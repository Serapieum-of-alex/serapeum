
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
        <meta name="description" content="LLM package">
      
      
      
        <link rel="canonical" href="https://serapieum-of-alex.github.io/Serapeum/develop/reference/providers/llama-cpp/examples/">
      
      
        <link rel="prev" href="../general/">
      
      
        <link rel="next" href="../llama_cpp_sequence/">
      
      
        
      
      
      <link rel="icon" href="../../../../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.6.1, mkdocs-material-9.7.3">
    
    
      
        <title>Examples - Serapeum</title>
      
    
    
      <link rel="stylesheet" href="../../../../assets/stylesheets/main.484c7ddc.min.css">
      
        
        <link rel="stylesheet" href="../../../../assets/stylesheets/palette.ab4e12ef.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
      <link rel="stylesheet" href="../../../../assets/_mkdocstrings.css">
    
      <link rel="stylesheet" href="../../../../assets/stylesheets/panzoom.css">
    
    <script>__md_scope=new URL("../../../..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
  <meta name="panzoom-theme" content="material">
<meta name="panzoom-data" content='{"selectors": [".mermaid", ".d2"], "initial_zoom_level": 1.0}'> </head>
  
  
    
    
      
    
    
    
    
    <body dir="ltr" data-md-color-scheme="default" data-md-color-primary="indigo" data-md-color-accent="indigo">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#llamacpp-usage-examples" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
      <div data-md-color-scheme="default" data-md-component="outdated" hidden>
        
      </div>
    
    
      

  

<header class="md-header md-header--shadow md-header--lifted" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href="../../../.." title="Serapeum" class="md-header__button md-logo" aria-label="Serapeum" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            Serapeum
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              Examples
            
          </span>
        </div>
      </div>
    </div>
    
      
        <form class="md-header__option" data-md-component="palette">
  
    
    
    
    <input class="md-option" data-md-color-media="(prefers-color-scheme)" data-md-color-scheme="default" data-md-color-primary="indigo" data-md-color-accent="indigo"  aria-label="Switch to light mode"  type="radio" name="__palette" id="__palette_0">
    
      <label class="md-header__button md-icon" title="Switch to light mode" for="__palette_1" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="m14.3 16-.7-2h-3.2l-.7 2H7.8L11 7h2l3.2 9zM20 8.69V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12zm-9.15 3.96h2.3L12 9z"/></svg>
      </label>
    
  
    
    
    
    <input class="md-option" data-md-color-media="(prefers-color-scheme: light)" data-md-color-scheme="default" data-md-color-primary="teal" data-md-color-accent="purple"  aria-label="Switch to dark mode"  type="radio" name="__palette" id="__palette_1">
    
      <label class="md-header__button md-icon" title="Switch to dark mode" for="__palette_2" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 7a5 5 0 0 1 5 5 5 5 0 0 1-5 5 5 5 0 0 1-5-5 5 5 0 0 1 5-5m0 2a3 3 0 0 0-3 3 3 3 0 0 0 3 3 3 3 0 0 0 3-3 3 3 0 0 0-3-3m0-7 2.39 3.42C13.65 5.15 12.84 5 12 5s-1.65.15-2.39.42zM3.34 7l4.16-.35A7.2 7.2 0 0 0 5.94 8.5c-.44.74-.69 1.5-.83 2.29zm.02 10 1.76-3.77a7.131 7.131 0 0 0 2.38 4.14zM20.65 7l-1.77 3.79a7.02 7.02 0 0 0-2.38-4.15zm-.01 10-4.14.36c.59-.51 1.12-1.14 1.54-1.86.42-.73.69-1.5.83-2.29zM12 22l-2.41-3.44c.74.27 1.55.44 2.41.44.82 0 1.63-.17 2.37-.44z"/></svg>
      </label>
    
  
    
    
    
    <input class="md-option" data-md-color-media="(prefers-color-scheme: dark)" data-md-color-scheme="slate" data-md-color-primary="black" data-md-color-accent="lime"  aria-label="Switch to system preference"  type="radio" name="__palette" id="__palette_2">
    
      <label class="md-header__button md-icon" title="Switch to system preference" for="__palette_0" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="m17.75 4.09-2.53 1.94.91 3.06-2.63-1.81-2.63 1.81.91-3.06-2.53-1.94L12.44 4l1.06-3 1.06 3zm3.5 6.91-1.64 1.25.59 1.98-1.7-1.17-1.7 1.17.59-1.98L15.75 11l2.06-.05L18.5 9l.69 1.95zm-2.28 4.95c.83-.08 1.72 1.1 1.19 1.85-.32.45-.66.87-1.08 1.27C15.17 23 8.84 23 4.94 19.07c-3.91-3.9-3.91-10.24 0-14.14.4-.4.82-.76 1.27-1.08.75-.53 1.93.36 1.85 1.19-.27 2.86.69 5.83 2.89 8.02a9.96 9.96 0 0 0 8.02 2.89m-1.64 2.02a12.08 12.08 0 0 1-7.8-3.47c-2.17-2.19-3.33-5-3.49-7.82-2.81 3.14-2.7 7.96.31 10.98 3.02 3.01 7.84 3.12 10.98.31"/></svg>
      </label>
    
  
</form>
      
    
    
      <script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script>
    
    
    
      
      
        <label class="md-header__button md-icon" for="__search">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        </label>
        <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg>
        </button>
      </nav>
      
        <div class="md-search__suggest" data-md-component="search-suggest"></div>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" tabindex="0" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
      
    
    
      <div class="md-header__source">
        <a href="https://github.com/Serapieum-of-alex/Serapeum" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 7.1.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M173.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6m-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3m44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9M252.8 8C114.1 8 8 113.3 8 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C436.2 457.8 504 362.9 504 252 504 113.3 391.5 8 252.8 8M105.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1m-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7m32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1m-11.4-14.7c-1.6 1-1.6 3.6 0 5.9s4.3 3.3 5.6 2.3c1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2"/></svg>
  </div>
  <div class="md-source__repository">
    Serapieum-of-alex/Serapeum
  </div>
</a>
      </div>
    
  </nav>
  
    
      
<nav class="md-tabs" aria-label="Tabs" data-md-component="tabs">
  <div class="md-grid">
    <ul class="md-tabs__list">
      
        
  
  
  
  
    <li class="md-tabs__item">
      <a href="../../../.." class="md-tabs__link">
        
  
  
    
  
  Home

      </a>
    </li>
  

      
        
  
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../../../../overview/core-package/" class="md-tabs__link">
          
  
  
    
  
  Overview

        </a>
      </li>
    
  

      
        
  
  
  
  
    
    
      
  
  
  
  
    
    
      
  
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../../../core/llms/llm-classes-comparison/" class="md-tabs__link">
          
  
  
    
  
  Reference

        </a>
      </li>
    
  

    
  

    
  

      
        
  
  
  
    
  
  
    
    
      
  
  
  
    
  
  
    
    
      <li class="md-tabs__item md-tabs__item--active">
        <a href="../../ollama/general/" class="md-tabs__link">
          
  
  
    
  
  Providers

        </a>
      </li>
    
  

    
  

      
        
  
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../../../../developer-guide/installation/" class="md-tabs__link">
          
  
  
    
  
  Developer Guide

        </a>
      </li>
    
  

      
        
  
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../../../../change-log/" class="md-tabs__link">
          
  
  
    
  
  About

        </a>
      </li>
    
  

      
    </ul>
  </div>
</nav>
    
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    


  


<nav class="md-nav md-nav--primary md-nav--lifted" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../../../.." title="Serapeum" class="md-nav__button md-logo" aria-label="Serapeum" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    Serapeum
  </label>
  
    <div class="md-nav__source">
      <a href="https://github.com/Serapieum-of-alex/Serapeum" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 7.1.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M173.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6m-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3m44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9M252.8 8C114.1 8 8 113.3 8 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C436.2 457.8 504 362.9 504 252 504 113.3 391.5 8 252.8 8M105.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1m-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7m32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1m-11.4-14.7c-1.6 1-1.6 3.6 0 5.9s4.3 3.3 5.6 2.3c1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2"/></svg>
  </div>
  <div class="md-source__repository">
    Serapieum-of-alex/Serapeum
  </div>
</a>
    </div>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../.." class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Home
  

    
  </span>
  
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    
    
      
        
      
        
      
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2" >
        
          
          <label class="md-nav__link" for="__nav_2" id="__nav_2_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    
  
    Overview
  

    
  </span>
  
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_2_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2">
            <span class="md-nav__icon md-icon"></span>
            
  
    Overview
  

          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../overview/core-package/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Core package
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    
    
      
        
      
        
      
        
      
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_2" >
        
          
          <label class="md-nav__link" for="__nav_2_2" id="__nav_2_2_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    
  
    Provider Integrations
  

    
  </span>
  
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_2_2_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2_2">
            <span class="md-nav__icon md-icon"></span>
            
  
    Provider Integrations
  

          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../overview/providers/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Overview
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../overview/providers/ollama/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Ollama
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../overview/providers/adding-new-providers/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Adding New Providers
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
      
        
      
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3" >
        
          
          <label class="md-nav__link" for="__nav_3" id="__nav_3_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    
  
    Reference
  

    
  </span>
  
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_3_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3">
            <span class="md-nav__icon md-icon"></span>
            
  
    Reference
  

          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    
    
      
        
      
        
      
        
      
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_1" >
        
          
          <label class="md-nav__link" for="__nav_3_1" id="__nav_3_1_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    
  
    Core
  

    
  </span>
  
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_3_1_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3_1">
            <span class="md-nav__icon md-icon"></span>
            
  
    Core
  

          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    
    
      
        
      
        
      
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_1_1" >
        
          
          <label class="md-nav__link" for="__nav_3_1_1" id="__nav_3_1_1_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    
  
    llms
  

    
  </span>
  
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_3_1_1_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3_1_1">
            <span class="md-nav__icon md-icon"></span>
            
  
    llms
  

          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../core/llms/llm-classes-comparison/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    LLM Classes Comparison
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    
    
      
        
      
        
      
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_1_1_2" >
        
          
          <label class="md-nav__link" for="__nav_3_1_1_2" id="__nav_3_1_1_2_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    
  
    orchestrators
  

    
  </span>
  
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="4" aria-labelledby="__nav_3_1_1_2_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3_1_1_2">
            <span class="md-nav__icon md-icon"></span>
            
  
    orchestrators
  

          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    
    
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_1_1_2_1" >
        
          
          <label class="md-nav__link" for="__nav_3_1_1_2_1" id="__nav_3_1_1_2_1_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    
  
    Text Completion LLM
  

    
  </span>
  
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="5" aria-labelledby="__nav_3_1_1_2_1_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3_1_1_2_1">
            <span class="md-nav__icon md-icon"></span>
            
  
    Text Completion LLM
  

          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../core/llms/orchestrators/text_completion_llm/general/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Overview
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../core/llms/orchestrators/text_completion_llm/examples/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Examples
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../core/llms/orchestrators/text_completion_llm/text_completion_llm_sequence/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Execution Flow
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../core/llms/orchestrators/text_completion_llm/text_completion_llm_class/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Architecture
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../core/llms/orchestrators/text_completion_llm/text_completion_llm_dataflow/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Data Flow
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../core/llms/orchestrators/text_completion_llm/text_completion_llm_components/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Components
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../core/llms/orchestrators/text_completion_llm/text_completion_llm_state/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Lifecycle
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_1_1_2_2" >
        
          
          <label class="md-nav__link" for="__nav_3_1_1_2_2" id="__nav_3_1_1_2_2_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    
  
    Tool Orchestrating LLM
  

    
  </span>
  
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="5" aria-labelledby="__nav_3_1_1_2_2_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3_1_1_2_2">
            <span class="md-nav__icon md-icon"></span>
            
  
    Tool Orchestrating LLM
  

          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../core/llms/orchestrators/tool_orchestrating_llm/general/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Overview
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../core/llms/orchestrators/tool_orchestrating_llm/examples/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Examples
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../core/llms/orchestrators/tool_orchestrating_llm/tool_orchestrating_llm_sequence/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Execution Flow
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../core/llms/orchestrators/tool_orchestrating_llm/tool_orchestrating_llm_class/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Architecture
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../core/llms/orchestrators/tool_orchestrating_llm/tool_orchestrating_llm_dataflow/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Data Flow
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../core/llms/orchestrators/tool_orchestrating_llm/tool_orchestrating_llm_components/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Components
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../core/llms/orchestrators/tool_orchestrating_llm/tool_orchestrating_llm_state/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Lifecycle
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
      
        
      
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_1_2" >
        
          
          <label class="md-nav__link" for="__nav_3_1_2" id="__nav_3_1_2_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    
  
    embeddings
  

    
  </span>
  
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_3_1_2_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3_1_2">
            <span class="md-nav__icon md-icon"></span>
            
  
    embeddings
  

          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../core/embeddings/module/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Core Embeddings
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
      
        
      
        
      
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_1_3" >
        
          
          <label class="md-nav__link" for="__nav_3_1_3" id="__nav_3_1_3_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    
  
    tools
  

    
  </span>
  
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_3_1_3_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3_1_3">
            <span class="md-nav__icon md-icon"></span>
            
  
    tools
  

          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../core/tools/tools/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Overview
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../core/tools/callable_tools/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    CallableTools
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
    
  
  
  
    
    
      
        
      
        
      
        
      
    
    
    
      
        
        
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--active md-nav__item--section md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_4" checked>
        
          
          <label class="md-nav__link" for="__nav_4" id="__nav_4_label" tabindex="">
            
  
  
  <span class="md-ellipsis">
    
  
    Providers
  

    
  </span>
  
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_4_label" aria-expanded="true">
          <label class="md-nav__title" for="__nav_4">
            <span class="md-nav__icon md-icon"></span>
            
  
    Providers
  

          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    
    
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
    
    
    
      
      
        
          
          
        
      
    
    
    <li class="md-nav__item md-nav__item--section md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_4_1" >
        
          
          <label class="md-nav__link" for="__nav_4_1" id="__nav_4_1_label" tabindex="">
            
  
  
  <span class="md-ellipsis">
    
  
    Ollama
  

    
  </span>
  
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_4_1_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_4_1">
            <span class="md-nav__icon md-icon"></span>
            
  
    Ollama
  

          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../ollama/general/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Overview
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../ollama/examples/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Examples
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../ollama/ollama_sequence/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Execution Flow
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../ollama/ollama_class/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Architecture
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../ollama/ollama_dataflow/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Data Flow
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../ollama/ollama_components/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Components
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../ollama/ollama_state/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Lifecycle
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../ollama/api_reference/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    API-Reference
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
    
  
  
  
    
    
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
    
    
    
      
      
        
          
          
        
      
    
    
    <li class="md-nav__item md-nav__item--active md-nav__item--section md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_4_2" checked>
        
          
          <label class="md-nav__link" for="__nav_4_2" id="__nav_4_2_label" tabindex="">
            
  
  
  <span class="md-ellipsis">
    
  
    LlamaCPP
  

    
  </span>
  
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_4_2_label" aria-expanded="true">
          <label class="md-nav__title" for="__nav_4_2">
            <span class="md-nav__icon md-icon"></span>
            
  
    LlamaCPP
  

          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../general/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Overview
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
    
  
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" type="checkbox" id="__toc">
      
      
        
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          
  
  
  <span class="md-ellipsis">
    
  
    Examples
  

    
  </span>
  
  

          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="./" class="md-nav__link md-nav__link--active">
        
  
  
  <span class="md-ellipsis">
    
  
    Examples
  

    
  </span>
  
  

      </a>
      
        

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#prerequisites-model-file" class="md-nav__link">
    <span class="md-ellipsis">
      
        Prerequisites: Model File
      
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#table-of-contents" class="md-nav__link">
    <span class="md-ellipsis">
      
        Table of Contents
      
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#basic-usage" class="md-nav__link">
    <span class="md-ellipsis">
      
        Basic Usage
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Basic Usage">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#simple-completion" class="md-nav__link">
    <span class="md-ellipsis">
      
        Simple Completion
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#initialization-patterns" class="md-nav__link">
    <span class="md-ellipsis">
      
        Initialization Patterns
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Initialization Patterns">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#1-from-local-model-path" class="md-nav__link">
    <span class="md-ellipsis">
      
        1. From Local Model Path
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#3-from-huggingface-hub" class="md-nav__link">
    <span class="md-ellipsis">
      
        3. From HuggingFace Hub
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#5-with-llama-2-formatters" class="md-nav__link">
    <span class="md-ellipsis">
      
        5. With Llama 2 Formatters
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2-completion-with-custom-parameters" class="md-nav__link">
    <span class="md-ellipsis">
      
        2. Completion with Custom Parameters
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#4-completion-with-stop-tokens" class="md-nav__link">
    <span class="md-ellipsis">
      
        4. Completion with Stop Tokens
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../llama_cpp_sequence/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Execution Flow
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../llama_cpp_class/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Architecture
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../llama_cpp_dataflow/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Data Flow
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../llama_cpp_components/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Components
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../llama_cpp_state/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Lifecycle
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../api_reference/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    API-Reference
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
      
        
      
    
    
    
      
      
        
          
          
        
      
    
    
    <li class="md-nav__item md-nav__item--section md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_4_3" >
        
          
          <label class="md-nav__link" for="__nav_4_3" id="__nav_4_3_label" tabindex="">
            
  
  
  <span class="md-ellipsis">
    
  
    Add New Providers
  

    
  </span>
  
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_4_3_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_4_3">
            <span class="md-nav__icon md-icon"></span>
            
  
    Add New Providers
  

          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../add_new_providers/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Instructions
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
      
        
      
        
      
        
      
        
      
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5" >
        
          
          <label class="md-nav__link" for="__nav_5" id="__nav_5_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    
  
    Developer Guide
  

    
  </span>
  
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_5_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_5">
            <span class="md-nav__icon md-icon"></span>
            
  
    Developer Guide
  

          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../developer-guide/installation/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Installation
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../developer-guide/contributing/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    contributing
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../developer-guide/taskfile/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Taskfile
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../developer-guide/testing-docs/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    testing-documentation
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
      
        
      
        
      
        
      
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_6" >
        
          
          <label class="md-nav__link" for="__nav_6" id="__nav_6_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    
  
    About
  

    
  </span>
  
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_6_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_6">
            <span class="md-nav__icon md-icon"></span>
            
  
    About
  

          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../change-log/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Change-log
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../contributing.md" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Contributing
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../LICENSE/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    License
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#prerequisites-model-file" class="md-nav__link">
    <span class="md-ellipsis">
      
        Prerequisites: Model File
      
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#table-of-contents" class="md-nav__link">
    <span class="md-ellipsis">
      
        Table of Contents
      
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#basic-usage" class="md-nav__link">
    <span class="md-ellipsis">
      
        Basic Usage
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Basic Usage">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#simple-completion" class="md-nav__link">
    <span class="md-ellipsis">
      
        Simple Completion
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#initialization-patterns" class="md-nav__link">
    <span class="md-ellipsis">
      
        Initialization Patterns
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Initialization Patterns">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#1-from-local-model-path" class="md-nav__link">
    <span class="md-ellipsis">
      
        1. From Local Model Path
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#3-from-huggingface-hub" class="md-nav__link">
    <span class="md-ellipsis">
      
        3. From HuggingFace Hub
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#5-with-llama-2-formatters" class="md-nav__link">
    <span class="md-ellipsis">
      
        5. With Llama 2 Formatters
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2-completion-with-custom-parameters" class="md-nav__link">
    <span class="md-ellipsis">
      
        2. Completion with Custom Parameters
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#4-completion-with-stop-tokens" class="md-nav__link">
    <span class="md-ellipsis">
      
        4. Completion with Stop Tokens
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              
              <article class="md-content__inner md-typeset">
                
                  


  
    <a href="https://github.com/Serapieum-of-alex/Serapeum/edit/master/docs/reference/providers/llama-cpp/examples.md" title="Edit this page" class="md-content__button md-icon" rel="edit">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M10 20H6V4h7v5h5v3.1l2-2V8l-6-6H6c-1.1 0-2 .9-2 2v16c0 1.1.9 2 2 2h4zm10.2-7c.1 0 .3.1.4.2l1.3 1.3c.2.2.2.6 0 .8l-1 1-2.1-2.1 1-1c.1-.1.2-.2.4-.2m0 3.9L14.1 23H12v-2.1l6.1-6.1z"/></svg>
    </a>
  
  
    
      
    
    <a href="https://github.com/Serapieum-of-alex/Serapeum/raw/master/docs/reference/providers/llama-cpp/examples.md" title="View source of this page" class="md-content__button md-icon">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M17 18c.56 0 1 .44 1 1s-.44 1-1 1-1-.44-1-1 .44-1 1-1m0-3c-2.73 0-5.06 1.66-6 4 .94 2.34 3.27 4 6 4s5.06-1.66 6-4c-.94-2.34-3.27-4-6-4m0 6.5a2.5 2.5 0 0 1-2.5-2.5 2.5 2.5 0 0 1 2.5-2.5 2.5 2.5 0 0 1 2.5 2.5 2.5 2.5 0 0 1-2.5 2.5M9.27 20H6V4h7v5h5v4.07c.7.08 1.36.25 2 .49V8l-6-6H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h4.5a8.2 8.2 0 0 1-1.23-2"/></svg>
    </a>
  


<h1 id="llamacpp-usage-examples"><a class="toclink" href="#llamacpp-usage-examples">LlamaCPP Usage Examples</a><a class="headerlink" href="#llamacpp-usage-examples" title="Permanent link">#</a></h1>
<p>This guide provides comprehensive examples covering all possible ways to use the <code>LlamaCPP</code> class based on real test cases from the codebase.</p>
<h2 id="prerequisites-model-file"><a class="toclink" href="#prerequisites-model-file">Prerequisites: Model File</a><a class="headerlink" href="#prerequisites-model-file" title="Permanent link">#</a></h2>
<p>LlamaCPP requires a local GGUF model file. You can:
- Download one manually from <a href="https://huggingface.co/models?search=gguf">HuggingFace</a>
- Let LlamaCPP download one automatically via <code>model_url</code> or <code>hf_model_id</code></p>
<p><strong>Example: set the model path via environment variable:</strong></p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-0-1" name="__codelineno-0-1" href="#__codelineno-0-1"></a><span class="nb">export</span><span class="w"> </span><span class="nv">LLAMA_CPP_MODEL_PATH</span><span class="o">=</span>/path/to/model.gguf
</code></pre></div>
<p>All examples below use a local model path. Replace it with your own.</p>
<hr />
<h2 id="table-of-contents"><a class="toclink" href="#table-of-contents">Table of Contents</a><a class="headerlink" href="#table-of-contents" title="Permanent link">#</a></h2>
<ol>
<li><a href="#basic-usage">Basic Usage</a></li>
<li><a href="#initialization-patterns">Initialization Patterns</a></li>
<li><a href="#completion-operations">Completion Operations</a></li>
<li><a href="#chat-operations">Chat Operations</a></li>
<li><a href="#streaming-operations">Streaming Operations</a></li>
<li><a href="#async-operations">Async Operations</a></li>
<li><a href="#integration-with-orchestrators">Integration with Orchestrators</a></li>
<li><a href="#prompt-formatters">Prompt Formatters</a></li>
</ol>
<hr />
<h2 id="basic-usage"><a class="toclink" href="#basic-usage">Basic Usage</a><a class="headerlink" href="#basic-usage" title="Permanent link">#</a></h2>
<h3 id="simple-completion"><a class="toclink" href="#simple-completion">Simple Completion</a><a class="headerlink" href="#simple-completion" title="Permanent link">#</a></h3>
<p>The most straightforward way to use <code>LlamaCPP</code>:</p>
<p>```python notest
from serapeum.llama_cpp import LlamaCPP
from serapeum.llama_cpp.formatters.llama3 import (
    messages_to_prompt_v3_instruct,
    completion_to_prompt_v3_instruct,
)</p>
<p>llm = LlamaCPP(
    model_path="/models/llama-3-8b-instruct.Q4_0.gguf",
    temperature=0.1,
    max_new_tokens=256,
    context_window=8192,
    messages_to_prompt=messages_to_prompt_v3_instruct,
    completion_to_prompt=completion_to_prompt_v3_instruct,
)</p>
<p>response = llm.complete("Say hello.")
print(response.text)  # "Hello! How are you?"
<div class="highlight"><pre><span></span><code><a id="__codelineno-1-1" name="__codelineno-1-1" href="#__codelineno-1-1"></a>### Simple Chat
<a id="__codelineno-1-2" name="__codelineno-1-2" href="#__codelineno-1-2"></a>
<a id="__codelineno-1-3" name="__codelineno-1-3" href="#__codelineno-1-3"></a>Using the chat API (via CompletionToChatMixin):
<a id="__codelineno-1-4" name="__codelineno-1-4" href="#__codelineno-1-4"></a>
<a id="__codelineno-1-5" name="__codelineno-1-5" href="#__codelineno-1-5"></a>```python notest
<a id="__codelineno-1-6" name="__codelineno-1-6" href="#__codelineno-1-6"></a>from serapeum.llama_cpp import LlamaCPP
<a id="__codelineno-1-7" name="__codelineno-1-7" href="#__codelineno-1-7"></a>from serapeum.core.llms import Message, MessageRole
<a id="__codelineno-1-8" name="__codelineno-1-8" href="#__codelineno-1-8"></a>from serapeum.llama_cpp.formatters.llama3 import (
<a id="__codelineno-1-9" name="__codelineno-1-9" href="#__codelineno-1-9"></a>    messages_to_prompt_v3_instruct,
<a id="__codelineno-1-10" name="__codelineno-1-10" href="#__codelineno-1-10"></a>    completion_to_prompt_v3_instruct,
<a id="__codelineno-1-11" name="__codelineno-1-11" href="#__codelineno-1-11"></a>)
<a id="__codelineno-1-12" name="__codelineno-1-12" href="#__codelineno-1-12"></a>
<a id="__codelineno-1-13" name="__codelineno-1-13" href="#__codelineno-1-13"></a>llm = LlamaCPP(
<a id="__codelineno-1-14" name="__codelineno-1-14" href="#__codelineno-1-14"></a>    model_path=&quot;/models/llama-3-8b-instruct.Q4_0.gguf&quot;,
<a id="__codelineno-1-15" name="__codelineno-1-15" href="#__codelineno-1-15"></a>    messages_to_prompt=messages_to_prompt_v3_instruct,
<a id="__codelineno-1-16" name="__codelineno-1-16" href="#__codelineno-1-16"></a>    completion_to_prompt=completion_to_prompt_v3_instruct,
<a id="__codelineno-1-17" name="__codelineno-1-17" href="#__codelineno-1-17"></a>)
<a id="__codelineno-1-18" name="__codelineno-1-18" href="#__codelineno-1-18"></a>
<a id="__codelineno-1-19" name="__codelineno-1-19" href="#__codelineno-1-19"></a>messages = [Message(role=MessageRole.USER, content=&quot;What is 2+2?&quot;)]
<a id="__codelineno-1-20" name="__codelineno-1-20" href="#__codelineno-1-20"></a>response = llm.chat(messages)
<a id="__codelineno-1-21" name="__codelineno-1-21" href="#__codelineno-1-21"></a>print(response.message.content)  # &quot;4&quot;
</code></pre></div></p>
<hr />
<h2 id="initialization-patterns"><a class="toclink" href="#initialization-patterns">Initialization Patterns</a><a class="headerlink" href="#initialization-patterns" title="Permanent link">#</a></h2>
<h3 id="1-from-local-model-path"><a class="toclink" href="#1-from-local-model-path">1. From Local Model Path</a><a class="headerlink" href="#1-from-local-model-path" title="Permanent link">#</a></h3>
<p>Load a GGUF file already on disk:</p>
<p>```python notest
from serapeum.llama_cpp import LlamaCPP
from serapeum.llama_cpp.formatters.llama3 import (
    messages_to_prompt_v3_instruct,
    completion_to_prompt_v3_instruct,
)</p>
<p>llm = LlamaCPP(
    model_path="/models/llama-3-8b-instruct.Q4_0.gguf",
    messages_to_prompt=messages_to_prompt_v3_instruct,
    completion_to_prompt=completion_to_prompt_v3_instruct,
)
<div class="highlight"><pre><span></span><code><a id="__codelineno-2-1" name="__codelineno-2-1" href="#__codelineno-2-1"></a>### 2. From URL Download
<a id="__codelineno-2-2" name="__codelineno-2-2" href="#__codelineno-2-2"></a>
<a id="__codelineno-2-3" name="__codelineno-2-3" href="#__codelineno-2-3"></a>Download and cache a GGUF model from a direct URL:
<a id="__codelineno-2-4" name="__codelineno-2-4" href="#__codelineno-2-4"></a>
<a id="__codelineno-2-5" name="__codelineno-2-5" href="#__codelineno-2-5"></a>```python notest
<a id="__codelineno-2-6" name="__codelineno-2-6" href="#__codelineno-2-6"></a>from serapeum.llama_cpp import LlamaCPP
<a id="__codelineno-2-7" name="__codelineno-2-7" href="#__codelineno-2-7"></a>from serapeum.llama_cpp.formatters.llama2 import (
<a id="__codelineno-2-8" name="__codelineno-2-8" href="#__codelineno-2-8"></a>    messages_to_prompt,
<a id="__codelineno-2-9" name="__codelineno-2-9" href="#__codelineno-2-9"></a>    completion_to_prompt,
<a id="__codelineno-2-10" name="__codelineno-2-10" href="#__codelineno-2-10"></a>)
<a id="__codelineno-2-11" name="__codelineno-2-11" href="#__codelineno-2-11"></a>
<a id="__codelineno-2-12" name="__codelineno-2-12" href="#__codelineno-2-12"></a>llm = LlamaCPP(
<a id="__codelineno-2-13" name="__codelineno-2-13" href="#__codelineno-2-13"></a>    model_url=&quot;https://huggingface.co/TheBloke/Llama-2-13B-chat-GGUF/resolve/main/llama-2-13b-chat.Q4_0.gguf&quot;,
<a id="__codelineno-2-14" name="__codelineno-2-14" href="#__codelineno-2-14"></a>    messages_to_prompt=messages_to_prompt,
<a id="__codelineno-2-15" name="__codelineno-2-15" href="#__codelineno-2-15"></a>    completion_to_prompt=completion_to_prompt,
<a id="__codelineno-2-16" name="__codelineno-2-16" href="#__codelineno-2-16"></a>)
</code></pre></div></p>
<h3 id="3-from-huggingface-hub"><a class="toclink" href="#3-from-huggingface-hub">3. From HuggingFace Hub</a><a class="headerlink" href="#3-from-huggingface-hub" title="Permanent link">#</a></h3>
<p>Download from HuggingFace Hub using <code>huggingface_hub</code>:</p>
<p>```python notest
from serapeum.llama_cpp import LlamaCPP
from serapeum.llama_cpp.formatters.llama2 import (
    messages_to_prompt,
    completion_to_prompt,
)</p>
<p>llm = LlamaCPP(
    hf_model_id="TheBloke/Llama-2-13B-chat-GGUF",
    hf_filename="llama-2-13b-chat.Q4_0.gguf",
    messages_to_prompt=messages_to_prompt,
    completion_to_prompt=completion_to_prompt,
)
<div class="highlight"><pre><span></span><code><a id="__codelineno-3-1" name="__codelineno-3-1" href="#__codelineno-3-1"></a>### 4. Full Configuration
<a id="__codelineno-3-2" name="__codelineno-3-2" href="#__codelineno-3-2"></a>
<a id="__codelineno-3-3" name="__codelineno-3-3" href="#__codelineno-3-3"></a>With all common parameters:
<a id="__codelineno-3-4" name="__codelineno-3-4" href="#__codelineno-3-4"></a>
<a id="__codelineno-3-5" name="__codelineno-3-5" href="#__codelineno-3-5"></a>```python notest
<a id="__codelineno-3-6" name="__codelineno-3-6" href="#__codelineno-3-6"></a>from serapeum.llama_cpp import LlamaCPP
<a id="__codelineno-3-7" name="__codelineno-3-7" href="#__codelineno-3-7"></a>from serapeum.llama_cpp.formatters.llama3 import (
<a id="__codelineno-3-8" name="__codelineno-3-8" href="#__codelineno-3-8"></a>    messages_to_prompt_v3_instruct,
<a id="__codelineno-3-9" name="__codelineno-3-9" href="#__codelineno-3-9"></a>    completion_to_prompt_v3_instruct,
<a id="__codelineno-3-10" name="__codelineno-3-10" href="#__codelineno-3-10"></a>)
<a id="__codelineno-3-11" name="__codelineno-3-11" href="#__codelineno-3-11"></a>
<a id="__codelineno-3-12" name="__codelineno-3-12" href="#__codelineno-3-12"></a>llm = LlamaCPP(
<a id="__codelineno-3-13" name="__codelineno-3-13" href="#__codelineno-3-13"></a>    model_path=&quot;/models/llama-3-8b-instruct.Q4_0.gguf&quot;,
<a id="__codelineno-3-14" name="__codelineno-3-14" href="#__codelineno-3-14"></a>    temperature=0.2,
<a id="__codelineno-3-15" name="__codelineno-3-15" href="#__codelineno-3-15"></a>    max_new_tokens=512,
<a id="__codelineno-3-16" name="__codelineno-3-16" href="#__codelineno-3-16"></a>    context_window=8192,
<a id="__codelineno-3-17" name="__codelineno-3-17" href="#__codelineno-3-17"></a>    n_gpu_layers=-1,           # Offload all layers to GPU
<a id="__codelineno-3-18" name="__codelineno-3-18" href="#__codelineno-3-18"></a>    stop=[&quot;&lt;/s&gt;&quot;, &quot;&lt;|eot_id|&gt;&quot;],
<a id="__codelineno-3-19" name="__codelineno-3-19" href="#__codelineno-3-19"></a>    verbose=False,
<a id="__codelineno-3-20" name="__codelineno-3-20" href="#__codelineno-3-20"></a>    generate_kwargs={&quot;top_p&quot;: 0.9, &quot;top_k&quot;: 40},
<a id="__codelineno-3-21" name="__codelineno-3-21" href="#__codelineno-3-21"></a>    model_kwargs={&quot;n_threads&quot;: 8},
<a id="__codelineno-3-22" name="__codelineno-3-22" href="#__codelineno-3-22"></a>    messages_to_prompt=messages_to_prompt_v3_instruct,
<a id="__codelineno-3-23" name="__codelineno-3-23" href="#__codelineno-3-23"></a>    completion_to_prompt=completion_to_prompt_v3_instruct,
<a id="__codelineno-3-24" name="__codelineno-3-24" href="#__codelineno-3-24"></a>)
</code></pre></div></p>
<h3 id="5-with-llama-2-formatters"><a class="toclink" href="#5-with-llama-2-formatters">5. With Llama 2 Formatters</a><a class="headerlink" href="#5-with-llama-2-formatters" title="Permanent link">#</a></h3>
<p>Using Llama 2 / Mistral-style prompt templates:</p>
<p>```python notest
from serapeum.llama_cpp import LlamaCPP
from serapeum.llama_cpp.formatters.llama2 import (
    messages_to_prompt,
    completion_to_prompt,
)</p>
<p>llm = LlamaCPP(
    model_path="/models/llama-2-13b-chat.Q4_0.gguf",
    temperature=0.1,
    max_new_tokens=256,
    context_window=4096,
    messages_to_prompt=messages_to_prompt,
    completion_to_prompt=completion_to_prompt,
)
<div class="highlight"><pre><span></span><code><a id="__codelineno-4-1" name="__codelineno-4-1" href="#__codelineno-4-1"></a>---
<a id="__codelineno-4-2" name="__codelineno-4-2" href="#__codelineno-4-2"></a>
<a id="__codelineno-4-3" name="__codelineno-4-3" href="#__codelineno-4-3"></a>## Completion Operations
<a id="__codelineno-4-4" name="__codelineno-4-4" href="#__codelineno-4-4"></a>
<a id="__codelineno-4-5" name="__codelineno-4-5" href="#__codelineno-4-5"></a>### 1. Basic Completion
<a id="__codelineno-4-6" name="__codelineno-4-6" href="#__codelineno-4-6"></a>
<a id="__codelineno-4-7" name="__codelineno-4-7" href="#__codelineno-4-7"></a>Simple text completion:
<a id="__codelineno-4-8" name="__codelineno-4-8" href="#__codelineno-4-8"></a>
<a id="__codelineno-4-9" name="__codelineno-4-9" href="#__codelineno-4-9"></a>```python notest
<a id="__codelineno-4-10" name="__codelineno-4-10" href="#__codelineno-4-10"></a>from serapeum.llama_cpp import LlamaCPP
<a id="__codelineno-4-11" name="__codelineno-4-11" href="#__codelineno-4-11"></a>from serapeum.llama_cpp.formatters.llama3 import (
<a id="__codelineno-4-12" name="__codelineno-4-12" href="#__codelineno-4-12"></a>    messages_to_prompt_v3_instruct,
<a id="__codelineno-4-13" name="__codelineno-4-13" href="#__codelineno-4-13"></a>    completion_to_prompt_v3_instruct,
<a id="__codelineno-4-14" name="__codelineno-4-14" href="#__codelineno-4-14"></a>)
<a id="__codelineno-4-15" name="__codelineno-4-15" href="#__codelineno-4-15"></a>
<a id="__codelineno-4-16" name="__codelineno-4-16" href="#__codelineno-4-16"></a>llm = LlamaCPP(
<a id="__codelineno-4-17" name="__codelineno-4-17" href="#__codelineno-4-17"></a>    model_path=&quot;/models/llama-3-8b-instruct.Q4_0.gguf&quot;,
<a id="__codelineno-4-18" name="__codelineno-4-18" href="#__codelineno-4-18"></a>    messages_to_prompt=messages_to_prompt_v3_instruct,
<a id="__codelineno-4-19" name="__codelineno-4-19" href="#__codelineno-4-19"></a>    completion_to_prompt=completion_to_prompt_v3_instruct,
<a id="__codelineno-4-20" name="__codelineno-4-20" href="#__codelineno-4-20"></a>)
<a id="__codelineno-4-21" name="__codelineno-4-21" href="#__codelineno-4-21"></a>
<a id="__codelineno-4-22" name="__codelineno-4-22" href="#__codelineno-4-22"></a>response = llm.complete(&quot;The capital of France is&quot;)
<a id="__codelineno-4-23" name="__codelineno-4-23" href="#__codelineno-4-23"></a>print(response.text)  # &quot;Paris&quot;
</code></pre></div></p>
<h3 id="2-completion-with-custom-parameters"><a class="toclink" href="#2-completion-with-custom-parameters">2. Completion with Custom Parameters</a><a class="headerlink" href="#2-completion-with-custom-parameters" title="Permanent link">#</a></h3>
<p>Override default generation settings:</p>
<p>```python notest
from serapeum.llama_cpp import LlamaCPP
from serapeum.llama_cpp.formatters.llama3 import (
    messages_to_prompt_v3_instruct,
    completion_to_prompt_v3_instruct,
)</p>
<p>llm = LlamaCPP(
    model_path="/models/llama-3-8b-instruct.Q4_0.gguf",
    messages_to_prompt=messages_to_prompt_v3_instruct,
    completion_to_prompt=completion_to_prompt_v3_instruct,
)</p>
<p>response = llm.complete(
    "Once upon a time",
    temperature=0.8,
    max_tokens=200,
)
print(response.text)
<div class="highlight"><pre><span></span><code><a id="__codelineno-5-1" name="__codelineno-5-1" href="#__codelineno-5-1"></a>### 3. Pre-formatted Prompt
<a id="__codelineno-5-2" name="__codelineno-5-2" href="#__codelineno-5-2"></a>
<a id="__codelineno-5-3" name="__codelineno-5-3" href="#__codelineno-5-3"></a>Pass an already-formatted prompt:
<a id="__codelineno-5-4" name="__codelineno-5-4" href="#__codelineno-5-4"></a>
<a id="__codelineno-5-5" name="__codelineno-5-5" href="#__codelineno-5-5"></a>```python notest
<a id="__codelineno-5-6" name="__codelineno-5-6" href="#__codelineno-5-6"></a>from serapeum.llama_cpp import LlamaCPP
<a id="__codelineno-5-7" name="__codelineno-5-7" href="#__codelineno-5-7"></a>from serapeum.llama_cpp.formatters.llama3 import (
<a id="__codelineno-5-8" name="__codelineno-5-8" href="#__codelineno-5-8"></a>    messages_to_prompt_v3_instruct,
<a id="__codelineno-5-9" name="__codelineno-5-9" href="#__codelineno-5-9"></a>    completion_to_prompt_v3_instruct,
<a id="__codelineno-5-10" name="__codelineno-5-10" href="#__codelineno-5-10"></a>)
<a id="__codelineno-5-11" name="__codelineno-5-11" href="#__codelineno-5-11"></a>
<a id="__codelineno-5-12" name="__codelineno-5-12" href="#__codelineno-5-12"></a>llm = LlamaCPP(
<a id="__codelineno-5-13" name="__codelineno-5-13" href="#__codelineno-5-13"></a>    model_path=&quot;/models/llama-3-8b-instruct.Q4_0.gguf&quot;,
<a id="__codelineno-5-14" name="__codelineno-5-14" href="#__codelineno-5-14"></a>    messages_to_prompt=messages_to_prompt_v3_instruct,
<a id="__codelineno-5-15" name="__codelineno-5-15" href="#__codelineno-5-15"></a>    completion_to_prompt=completion_to_prompt_v3_instruct,
<a id="__codelineno-5-16" name="__codelineno-5-16" href="#__codelineno-5-16"></a>)
<a id="__codelineno-5-17" name="__codelineno-5-17" href="#__codelineno-5-17"></a>
<a id="__codelineno-5-18" name="__codelineno-5-18" href="#__codelineno-5-18"></a># Build the prompt manually
<a id="__codelineno-5-19" name="__codelineno-5-19" href="#__codelineno-5-19"></a>prompt = completion_to_prompt_v3_instruct(&quot;Say hello.&quot;)
<a id="__codelineno-5-20" name="__codelineno-5-20" href="#__codelineno-5-20"></a>
<a id="__codelineno-5-21" name="__codelineno-5-21" href="#__codelineno-5-21"></a># Pass formatted=True to skip the automatic formatter
<a id="__codelineno-5-22" name="__codelineno-5-22" href="#__codelineno-5-22"></a>response = llm.complete(prompt, formatted=True)
<a id="__codelineno-5-23" name="__codelineno-5-23" href="#__codelineno-5-23"></a>print(response.text)
</code></pre></div></p>
<h3 id="4-completion-with-stop-tokens"><a class="toclink" href="#4-completion-with-stop-tokens">4. Completion with Stop Tokens</a><a class="headerlink" href="#4-completion-with-stop-tokens" title="Permanent link">#</a></h3>
<p>Stop generation at specific tokens:</p>
<p>```python notest
from serapeum.llama_cpp import LlamaCPP
from serapeum.llama_cpp.formatters.llama3 import (
    messages_to_prompt_v3_instruct,
    completion_to_prompt_v3_instruct,
)</p>
<p>llm = LlamaCPP(
    model_path="/models/llama-3-8b-instruct.Q4_0.gguf",
    stop=["</s>", "&lt;|eot_id|&gt;", "&lt;|end|&gt;"],
    messages_to_prompt=messages_to_prompt_v3_instruct,
    completion_to_prompt=completion_to_prompt_v3_instruct,
)</p>
<p>response = llm.complete("Say hello.")</p>
<h1 id="stop-tokens-are-stripped-from-output"><a class="toclink" href="#stop-tokens-are-stripped-from-output">Stop tokens are stripped from output</a><a class="headerlink" href="#stop-tokens-are-stripped-from-output" title="Permanent link">#</a></h1>
<div class="highlight"><pre><span></span><code><a id="__codelineno-6-1" name="__codelineno-6-1" href="#__codelineno-6-1"></a>### 5. Raw Response Access
<a id="__codelineno-6-2" name="__codelineno-6-2" href="#__codelineno-6-2"></a>
<a id="__codelineno-6-3" name="__codelineno-6-3" href="#__codelineno-6-3"></a>Access the underlying llama-cpp-python response:
<a id="__codelineno-6-4" name="__codelineno-6-4" href="#__codelineno-6-4"></a>
<a id="__codelineno-6-5" name="__codelineno-6-5" href="#__codelineno-6-5"></a>```python notest
<a id="__codelineno-6-6" name="__codelineno-6-6" href="#__codelineno-6-6"></a>from serapeum.llama_cpp import LlamaCPP
<a id="__codelineno-6-7" name="__codelineno-6-7" href="#__codelineno-6-7"></a>from serapeum.llama_cpp.formatters.llama3 import (
<a id="__codelineno-6-8" name="__codelineno-6-8" href="#__codelineno-6-8"></a>    messages_to_prompt_v3_instruct,
<a id="__codelineno-6-9" name="__codelineno-6-9" href="#__codelineno-6-9"></a>    completion_to_prompt_v3_instruct,
<a id="__codelineno-6-10" name="__codelineno-6-10" href="#__codelineno-6-10"></a>)
<a id="__codelineno-6-11" name="__codelineno-6-11" href="#__codelineno-6-11"></a>
<a id="__codelineno-6-12" name="__codelineno-6-12" href="#__codelineno-6-12"></a>llm = LlamaCPP(
<a id="__codelineno-6-13" name="__codelineno-6-13" href="#__codelineno-6-13"></a>    model_path=&quot;/models/llama-3-8b-instruct.Q4_0.gguf&quot;,
<a id="__codelineno-6-14" name="__codelineno-6-14" href="#__codelineno-6-14"></a>    messages_to_prompt=messages_to_prompt_v3_instruct,
<a id="__codelineno-6-15" name="__codelineno-6-15" href="#__codelineno-6-15"></a>    completion_to_prompt=completion_to_prompt_v3_instruct,
<a id="__codelineno-6-16" name="__codelineno-6-16" href="#__codelineno-6-16"></a>)
<a id="__codelineno-6-17" name="__codelineno-6-17" href="#__codelineno-6-17"></a>
<a id="__codelineno-6-18" name="__codelineno-6-18" href="#__codelineno-6-18"></a>response = llm.complete(&quot;Say hello.&quot;)
<a id="__codelineno-6-19" name="__codelineno-6-19" href="#__codelineno-6-19"></a>print(response.raw)  # Full dict from llama-cpp-python
<a id="__codelineno-6-20" name="__codelineno-6-20" href="#__codelineno-6-20"></a>print(response.raw[&quot;choices&quot;])  # List of generated choices
<a id="__codelineno-6-21" name="__codelineno-6-21" href="#__codelineno-6-21"></a>print(response.raw[&quot;usage&quot;])    # Token usage statistics
</code></pre></div>
<hr />
<h2 id="chat-operations"><a class="toclink" href="#chat-operations">Chat Operations</a><a class="headerlink" href="#chat-operations" title="Permanent link">#</a></h2>
<p>Chat is provided by <code>CompletionToChatMixin</code>, which formats messages using
<code>messages_to_prompt</code> and delegates to <code>complete()</code>.</p>
<h3 id="1-single-turn-chat"><a class="toclink" href="#1-single-turn-chat">1. Single Turn Chat</a><a class="headerlink" href="#1-single-turn-chat" title="Permanent link">#</a></h3>
<p>Basic conversation:</p>
<p>```python notest
from serapeum.llama_cpp import LlamaCPP
from serapeum.core.llms import Message, MessageRole
from serapeum.llama_cpp.formatters.llama3 import (
    messages_to_prompt_v3_instruct,
    completion_to_prompt_v3_instruct,
)</p>
<p>llm = LlamaCPP(
    model_path="/models/llama-3-8b-instruct.Q4_0.gguf",
    messages_to_prompt=messages_to_prompt_v3_instruct,
    completion_to_prompt=completion_to_prompt_v3_instruct,
)</p>
<p>messages = [Message(role=MessageRole.USER, content="What is 2+2?")]
response = llm.chat(messages)
print(response.message.content)  # "4"
print(response.message.role)     # MessageRole.ASSISTANT
<div class="highlight"><pre><span></span><code><a id="__codelineno-7-1" name="__codelineno-7-1" href="#__codelineno-7-1"></a>### 2. Multi-turn Conversation
<a id="__codelineno-7-2" name="__codelineno-7-2" href="#__codelineno-7-2"></a>
<a id="__codelineno-7-3" name="__codelineno-7-3" href="#__codelineno-7-3"></a>With conversation history:
<a id="__codelineno-7-4" name="__codelineno-7-4" href="#__codelineno-7-4"></a>
<a id="__codelineno-7-5" name="__codelineno-7-5" href="#__codelineno-7-5"></a>```python notest
<a id="__codelineno-7-6" name="__codelineno-7-6" href="#__codelineno-7-6"></a>from serapeum.llama_cpp import LlamaCPP
<a id="__codelineno-7-7" name="__codelineno-7-7" href="#__codelineno-7-7"></a>from serapeum.core.llms import Message, MessageRole
<a id="__codelineno-7-8" name="__codelineno-7-8" href="#__codelineno-7-8"></a>from serapeum.llama_cpp.formatters.llama3 import (
<a id="__codelineno-7-9" name="__codelineno-7-9" href="#__codelineno-7-9"></a>    messages_to_prompt_v3_instruct,
<a id="__codelineno-7-10" name="__codelineno-7-10" href="#__codelineno-7-10"></a>    completion_to_prompt_v3_instruct,
<a id="__codelineno-7-11" name="__codelineno-7-11" href="#__codelineno-7-11"></a>)
<a id="__codelineno-7-12" name="__codelineno-7-12" href="#__codelineno-7-12"></a>
<a id="__codelineno-7-13" name="__codelineno-7-13" href="#__codelineno-7-13"></a>llm = LlamaCPP(
<a id="__codelineno-7-14" name="__codelineno-7-14" href="#__codelineno-7-14"></a>    model_path=&quot;/models/llama-3-8b-instruct.Q4_0.gguf&quot;,
<a id="__codelineno-7-15" name="__codelineno-7-15" href="#__codelineno-7-15"></a>    messages_to_prompt=messages_to_prompt_v3_instruct,
<a id="__codelineno-7-16" name="__codelineno-7-16" href="#__codelineno-7-16"></a>    completion_to_prompt=completion_to_prompt_v3_instruct,
<a id="__codelineno-7-17" name="__codelineno-7-17" href="#__codelineno-7-17"></a>)
<a id="__codelineno-7-18" name="__codelineno-7-18" href="#__codelineno-7-18"></a>
<a id="__codelineno-7-19" name="__codelineno-7-19" href="#__codelineno-7-19"></a>messages = [
<a id="__codelineno-7-20" name="__codelineno-7-20" href="#__codelineno-7-20"></a>    Message(role=MessageRole.USER, content=&quot;My name is Alex.&quot;),
<a id="__codelineno-7-21" name="__codelineno-7-21" href="#__codelineno-7-21"></a>    Message(role=MessageRole.ASSISTANT, content=&quot;Nice to meet you, Alex!&quot;),
<a id="__codelineno-7-22" name="__codelineno-7-22" href="#__codelineno-7-22"></a>    Message(role=MessageRole.USER, content=&quot;What is my name?&quot;),
<a id="__codelineno-7-23" name="__codelineno-7-23" href="#__codelineno-7-23"></a>]
<a id="__codelineno-7-24" name="__codelineno-7-24" href="#__codelineno-7-24"></a>
<a id="__codelineno-7-25" name="__codelineno-7-25" href="#__codelineno-7-25"></a>response = llm.chat(messages)
<a id="__codelineno-7-26" name="__codelineno-7-26" href="#__codelineno-7-26"></a>print(response.message.content)  # Should mention &quot;Alex&quot;
</code></pre></div></p>
<h3 id="3-chat-with-system-message"><a class="toclink" href="#3-chat-with-system-message">3. Chat with System Message</a><a class="headerlink" href="#3-chat-with-system-message" title="Permanent link">#</a></h3>
<p>System message for context:</p>
<p>```python notest
from serapeum.llama_cpp import LlamaCPP
from serapeum.core.llms import Message, MessageRole
from serapeum.llama_cpp.formatters.llama3 import (
    messages_to_prompt_v3_instruct,
    completion_to_prompt_v3_instruct,
)</p>
<p>llm = LlamaCPP(
    model_path="/models/llama-3-8b-instruct.Q4_0.gguf",
    messages_to_prompt=messages_to_prompt_v3_instruct,
    completion_to_prompt=completion_to_prompt_v3_instruct,
)</p>
<p>messages = [
    Message(role=MessageRole.SYSTEM, content="You are a helpful math tutor."),
    Message(role=MessageRole.USER, content="What is the square root of 144?"),
]</p>
<p>response = llm.chat(messages)
print(response.message.content)  # "12"
<div class="highlight"><pre><span></span><code><a id="__codelineno-8-1" name="__codelineno-8-1" href="#__codelineno-8-1"></a>---
<a id="__codelineno-8-2" name="__codelineno-8-2" href="#__codelineno-8-2"></a>
<a id="__codelineno-8-3" name="__codelineno-8-3" href="#__codelineno-8-3"></a>## Streaming Operations
<a id="__codelineno-8-4" name="__codelineno-8-4" href="#__codelineno-8-4"></a>
<a id="__codelineno-8-5" name="__codelineno-8-5" href="#__codelineno-8-5"></a>### 1. Stream Completion
<a id="__codelineno-8-6" name="__codelineno-8-6" href="#__codelineno-8-6"></a>
<a id="__codelineno-8-7" name="__codelineno-8-7" href="#__codelineno-8-7"></a>Real-time streaming completion:
<a id="__codelineno-8-8" name="__codelineno-8-8" href="#__codelineno-8-8"></a>
<a id="__codelineno-8-9" name="__codelineno-8-9" href="#__codelineno-8-9"></a>```python notest
<a id="__codelineno-8-10" name="__codelineno-8-10" href="#__codelineno-8-10"></a>from serapeum.llama_cpp import LlamaCPP
<a id="__codelineno-8-11" name="__codelineno-8-11" href="#__codelineno-8-11"></a>from serapeum.llama_cpp.formatters.llama3 import (
<a id="__codelineno-8-12" name="__codelineno-8-12" href="#__codelineno-8-12"></a>    messages_to_prompt_v3_instruct,
<a id="__codelineno-8-13" name="__codelineno-8-13" href="#__codelineno-8-13"></a>    completion_to_prompt_v3_instruct,
<a id="__codelineno-8-14" name="__codelineno-8-14" href="#__codelineno-8-14"></a>)
<a id="__codelineno-8-15" name="__codelineno-8-15" href="#__codelineno-8-15"></a>
<a id="__codelineno-8-16" name="__codelineno-8-16" href="#__codelineno-8-16"></a>llm = LlamaCPP(
<a id="__codelineno-8-17" name="__codelineno-8-17" href="#__codelineno-8-17"></a>    model_path=&quot;/models/llama-3-8b-instruct.Q4_0.gguf&quot;,
<a id="__codelineno-8-18" name="__codelineno-8-18" href="#__codelineno-8-18"></a>    messages_to_prompt=messages_to_prompt_v3_instruct,
<a id="__codelineno-8-19" name="__codelineno-8-19" href="#__codelineno-8-19"></a>    completion_to_prompt=completion_to_prompt_v3_instruct,
<a id="__codelineno-8-20" name="__codelineno-8-20" href="#__codelineno-8-20"></a>)
<a id="__codelineno-8-21" name="__codelineno-8-21" href="#__codelineno-8-21"></a>
<a id="__codelineno-8-22" name="__codelineno-8-22" href="#__codelineno-8-22"></a>for chunk in llm.complete(&quot;Count from 1 to 5.&quot;, stream=True):
<a id="__codelineno-8-23" name="__codelineno-8-23" href="#__codelineno-8-23"></a>    print(chunk.delta, end=&quot;&quot;, flush=True)
<a id="__codelineno-8-24" name="__codelineno-8-24" href="#__codelineno-8-24"></a>    # &quot;1&quot; &quot;, &quot; &quot;2&quot; &quot;, &quot; &quot;3&quot; &quot;, &quot; &quot;4&quot; &quot;, &quot; &quot;5&quot;
</code></pre></div></p>
<h3 id="2-stream-chat"><a class="toclink" href="#2-stream-chat">2. Stream Chat</a><a class="headerlink" href="#2-stream-chat" title="Permanent link">#</a></h3>
<p>Real-time streaming chat:</p>
<p>```python notest
from serapeum.llama_cpp import LlamaCPP
from serapeum.core.llms import Message, MessageRole
from serapeum.llama_cpp.formatters.llama3 import (
    messages_to_prompt_v3_instruct,
    completion_to_prompt_v3_instruct,
)</p>
<p>llm = LlamaCPP(
    model_path="/models/llama-3-8b-instruct.Q4_0.gguf",
    messages_to_prompt=messages_to_prompt_v3_instruct,
    completion_to_prompt=completion_to_prompt_v3_instruct,
)</p>
<p>messages = [Message(role=MessageRole.USER, content="Tell me a joke.")]
for chunk in llm.chat(messages, stream=True):
    print(chunk.delta, end="", flush=True)
<div class="highlight"><pre><span></span><code><a id="__codelineno-9-1" name="__codelineno-9-1" href="#__codelineno-9-1"></a>### 3. Processing Stream with Delta
<a id="__codelineno-9-2" name="__codelineno-9-2" href="#__codelineno-9-2"></a>
<a id="__codelineno-9-3" name="__codelineno-9-3" href="#__codelineno-9-3"></a>Access incremental content:
<a id="__codelineno-9-4" name="__codelineno-9-4" href="#__codelineno-9-4"></a>
<a id="__codelineno-9-5" name="__codelineno-9-5" href="#__codelineno-9-5"></a>```python notest
<a id="__codelineno-9-6" name="__codelineno-9-6" href="#__codelineno-9-6"></a>from serapeum.llama_cpp import LlamaCPP
<a id="__codelineno-9-7" name="__codelineno-9-7" href="#__codelineno-9-7"></a>from serapeum.llama_cpp.formatters.llama3 import (
<a id="__codelineno-9-8" name="__codelineno-9-8" href="#__codelineno-9-8"></a>    messages_to_prompt_v3_instruct,
<a id="__codelineno-9-9" name="__codelineno-9-9" href="#__codelineno-9-9"></a>    completion_to_prompt_v3_instruct,
<a id="__codelineno-9-10" name="__codelineno-9-10" href="#__codelineno-9-10"></a>)
<a id="__codelineno-9-11" name="__codelineno-9-11" href="#__codelineno-9-11"></a>
<a id="__codelineno-9-12" name="__codelineno-9-12" href="#__codelineno-9-12"></a>llm = LlamaCPP(
<a id="__codelineno-9-13" name="__codelineno-9-13" href="#__codelineno-9-13"></a>    model_path=&quot;/models/llama-3-8b-instruct.Q4_0.gguf&quot;,
<a id="__codelineno-9-14" name="__codelineno-9-14" href="#__codelineno-9-14"></a>    messages_to_prompt=messages_to_prompt_v3_instruct,
<a id="__codelineno-9-15" name="__codelineno-9-15" href="#__codelineno-9-15"></a>    completion_to_prompt=completion_to_prompt_v3_instruct,
<a id="__codelineno-9-16" name="__codelineno-9-16" href="#__codelineno-9-16"></a>)
<a id="__codelineno-9-17" name="__codelineno-9-17" href="#__codelineno-9-17"></a>
<a id="__codelineno-9-18" name="__codelineno-9-18" href="#__codelineno-9-18"></a>full_response = &quot;&quot;
<a id="__codelineno-9-19" name="__codelineno-9-19" href="#__codelineno-9-19"></a>for chunk in llm.complete(&quot;Say hello.&quot;, stream=True):
<a id="__codelineno-9-20" name="__codelineno-9-20" href="#__codelineno-9-20"></a>    delta = chunk.delta  # Incremental content
<a id="__codelineno-9-21" name="__codelineno-9-21" href="#__codelineno-9-21"></a>    if delta:
<a id="__codelineno-9-22" name="__codelineno-9-22" href="#__codelineno-9-22"></a>        full_response += delta
<a id="__codelineno-9-23" name="__codelineno-9-23" href="#__codelineno-9-23"></a>        print(delta, end=&quot;&quot;, flush=True)
<a id="__codelineno-9-24" name="__codelineno-9-24" href="#__codelineno-9-24"></a>
<a id="__codelineno-9-25" name="__codelineno-9-25" href="#__codelineno-9-25"></a>print(f&quot;\n\nFull response: {full_response}&quot;)
<a id="__codelineno-9-26" name="__codelineno-9-26" href="#__codelineno-9-26"></a>
<a id="__codelineno-9-27" name="__codelineno-9-27" href="#__codelineno-9-27"></a># Alternatively, the last chunk&#39;s .text contains the full accumulated text:
<a id="__codelineno-9-28" name="__codelineno-9-28" href="#__codelineno-9-28"></a>chunks = list(llm.complete(&quot;Say hello.&quot;, stream=True))
<a id="__codelineno-9-29" name="__codelineno-9-29" href="#__codelineno-9-29"></a>final_text = chunks[-1].text  # Same as joining all deltas
</code></pre></div></p>
<hr />
<h2 id="async-operations"><a class="toclink" href="#async-operations">Async Operations</a><a class="headerlink" href="#async-operations" title="Permanent link">#</a></h2>
<p>LlamaCPP offloads CPU-bound inference to a thread pool via <code>asyncio.to_thread</code>,
so async calls do not block the event loop.</p>
<h3 id="1-async-completion"><a class="toclink" href="#1-async-completion">1. Async Completion</a><a class="headerlink" href="#1-async-completion" title="Permanent link">#</a></h3>
<p>Non-blocking completion:</p>
<p>```python notest
import asyncio
from serapeum.llama_cpp import LlamaCPP
from serapeum.llama_cpp.formatters.llama3 import (
    messages_to_prompt_v3_instruct,
    completion_to_prompt_v3_instruct,
)</p>
<p>async def main():
    llm = await asyncio.to_thread(
        LlamaCPP,
        model_path="/models/llama-3-8b-instruct.Q4_0.gguf",
        messages_to_prompt=messages_to_prompt_v3_instruct,
        completion_to_prompt=completion_to_prompt_v3_instruct,
    )</p>
<div class="highlight"><pre><span></span><code>response = await llm.acomplete(&quot;Say hello.&quot;)
print(response.text)
</code></pre></div>
<p>asyncio.run(main())
<div class="highlight"><pre><span></span><code><a id="__codelineno-10-1" name="__codelineno-10-1" href="#__codelineno-10-1"></a>### 2. Async Chat
<a id="__codelineno-10-2" name="__codelineno-10-2" href="#__codelineno-10-2"></a>
<a id="__codelineno-10-3" name="__codelineno-10-3" href="#__codelineno-10-3"></a>Non-blocking chat:
<a id="__codelineno-10-4" name="__codelineno-10-4" href="#__codelineno-10-4"></a>
<a id="__codelineno-10-5" name="__codelineno-10-5" href="#__codelineno-10-5"></a>```python notest
<a id="__codelineno-10-6" name="__codelineno-10-6" href="#__codelineno-10-6"></a>import asyncio
<a id="__codelineno-10-7" name="__codelineno-10-7" href="#__codelineno-10-7"></a>from serapeum.llama_cpp import LlamaCPP
<a id="__codelineno-10-8" name="__codelineno-10-8" href="#__codelineno-10-8"></a>from serapeum.core.llms import Message, MessageRole
<a id="__codelineno-10-9" name="__codelineno-10-9" href="#__codelineno-10-9"></a>from serapeum.llama_cpp.formatters.llama3 import (
<a id="__codelineno-10-10" name="__codelineno-10-10" href="#__codelineno-10-10"></a>    messages_to_prompt_v3_instruct,
<a id="__codelineno-10-11" name="__codelineno-10-11" href="#__codelineno-10-11"></a>    completion_to_prompt_v3_instruct,
<a id="__codelineno-10-12" name="__codelineno-10-12" href="#__codelineno-10-12"></a>)
<a id="__codelineno-10-13" name="__codelineno-10-13" href="#__codelineno-10-13"></a>
<a id="__codelineno-10-14" name="__codelineno-10-14" href="#__codelineno-10-14"></a>
<a id="__codelineno-10-15" name="__codelineno-10-15" href="#__codelineno-10-15"></a>async def main():
<a id="__codelineno-10-16" name="__codelineno-10-16" href="#__codelineno-10-16"></a>    llm = await asyncio.to_thread(
<a id="__codelineno-10-17" name="__codelineno-10-17" href="#__codelineno-10-17"></a>        LlamaCPP,
<a id="__codelineno-10-18" name="__codelineno-10-18" href="#__codelineno-10-18"></a>        model_path=&quot;/models/llama-3-8b-instruct.Q4_0.gguf&quot;,
<a id="__codelineno-10-19" name="__codelineno-10-19" href="#__codelineno-10-19"></a>        messages_to_prompt=messages_to_prompt_v3_instruct,
<a id="__codelineno-10-20" name="__codelineno-10-20" href="#__codelineno-10-20"></a>        completion_to_prompt=completion_to_prompt_v3_instruct,
<a id="__codelineno-10-21" name="__codelineno-10-21" href="#__codelineno-10-21"></a>    )
<a id="__codelineno-10-22" name="__codelineno-10-22" href="#__codelineno-10-22"></a>
<a id="__codelineno-10-23" name="__codelineno-10-23" href="#__codelineno-10-23"></a>    messages = [Message(role=MessageRole.USER, content=&quot;Hello!&quot;)]
<a id="__codelineno-10-24" name="__codelineno-10-24" href="#__codelineno-10-24"></a>    response = await llm.achat(messages)
<a id="__codelineno-10-25" name="__codelineno-10-25" href="#__codelineno-10-25"></a>    print(response.message.content)
<a id="__codelineno-10-26" name="__codelineno-10-26" href="#__codelineno-10-26"></a>
<a id="__codelineno-10-27" name="__codelineno-10-27" href="#__codelineno-10-27"></a>
<a id="__codelineno-10-28" name="__codelineno-10-28" href="#__codelineno-10-28"></a>asyncio.run(main())
</code></pre></div></p>
<h3 id="3-async-streaming"><a class="toclink" href="#3-async-streaming">3. Async Streaming</a><a class="headerlink" href="#3-async-streaming" title="Permanent link">#</a></h3>
<p>Non-blocking streaming completion:</p>
<p>```python notest
import asyncio
from serapeum.llama_cpp import LlamaCPP
from serapeum.llama_cpp.formatters.llama3 import (
    messages_to_prompt_v3_instruct,
    completion_to_prompt_v3_instruct,
)</p>
<p>async def main():
    llm = await asyncio.to_thread(
        LlamaCPP,
        model_path="/models/llama-3-8b-instruct.Q4_0.gguf",
        messages_to_prompt=messages_to_prompt_v3_instruct,
        completion_to_prompt=completion_to_prompt_v3_instruct,
    )</p>
<div class="highlight"><pre><span></span><code>gen = await llm.acomplete(&quot;Count to 5&quot;, stream=True)
async for chunk in gen:
    print(chunk.delta, end=&quot;&quot;, flush=True)
</code></pre></div>
<p>asyncio.run(main())
<div class="highlight"><pre><span></span><code><a id="__codelineno-11-1" name="__codelineno-11-1" href="#__codelineno-11-1"></a>### 4. Concurrent Async Requests
<a id="__codelineno-11-2" name="__codelineno-11-2" href="#__codelineno-11-2"></a>
<a id="__codelineno-11-3" name="__codelineno-11-3" href="#__codelineno-11-3"></a>Process multiple requests concurrently:
<a id="__codelineno-11-4" name="__codelineno-11-4" href="#__codelineno-11-4"></a>
<a id="__codelineno-11-5" name="__codelineno-11-5" href="#__codelineno-11-5"></a>```python notest
<a id="__codelineno-11-6" name="__codelineno-11-6" href="#__codelineno-11-6"></a>import asyncio
<a id="__codelineno-11-7" name="__codelineno-11-7" href="#__codelineno-11-7"></a>from serapeum.llama_cpp import LlamaCPP
<a id="__codelineno-11-8" name="__codelineno-11-8" href="#__codelineno-11-8"></a>from serapeum.llama_cpp.formatters.llama3 import (
<a id="__codelineno-11-9" name="__codelineno-11-9" href="#__codelineno-11-9"></a>    messages_to_prompt_v3_instruct,
<a id="__codelineno-11-10" name="__codelineno-11-10" href="#__codelineno-11-10"></a>    completion_to_prompt_v3_instruct,
<a id="__codelineno-11-11" name="__codelineno-11-11" href="#__codelineno-11-11"></a>)
<a id="__codelineno-11-12" name="__codelineno-11-12" href="#__codelineno-11-12"></a>
<a id="__codelineno-11-13" name="__codelineno-11-13" href="#__codelineno-11-13"></a>
<a id="__codelineno-11-14" name="__codelineno-11-14" href="#__codelineno-11-14"></a>async def main():
<a id="__codelineno-11-15" name="__codelineno-11-15" href="#__codelineno-11-15"></a>    llm = await asyncio.to_thread(
<a id="__codelineno-11-16" name="__codelineno-11-16" href="#__codelineno-11-16"></a>        LlamaCPP,
<a id="__codelineno-11-17" name="__codelineno-11-17" href="#__codelineno-11-17"></a>        model_path=&quot;/models/llama-3-8b-instruct.Q4_0.gguf&quot;,
<a id="__codelineno-11-18" name="__codelineno-11-18" href="#__codelineno-11-18"></a>        messages_to_prompt=messages_to_prompt_v3_instruct,
<a id="__codelineno-11-19" name="__codelineno-11-19" href="#__codelineno-11-19"></a>        completion_to_prompt=completion_to_prompt_v3_instruct,
<a id="__codelineno-11-20" name="__codelineno-11-20" href="#__codelineno-11-20"></a>    )
<a id="__codelineno-11-21" name="__codelineno-11-21" href="#__codelineno-11-21"></a>
<a id="__codelineno-11-22" name="__codelineno-11-22" href="#__codelineno-11-22"></a>    prompts = [&quot;What is 2+2?&quot;, &quot;What is 3+3?&quot;, &quot;What is 4+4?&quot;]
<a id="__codelineno-11-23" name="__codelineno-11-23" href="#__codelineno-11-23"></a>
<a id="__codelineno-11-24" name="__codelineno-11-24" href="#__codelineno-11-24"></a>    tasks = [llm.acomplete(prompt) for prompt in prompts]
<a id="__codelineno-11-25" name="__codelineno-11-25" href="#__codelineno-11-25"></a>    responses = await asyncio.gather(*tasks)
<a id="__codelineno-11-26" name="__codelineno-11-26" href="#__codelineno-11-26"></a>
<a id="__codelineno-11-27" name="__codelineno-11-27" href="#__codelineno-11-27"></a>    for prompt, response in zip(prompts, responses):
<a id="__codelineno-11-28" name="__codelineno-11-28" href="#__codelineno-11-28"></a>        print(f&quot;{prompt} -&gt; {response.text}&quot;)
<a id="__codelineno-11-29" name="__codelineno-11-29" href="#__codelineno-11-29"></a>
<a id="__codelineno-11-30" name="__codelineno-11-30" href="#__codelineno-11-30"></a>
<a id="__codelineno-11-31" name="__codelineno-11-31" href="#__codelineno-11-31"></a>asyncio.run(main())
</code></pre></div></p>
<hr />
<h2 id="integration-with-orchestrators"><a class="toclink" href="#integration-with-orchestrators">Integration with Orchestrators</a><a class="headerlink" href="#integration-with-orchestrators" title="Permanent link">#</a></h2>
<h3 id="1-with-textcompletionllm"><a class="toclink" href="#1-with-textcompletionllm">1. With TextCompletionLLM</a><a class="headerlink" href="#1-with-textcompletionllm" title="Permanent link">#</a></h3>
<p>Use LlamaCPP with <code>TextCompletionLLM</code> for structured outputs:</p>
<p>```python notest
from pydantic import BaseModel
from serapeum.core.output_parsers import PydanticParser
from serapeum.core.llms import TextCompletionLLM
from serapeum.llama_cpp import LlamaCPP
from serapeum.llama_cpp.formatters.llama3 import (
    messages_to_prompt_v3_instruct,
    completion_to_prompt_v3_instruct,
)</p>
<p>class DummyModel(BaseModel):
    value: str</p>
<p>llm = LlamaCPP(
    model_path="/models/llama-3-8b-instruct.Q4_0.gguf",
    messages_to_prompt=messages_to_prompt_v3_instruct,
    completion_to_prompt=completion_to_prompt_v3_instruct,
)</p>
<p>parser = PydanticParser(output_cls=DummyModel)</p>
<p>text_llm = TextCompletionLLM(
    output_parser=parser,
    prompt="Value: {value}",
    llm=llm,
)</p>
<p>result = text_llm(value="input")
print(result.value)  # "input"
<div class="highlight"><pre><span></span><code><a id="__codelineno-12-1" name="__codelineno-12-1" href="#__codelineno-12-1"></a>---
<a id="__codelineno-12-2" name="__codelineno-12-2" href="#__codelineno-12-2"></a>
<a id="__codelineno-12-3" name="__codelineno-12-3" href="#__codelineno-12-3"></a>## Prompt Formatters
<a id="__codelineno-12-4" name="__codelineno-12-4" href="#__codelineno-12-4"></a>
<a id="__codelineno-12-5" name="__codelineno-12-5" href="#__codelineno-12-5"></a>LlamaCPP **requires** explicit prompt formatters. Using the wrong formatter
<a id="__codelineno-12-6" name="__codelineno-12-6" href="#__codelineno-12-6"></a>for your model produces garbage output.
<a id="__codelineno-12-7" name="__codelineno-12-7" href="#__codelineno-12-7"></a>
<a id="__codelineno-12-8" name="__codelineno-12-8" href="#__codelineno-12-8"></a>### Available Formatters
<a id="__codelineno-12-9" name="__codelineno-12-9" href="#__codelineno-12-9"></a>
<a id="__codelineno-12-10" name="__codelineno-12-10" href="#__codelineno-12-10"></a>#### Llama 2 / Mistral
<a id="__codelineno-12-11" name="__codelineno-12-11" href="#__codelineno-12-11"></a>
<a id="__codelineno-12-12" name="__codelineno-12-12" href="#__codelineno-12-12"></a>```python notest
<a id="__codelineno-12-13" name="__codelineno-12-13" href="#__codelineno-12-13"></a>from serapeum.llama_cpp.formatters.llama2 import (
<a id="__codelineno-12-14" name="__codelineno-12-14" href="#__codelineno-12-14"></a>    messages_to_prompt,
<a id="__codelineno-12-15" name="__codelineno-12-15" href="#__codelineno-12-15"></a>    completion_to_prompt,
<a id="__codelineno-12-16" name="__codelineno-12-16" href="#__codelineno-12-16"></a>)
</code></pre></div></p>
<p>Uses the <code>[INST]...[/INST]</code> format:
<div class="highlight"><pre><span></span><code><a id="__codelineno-13-1" name="__codelineno-13-1" href="#__codelineno-13-1"></a>&lt;s&gt; [INST] &lt;&lt;SYS&gt;&gt; system prompt &lt;&lt;/SYS&gt;&gt; user message [/INST] assistant reply &lt;/s&gt;
</code></pre></div></p>
<h4 id="llama-3-instruct"><a class="toclink" href="#llama-3-instruct">Llama 3 Instruct</a><a class="headerlink" href="#llama-3-instruct" title="Permanent link">#</a></h4>
<p>```python notest
from serapeum.llama_cpp.formatters.llama3 import (
    messages_to_prompt_v3_instruct,
    completion_to_prompt_v3_instruct,
)
<div class="highlight"><pre><span></span><code><a id="__codelineno-14-1" name="__codelineno-14-1" href="#__codelineno-14-1"></a>Uses the `&lt;|start_header_id|&gt;...&lt;|eot_id|&gt;` format:
</code></pre></div>
&lt;|start_header_id|&gt;system&lt;|end_header_id|&gt;
system prompt&lt;|eot_id|&gt;
&lt;|start_header_id|&gt;user&lt;|end_header_id|&gt;
user message&lt;|eot_id|&gt;
&lt;|start_header_id|&gt;assistant&lt;|end_header_id|&gt;
<div class="highlight"><pre><span></span><code><a id="__codelineno-15-1" name="__codelineno-15-1" href="#__codelineno-15-1"></a>### Formatter Behavior
<a id="__codelineno-15-2" name="__codelineno-15-2" href="#__codelineno-15-2"></a>
<a id="__codelineno-15-3" name="__codelineno-15-3" href="#__codelineno-15-3"></a>**messages_to_prompt**: Converts a list of `Message` objects to a single formatted
<a id="__codelineno-15-4" name="__codelineno-15-4" href="#__codelineno-15-4"></a>string. Handles system, user, and assistant roles, including multi-turn conversations.
<a id="__codelineno-15-5" name="__codelineno-15-5" href="#__codelineno-15-5"></a>
<a id="__codelineno-15-6" name="__codelineno-15-6" href="#__codelineno-15-6"></a>**completion_to_prompt**: Wraps a plain string in the model&#39;s instruct format
<a id="__codelineno-15-7" name="__codelineno-15-7" href="#__codelineno-15-7"></a>with a default system prompt.
<a id="__codelineno-15-8" name="__codelineno-15-8" href="#__codelineno-15-8"></a>
<a id="__codelineno-15-9" name="__codelineno-15-9" href="#__codelineno-15-9"></a>### Custom Formatters
<a id="__codelineno-15-10" name="__codelineno-15-10" href="#__codelineno-15-10"></a>
<a id="__codelineno-15-11" name="__codelineno-15-11" href="#__codelineno-15-11"></a>You can write your own formatters for other model families:
<a id="__codelineno-15-12" name="__codelineno-15-12" href="#__codelineno-15-12"></a>
<a id="__codelineno-15-13" name="__codelineno-15-13" href="#__codelineno-15-13"></a>```python notest
<a id="__codelineno-15-14" name="__codelineno-15-14" href="#__codelineno-15-14"></a>from collections.abc import Sequence
<a id="__codelineno-15-15" name="__codelineno-15-15" href="#__codelineno-15-15"></a>from serapeum.core.llms import Message, MessageRole
<a id="__codelineno-15-16" name="__codelineno-15-16" href="#__codelineno-15-16"></a>from serapeum.llama_cpp import LlamaCPP
<a id="__codelineno-15-17" name="__codelineno-15-17" href="#__codelineno-15-17"></a>
<a id="__codelineno-15-18" name="__codelineno-15-18" href="#__codelineno-15-18"></a>
<a id="__codelineno-15-19" name="__codelineno-15-19" href="#__codelineno-15-19"></a>def my_messages_to_prompt(messages: Sequence[Message]) -&gt; str:
<a id="__codelineno-15-20" name="__codelineno-15-20" href="#__codelineno-15-20"></a>    &quot;&quot;&quot;Custom formatter for your model.&quot;&quot;&quot;
<a id="__codelineno-15-21" name="__codelineno-15-21" href="#__codelineno-15-21"></a>    parts = []
<a id="__codelineno-15-22" name="__codelineno-15-22" href="#__codelineno-15-22"></a>    for msg in messages:
<a id="__codelineno-15-23" name="__codelineno-15-23" href="#__codelineno-15-23"></a>        if msg.role == MessageRole.SYSTEM:
<a id="__codelineno-15-24" name="__codelineno-15-24" href="#__codelineno-15-24"></a>            parts.append(f&quot;[SYSTEM] {msg.content}&quot;)
<a id="__codelineno-15-25" name="__codelineno-15-25" href="#__codelineno-15-25"></a>        elif msg.role == MessageRole.USER:
<a id="__codelineno-15-26" name="__codelineno-15-26" href="#__codelineno-15-26"></a>            parts.append(f&quot;[USER] {msg.content}&quot;)
<a id="__codelineno-15-27" name="__codelineno-15-27" href="#__codelineno-15-27"></a>        elif msg.role == MessageRole.ASSISTANT:
<a id="__codelineno-15-28" name="__codelineno-15-28" href="#__codelineno-15-28"></a>            parts.append(f&quot;[ASSISTANT] {msg.content}&quot;)
<a id="__codelineno-15-29" name="__codelineno-15-29" href="#__codelineno-15-29"></a>    parts.append(&quot;[ASSISTANT]&quot;)
<a id="__codelineno-15-30" name="__codelineno-15-30" href="#__codelineno-15-30"></a>    return &quot;\n&quot;.join(parts)
<a id="__codelineno-15-31" name="__codelineno-15-31" href="#__codelineno-15-31"></a>
<a id="__codelineno-15-32" name="__codelineno-15-32" href="#__codelineno-15-32"></a>
<a id="__codelineno-15-33" name="__codelineno-15-33" href="#__codelineno-15-33"></a>def my_completion_to_prompt(completion: str) -&gt; str:
<a id="__codelineno-15-34" name="__codelineno-15-34" href="#__codelineno-15-34"></a>    &quot;&quot;&quot;Custom formatter for completion.&quot;&quot;&quot;
<a id="__codelineno-15-35" name="__codelineno-15-35" href="#__codelineno-15-35"></a>    return f&quot;[SYSTEM] You are helpful.\n[USER] {completion}\n[ASSISTANT]&quot;
<a id="__codelineno-15-36" name="__codelineno-15-36" href="#__codelineno-15-36"></a>
<a id="__codelineno-15-37" name="__codelineno-15-37" href="#__codelineno-15-37"></a>
<a id="__codelineno-15-38" name="__codelineno-15-38" href="#__codelineno-15-38"></a>llm = LlamaCPP(
<a id="__codelineno-15-39" name="__codelineno-15-39" href="#__codelineno-15-39"></a>    model_path=&quot;/models/my-model.gguf&quot;,
<a id="__codelineno-15-40" name="__codelineno-15-40" href="#__codelineno-15-40"></a>    messages_to_prompt=my_messages_to_prompt,
<a id="__codelineno-15-41" name="__codelineno-15-41" href="#__codelineno-15-41"></a>    completion_to_prompt=my_completion_to_prompt,
<a id="__codelineno-15-42" name="__codelineno-15-42" href="#__codelineno-15-42"></a>)
</code></pre></div></p>
<hr />
<h2 id="tokenization"><a class="toclink" href="#tokenization">Tokenization</a><a class="headerlink" href="#tokenization" title="Permanent link">#</a></h2>
<p>LlamaCPP exposes the model's tokenizer for prompt length checking:</p>
<p>```python notest
from serapeum.llama_cpp import LlamaCPP
from serapeum.llama_cpp.formatters.llama3 import (
    messages_to_prompt_v3_instruct,
    completion_to_prompt_v3_instruct,
)</p>
<p>llm = LlamaCPP(
    model_path="/models/llama-3-8b-instruct.Q4_0.gguf",
    messages_to_prompt=messages_to_prompt_v3_instruct,
    completion_to_prompt=completion_to_prompt_v3_instruct,
)</p>
<h1 id="get-token-ids"><a class="toclink" href="#get-token-ids">Get token IDs</a><a class="headerlink" href="#get-token-ids" title="Permanent link">#</a></h1>
<p>tokens = llm.tokenize("Hello, world!")
print(tokens)  # [1, 15043, 29892, 3186, 29991]</p>
<h1 id="count-tokens"><a class="toclink" href="#count-tokens">Count tokens</a><a class="headerlink" href="#count-tokens" title="Permanent link">#</a></h1>
<p>count = llm.count_tokens("Hello, world!")
print(count)  # 5
<div class="highlight"><pre><span></span><code><a id="__codelineno-16-1" name="__codelineno-16-1" href="#__codelineno-16-1"></a>---
<a id="__codelineno-16-2" name="__codelineno-16-2" href="#__codelineno-16-2"></a>
<a id="__codelineno-16-3" name="__codelineno-16-3" href="#__codelineno-16-3"></a>## Best Practices
<a id="__codelineno-16-4" name="__codelineno-16-4" href="#__codelineno-16-4"></a>
<a id="__codelineno-16-5" name="__codelineno-16-5" href="#__codelineno-16-5"></a>### 1. Reuse LLM Instances
<a id="__codelineno-16-6" name="__codelineno-16-6" href="#__codelineno-16-6"></a>
<a id="__codelineno-16-7" name="__codelineno-16-7" href="#__codelineno-16-7"></a>Create once, use many times  model loading is expensive:
<a id="__codelineno-16-8" name="__codelineno-16-8" href="#__codelineno-16-8"></a>
<a id="__codelineno-16-9" name="__codelineno-16-9" href="#__codelineno-16-9"></a>```python notest
<a id="__codelineno-16-10" name="__codelineno-16-10" href="#__codelineno-16-10"></a>from serapeum.llama_cpp import LlamaCPP
<a id="__codelineno-16-11" name="__codelineno-16-11" href="#__codelineno-16-11"></a>from serapeum.llama_cpp.formatters.llama3 import (
<a id="__codelineno-16-12" name="__codelineno-16-12" href="#__codelineno-16-12"></a>    messages_to_prompt_v3_instruct,
<a id="__codelineno-16-13" name="__codelineno-16-13" href="#__codelineno-16-13"></a>    completion_to_prompt_v3_instruct,
<a id="__codelineno-16-14" name="__codelineno-16-14" href="#__codelineno-16-14"></a>)
<a id="__codelineno-16-15" name="__codelineno-16-15" href="#__codelineno-16-15"></a>
<a id="__codelineno-16-16" name="__codelineno-16-16" href="#__codelineno-16-16"></a># Good: Create once
<a id="__codelineno-16-17" name="__codelineno-16-17" href="#__codelineno-16-17"></a>llm = LlamaCPP(
<a id="__codelineno-16-18" name="__codelineno-16-18" href="#__codelineno-16-18"></a>    model_path=&quot;/models/llama-3-8b-instruct.Q4_0.gguf&quot;,
<a id="__codelineno-16-19" name="__codelineno-16-19" href="#__codelineno-16-19"></a>    messages_to_prompt=messages_to_prompt_v3_instruct,
<a id="__codelineno-16-20" name="__codelineno-16-20" href="#__codelineno-16-20"></a>    completion_to_prompt=completion_to_prompt_v3_instruct,
<a id="__codelineno-16-21" name="__codelineno-16-21" href="#__codelineno-16-21"></a>)
<a id="__codelineno-16-22" name="__codelineno-16-22" href="#__codelineno-16-22"></a>
<a id="__codelineno-16-23" name="__codelineno-16-23" href="#__codelineno-16-23"></a># Reuse for multiple calls (model stays loaded)
<a id="__codelineno-16-24" name="__codelineno-16-24" href="#__codelineno-16-24"></a>response1 = llm.complete(&quot;Hello!&quot;)
<a id="__codelineno-16-25" name="__codelineno-16-25" href="#__codelineno-16-25"></a>response2 = llm.complete(&quot;How are you?&quot;)
<a id="__codelineno-16-26" name="__codelineno-16-26" href="#__codelineno-16-26"></a>
<a id="__codelineno-16-27" name="__codelineno-16-27" href="#__codelineno-16-27"></a># Bad: Don&#39;t recreate for each call  model reloads every time
<a id="__codelineno-16-28" name="__codelineno-16-28" href="#__codelineno-16-28"></a>def process(prompt):
<a id="__codelineno-16-29" name="__codelineno-16-29" href="#__codelineno-16-29"></a>    llm = LlamaCPP(model_path=&quot;...&quot;, ...)  # Expensive!
<a id="__codelineno-16-30" name="__codelineno-16-30" href="#__codelineno-16-30"></a>    return llm.complete(prompt)
</code></pre></div></p>
<h3 id="2-match-formatter-to-model"><a class="toclink" href="#2-match-formatter-to-model">2. Match Formatter to Model</a><a class="headerlink" href="#2-match-formatter-to-model" title="Permanent link">#</a></h3>
<p>Always use the correct formatter for your model family:</p>
<p>```python notest
from serapeum.llama_cpp import LlamaCPP</p>
<h1 id="llama-2-mistral-models"><a class="toclink" href="#llama-2-mistral-models">Llama 2 / Mistral models</a><a class="headerlink" href="#llama-2-mistral-models" title="Permanent link">#</a></h1>
<p>from serapeum.llama_cpp.formatters.llama2 import messages_to_prompt, completion_to_prompt</p>
<h1 id="llama-3-instruct-models"><a class="toclink" href="#llama-3-instruct-models">Llama 3 Instruct models</a><a class="headerlink" href="#llama-3-instruct-models" title="Permanent link">#</a></h1>
<p>from serapeum.llama_cpp.formatters.llama3 import (
    messages_to_prompt_v3_instruct, completion_to_prompt_v3_instruct
)
<div class="highlight"><pre><span></span><code><a id="__codelineno-17-1" name="__codelineno-17-1" href="#__codelineno-17-1"></a>### 3. Handle Errors Gracefully
<a id="__codelineno-17-2" name="__codelineno-17-2" href="#__codelineno-17-2"></a>
<a id="__codelineno-17-3" name="__codelineno-17-3" href="#__codelineno-17-3"></a>```python notest
<a id="__codelineno-17-4" name="__codelineno-17-4" href="#__codelineno-17-4"></a>from serapeum.llama_cpp import LlamaCPP
<a id="__codelineno-17-5" name="__codelineno-17-5" href="#__codelineno-17-5"></a>from serapeum.llama_cpp.formatters.llama3 import (
<a id="__codelineno-17-6" name="__codelineno-17-6" href="#__codelineno-17-6"></a>    messages_to_prompt_v3_instruct,
<a id="__codelineno-17-7" name="__codelineno-17-7" href="#__codelineno-17-7"></a>    completion_to_prompt_v3_instruct,
<a id="__codelineno-17-8" name="__codelineno-17-8" href="#__codelineno-17-8"></a>)
<a id="__codelineno-17-9" name="__codelineno-17-9" href="#__codelineno-17-9"></a>
<a id="__codelineno-17-10" name="__codelineno-17-10" href="#__codelineno-17-10"></a>llm = LlamaCPP(
<a id="__codelineno-17-11" name="__codelineno-17-11" href="#__codelineno-17-11"></a>    model_path=&quot;/models/llama-3-8b-instruct.Q4_0.gguf&quot;,
<a id="__codelineno-17-12" name="__codelineno-17-12" href="#__codelineno-17-12"></a>    messages_to_prompt=messages_to_prompt_v3_instruct,
<a id="__codelineno-17-13" name="__codelineno-17-13" href="#__codelineno-17-13"></a>    completion_to_prompt=completion_to_prompt_v3_instruct,
<a id="__codelineno-17-14" name="__codelineno-17-14" href="#__codelineno-17-14"></a>)
<a id="__codelineno-17-15" name="__codelineno-17-15" href="#__codelineno-17-15"></a>
<a id="__codelineno-17-16" name="__codelineno-17-16" href="#__codelineno-17-16"></a>try:
<a id="__codelineno-17-17" name="__codelineno-17-17" href="#__codelineno-17-17"></a>    response = llm.complete(&quot;Hello&quot;)
<a id="__codelineno-17-18" name="__codelineno-17-18" href="#__codelineno-17-18"></a>except ValueError as e:
<a id="__codelineno-17-19" name="__codelineno-17-19" href="#__codelineno-17-19"></a>    print(f&quot;Prompt too long: {e}&quot;)
<a id="__codelineno-17-20" name="__codelineno-17-20" href="#__codelineno-17-20"></a>except RuntimeError as e:
<a id="__codelineno-17-21" name="__codelineno-17-21" href="#__codelineno-17-21"></a>    print(f&quot;Model error: {e}&quot;)
</code></pre></div></p>
<h3 id="4-use-gpu-for-performance"><a class="toclink" href="#4-use-gpu-for-performance">4. Use GPU for Performance</a><a class="headerlink" href="#4-use-gpu-for-performance" title="Permanent link">#</a></h3>
<p>```python notest
from serapeum.llama_cpp import LlamaCPP
from serapeum.llama_cpp.formatters.llama3 import (
    messages_to_prompt_v3_instruct,
    completion_to_prompt_v3_instruct,
)</p>
<h1 id="offload-all-layers-to-gpu"><a class="toclink" href="#offload-all-layers-to-gpu">Offload all layers to GPU</a><a class="headerlink" href="#offload-all-layers-to-gpu" title="Permanent link">#</a></h1>
<p>llm = LlamaCPP(
    model_path="/models/llama-3-8b-instruct.Q4_0.gguf",
    n_gpu_layers=-1,
    messages_to_prompt=messages_to_prompt_v3_instruct,
    completion_to_prompt=completion_to_prompt_v3_instruct,
)
<div class="highlight"><pre><span></span><code><a id="__codelineno-18-1" name="__codelineno-18-1" href="#__codelineno-18-1"></a>### 5. Use Temperature=0 for Deterministic Output
<a id="__codelineno-18-2" name="__codelineno-18-2" href="#__codelineno-18-2"></a>
<a id="__codelineno-18-3" name="__codelineno-18-3" href="#__codelineno-18-3"></a>```python notest
<a id="__codelineno-18-4" name="__codelineno-18-4" href="#__codelineno-18-4"></a>from serapeum.llama_cpp import LlamaCPP
<a id="__codelineno-18-5" name="__codelineno-18-5" href="#__codelineno-18-5"></a>from serapeum.llama_cpp.formatters.llama3 import (
<a id="__codelineno-18-6" name="__codelineno-18-6" href="#__codelineno-18-6"></a>    messages_to_prompt_v3_instruct,
<a id="__codelineno-18-7" name="__codelineno-18-7" href="#__codelineno-18-7"></a>    completion_to_prompt_v3_instruct,
<a id="__codelineno-18-8" name="__codelineno-18-8" href="#__codelineno-18-8"></a>)
<a id="__codelineno-18-9" name="__codelineno-18-9" href="#__codelineno-18-9"></a>
<a id="__codelineno-18-10" name="__codelineno-18-10" href="#__codelineno-18-10"></a># Greedy decoding for reproducible results
<a id="__codelineno-18-11" name="__codelineno-18-11" href="#__codelineno-18-11"></a>llm = LlamaCPP(
<a id="__codelineno-18-12" name="__codelineno-18-12" href="#__codelineno-18-12"></a>    model_path=&quot;/models/llama-3-8b-instruct.Q4_0.gguf&quot;,
<a id="__codelineno-18-13" name="__codelineno-18-13" href="#__codelineno-18-13"></a>    temperature=0.0,
<a id="__codelineno-18-14" name="__codelineno-18-14" href="#__codelineno-18-14"></a>    messages_to_prompt=messages_to_prompt_v3_instruct,
<a id="__codelineno-18-15" name="__codelineno-18-15" href="#__codelineno-18-15"></a>    completion_to_prompt=completion_to_prompt_v3_instruct,
<a id="__codelineno-18-16" name="__codelineno-18-16" href="#__codelineno-18-16"></a>)
<a id="__codelineno-18-17" name="__codelineno-18-17" href="#__codelineno-18-17"></a>
<a id="__codelineno-18-18" name="__codelineno-18-18" href="#__codelineno-18-18"></a># Same prompt always produces same output
<a id="__codelineno-18-19" name="__codelineno-18-19" href="#__codelineno-18-19"></a>r1 = llm.complete(&quot;The capital of France is&quot;)
<a id="__codelineno-18-20" name="__codelineno-18-20" href="#__codelineno-18-20"></a>r2 = llm.complete(&quot;The capital of France is&quot;)
<a id="__codelineno-18-21" name="__codelineno-18-21" href="#__codelineno-18-21"></a>assert r1.text == r2.text
</code></pre></div></p>
<hr />
<h2 id="see-also"><a class="toclink" href="#see-also">See Also</a><a class="headerlink" href="#see-also" title="Permanent link">#</a></h2>
<ul>
<li><a href="../llama_cpp_sequence/">Execution Flow and Method Calls</a> - Detailed sequence diagrams</li>
<li><a href="../llama_cpp_class/">Architecture and Class Relationships</a> - Class structure</li>
<li><a href="../llama_cpp_dataflow/">Data Transformations and Validation</a> - Data flow details</li>
<li><a href="../llama_cpp_components/">Component Boundaries and Interactions</a> - System components</li>
<li><a href="../llama_cpp_state/">Lifecycle and State Management</a> - State management</li>
</ul>












                
              </article>
            </div>
          
          
<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
          <button type="button" class="md-top md-icon" data-md-component="top" hidden>
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8z"/></svg>
  Back to top
</button>
        
      </main>
      
        <footer class="md-footer">
  
    
      
      <nav class="md-footer__inner md-grid" aria-label="Footer" >
        
          
          <a href="../general/" class="md-footer__link md-footer__link--prev" aria-label="Previous: Overview">
            <div class="md-footer__button md-icon">
              
              <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg>
            </div>
            <div class="md-footer__title">
              <span class="md-footer__direction">
                Previous
              </span>
              <div class="md-ellipsis">
                Overview
              </div>
            </div>
          </a>
        
        
          
          <a href="../llama_cpp_sequence/" class="md-footer__link md-footer__link--next" aria-label="Next: Execution Flow">
            <div class="md-footer__title">
              <span class="md-footer__direction">
                Next
              </span>
              <div class="md-ellipsis">
                Execution Flow
              </div>
            </div>
            <div class="md-footer__button md-icon">
              
              <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M4 11v2h12l-5.5 5.5 1.42 1.42L19.84 12l-7.92-7.92L10.5 5.5 16 11z"/></svg>
            </div>
          </a>
        
      </nav>
    
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
        
<div class="md-social">
  
    
    
    
    
      
      
    
    <a href="https://github.com/Serapieum-of-alex/Serapeum" target="_blank" rel="noopener" title="github.com" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 7.1.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M173.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6m-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3m44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9M252.8 8C114.1 8 8 113.3 8 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C436.2 457.8 504 362.9 504 252 504 113.3 391.5 8 252.8 8M105.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1m-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7m32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1m-11.4-14.7c-1.6 1-1.6 3.6 0 5.9s4.3 3.3 5.6 2.3c1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2"/></svg>
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    
      
      
      <script id="__config" type="application/json">{"annotate": null, "base": "../../../..", "features": ["navigation.instant", "navigation.tracking", "navigation.indexes", "navigation.footer", "navigation.sections", "navigation.tabs", "navigation.tabs.sticky", "navigation.top", "content.code.copy", "content.code.annotate", "content.action.edit", "content.action.view", "content.tooltips", "search.highlight", "search.suggest", "toc.follow", "announce.dismiss"], "search": "../../../../assets/javascripts/workers/search.2c215733.min.js", "tags": null, "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}, "version": {"provider": "mike"}}</script>
    
    
      <script src="../../../../assets/javascripts/bundle.79ae519e.min.js"></script>
      
        <script src="../../../../assets/javascripts/panzoom.min.js"></script>
      
        <script src="../../../../assets/javascripts/zoompan.js"></script>
      
    
  </body>
</html>