{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"AUTHORS/","title":"Credits","text":""},{"location":"AUTHORS/#development-lead","title":"Development Lead","text":"<ul> <li>Mostafa Farrag moah.farag@gmail.com</li> </ul>"},{"location":"LICENSE/","title":"License","text":""},{"location":"LICENSE/#gnu-general-public-license","title":"GNU GENERAL PUBLIC LICENSE","text":"<p>Version 3, 29 June 2007</p> <p>Copyright (C) 2007 Free Software Foundation, Inc. https://fsf.org/</p> <p>Everyone is permitted to copy and distribute verbatim copies of this license document, but changing it is not allowed.</p>"},{"location":"LICENSE/#preamble","title":"Preamble","text":"<p>The GNU General Public License is a free, copyleft license for software and other kinds of works.</p> <p>The licenses for most software and other practical works are designed to take away your freedom to share and change the works. By contrast, the GNU General Public License is intended to guarantee your freedom to share and change all versions of a program--to make sure it remains free software for all its users. We, the Free Software Foundation, use the GNU General Public License for most of our software; it applies also to any other work released this way by its authors. You can apply it to your programs, too.</p> <p>When we speak of free software, we are referring to freedom, not price. Our General Public Licenses are designed to make sure that you have the freedom to distribute copies of free software (and charge for them if you wish), that you receive source code or can get it if you want it, that you can change the software or use pieces of it in new free programs, and that you know you can do these things.</p> <p>To protect your rights, we need to prevent others from denying you these rights or asking you to surrender the rights. Therefore, you have certain responsibilities if you distribute copies of the software, or if you modify it: responsibilities to respect the freedom of others.</p> <p>For example, if you distribute copies of such a program, whether gratis or for a fee, you must pass on to the recipients the same freedoms that you received. You must make sure that they, too, receive or can get the source code. And you must show them these terms so they know their rights.</p> <p>Developers that use the GNU GPL protect your rights with two steps: (1) assert copyright on the software, and (2) offer you this License giving you legal permission to copy, distribute and/or modify it.</p> <p>For the developers' and authors' protection, the GPL clearly explains that there is no warranty for this free software. For both users' and authors' sake, the GPL requires that modified versions be marked as changed, so that their problems will not be attributed erroneously to authors of previous versions.</p> <p>Some devices are designed to deny users access to install or run modified versions of the software inside them, although the manufacturer can do so. This is fundamentally incompatible with the aim of protecting users' freedom to change the software. The systematic pattern of such abuse occurs in the area of products for individuals to use, which is precisely where it is most unacceptable. Therefore, we have designed this version of the GPL to prohibit the practice for those products. If such problems arise substantially in other domains, we stand ready to extend this provision to those domains in future versions of the GPL, as needed to protect the freedom of users.</p> <p>Finally, every program is threatened constantly by software patents. States should not allow patents to restrict development and use of software on general-purpose computers, but in those that do, we wish to avoid the special danger that patents applied to a free program could make it effectively proprietary. To prevent this, the GPL assures that patents cannot be used to render the program non-free.</p> <p>The precise terms and conditions for copying, distribution and modification follow.</p>"},{"location":"LICENSE/#terms-and-conditions","title":"TERMS AND CONDITIONS","text":""},{"location":"LICENSE/#0-definitions","title":"0. Definitions.","text":"<p>\"This License\" refers to version 3 of the GNU General Public License.</p> <p>\"Copyright\" also means copyright-like laws that apply to other kinds of works, such as semiconductor masks.</p> <p>\"The Program\" refers to any copyrightable work licensed under this License. Each licensee is addressed as \"you\". \"Licensees\" and \"recipients\" may be individuals or organizations.</p> <p>To \"modify\" a work means to copy from or adapt all or part of the work in a fashion requiring copyright permission, other than the making of an exact copy. The resulting work is called a \"modified version\" of the earlier work or a work \"based on\" the earlier work.</p> <p>A \"covered work\" means either the unmodified Program or a work based on the Program.</p> <p>To \"propagate\" a work means to do anything with it that, without permission, would make you directly or secondarily liable for infringement under applicable copyright law, except executing it on a computer or modifying a private copy. Propagation includes copying, distribution (with or without modification), making available to the public, and in some countries other activities as well.</p> <p>To \"convey\" a work means any kind of propagation that enables other parties to make or receive copies. Mere interaction with a user through a computer network, with no transfer of a copy, is not conveying.</p> <p>An interactive user interface displays \"Appropriate Legal Notices\" to the extent that it includes a convenient and prominently visible feature that (1) displays an appropriate copyright notice, and (2) tells the user that there is no warranty for the work (except to the extent that warranties are provided), that licensees may convey the work under this License, and how to view a copy of this License. If the interface presents a list of user commands or options, such as a menu, a prominent item in the list meets this criterion.</p>"},{"location":"LICENSE/#1-source-code","title":"1. Source Code.","text":"<p>The \"source code\" for a work means the preferred form of the work for making modifications to it. \"Object code\" means any non-source form of a work.</p> <p>A \"Standard Interface\" means an interface that either is an official standard defined by a recognized standards body, or, in the case of interfaces specified for a particular programming language, one that is widely used among developers working in that language.</p> <p>The \"System Libraries\" of an executable work include anything, other than the work as a whole, that (a) is included in the normal form of packaging a Major Component, but which is not part of that Major Component, and (b) serves only to enable use of the work with that Major Component, or to implement a Standard Interface for which an implementation is available to the public in source code form. A \"Major Component\", in this context, means a major essential component (kernel, window system, and so on) of the specific operating system (if any) on which the executable work runs, or a compiler used to produce the work, or an object code interpreter used to run it.</p> <p>The \"Corresponding Source\" for a work in object code form means all the source code needed to generate, install, and (for an executable work) run the object code and to modify the work, including scripts to control those activities. However, it does not include the work's System Libraries, or general-purpose tools or generally available free programs which are used unmodified in performing those activities but which are not part of the work. For example, Corresponding Source includes interface definition files associated with source files for the work, and the source code for shared libraries and dynamically linked subprograms that the work is specifically designed to require, such as by intimate data communication or control flow between those subprograms and other parts of the work.</p> <p>The Corresponding Source need not include anything that users can regenerate automatically from other parts of the Corresponding Source.</p> <p>The Corresponding Source for a work in source code form is that same work.</p>"},{"location":"LICENSE/#2-basic-permissions","title":"2. Basic Permissions.","text":"<p>All rights granted under this License are granted for the term of copyright on the Program, and are irrevocable provided the stated conditions are met. This License explicitly affirms your unlimited permission to run the unmodified Program. The output from running a covered work is covered by this License only if the output, given its content, constitutes a covered work. This License acknowledges your rights of fair use or other equivalent, as provided by copyright law.</p> <p>You may make, run and propagate covered works that you do not convey, without conditions so long as your license otherwise remains in force. You may convey covered works to others for the sole purpose of having them make modifications exclusively for you, or provide you with facilities for running those works, provided that you comply with the terms of this License in conveying all material for which you do not control copyright. Those thus making or running the covered works for you must do so exclusively on your behalf, under your direction and control, on terms that prohibit them from making any copies of your copyrighted material outside their relationship with you.</p> <p>Conveying under any other circumstances is permitted solely under the conditions stated below. Sublicensing is not allowed; section 10 makes it unnecessary.</p>"},{"location":"LICENSE/#3-protecting-users-legal-rights-from-anti-circumvention-law","title":"3. Protecting Users' Legal Rights From Anti-Circumvention Law.","text":"<p>No covered work shall be deemed part of an effective technological measure under any applicable law fulfilling obligations under article 11 of the WIPO copyright treaty adopted on 20 December 1996, or similar laws prohibiting or restricting circumvention of such measures.</p> <p>When you convey a covered work, you waive any legal power to forbid circumvention of technological measures to the extent such circumvention is effected by exercising rights under this License with respect to the covered work, and you disclaim any intention to limit operation or modification of the work as a means of enforcing, against the work's users, your or third parties' legal rights to forbid circumvention of technological measures.</p>"},{"location":"LICENSE/#4-conveying-verbatim-copies","title":"4. Conveying Verbatim Copies.","text":"<p>You may convey verbatim copies of the Program's source code as you receive it, in any medium, provided that you conspicuously and appropriately publish on each copy an appropriate copyright notice; keep intact all notices stating that this License and any non-permissive terms added in accord with section 7 apply to the code; keep intact all notices of the absence of any warranty; and give all recipients a copy of this License along with the Program.</p> <p>You may charge any price or no price for each copy that you convey, and you may offer support or warranty protection for a fee.</p>"},{"location":"LICENSE/#5-conveying-modified-source-versions","title":"5. Conveying Modified Source Versions.","text":"<p>You may convey a work based on the Program, or the modifications to produce it from the Program, in the form of source code under the terms of section 4, provided that you also meet all of these conditions:</p> <ul> <li>a) The work must carry prominent notices stating that you modified     it, and giving a relevant date.</li> <li>b) The work must carry prominent notices stating that it is     released under this License and any conditions added under     section 7. This requirement modifies the requirement in section 4     to \"keep intact all notices\".</li> <li>c) You must license the entire work, as a whole, under this     License to anyone who comes into possession of a copy. This     License will therefore apply, along with any applicable section 7     additional terms, to the whole of the work, and all its parts,     regardless of how they are packaged. This License gives no     permission to license the work in any other way, but it does not     invalidate such permission if you have separately received it.</li> <li>d) If the work has interactive user interfaces, each must display     Appropriate Legal Notices; however, if the Program has interactive     interfaces that do not display Appropriate Legal Notices, your     work need not make them do so.</li> </ul> <p>A compilation of a covered work with other separate and independent works, which are not by their nature extensions of the covered work, and which are not combined with it such as to form a larger program, in or on a volume of a storage or distribution medium, is called an \"aggregate\" if the compilation and its resulting copyright are not used to limit the access or legal rights of the compilation's users beyond what the individual works permit. Inclusion of a covered work in an aggregate does not cause this License to apply to the other parts of the aggregate.</p>"},{"location":"LICENSE/#6-conveying-non-source-forms","title":"6. Conveying Non-Source Forms.","text":"<p>You may convey a covered work in object code form under the terms of sections 4 and 5, provided that you also convey the machine-readable Corresponding Source under the terms of this License, in one of these ways:</p> <ul> <li>a) Convey the object code in, or embodied in, a physical product     (including a physical distribution medium), accompanied by the     Corresponding Source fixed on a durable physical medium     customarily used for software interchange.</li> <li>b) Convey the object code in, or embodied in, a physical product     (including a physical distribution medium), accompanied by a     written offer, valid for at least three years and valid for as     long as you offer spare parts or customer support for that product     model, to give anyone who possesses the object code either (1) a     copy of the Corresponding Source for all the software in the     product that is covered by this License, on a durable physical     medium customarily used for software interchange, for a price no     more than your reasonable cost of physically performing this     conveying of source, or (2) access to copy the Corresponding     Source from a network server at no charge.</li> <li>c) Convey individual copies of the object code with a copy of the     written offer to provide the Corresponding Source. This     alternative is allowed only occasionally and noncommercially, and     only if you received the object code with such an offer, in accord     with subsection 6b.</li> <li>d) Convey the object code by offering access from a designated     place (gratis or for a charge), and offer equivalent access to the     Corresponding Source in the same way through the same place at no     further charge. You need not require recipients to copy the     Corresponding Source along with the object code. If the place to     copy the object code is a network server, the Corresponding Source     may be on a different server (operated by you or a third party)     that supports equivalent copying facilities, provided you maintain     clear directions next to the object code saying where to find the     Corresponding Source. Regardless of what server hosts the     Corresponding Source, you remain obligated to ensure that it is     available for as long as needed to satisfy these requirements.</li> <li>e) Convey the object code using peer-to-peer transmission,     provided you inform other peers where the object code and     Corresponding Source of the work are being offered to the general     public at no charge under subsection 6d.</li> </ul> <p>A separable portion of the object code, whose source code is excluded from the Corresponding Source as a System Library, need not be included in conveying the object code work.</p> <p>A \"User Product\" is either (1) a \"consumer product\", which means any tangible personal property which is normally used for personal, family, or household purposes, or (2) anything designed or sold for incorporation into a dwelling. In determining whether a product is a consumer product, doubtful cases shall be resolved in favor of coverage. For a particular product received by a particular user, \"normally used\" refers to a typical or common use of that class of product, regardless of the status of the particular user or of the way in which the particular user actually uses, or expects or is expected to use, the product. A product is a consumer product regardless of whether the product has substantial commercial, industrial or non-consumer uses, unless such uses represent the only significant mode of use of the product.</p> <p>\"Installation Information\" for a User Product means any methods, procedures, authorization keys, or other information required to install and execute modified versions of a covered work in that User Product from a modified version of its Corresponding Source. The information must suffice to ensure that the continued functioning of the modified object code is in no case prevented or interfered with solely because modification has been made.</p> <p>If you convey an object code work under this section in, or with, or specifically for use in, a User Product, and the conveying occurs as part of a transaction in which the right of possession and use of the User Product is transferred to the recipient in perpetuity or for a fixed term (regardless of how the transaction is characterized), the Corresponding Source conveyed under this section must be accompanied by the Installation Information. But this requirement does not apply if neither you nor any third party retains the ability to install modified object code on the User Product (for example, the work has been installed in ROM).</p> <p>The requirement to provide Installation Information does not include a requirement to continue to provide support service, warranty, or updates for a work that has been modified or installed by the recipient, or for the User Product in which it has been modified or installed. Access to a network may be denied when the modification itself materially and adversely affects the operation of the network or violates the rules and protocols for communication across the network.</p> <p>Corresponding Source conveyed, and Installation Information provided, in accord with this section must be in a format that is publicly documented (and with an implementation available to the public in source code form), and must require no special password or key for unpacking, reading or copying.</p>"},{"location":"LICENSE/#7-additional-terms","title":"7. Additional Terms.","text":"<p>\"Additional permissions\" are terms that supplement the terms of this License by making exceptions from one or more of its conditions. Additional permissions that are applicable to the entire Program shall be treated as though they were included in this License, to the extent that they are valid under applicable law. If additional permissions apply only to part of the Program, that part may be used separately under those permissions, but the entire Program remains governed by this License without regard to the additional permissions.</p> <p>When you convey a copy of a covered work, you may at your option remove any additional permissions from that copy, or from any part of it. (Additional permissions may be written to require their own removal in certain cases when you modify the work.) You may place additional permissions on material, added by you to a covered work, for which you have or can give appropriate copyright permission.</p> <p>Notwithstanding any other provision of this License, for material you add to a covered work, you may (if authorized by the copyright holders of that material) supplement the terms of this License with terms:</p> <ul> <li>a) Disclaiming warranty or limiting liability differently from the     terms of sections 15 and 16 of this License; or</li> <li>b) Requiring preservation of specified reasonable legal notices or     author attributions in that material or in the Appropriate Legal     Notices displayed by works containing it; or</li> <li>c) Prohibiting misrepresentation of the origin of that material,     or requiring that modified versions of such material be marked in     reasonable ways as different from the original version; or</li> <li>d) Limiting the use for publicity purposes of names of licensors     or authors of the material; or</li> <li>e) Declining to grant rights under trademark law for use of some     trade names, trademarks, or service marks; or</li> <li>f) Requiring indemnification of licensors and authors of that     material by anyone who conveys the material (or modified versions     of it) with contractual assumptions of liability to the recipient,     for any liability that these contractual assumptions directly     impose on those licensors and authors.</li> </ul> <p>All other non-permissive additional terms are considered \"further restrictions\" within the meaning of section 10. If the Program as you received it, or any part of it, contains a notice stating that it is governed by this License along with a term that is a further restriction, you may remove that term. If a license document contains a further restriction but permits relicensing or conveying under this License, you may add to a covered work material governed by the terms of that license document, provided that the further restriction does not survive such relicensing or conveying.</p> <p>If you add terms to a covered work in accord with this section, you must place, in the relevant source files, a statement of the additional terms that apply to those files, or a notice indicating where to find the applicable terms.</p> <p>Additional terms, permissive or non-permissive, may be stated in the form of a separately written license, or stated as exceptions; the above requirements apply either way.</p>"},{"location":"LICENSE/#8-termination","title":"8. Termination.","text":"<p>You may not propagate or modify a covered work except as expressly provided under this License. Any attempt otherwise to propagate or modify it is void, and will automatically terminate your rights under this License (including any patent licenses granted under the third paragraph of section 11).</p> <p>However, if you cease all violation of this License, then your license from a particular copyright holder is reinstated (a) provisionally, unless and until the copyright holder explicitly and finally terminates your license, and (b) permanently, if the copyright holder fails to notify you of the violation by some reasonable means prior to 60 days after the cessation.</p> <p>Moreover, your license from a particular copyright holder is reinstated permanently if the copyright holder notifies you of the violation by some reasonable means, this is the first time you have received notice of violation of this License (for any work) from that copyright holder, and you cure the violation prior to 30 days after your receipt of the notice.</p> <p>Termination of your rights under this section does not terminate the licenses of parties who have received copies or rights from you under this License. If your rights have been terminated and not permanently reinstated, you do not qualify to receive new licenses for the same material under section 10.</p>"},{"location":"LICENSE/#9-acceptance-not-required-for-having-copies","title":"9. Acceptance Not Required for Having Copies.","text":"<p>You are not required to accept this License in order to receive or run a copy of the Program. Ancillary propagation of a covered work occurring solely as a consequence of using peer-to-peer transmission to receive a copy likewise does not require acceptance. However, nothing other than this License grants you permission to propagate or modify any covered work. These actions infringe copyright if you do not accept this License. Therefore, by modifying or propagating a covered work, you indicate your acceptance of this License to do so.</p>"},{"location":"LICENSE/#10-automatic-licensing-of-downstream-recipients","title":"10. Automatic Licensing of Downstream Recipients.","text":"<p>Each time you convey a covered work, the recipient automatically receives a license from the original licensors, to run, modify and propagate that work, subject to this License. You are not responsible for enforcing compliance by third parties with this License.</p> <p>An \"entity transaction\" is a transaction transferring control of an organization, or substantially all assets of one, or subdividing an organization, or merging organizations. If propagation of a covered work results from an entity transaction, each party to that transaction who receives a copy of the work also receives whatever licenses to the work the party's predecessor in interest had or could give under the previous paragraph, plus a right to possession of the Corresponding Source of the work from the predecessor in interest, if the predecessor has it or can get it with reasonable efforts.</p> <p>You may not impose any further restrictions on the exercise of the rights granted or affirmed under this License. For example, you may not impose a license fee, royalty, or other charge for exercise of rights granted under this License, and you may not initiate litigation (including a cross-claim or counterclaim in a lawsuit) alleging that any patent claim is infringed by making, using, selling, offering for sale, or importing the Program or any portion of it.</p>"},{"location":"LICENSE/#11-patents","title":"11. Patents.","text":"<p>A \"contributor\" is a copyright holder who authorizes use under this License of the Program or a work on which the Program is based. The work thus licensed is called the contributor's \"contributor version\".</p> <p>A contributor's \"essential patent claims\" are all patent claims owned or controlled by the contributor, whether already acquired or hereafter acquired, that would be infringed by some manner, permitted by this License, of making, using, or selling its contributor version, but do not include claims that would be infringed only as a consequence of further modification of the contributor version. For purposes of this definition, \"control\" includes the right to grant patent sublicenses in a manner consistent with the requirements of this License.</p> <p>Each contributor grants you a non-exclusive, worldwide, royalty-free patent license under the contributor's essential patent claims, to make, use, sell, offer for sale, import and otherwise run, modify and propagate the contents of its contributor version.</p> <p>In the following three paragraphs, a \"patent license\" is any express agreement or commitment, however denominated, not to enforce a patent (such as an express permission to practice a patent or covenant not to sue for patent infringement). To \"grant\" such a patent license to a party means to make such an agreement or commitment not to enforce a patent against the party.</p> <p>If you convey a covered work, knowingly relying on a patent license, and the Corresponding Source of the work is not available for anyone to copy, free of charge and under the terms of this License, through a publicly available network server or other readily accessible means, then you must either (1) cause the Corresponding Source to be so available, or (2) arrange to deprive yourself of the benefit of the patent license for this particular work, or (3) arrange, in a manner consistent with the requirements of this License, to extend the patent license to downstream recipients. \"Knowingly relying\" means you have actual knowledge that, but for the patent license, your conveying the covered work in a country, or your recipient's use of the covered work in a country, would infringe one or more identifiable patents in that country that you have reason to believe are valid.</p> <p>If, pursuant to or in connection with a single transaction or arrangement, you convey, or propagate by procuring conveyance of, a covered work, and grant a patent license to some of the parties receiving the covered work authorizing them to use, propagate, modify or convey a specific copy of the covered work, then the patent license you grant is automatically extended to all recipients of the covered work and works based on it.</p> <p>A patent license is \"discriminatory\" if it does not include within the scope of its coverage, prohibits the exercise of, or is conditioned on the non-exercise of one or more of the rights that are specifically granted under this License. You may not convey a covered work if you are a party to an arrangement with a third party that is in the business of distributing software, under which you make payment to the third party based on the extent of your activity of conveying the work, and under which the third party grants, to any of the parties who would receive the covered work from you, a discriminatory patent license (a) in connection with copies of the covered work conveyed by you (or copies made from those copies), or (b) primarily for and in connection with specific products or compilations that contain the covered work, unless you entered into that arrangement, or that patent license was granted, prior to 28 March 2007.</p> <p>Nothing in this License shall be construed as excluding or limiting any implied license or other defenses to infringement that may otherwise be available to you under applicable patent law.</p>"},{"location":"LICENSE/#12-no-surrender-of-others-freedom","title":"12. No Surrender of Others' Freedom.","text":"<p>If conditions are imposed on you (whether by court order, agreement or otherwise) that contradict the conditions of this License, they do not excuse you from the conditions of this License. If you cannot convey a covered work so as to satisfy simultaneously your obligations under this License and any other pertinent obligations, then as a consequence you may not convey it at all. For example, if you agree to terms that obligate you to collect a royalty for further conveying from those to whom you convey the Program, the only way you could satisfy both those terms and this License would be to refrain entirely from conveying the Program.</p>"},{"location":"LICENSE/#13-use-with-the-gnu-affero-general-public-license","title":"13. Use with the GNU Affero General Public License.","text":"<p>Notwithstanding any other provision of this License, you have permission to link or combine any covered work with a work licensed under version 3 of the GNU Affero General Public License into a single combined work, and to convey the resulting work. The terms of this License will continue to apply to the part which is the covered work, but the special requirements of the GNU Affero General Public License, section 13, concerning interaction through a network will apply to the combination as such.</p>"},{"location":"LICENSE/#14-revised-versions-of-this-license","title":"14. Revised Versions of this License.","text":"<p>The Free Software Foundation may publish revised and/or new versions of the GNU General Public License from time to time. Such new versions will be similar in spirit to the present version, but may differ in detail to address new problems or concerns.</p> <p>Each version is given a distinguishing version number. If the Program specifies that a certain numbered version of the GNU General Public License \"or any later version\" applies to it, you have the option of following the terms and conditions either of that numbered version or of any later version published by the Free Software Foundation. If the Program does not specify a version number of the GNU General Public License, you may choose any version ever published by the Free Software Foundation.</p> <p>If the Program specifies that a proxy can decide which future versions of the GNU General Public License can be used, that proxy's public statement of acceptance of a version permanently authorizes you to choose that version for the Program.</p> <p>Later license versions may give you additional or different permissions. However, no additional obligations are imposed on any author or copyright holder as a result of your choosing to follow a later version.</p>"},{"location":"LICENSE/#15-disclaimer-of-warranty","title":"15. Disclaimer of Warranty.","text":"<p>THERE IS NO WARRANTY FOR THE PROGRAM, TO THE EXTENT PERMITTED BY APPLICABLE LAW. EXCEPT WHEN OTHERWISE STATED IN WRITING THE COPYRIGHT HOLDERS AND/OR OTHER PARTIES PROVIDE THE PROGRAM \"AS IS\" WITHOUT WARRANTY OF ANY KIND, EITHER EXPRESSED OR IMPLIED, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE. THE ENTIRE RISK AS TO THE QUALITY AND PERFORMANCE OF THE PROGRAM IS WITH YOU. SHOULD THE PROGRAM PROVE DEFECTIVE, YOU ASSUME THE COST OF ALL NECESSARY SERVICING, REPAIR OR CORRECTION.</p>"},{"location":"LICENSE/#16-limitation-of-liability","title":"16. Limitation of Liability.","text":"<p>IN NO EVENT UNLESS REQUIRED BY APPLICABLE LAW OR AGREED TO IN WRITING WILL ANY COPYRIGHT HOLDER, OR ANY OTHER PARTY WHO MODIFIES AND/OR CONVEYS THE PROGRAM AS PERMITTED ABOVE, BE LIABLE TO YOU FOR DAMAGES, INCLUDING ANY GENERAL, SPECIAL, INCIDENTAL OR CONSEQUENTIAL DAMAGES ARISING OUT OF THE USE OR INABILITY TO USE THE PROGRAM (INCLUDING BUT NOT LIMITED TO LOSS OF DATA OR DATA BEING RENDERED INACCURATE OR LOSSES SUSTAINED BY YOU OR THIRD PARTIES OR A FAILURE OF THE PROGRAM TO OPERATE WITH ANY OTHER PROGRAMS), EVEN IF SUCH HOLDER OR OTHER PARTY HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES.</p>"},{"location":"LICENSE/#17-interpretation-of-sections-15-and-16","title":"17. Interpretation of Sections 15 and 16.","text":"<p>If the disclaimer of warranty and limitation of liability provided above cannot be given local legal effect according to their terms, reviewing courts shall apply local law that most closely approximates an absolute waiver of all civil liability in connection with the Program, unless a warranty or assumption of liability accompanies a copy of the Program in return for a fee.</p> <p>END OF TERMS AND CONDITIONS</p>"},{"location":"LICENSE/#how-to-apply-these-terms-to-your-new-programs","title":"How to Apply These Terms to Your New Programs","text":"<p>If you develop a new program, and you want it to be of the greatest possible use to the public, the best way to achieve this is to make it free software which everyone can redistribute and change under these terms.</p> <p>To do so, attach the following notices to the program. It is safest to attach them to the start of each source file to most effectively state the exclusion of warranty; and each file should have at least the \"copyright\" line and a pointer to where the full notice is found.</p> <pre><code>    &lt;one line to give the program's name and a brief idea of what it does.&gt;\n    Copyright (C) &lt;year&gt;  &lt;name of author&gt;\n\n    This program is free software: you can redistribute it and/or modify\n    it under the terms of the GNU General Public License as published by\n    the Free Software Foundation, either version 3 of the License, or\n    (at your option) any later version.\n\n    This program is distributed in the hope that it will be useful,\n    but WITHOUT ANY WARRANTY; without even the implied warranty of\n    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n    GNU General Public License for more details.\n\n    You should have received a copy of the GNU General Public License\n    along with this program.  If not, see &lt;https://www.gnu.org/licenses/&gt;.\n</code></pre> <p>Also add information on how to contact you by electronic and paper mail.</p> <p>If the program does terminal interaction, make it output a short notice like this when it starts in an interactive mode:</p> <pre><code>    &lt;program&gt;  Copyright (C) &lt;year&gt;  &lt;name of author&gt;\n    This program comes with ABSOLUTELY NO WARRANTY; for details type `show w'.\n    This is free software, and you are welcome to redistribute it\n    under certain conditions; type `show c' for details.\n</code></pre> <p>The hypothetical commands `show w' and `show c' should show the appropriate parts of the General Public License. Of course, your program's commands might be different; for a GUI interface, you would use an \"about box\".</p> <p>You should also get your employer (if you work as a programmer) or school, if any, to sign a \"copyright disclaimer\" for the program, if necessary. For more information on this, and how to apply and follow the GNU GPL, see https://www.gnu.org/licenses/.</p> <p>The GNU General Public License does not permit incorporating your program into proprietary programs. If your program is a subroutine library, you may consider it more useful to permit linking proprietary applications with the library. If this is what you want to do, use the GNU Lesser General Public License instead of this License. But first, please read https://www.gnu.org/licenses/why-not-lgpl.html.</p>"},{"location":"change-log/","title":"Changelog","text":""},{"location":"developer-guide/SECURITY/","title":"Security Policy","text":""},{"location":"developer-guide/SECURITY/#supported-versions","title":"Supported Versions","text":"<p>Use this section to tell people about which versions of your project are currently being supported with security updates.</p> Version Supported 5.1.x 5.0.x 4.0.x &lt; 4.0"},{"location":"developer-guide/SECURITY/#reporting-a-vulnerability","title":"Reporting a Vulnerability","text":"<p>Use this section to tell people how to report a vulnerability.</p> <p>Tell them where to go, how often they can expect to get an update on a reported vulnerability, what to expect if the vulnerability is accepted or declined, etc.</p>"},{"location":"developer-guide/contributing/","title":"Contributing","text":"<p>When contributing to this repository, please first discuss the change you wish to make via issue, email, or any other method with the owners of this repository before making a change.</p> <p>Please note we have a code of conduct, please follow it in all your interactions with the project.</p>"},{"location":"developer-guide/contributing/#pull-request-process","title":"Pull Request Process","text":"<ol> <li>Ensure any install or build dependencies are removed before the end of the layer when doing a    build.</li> <li>Update the README.md with details of changes to the interface, this includes new environment    variables, exposed ports, useful file locations and container parameters.</li> <li>Increase the version numbers in any examples files and the README.md to the new version that this    Pull Request would represent. The versioning scheme we use is SemVer.</li> <li>You may merge the Pull Request in once you have the sign-off of two other developers, or if you    do not have permission to do that, you may request the second reviewer to merge it for you.</li> </ol>"},{"location":"developer-guide/contributing/#code-of-conduct","title":"Code of Conduct","text":""},{"location":"developer-guide/contributing/#our-pledge","title":"Our Pledge","text":"<p>In the interest of fostering an open and welcoming environment, we as contributors and maintainers pledge to making participation in our project and our community a harassment-free experience for everyone, regardless of age, body size, disability, ethnicity, gender identity and expression, level of experience, nationality, personal appearance, race, religion, or sexual identity and orientation.</p>"},{"location":"developer-guide/contributing/#our-standards","title":"Our Standards","text":"<p>Examples of behavior that contributes to creating a positive environment include:</p> <ul> <li>Using welcoming and inclusive language</li> <li>Being respectful of differing viewpoints and experiences</li> <li>Gracefully accepting constructive criticism</li> <li>Focusing on what is best for the community</li> <li>Showing empathy towards other community members</li> </ul> <p>Examples of unacceptable behavior by participants include:</p> <ul> <li>The use of sexualized language or imagery and unwelcome sexual attention or advances</li> <li>Trolling, insulting/derogatory comments, and personal or political attacks</li> <li>Public or private harassment</li> <li>Publishing others' private information, such as a physical or electronic   address, without explicit permission</li> <li>Other conduct which could reasonably be considered inappropriate in a   professional setting</li> </ul>"},{"location":"developer-guide/contributing/#our-responsibilities","title":"Our Responsibilities","text":"<p>Project maintainers are responsible for clarifying the standards of acceptable behavior and are expected to take appropriate and fair corrective action in response to any instances of unacceptable behavior.</p> <p>Project maintainers have the right and responsibility to remove, edit, or reject comments, commits, code, wiki edits, issues, and other contributions that are not aligned to this Code of Conduct, or to ban temporarily or permanently any contributor for other behaviors that they deem inappropriate, threatening, offensive, or harmful.</p>"},{"location":"developer-guide/contributing/#scope","title":"Scope","text":"<p>This Code of Conduct applies both within project spaces and in public spaces when an individual is representing the project or its community. Examples of representing a project or community include using an official project e-mail address, posting via an official social media account, or acting as an appointed representative at an online or offline event. Representation of a project may be further defined and clarified by project maintainers.</p>"},{"location":"developer-guide/contributing/#enforcement","title":"Enforcement","text":"<p>Instances of abusive, harassing, or otherwise unacceptable behavior may be reported by contacting the project team at [INSERT EMAIL ADDRESS]. All complaints will be reviewed and investigated and will result in a response that is deemed necessary and appropriate to the circumstances. The project team is obligated to maintain confidentiality with regard to the reporter of an incident. Further details of specific enforcement policies may be posted separately.</p> <p>Project maintainers who do not follow or enforce the Code of Conduct in good faith may face temporary or permanent repercussions as determined by other members of the project's leadership.</p>"},{"location":"developer-guide/contributing/#attribution","title":"Attribution","text":"<p>This Code of Conduct is adapted from the Contributor Covenant, version 1.4, available at http://contributor-covenant.org/version/1/4</p>"},{"location":"developer-guide/installation/","title":"Installation and Usage Guide","text":"<p>This page explains how to install Serapeum and its dependencies, using the recommended uv package manager, or plain pip. It also provides troubleshooting tips and common commands for development.</p>"},{"location":"developer-guide/installation/#project-information","title":"Project Information","text":"<ul> <li>Package name: serapeum</li> <li>Current version: 0.1.0</li> <li>Supported Python versions: 3.11\u20133.12 (requires Python &gt;=3.11,&lt;4.0)</li> </ul>"},{"location":"developer-guide/installation/#dependencies","title":"Dependencies","text":"<ul> <li>Core runtime: ollama &gt;= 0.5.4, numpy, filetype &gt;= 1.2.0, requests &gt;= 2.32.5</li> <li>Development group: pytest, pytest-cov, pre-commit, pre-commit-hooks, pytest-asyncio, nest-asyncio, nbval</li> <li>Docs group: mkdocs, mkdocs-material, mkdocstrings, mkdocstrings-python, mike, mkdocs-jupyter, mkdocs-autorefs, mkdocs-macros-plugin, mkdocs-table-reader-plugin, mkdocs-mermaid2-plugin, jupyter, notebook&lt;7, commitizen, mkdocs-panzoom-plugin</li> </ul>"},{"location":"developer-guide/installation/#installing-uv","title":"Installing uv","text":"<pre><code># On macOS and Linux\ncurl -LsSf https://astral.sh/uv/install.sh | sh\n\n# On Windows\npowershell -c \"irm https://astral.sh/uv/install.ps1 | iex\"\n\n# Using pip (alternative)\npip install uv\n</code></pre>"},{"location":"developer-guide/installation/#setting-up-the-project-recommended-uv","title":"Setting Up the Project (Recommended: uv)","text":"<ol> <li> <p>Create a virtual environment <pre><code>uv venv\n</code></pre>    This creates a <code>.venv</code> directory in your project root.</p> </li> <li> <p>Activate the virtual environment <pre><code># On macOS/Linux\nsource .venv/bin/activate\n# On Windows\n.venv\\Scripts\\activate\n</code></pre></p> </li> <li> <p>Install the package (editable/local) <pre><code>uv pip install -e .\n</code></pre></p> </li> <li> <p>Optionally add development or docs tools <pre><code>uv pip install --group dev\nuv pip install --group docs\n</code></pre></p> </li> <li> <p>Run tests <pre><code>uv run pytest -q\n</code></pre></p> </li> <li> <p>Sync dependencies from <code>pyproject.toml</code> <pre><code>uv sync --active\n</code></pre></p> </li> </ol>"},{"location":"developer-guide/installation/#install-from-github","title":"Install from GitHub","text":"<pre><code>uv pip install \"git+https://github.com/Serapieum-of-alex/serapeum.git\"\n# or a specific tag (example: v0.1.0)\nuv pip install \"git+https://github.com/Serapieum-of-alex/serapeum.git@v0.1.0\"\n</code></pre>"},{"location":"developer-guide/installation/#alternative-using-pip","title":"Alternative: Using pip","text":"<ol> <li> <p>Create and activate a venv <pre><code>python -m venv .venv\n# macOS/Linux\nsource .venv/bin/activate\n# Windows\n.venv\\Scripts\\activate\n</code></pre></p> </li> <li> <p>Install the package (editable/local) <pre><code>pip install -e .\n</code></pre></p> </li> <li> <p>(Optional) Install dev/docs tools</p> </li> <li>Note: dev/docs are defined as groups, not pip extras. With pip, install the needed packages manually according to <code>pyproject.toml</code>.</li> <li> <p>Example (partial):      <pre><code>pip install pytest pytest-cov pre-commit pytest-asyncio nbval\n</code></pre></p> </li> <li> <p>Install from GitHub with pip <pre><code>pip install \"git+https://github.com/Serapieum-of-alex/serapeum.git\"\n# or a specific tag\npip install \"git+https://github.com/Serapieum-of-alex/serapeum.git@v0.1.0\"\n</code></pre></p> </li> </ol>"},{"location":"developer-guide/installation/#common-uv-commands","title":"Common uv Commands","text":"<ul> <li>Install a new package: <pre><code>uv pip install &lt;package-name&gt;\nuv pip install &lt;package-name&gt; --group dev  # as dev dependency\n</code></pre></li> <li>Update a package: <pre><code>uv pip install --upgrade &lt;package-name&gt;\n</code></pre></li> <li>Run Python scripts: <pre><code>uv run python script.py\nuv run pytest\n</code></pre></li> </ul>"},{"location":"developer-guide/installation/#troubleshooting","title":"Troubleshooting","text":"<ul> <li>Virtual environment not activating:   Make sure you've created the virtual environment first:   <pre><code>uv venv\n</code></pre></li> <li>Package installation fails:   Try clearing the cache:   <pre><code>uv cache clean\n</code></pre></li> <li>ImportError after installation:   Ensure you've installed the package in editable mode:   <pre><code>uv pip install -e .\n</code></pre></li> </ul>"},{"location":"developer-guide/installation/#quick-check","title":"Quick Check","text":"<p>After installation, open Python and run: <pre><code>import serapeum\nprint(serapeum.__version__)\n</code></pre></p>"},{"location":"developer-guide/installation/#additional-resources","title":"Additional Resources","text":"<ul> <li>uv Documentation</li> <li>PEP 621 - Storing project metadata in pyproject.toml</li> <li>Project homepage: https://github.com/Serapieum-of-alex/serapeum</li> <li>Documentation: https://serapeum.readthedocs.io/</li> </ul>"},{"location":"overview/codebase-map/","title":"Codebase Map","text":"<p>This page summarizes the main modules, key classes, and the public API surface of the <code>serapeum</code> package.</p>"},{"location":"overview/codebase-map/#packages-and-modules","title":"Packages and Modules","text":"<ul> <li>serapeum.core.base.llms.base</li> <li>Base interface for all LLM backends (<code>BaseLLM</code>): sync/async chat and completion, streaming endpoints, and message conversion helpers.</li> <li>serapeum.core.base.llms.models</li> <li>Core data models: <code>Message</code>, <code>MessageList</code>, <code>ChatResponse</code>, <code>CompletionResponse</code>, <code>Metadata</code>, <code>MessageRole</code>, and multimodal chunks (<code>TextChunk</code>, <code>Image</code>, <code>Audio</code>).</li> <li>serapeum.core.base.llms.utils</li> <li>Adapters/decorators to adapt chat endpoints to completion-style calls (<code>chat_to_completion_decorator</code>, <code>achat_to_completion_decorator</code>).</li> <li>serapeum.core.llm.base</li> <li>High-level LLM orchestration (<code>LLM</code>) built on <code>BaseLLM</code>: prompt/message formatting, structured prediction to Pydantic models, and sync/async streaming utilities.</li> <li>serapeum.core.llm.function_calling</li> <li>Tool-calling specialization (<code>FunctionCallingLLM</code>): chat with tools, tool call extraction/validation, predict-and-call helpers (sync/async, streaming).</li> <li>serapeum.core.llm.structured_llm</li> <li>Wrapper LLM (<code>StructuredLLM</code>) that forces structured outputs (<code>BaseModel</code>) from another <code>LLM</code> while keeping chat/completion interfaces.</li> <li>serapeum.core.chat.models</li> <li><code>AgentChatResponse</code>: aggregates model/tool outputs and provides sync/async streaming generators and tool output parsing.</li> <li>serapeum.core.prompts.base</li> <li>Prompt abstractions: <code>PromptTemplate</code> (string prompts) and <code>ChatPromptTemplate</code> (message-based prompts) with variable/function mappings.</li> <li>serapeum.core.prompts.models</li> <li>Prompt-related data models and types used by templates and LLMs.</li> <li>serapeum.core.prompts.utils</li> <li>Utilities for working with prompts and templates.</li> <li>serapeum.core.tools.models</li> <li>Tool system: <code>ToolMetadata</code>, JSON schema utilities, <code>BaseTool</code>, <code>AsyncBaseTool</code>, <code>ToolOutput</code>, <code>ToolCallArguments</code>, and adapters (e.g., <code>adapt_to_async_tool</code>).</li> <li>serapeum.core.tools.callable_tool</li> <li><code>CallableTool</code>: create tools from Python callables or Pydantic models; handles sync/async bridging and output parsing.</li> <li>serapeum.core.tools.utils</li> <li>General-purpose helpers for the tool subsystem.</li> <li>serapeum.core.structured_tools.tools_llm</li> <li><code>ToolOrchestratingLLM</code>: composes prompts, an LLM, and a toolset to drive structured tool-calling conversations (sync/async, streaming).</li> <li>serapeum.core.structured_tools.text_completion_llm</li> <li>Utilities/classes for orchestrating text-completion style LLMs in the structured tools pipeline.</li> <li>serapeum.core.structured_tools.utils</li> <li>Support utilities for the structured tools orchestration layer.</li> <li>serapeum.core.output_parsers.models</li> <li>Parsers and base models for converting raw LLM output into typed structures.</li> <li>serapeum.core.output_parsers.utils</li> <li>Helper functions for robust output parsing and error handling.</li> <li>serapeum.core.configs.configs / serapeum.core.configs.defaults</li> <li>Configuration objects and defaults used across the package.</li> <li>serapeum.core.utils.base / serapeum.core.utils.async_utils</li> <li>Common utilities (sync/async helpers, base helpers) shared across modules.</li> <li>serapeum.core.models.base</li> <li><code>SerializableModel</code> (JSON/pickle serialization helpers) and <code>StructuredLLMMode</code> enum.</li> <li>serapeum.llms.ollama.base</li> <li><code>Ollama</code>: concrete <code>FunctionCallingLLM</code> implementation for the Ollama server; supports chat/completion, tool calling, and structured prediction (sync/async/streaming).</li> <li>serapeum.datasource</li> <li>Convenience helpers for integrating external data sources (if used by your application code).</li> </ul>"},{"location":"overview/codebase-map/#key-public-classes","title":"Key Public Classes","text":"<ul> <li>serapeum.core.base.llms.base.BaseLLM</li> <li>serapeum.core.llm.base.LLM</li> <li>serapeum.core.llm.function_calling.FunctionCallingLLM</li> <li>serapeum.core.llm.structured_llm.StructuredLLM</li> <li>serapeum.core.structured_tools.tools_llm.ToolOrchestratingLLM</li> <li>serapeum.core.tools.callable_tool.CallableTool</li> <li>serapeum.core.tools.models.BaseTool / AsyncBaseTool</li> <li>serapeum.core.base.llms.models.Message / MessageList / ChatResponse / CompletionResponse / Metadata</li> <li>serapeum.core.prompts.base.PromptTemplate / ChatPromptTemplate</li> <li>serapeum.core.models.base.SerializableModel</li> <li>serapeum.llms.ollama.base.Ollama</li> </ul>"},{"location":"overview/codebase-map/#representative-public-methods","title":"Representative Public Methods","text":"<ul> <li>BaseLLM</li> <li>chat(messages, **kwargs) \u2192 ChatResponse</li> <li>complete(prompt, formatted=False, **kwargs) \u2192 CompletionResponse</li> <li>stream_chat(...), stream_complete(...)</li> <li>achat(...), acomplete(...), astream_chat(...), astream_complete(...)</li> <li>LLM</li> <li>predict(prompt: BasePromptTemplate, ...) \u2192 CompletionResponse</li> <li>stream(...), apredict(...), astream(...)</li> <li>structured_predict(output_cls, prompt, llm_kwargs=None, **prompt_args)</li> <li>astructured_predict(...), stream_structured_predict(...), astream_structured_predict(...)</li> <li>FunctionCallingLLM</li> <li>chat_with_tools(tools, user_msg=None, chat_history=None, ...)</li> <li>predict_and_call(tools, user_msg=None, chat_history=None, ...)</li> <li>get_tool_calls_from_response(response, ...)</li> <li>stream_chat_with_tools(...), astream_chat_with_tools(...)</li> <li>CallableTool</li> <li>from_function(func, ...), from_model(output_cls)</li> <li>call(...), acall(...)</li> <li>ToolOrchestratingLLM</li> <li>call(..., llm_kwargs=None, ...), acall(...)</li> <li>stream_call(...), astream_call(...)</li> <li>Ollama</li> <li>chat/achat/stream_chat/astream_chat, complete/acomplete/stream_complete/astream_complete</li> <li>structured_predict/astr.../stream_str.../astream_str...</li> </ul>"},{"location":"overview/codebase-map/#data-flow-high-level","title":"Data Flow (High Level)","text":"<ul> <li>User input/messages -&gt; Prompt building (<code>PromptTemplate</code> / <code>ChatPromptTemplate</code>) -&gt; LLM (<code>LLM</code> or concrete impl like <code>Ollama</code>)</li> <li>Optional structured path: <code>LLM.structured_predict(...)</code> -&gt; Pydantic <code>BaseModel</code> outputs</li> <li>Tool-calling path: <code>FunctionCallingLLM</code> predicts tool calls -&gt; <code>BaseTool</code>/<code>CallableTool</code> executed -&gt; <code>ToolOutput</code> aggregated -&gt; <code>AgentChatResponse</code></li> <li>Outputs -&gt; <code>ChatResponse</code> / <code>CompletionResponse</code> (optionally converted/parsed to domain models)</li> </ul> <p>See the Architecture section for diagrams and deeper internals, and the API Reference for exhaustive signatures.</p>"},{"location":"overview/codebase-map/#class-dependency-graph","title":"Class &amp; Dependency Graph","text":"<p>Below is a high-level Mermaid class/dependency diagram showing the main modules and their primary classes, plus key relationships between them.</p>  Hold \"Ctrl\" to enable pan &amp; zoom  <pre><code>classDiagram\n  class SerializableModel { &lt;&lt;base&gt;&gt; }\n  class BaseLLM { &lt;&lt;abstract&gt;&gt; }\n  class LLM { }\n  class FunctionCallingLLM { }\n  class StructuredLLM { }\n  class ToolOrchestratingLLM { }\n  class BaseTool { }\n  class AsyncBaseTool { }\n  class CallableTool { }\n  class PromptTemplate { }\n  class ChatPromptTemplate { }\n  class Message { }\n  class MessageList { }\n  class ChatResponse { }\n  class CompletionResponse { }\n  class Ollama { }\n\n  SerializableModel &lt;|-- BaseLLM\n  BaseLLM &lt;|-- LLM\n  LLM &lt;|-- FunctionCallingLLM\n  LLM &lt;|-- StructuredLLM\n  FunctionCallingLLM &lt;|-- Ollama\n  ToolOrchestratingLLM ..&gt; FunctionCallingLLM : uses\n  ToolOrchestratingLLM ..&gt; PromptTemplate : composes\n  ToolOrchestratingLLM ..&gt; ChatPromptTemplate : composes\n  AsyncBaseTool &lt;|-- CallableTool\n  BaseTool &lt;|-- AsyncBaseTool\n  ToolOrchestratingLLM ..&gt; BaseTool : orchestrates\n  LLM ..&gt; MessageList : formats\n  LLM ..&gt; ChatResponse : returns\n  LLM ..&gt; CompletionResponse : returns</code></pre> <p>Notes: - The diagram abstracts many modules for clarity; see source files for full method signatures and additional classes. - <code>Ollama</code> is one concrete backend; additional backends would subclass <code>FunctionCallingLLM</code> or <code>LLM</code>.</p>"},{"location":"reference/core/structured_tools/text_completion_llm/examples/","title":"TextCompletionLLM Usage Examples","text":"<p>This guide provides comprehensive examples covering all possible ways to use <code>TextCompletionLLM</code>.</p>"},{"location":"reference/core/structured_tools/text_completion_llm/examples/#table-of-contents","title":"Table of Contents","text":"<ol> <li>Basic Usage</li> <li>Initialization Patterns</li> <li>Prompt Formats</li> <li>Execution Modes</li> <li>Advanced Usage</li> <li>Error Handling</li> </ol>"},{"location":"reference/core/structured_tools/text_completion_llm/examples/#basic-usage","title":"Basic Usage","text":""},{"location":"reference/core/structured_tools/text_completion_llm/examples/#simple-string-prompt-with-variables","title":"Simple String Prompt with Variables","text":"<p>The most straightforward way to use <code>TextCompletionLLM</code>:</p> <pre><code>from pydantic import BaseModel\nfrom serapeum.core.output_parsers import PydanticParser\nfrom serapeum.core.structured_tools.text_completion_llm import TextCompletionLLM\nfrom serapeum.llms.ollama import Ollama\n\n\n# Define your output schema\nclass Greeting(BaseModel):\n    message: str\n    language: str\n\n# Initialize the LLM\nllm = Ollama(model=\"llama3.1\", request_timeout=180)\n\n# Create output parser\noutput_parser = PydanticParser(output_cls=Greeting)\n\n# Create TextCompletionLLM with string prompt\ntext_llm = TextCompletionLLM(\n    output_parser=output_parser,\n    prompt=\"Generate a greeting in {language} for {name}. Return as JSON.\",\n    llm=llm,\n)\n\n# Execute with variables\nresult = text_llm(language=\"dutch\", name=\"Ahmed\")\nprint(result.message)  # \"Hallo Ahmed\"\nprint(result.language)  # \"Dutch\"\n</code></pre>"},{"location":"reference/core/structured_tools/text_completion_llm/examples/#initialization-patterns","title":"Initialization Patterns","text":""},{"location":"reference/core/structured_tools/text_completion_llm/examples/#1-with-explicit-output-parser","title":"1. With Explicit Output Parser","text":"<p>Provide a fully configured <code>PydanticParser</code>:</p> <pre><code>from pydantic import BaseModel, Field\nfrom serapeum.core.output_parsers import PydanticParser\nfrom serapeum.core.structured_tools.text_completion_llm import TextCompletionLLM\nfrom serapeum.llms.ollama import Ollama\n\n\nclass Product(BaseModel):\n    name: str = Field(description=\"Product name\")\n    price: float = Field(description=\"Product price in USD\")\n    in_stock: bool = Field(description=\"Availability status\")\n\nllm = Ollama(model=\"llama3.1\", request_timeout=180)\noutput_parser = PydanticParser(output_cls=Product)\n\ntext_llm = TextCompletionLLM(\n    output_parser=output_parser,\n    prompt=\"Extract product information from: {text}\",\n    llm=llm,\n)\n\nresult = text_llm(text=\"iPhone 15 costs $999 and is available\")\n# Returns: Product(name=\"iPhone 15\", price=999.0, in_stock=True)\n</code></pre>"},{"location":"reference/core/structured_tools/text_completion_llm/examples/#2-with-output-class-only-auto-creates-parser","title":"2. With Output Class Only (Auto-creates Parser)","text":"<p>Let <code>TextCompletionLLM</code> create the parser for you:</p> <pre><code>from pydantic import BaseModel\nfrom serapeum.core.structured_tools.text_completion_llm import TextCompletionLLM\nfrom serapeum.llms.ollama import Ollama\n\nclass Person(BaseModel):\n    name: str\n    age: int\n\nllm = Ollama(model=\"llama3.1\", request_timeout=180)\n\n# Parser is created automatically from output_cls\ntext_llm = TextCompletionLLM(\n    output_parser=None,  # Will be auto-created\n    prompt=\"Extract person info from: {bio}\",\n    output_cls=Person,  # Parser created from this\n    llm=llm,\n)\n\nresult = text_llm(bio=\"John Smith is 30 years old\")\n# Returns: Person(name=\"John Smith\", age=30)\n</code></pre>"},{"location":"reference/core/structured_tools/text_completion_llm/examples/#3-using-global-llm-from-configs","title":"3. Using Global LLM from Configs","text":"<p>Set a default LLM for the entire application:</p> <pre><code>from pydantic import BaseModel\nfrom serapeum.core.configs.configs import Configs\nfrom serapeum.core.structured_tools.text_completion_llm import TextCompletionLLM\nfrom serapeum.llms.ollama import Ollama\n\n# Set global LLM\nConfigs.llm = Ollama(model=\"llama3.1\", request_timeout=180)\n\nclass Task(BaseModel):\n    id: str\n    priority: int\n\n# No need to pass llm parameter\ntext_llm = TextCompletionLLM(\n    output_cls=Task,\n    prompt=\"Create a task from: {description}\",\n    # llm=None uses Configs.llm by default\n)\n\nresult = text_llm(description=\"Fix critical bug in authentication\")\n# Returns: Task(id=\"Fix critical bug in authentication\", priority=1)\n</code></pre>"},{"location":"reference/core/structured_tools/text_completion_llm/examples/#prompt-formats","title":"Prompt Formats","text":""},{"location":"reference/core/structured_tools/text_completion_llm/examples/#1-string-prompt-auto-converted-to-prompttemplate","title":"1. String Prompt (Auto-converted to PromptTemplate)","text":"<p>Simple string prompts are automatically wrapped in <code>PromptTemplate</code>:</p> <pre><code>from pydantic import BaseModel\nfrom serapeum.core.structured_tools.text_completion_llm import TextCompletionLLM\nfrom serapeum.llms.ollama import Ollama\n\nclass Summary(BaseModel):\n    summary: str\n    word_count: int\n\nllm = Ollama(model=\"llama3.1\", request_timeout=180)\n\ntext_llm = TextCompletionLLM(\n    output_cls=Summary,\n    prompt=\"Generate an essay with a maximum number of words {max_words} about the topic: {text}\",  # String prompt\n    llm=llm,\n)\n\nresult = text_llm(\n    text=\"AI\",\n    max_words=50\n)\n</code></pre>"},{"location":"reference/core/structured_tools/text_completion_llm/examples/#2-prompttemplate-object","title":"2. PromptTemplate Object","text":"<p>Use <code>PromptTemplate</code> for more control:</p> <pre><code>from pydantic import BaseModel\nfrom serapeum.core.prompts.base import PromptTemplate\nfrom serapeum.core.structured_tools.text_completion_llm import TextCompletionLLM\nfrom serapeum.llms.ollama import Ollama\n\nclass Sentiment(BaseModel):\n    sentiment: str\n    confidence: float\n\nllm = Ollama(model=\"llama3.1\", request_timeout=180)\n\n# Create explicit PromptTemplate\nprompt_template = PromptTemplate(\n    \"Analyze sentiment of: {review}\"\n)\n\ntext_llm = TextCompletionLLM(\n    output_cls=Sentiment,\n    prompt=prompt_template,\n    llm=llm,\n)\n\nresult = text_llm(review=\"This product is amazing!\")\n# Returns: Sentiment(sentiment=\"positive\", confidence=1.0)\n</code></pre>"},{"location":"reference/core/structured_tools/text_completion_llm/examples/#3-chatprompttemplate-with-messages","title":"3. ChatPromptTemplate with Messages","text":"<p>Use structured message templates for chat models:</p> <pre><code>from pydantic import BaseModel\nfrom serapeum.core.base.llms.models import Message, MessageRole\nfrom serapeum.core.prompts import ChatPromptTemplate\nfrom serapeum.core.structured_tools.text_completion_llm import TextCompletionLLM\nfrom serapeum.llms.ollama import Ollama\n\nclass Translation(BaseModel):\n    translated_text: str\n    source_language: str\n    target_language: str\n\nllm = Ollama(model=\"llama3.1\", request_timeout=180)\n\n# Create message templates\nmessages = [\n    Message(\n        role=MessageRole.SYSTEM,\n        content=\"You are a professional translator.\"\n    ),\n    Message(\n        role=MessageRole.USER,\n        content=\"Translate to {target_lang}: {text}\"\n    ),\n]\n\nprompt = ChatPromptTemplate(message_templates=messages)\n\ntext_llm = TextCompletionLLM(\n    output_cls=Translation,\n    prompt=prompt,\n    llm=llm,\n)\n\nresult = text_llm(target_lang=\"French\", text=\"Hello, world!\")\n# Returns: Translation(translated_text=\"Bonjour, monde!\", ...)\n</code></pre>"},{"location":"reference/core/structured_tools/text_completion_llm/examples/#execution-modes","title":"Execution Modes","text":""},{"location":"reference/core/structured_tools/text_completion_llm/examples/#1-synchronous-execution","title":"1. Synchronous Execution","text":"<p>Standard blocking execution:</p> <pre><code>from pydantic import BaseModel\nfrom serapeum.core.structured_tools.text_completion_llm import TextCompletionLLM\nfrom serapeum.llms.ollama import Ollama\n\nclass Answer(BaseModel):\n    answer: str\n    reasoning: str\n\nllm = Ollama(model=\"llama3.1\", request_timeout=180)\n\ntext_llm = TextCompletionLLM(\n    output_cls=Answer,\n    prompt=\"Answer this question: {question}\",\n    llm=llm,\n)\n\n# Synchronous call using __call__\nresult = text_llm(question=\"What is the capital of France?\")\nprint(result.answer)  # \"Paris\"\n</code></pre>"},{"location":"reference/core/structured_tools/text_completion_llm/examples/#2-asynchronous-execution","title":"2. Asynchronous Execution","text":"<p>Non-blocking async execution:</p> <pre><code>import asyncio\nfrom pydantic import BaseModel\nfrom serapeum.core.structured_tools.text_completion_llm import TextCompletionLLM\nfrom serapeum.llms.ollama import Ollama\n\nclass Analysis(BaseModel):\n    result: str\n    confidence: float\n\nllm = Ollama(model=\"llama3.1\", request_timeout=180)\n\ntext_llm = TextCompletionLLM(\n    output_cls=Analysis,\n    prompt=\"Analyze: {data}\",\n    llm=llm,\n)\n\nasync def analyze_data(data: str) -&gt; Analysis:\n    # Asynchronous call using acall\n    result = await text_llm.acall(data=data)\n    return result\n\n# Run async function\nresult = asyncio.run(analyze_data(\"Sample data\"))\n</code></pre>"},{"location":"reference/core/structured_tools/text_completion_llm/examples/#3-batch-processing","title":"3. Batch Processing","text":"<p>Process multiple inputs efficiently:</p> <pre><code>import asyncio\nfrom pydantic import BaseModel\nfrom serapeum.core.structured_tools.text_completion_llm import TextCompletionLLM\nfrom serapeum.llms.ollama import Ollama\n\nclass Category(BaseModel):\n    category: str\n    subcategory: str\n\nllm = Ollama(model=\"llama3.1\", request_timeout=180)\n\ntext_llm = TextCompletionLLM(\n    output_cls=Category,\n    prompt=\"Categorize this item: {item}\",\n    llm=llm,\n)\n\nasync def categorize_batch(items: list[str]) -&gt; list[Category]:\n    tasks = [text_llm.acall(item=item) for item in items]\n    results = await asyncio.gather(*tasks)\n    return results\n\nitems = [\"Laptop\", \"Apple\", \"T-shirt\", \"Novel\"]\ncategories = asyncio.run(categorize_batch(items))\nfor item, cat in zip(items, categories):\n    print(f\"{item}: {cat.category}/{cat.subcategory}\")\n</code></pre>"},{"location":"reference/core/structured_tools/text_completion_llm/examples/#4-passing-llm-specific-parameters","title":"4. Passing LLM-specific Parameters","text":"<p>Forward parameters directly to the LLM:</p> <pre><code>from pydantic import BaseModel\nfrom serapeum.core.structured_tools.text_completion_llm import TextCompletionLLM\nfrom serapeum.llms.ollama import Ollama\n\nclass Story(BaseModel):\n    title: str\n    content: str\n\nllm = Ollama(model=\"llama3.1\", request_timeout=180)\n\ntext_llm = TextCompletionLLM(\n    output_cls=Story,\n    prompt=\"Write a {genre} story about {topic}\",\n    llm=llm,\n)\n\n# Pass LLM-specific kwargs\nresult = text_llm(\n    llm_kwargs={\n        \"temperature\": 0.8,     # Higher temperature for creativity\n        \"top_p\": 0.9,\n        \"max_tokens\": 500,\n    },\n    genre=\"sci-fi\",\n    topic=\"time travel\"\n)\n</code></pre>"},{"location":"reference/core/structured_tools/text_completion_llm/examples/#advanced-usage","title":"Advanced Usage","text":""},{"location":"reference/core/structured_tools/text_completion_llm/examples/#1-dynamic-prompt-updates","title":"1. Dynamic Prompt Updates","text":"<p>Change the prompt at runtime:</p> <pre><code>from pydantic import BaseModel\nfrom serapeum.core.prompts.base import PromptTemplate\nfrom serapeum.core.structured_tools.text_completion_llm import TextCompletionLLM\nfrom serapeum.llms.ollama import Ollama\n\nclass Response(BaseModel):\n    response: str\n\nllm = Ollama(model=\"llama3.1\", request_timeout=180)\n\ntext_llm = TextCompletionLLM(\n    output_cls=Response,\n    prompt=\"Default prompt: {input}\",\n    llm=llm,\n)\n\n# Use with initial prompt\nresult1 = text_llm(input=\"test\")\n\n# Update prompt dynamically\ntext_llm.prompt = PromptTemplate(\"Updated prompt: {input}\")\n\n# Use with new prompt\nresult2 = text_llm(input=\"test\")\n</code></pre>"},{"location":"reference/core/structured_tools/text_completion_llm/examples/#2-reusable-instance-pattern","title":"2. Reusable Instance Pattern","text":"<p>Create once, use many times:</p> <pre><code>from pydantic import BaseModel\nfrom serapeum.core.structured_tools.text_completion_llm import TextCompletionLLM\nfrom serapeum.llms.ollama import Ollama\n\nclass Entity(BaseModel):\n    name: str\n    type: str\n\nllm = Ollama(model=\"llama3.1\", request_timeout=180)\n\n# Create reusable instance\nentity_extractor = TextCompletionLLM(\n    output_cls=Entity,\n    prompt=\"Extract the main entity from: {text}\",\n    llm=llm,\n)\n\n# Reuse multiple times\ntexts = [\n    \"Apple Inc. announced new products\",\n    \"Paris is the capital of France\",\n    \"Python is a programming language\",\n]\n\nfor text in texts:\n    entity = entity_extractor(text=text)\n    print(f\"{entity.name} ({entity.type})\")\n</code></pre>"},{"location":"reference/core/structured_tools/text_completion_llm/examples/#3-complex-nested-models","title":"3. Complex Nested Models","text":"<p>Use deeply nested Pydantic models:</p> <pre><code>from pydantic import BaseModel, Field\nfrom serapeum.core.structured_tools.text_completion_llm import TextCompletionLLM\nfrom serapeum.llms.ollama import Ollama\n\nclass Address(BaseModel):\n    street: str\n    city: str\n    country: str\n\nclass Contact(BaseModel):\n    email: str\n    phone: str\n\nclass Person(BaseModel):\n    name: str\n    age: int\n    address: Address\n    contact: Contact\n\nllm = Ollama(model=\"llama3.1\", request_timeout=180)\n\ntext_llm = TextCompletionLLM(\n    output_cls=Person,\n    prompt=\"Extract person information from: {bio}\",\n    llm=llm,\n)\n\nbio = \"\"\"\nJohn Doe, 30 years old\nLives at 123 Main St, New York, USA\nEmail: john@example.com\nPhone: +1-555-0123\n\"\"\"\n\nresult = text_llm(bio=bio)\nprint(result.name)              # \"John Doe\"\nprint(result.address.city)      # \"New York\"\nprint(result.contact.email)     # \"john@example.com\"\n</code></pre>"},{"location":"reference/core/structured_tools/text_completion_llm/examples/#4-optional-and-union-types","title":"4. Optional and Union Types","text":"<p>Handle optional fields and union types:</p> <pre><code>from typing import Optional, Union\nfrom pydantic import BaseModel\nfrom serapeum.core.structured_tools.text_completion_llm import TextCompletionLLM\nfrom serapeum.llms.ollama import Ollama\n\nclass Event(BaseModel):\n    title: str\n    date: str\n    location: Optional[str] = None\n    attendees: Optional[int] = None\n    type: Union[str, None] = \"general\"\n\nllm = Ollama(model=\"llama3.1\", request_timeout=180)\n\ntext_llm = TextCompletionLLM(\n    output_cls=Event,\n    prompt=\"Extract event details from: {text}\",\n    llm=llm,\n)\n\n# Works with partial information\nresult = text_llm(text=\"Python Conference on March 15\")\nprint(result.title)      # \"Python Conference\"\nprint(result.date)       # \"March 15\"\nprint(result.location)   # None (optional field)\n</code></pre>"},{"location":"reference/core/structured_tools/text_completion_llm/examples/#5-list-and-array-fields","title":"5. List and Array Fields","text":"<p>Extract lists of items:</p> <pre><code>from pydantic import BaseModel\nfrom serapeum.core.structured_tools.text_completion_llm import TextCompletionLLM\nfrom serapeum.llms.ollama import Ollama\n\nclass Recipe(BaseModel):\n    name: str\n    ingredients: list[str]\n    steps: list[str]\n    prep_time: int  # in minutes\n\nllm = Ollama(model=\"llama3.1\", request_timeout=180)\n\ntext_llm = TextCompletionLLM(\n    output_cls=Recipe,\n    prompt=\"Extract recipe from: {text}\",\n    llm=llm,\n)\n\nrecipe_text = \"\"\"\nChocolate Chip Cookies\nIngredients: flour, butter, sugar, eggs, chocolate chips\nSteps: Mix dry ingredients, cream butter and sugar, add eggs, fold in chips, bake\nPrep time: 30 minutes\n\"\"\"\n\nresult = text_llm(text=recipe_text)\nprint(result.name)\nprint(f\"Ingredients: {', '.join(result.ingredients)}\")\nprint(f\"Steps: {len(result.steps)}\")\n</code></pre>"},{"location":"reference/core/structured_tools/text_completion_llm/examples/#error-handling","title":"Error Handling","text":""},{"location":"reference/core/structured_tools/text_completion_llm/examples/#1-handling-validation-errors","title":"1. Handling Validation Errors","text":"<p>Catch and handle Pydantic validation errors:</p> <pre><code>from pydantic import BaseModel, ValidationError\nfrom serapeum.core.structured_tools.text_completion_llm import TextCompletionLLM\nfrom serapeum.llms.ollama import Ollama\n\nclass StrictModel(BaseModel):\n    count: int  # Must be integer\n    ratio: float  # Must be float\n\nllm = Ollama(model=\"llama3.1\", request_timeout=180)\n\ntext_llm = TextCompletionLLM(\n    output_cls=StrictModel,\n    prompt=\"Extract numbers from: {text}\",\n    llm=llm,\n)\n\ntry:\n    result = text_llm(text=\"Invalid data\")\nexcept ValidationError as e:\n    print(f\"Validation failed: {e}\")\n    # Handle invalid response format\nexcept ValueError as e:\n    print(f\"Type mismatch: {e}\")\n    # Handle type checking errors\n</code></pre>"},{"location":"reference/core/structured_tools/text_completion_llm/examples/#2-handling-missing-llm","title":"2. Handling Missing LLM","text":"<p>Gracefully handle missing LLM configuration:</p> <pre><code>from pydantic import BaseModel\nfrom serapeum.core.configs.configs import Configs\nfrom serapeum.core.structured_tools.text_completion_llm import TextCompletionLLM\n\nclass Data(BaseModel):\n    value: str\n\n# Clear global LLM\nConfigs.llm = None\n\ntry:\n    text_llm = TextCompletionLLM(\n        output_cls=Data,\n        prompt=\"Process: {input}\",\n        llm=None,  # No LLM provided\n    )\nexcept AssertionError as e:\n    print(\"LLM must be provided or set in Configs\")\n    # Provide fallback or configuration instructions\n</code></pre>"},{"location":"reference/core/structured_tools/text_completion_llm/examples/#3-handling-type-mismatches","title":"3. Handling Type Mismatches","text":"<p>Handle cases where parser returns wrong type:</p> <pre><code>from pydantic import BaseModel\nfrom serapeum.core.output_parsers import BaseParser\nfrom serapeum.core.structured_tools.text_completion_llm import TextCompletionLLM\nfrom serapeum.llms.ollama import Ollama\n\n\nclass ExpectedModel(BaseModel):\n    value: str\n\nclass WrongModel(BaseModel):\n    other: str\n\n# Custom parser that returns wrong type\nclass FaultyParser(BaseParser):\n\n    def parse(self, output: str):\n        return WrongModel(other=output)  # Wrong type!\n\nllm = Ollama(model=\"llama3.1\", request_timeout=180)\n\ntext_llm = TextCompletionLLM(\n    output_parser=FaultyParser(),\n    output_cls=ExpectedModel,\n    prompt=\"Process: {input}\",\n    llm=llm,\n)\n\ntry:\n    result = text_llm(input=\"test\")\nexcept ValueError as e:\n    print(f\"Type check failed: {e}\")\n    # Parser returned wrong type\n</code></pre>"},{"location":"reference/core/structured_tools/text_completion_llm/examples/#4-retry-logic","title":"4. Retry Logic","text":"<p>Implement retry logic for robustness:</p> <pre><code>import asyncio\nfrom pydantic import BaseModel\nfrom serapeum.core.structured_tools.text_completion_llm import TextCompletionLLM\nfrom serapeum.llms.ollama import Ollama\n\nclass Result(BaseModel):\n    data: str\n\nllm = Ollama(model=\"llama3.1\", request_timeout=180)\n\ntext_llm = TextCompletionLLM(\n    output_cls=Result,\n    prompt=\"Process: {input}\",\n    llm=llm,\n)\n\nasync def call_with_retry(\n    text_llm: TextCompletionLLM,\n    max_retries: int = 3,\n    **kwargs\n) -&gt; Result:\n    \"\"\"Call TextCompletionLLM with retry logic.\"\"\"\n    for attempt in range(max_retries):\n        try:\n            return await text_llm.acall(**kwargs)\n        except Exception as e:\n            if attempt == max_retries - 1:\n                raise\n            print(f\"Attempt {attempt + 1} failed: {e}. Retrying...\")\n            await asyncio.sleep(2 ** attempt)  # Exponential backoff\n\n# Use with retry\nresult = asyncio.run(call_with_retry(text_llm, input=\"test\"))\n</code></pre>"},{"location":"reference/core/structured_tools/text_completion_llm/examples/#best-practices","title":"Best Practices","text":""},{"location":"reference/core/structured_tools/text_completion_llm/examples/#1-model-validation","title":"1. Model Validation","text":"<p>Always define clear Pydantic models with validation:</p> <pre><code>from pydantic import BaseModel, Field, field_validator\nfrom serapeum.core.structured_tools.text_completion_llm import TextCompletionLLM\nfrom serapeum.llms.ollama import Ollama\n\nclass ValidatedData(BaseModel):\n    email: str = Field(pattern=r'^[\\w\\.-]+@[\\w\\.-]+\\.\\w+$')\n    age: int = Field(ge=0, le=150)\n    score: float = Field(ge=0.0, le=1.0)\n\n    @field_validator('email')\n    def validate_email(cls, v):\n        if '@' not in v:\n            raise ValueError('Invalid email format')\n        return v.lower()\n\nllm = Ollama(model=\"llama3.1\", request_timeout=180)\n\ntext_llm = TextCompletionLLM(\n    output_cls=ValidatedData,\n    prompt=\"Extract data from: {text}\",\n    llm=llm,\n)\n</code></pre>"},{"location":"reference/core/structured_tools/text_completion_llm/examples/#2-clear-prompt-instructions","title":"2. Clear Prompt Instructions","text":"<p>Provide clear instructions for JSON output:</p> <pre><code>from pydantic import BaseModel\nfrom serapeum.core.structured_tools.text_completion_llm import TextCompletionLLM\nfrom serapeum.llms.ollama import Ollama\n\nclass Output(BaseModel):\n    result: str\n\nllm = Ollama(model=\"llama3.1\", request_timeout=180)\n\n# Good: Clear instructions\ntext_llm = TextCompletionLLM(\n    output_cls=Output,\n    prompt=\"\"\"\n    Analyze the following text and return ONLY valid JSON.\n    Do not include any explanation or markdown formatting.\n\n    Text: {text}\n\n    Return format: {{\"result\": \"your analysis here\"}}\n    \"\"\",\n    llm=llm,\n)\n</code></pre>"},{"location":"reference/core/structured_tools/text_completion_llm/examples/#3-instance-reuse","title":"3. Instance Reuse","text":"<p>Create instances once and reuse them:</p> <pre><code>from pydantic import BaseModel\nfrom serapeum.core.structured_tools.text_completion_llm import TextCompletionLLM\nfrom serapeum.llms.ollama import Ollama\n\nclass Classification(BaseModel):\n    category: str\n    confidence: float\n\n# Create once\nllm = Ollama(model=\"llama3.1\", request_timeout=180)\nclassifier = TextCompletionLLM(\n    output_cls=Classification,\n    prompt=\"Classify: {text}\",\n    llm=llm,\n)\n\n# Reuse many times - this is efficient!\nfor text in texts:\n    result = classifier(text=text)\n</code></pre>"},{"location":"reference/core/structured_tools/text_completion_llm/examples/#see-also","title":"See Also","text":"<ul> <li>Execution Flow and Method Calls - Detailed sequence diagram</li> <li>Architecture and Class Relationships - Class structure</li> <li>Data Transformations and Validation - Data flow details</li> <li>Component Boundaries and Interactions - System components</li> <li>Lifecycle States and Transitions - State management</li> </ul>"},{"location":"reference/core/structured_tools/text_completion_llm/general/","title":"TextCompletionLLM Workflow","text":"<p>This directory contains comprehensive explaining the complete workflow of the <code>TextCompletionLLM</code> class, from initialization to execution and output parsing.</p>"},{"location":"reference/core/structured_tools/text_completion_llm/general/#overview","title":"Overview","text":"<p>The <code>TextCompletionLLM</code> is a structured text completion runner that orchestrates: 1. Prompt formatting with template variables 2. LLM execution (chat or completion mode) 3. Output parsing into validated Pydantic models</p>"},{"location":"reference/core/structured_tools/text_completion_llm/general/#example-usage","title":"Example Usage","text":"<pre><code>from pydantic import BaseModel\nfrom serapeum.core.output_parsers import PydanticParser\nfrom serapeum.core.structured_tools.text_completion_llm import TextCompletionLLM\nfrom serapeum.llms.ollama import Ollama\n\n\n# Define the output schema\nclass ModelTest(BaseModel):\n    hello: str\n\n# Initialize components\nLLM = Ollama(model=\"llama3.1\", request_timeout=180)\noutput_parser = PydanticParser(output_cls=ModelTest)\n\n# Create TextCompletionLLM instance\ntext_llm = TextCompletionLLM(\n    output_parser=output_parser,\n    prompt=\"This is a test prompt with a {test_input}.\",\n    llm=LLM,\n)\n\n# Execute and get structured output\nobj_output = text_llm(test_input=\"hello\")\n# Returns: ModelTest(hello=\"...\")\n</code></pre>"},{"location":"reference/core/structured_tools/text_completion_llm/general/#understanding-the-workflow","title":"Understanding the Workflow","text":""},{"location":"reference/core/structured_tools/text_completion_llm/general/#1-execution-flow-and-method-calls","title":"1. Execution Flow and Method Calls","text":"<p>Shows the chronological flow of method calls and interactions.</p> <p>Best for: - Understanding the order of operations - Seeing how objects communicate - Debugging execution flow</p> <p>Key Sections: - Initialization phase (validation and component setup) - Execution phase (prompt formatting and LLM invocation) - Parsing phase (JSON extraction and Pydantic validation)</p>"},{"location":"reference/core/structured_tools/text_completion_llm/general/#2-architecture-and-class-relationships","title":"2. Architecture and Class Relationships","text":"<p>Illustrates the static structure and relationships between classes.</p> <p>Best for: - Understanding the architecture - Seeing inheritance and composition - Identifying class responsibilities</p> <p>Key Classes: - <code>TextCompletionLLM</code>: Main orchestrator - <code>PydanticParser</code>: Output validation - <code>BasePromptTemplate</code> hierarchy: Prompt formatting - <code>LLM</code> hierarchy: Model execution</p>"},{"location":"reference/core/structured_tools/text_completion_llm/general/#3-data-transformations-and-validation","title":"3. Data Transformations and Validation","text":"<p>Tracks how data transforms through the system.</p> <p>Best for: - Understanding data transformations - Identifying validation points - Seeing error handling paths</p> <p>Key Flows: - Initialization validation pipeline - Chat model execution path - Completion model execution path - JSON parsing and validation</p>"},{"location":"reference/core/structured_tools/text_completion_llm/general/#4-component-boundaries-and-interactions","title":"4. Component Boundaries and Interactions","text":"<p>Shows component boundaries and interaction patterns.</p> <p>Best for: - Understanding system architecture - Seeing component responsibilities - Identifying interaction patterns</p> <p>Key Components: - User space (application code) - TextCompletionLLM layer (orchestration) - Prompt layer (template formatting) - LLM layer (model execution) - Parser layer (output validation) - External service (Ollama server)</p>"},{"location":"reference/core/structured_tools/text_completion_llm/general/#5-lifecycle-states-and-transitions","title":"5. Lifecycle States and Transitions","text":"<p>Depicts the lifecycle states and transitions.</p> <p>Best for: - Understanding instance lifecycle - Seeing state transitions - Identifying error states</p> <p>Key States: - Initialization states (validation) - Execution states (chat vs completion) - Parsing states (JSON extraction and validation) - Error states (initialization, execution, parsing)</p>"},{"location":"reference/core/structured_tools/text_completion_llm/general/#workflow-summary","title":"Workflow Summary","text":""},{"location":"reference/core/structured_tools/text_completion_llm/general/#initialization-workflow","title":"Initialization Workflow","text":"<pre><code>1. Create PydanticParser with output schema (ModelTest)\n2. Initialize TextCompletionLLM with:\n   - output_parser: PydanticParser\n   - prompt: String or BasePromptTemplate\n   - llm: Ollama instance\n3. Validation occurs:\n   - Parser type and output_cls extraction\n   - LLM instance availability check\n   - Prompt conversion to template if needed\n4. Components stored in instance\n5. Instance ready for execution\n</code></pre>"},{"location":"reference/core/structured_tools/text_completion_llm/general/#execution-workflow","title":"Execution Workflow","text":"<pre><code>1. User calls text_llm(test_input=\"hello\")\n2. Check LLM.metadata.is_chat_model:\n\n   If Chat Model (True):\n   a. Format prompt to messages with variables\n   b. Extend messages with system prompts\n   c. Call Ollama.chat() \u2192 HTTP POST /api/chat\n   d. Extract message.content from ChatResponse\n\n   If Completion Model (False):\n   a. Format prompt to string with variables\n   b. Extend prompt with system prompts\n   c. Call Ollama.complete() \u2192 HTTP POST /api/generate\n   d. Extract text from CompletionResponse\n\n3. Parse output:\n   a. Extract JSON string from raw text\n   b. Validate JSON against Pydantic schema\n   c. Create ModelTest instance\n   d. Type check output\n\n4. Return validated ModelTest instance\n</code></pre>"},{"location":"reference/core/structured_tools/text_completion_llm/general/#key-design-patterns","title":"Key Design Patterns","text":""},{"location":"reference/core/structured_tools/text_completion_llm/general/#1-validation-at-construction","title":"1. Validation at Construction","text":"<p>All components are validated during <code>__init__</code>, ensuring errors are caught early: - Parser type checking - LLM availability verification - Prompt format conversion</p>"},{"location":"reference/core/structured_tools/text_completion_llm/general/#2-adapter-pattern","title":"2. Adapter Pattern","text":"<p><code>TextCompletionLLM</code> adapts different prompt types and LLM modes: - String prompts \u2192 PromptTemplate - Chat models \u2192 format_messages - Completion models \u2192 format string</p>"},{"location":"reference/core/structured_tools/text_completion_llm/general/#3-strategy-pattern","title":"3. Strategy Pattern","text":"<p>Output parsing strategy is injected via <code>PydanticParser</code>: - Customizable JSON extraction - Flexible schema validation - Extensible error handling</p>"},{"location":"reference/core/structured_tools/text_completion_llm/general/#4-template-method-pattern","title":"4. Template Method Pattern","text":"<p><code>__call__</code> defines the algorithm skeleton: 1. Check model type 2. Format prompt (strategy varies) 3. Execute LLM (path varies) 4. Parse output (consistent)</p>"},{"location":"reference/core/structured_tools/text_completion_llm/general/#performance-considerations","title":"Performance Considerations","text":"<ol> <li>Reusable Instances: <code>TextCompletionLLM</code> instances are reusable after initialization</li> <li>Async Support: <code>acall()</code> method provides async execution</li> <li>Streaming Support: LLM layer supports streaming responses</li> <li>Stateless Execution: Each call creates independent transient state</li> </ol>"},{"location":"reference/core/structured_tools/text_completion_llm/text_completion_llm_class/","title":"Architecture and Class Relationships","text":"<p>This diagram shows the class relationships and structure for <code>TextCompletionLLM</code>.</p>  Hold \"Ctrl\" to enable pan &amp; zoom  <pre><code>classDiagram\n    class BasePydanticLLM~BaseModel~ {\n        &lt;&lt;abstract&gt;&gt;\n        +output_cls: Type[BaseModel]\n        +__call__(**kwargs) BaseModel\n        +acall(**kwargs) BaseModel\n    }\n\n    class TextCompletionLLM~BaseModel~ {\n        +__init__(output_parser, prompt, output_cls, llm, verbose)\n        +prompt: BasePromptTemplate\n        +output_cls: Type[BaseModel]\n        +__call__(llm_kwargs, **kwargs) BaseModel\n        +acall(llm_kwargs, **kwargs) BaseModel\n        -_output_parser: BaseParser\n        -_output_cls: Type[BaseModel]\n        -_llm: LLM\n        -_prompt: BasePromptTemplate\n        -_verbose: bool\n        +_validate_prompt(prompt) BasePromptTemplate\n        +_validate_llm(llm) LLM\n        +_validate_output_parser_cls(parser, cls) Tuple\n    }\n\n    class BaseParser {\n        &lt;&lt;abstract&gt;&gt;\n        +parse(output: str) Any\n        +format(query: str) str\n        +format_messages(messages) List[Message]\n    }\n\n    class PydanticParser~Model~ {\n        -_output_cls: Type[Model]\n        -_excluded_schema_keys_from_format: List\n        -_pydantic_format_tmpl: str\n        +__init__(output_cls, excluded_schema_keys, pydantic_format_tmpl)\n        +output_cls: Type[Model]\n        +format_string: str\n        +get_format_string(escape_json) str\n        +parse(text: str) Any\n        +format(query: str) str\n    }\n\n    class BasePromptTemplate {\n        &lt;&lt;abstract&gt;&gt;\n        +metadata: Dict[str, Any]\n        +template_vars: List[str]\n        +kwargs: Dict[str, str]\n        +output_parser: Optional[BaseParser]\n        +template_var_mappings: Optional[Dict]\n        +function_mappings: Optional[Dict]\n        +partial_format(**kwargs) BasePromptTemplate\n        +format(llm, **kwargs) str\n        +format_messages(llm, **kwargs) List[Message]\n        +get_template(llm) str\n    }\n\n    class PromptTemplate {\n        +template: str\n        +__init__(template, prompt_type, output_parser, metadata, ...)\n        +partial_format(**kwargs) PromptTemplate\n        +format(llm, completion_to_prompt, **kwargs) str\n        +format_messages(llm, **kwargs) List[Message]\n        +get_template(llm) str\n    }\n\n    class ChatPromptTemplate {\n        +message_templates: List[Message]\n        +__init__(message_templates, prompt_type, output_parser, ...)\n        +from_messages(message_templates, **kwargs) ChatPromptTemplate\n        +partial_format(**kwargs) ChatPromptTemplate\n        +format(llm, messages_to_prompt, **kwargs) str\n        +format_messages(llm, **kwargs) List[Message]\n        +get_template(llm) str\n    }\n\n    class BaseLLM {\n        &lt;&lt;abstract&gt;&gt;\n        +metadata: Metadata\n        +chat(messages, **kwargs) ChatResponse\n        +stream_chat(messages, **kwargs) ChatResponseGen\n        +achat(messages, **kwargs) ChatResponse\n        +astream_chat(messages, **kwargs) ChatResponseAsyncGen\n        +complete(prompt, **kwargs) CompletionResponse\n        +stream_complete(prompt, **kwargs) CompletionResponseGen\n        +acomplete(prompt, **kwargs) CompletionResponse\n        +astream_complete(prompt, **kwargs) CompletionResponseAsyncGen\n    }\n\n    class LLM {\n        +system_prompt: Optional[str]\n        +messages_to_prompt: MessagesToPromptCallable\n        +completion_to_prompt: CompletionToPromptCallable\n        +output_parser: Optional[BaseParser]\n        +pydantic_program_mode: StructuredLLMMode\n        +_get_prompt(prompt, **kwargs) str\n        +_get_messages(prompt, **kwargs) List[Message]\n        +_parse_output(output) str\n        +_extend_prompt(formatted_prompt) str\n        +_extend_messages(messages) List[Message]\n        +predict(prompt, **kwargs) str\n        +stream(prompt, **kwargs) TokenGen\n        +apredict(prompt, **kwargs) str\n        +astream(prompt, **kwargs) TokenAsyncGen\n        +structured_predict(output_cls, prompt, **kwargs) Model\n    }\n\n    class Ollama {\n        +model: str\n        +base_url: str\n        +request_timeout: int\n        +temperature: float\n        +metadata: Metadata\n        +__init__(model, base_url, request_timeout, ...)\n        +chat(messages, **kwargs) ChatResponse\n        +achat(messages, **kwargs) ChatResponse\n        +complete(prompt, **kwargs) CompletionResponse\n        +acomplete(prompt, **kwargs) CompletionResponse\n        -_chat_request(messages, stream, **kwargs) dict\n        -_complete_request(prompt, stream, **kwargs) dict\n    }\n\n    class BaseModel {\n        &lt;&lt;pydantic&gt;&gt;\n        +model_validate_json(json_data) BaseModel\n        +model_json_schema() dict\n    }\n\n    class ModelTest {\n        +hello: str\n    }\n\n    BasePydanticLLM &lt;|-- TextCompletionLLM\n    BaseLLM &lt;|-- LLM\n    LLM &lt;|-- Ollama\n    BaseParser &lt;|-- PydanticParser\n    BasePromptTemplate &lt;|-- PromptTemplate\n    BasePromptTemplate &lt;|-- ChatPromptTemplate\n    BaseModel &lt;|-- ModelTest\n\n    TextCompletionLLM o-- PydanticParser : uses\n    TextCompletionLLM o-- BasePromptTemplate : uses\n    TextCompletionLLM o-- LLM : uses\n    TextCompletionLLM ..&gt; ModelTest : produces\n    PydanticParser o-- ModelTest : validates against\n    PromptTemplate --|&gt; BasePromptTemplate\n    ChatPromptTemplate --|&gt; BasePromptTemplate\n\n    note for TextCompletionLLM \"Main orchestrator that:\\n1. Validates parser, prompt, LLM\\n2. Formats prompts with variables\\n3. Calls LLM (chat or complete)\\n4. Parses output to Pydantic model\"\n    note for PydanticParser \"Extracts JSON from text\\nand validates against schema\"\n    note for Ollama \"Concrete LLM implementation\\nfor Ollama server\"</code></pre>"},{"location":"reference/core/structured_tools/text_completion_llm/text_completion_llm_class/#class-responsibilities","title":"Class Responsibilities","text":""},{"location":"reference/core/structured_tools/text_completion_llm/text_completion_llm_class/#textcompletionllm","title":"TextCompletionLLM","text":"<ul> <li>Orchestrates the complete workflow from prompt to structured output</li> <li>Validates all components during initialization</li> <li>Routes to chat or completion based on LLM metadata</li> <li>Ensures type safety of output</li> </ul>"},{"location":"reference/core/structured_tools/text_completion_llm/text_completion_llm_class/#pydanticparser","title":"PydanticParser","text":"<ul> <li>Extracts JSON from raw LLM output</li> <li>Validates JSON against Pydantic schema</li> <li>Formats prompts with schema hints</li> </ul>"},{"location":"reference/core/structured_tools/text_completion_llm/text_completion_llm_class/#ollama-llm","title":"Ollama (LLM)","text":"<ul> <li>Executes requests to Ollama server</li> <li>Supports both chat and completion modes</li> <li>Handles streaming and async operations</li> </ul>"},{"location":"reference/core/structured_tools/text_completion_llm/text_completion_llm_class/#prompttemplatechatprompttemplate","title":"PromptTemplate/ChatPromptTemplate","text":"<ul> <li>Formats prompts with variables</li> <li>Supports both string and message-based templates</li> <li>Manages template variables and mappings</li> </ul>"},{"location":"reference/core/structured_tools/text_completion_llm/text_completion_llm_components/","title":"Component Boundaries and Interactions","text":"<p>This diagram shows how components interact during the complete lifecycle.</p>  Hold \"Ctrl\" to enable pan &amp; zoom  <pre><code>graph TB\n    subgraph User Space\n        UC[User Code]\n        MT[ModelTest Schema]\n    end\n\n    subgraph TextCompletionLLM Components\n        TCL[TextCompletionLLM]\n\n        subgraph Validators\n            VP[validate_prompt]\n            VL[validate_llm]\n            VO[validate_output_parser_cls]\n        end\n\n        subgraph Stored State\n            SP[\"_prompt: BasePromptTemplate\"]\n            SL[\"_llm: LLM\"]\n            SO[\"_output_parser: PydanticParser\"]\n            SC[\"_output_cls: Type ModelTest\"]\n        end\n    end\n\n    subgraph Output Parser Layer\n        POP[PydanticParser]\n        EJ[extract_json_str]\n        VJ[model_validate_json]\n    end\n\n    subgraph Prompt Layer\n        PT[PromptTemplate]\n        CPT[ChatPromptTemplate]\n\n        subgraph Prompt Operations\n            PF[format]\n            FM[format_messages]\n            AV[Apply Variables]\n        end\n    end\n\n    subgraph LLM Layer\n        OL[Ollama]\n\n        subgraph LLM Operations\n            GM[_get_messages]\n            GP[_get_prompt]\n            EM[_extend_messages]\n            EP[_extend_prompt]\n        end\n\n        subgraph API Methods\n            CH[chat]\n            CO[complete]\n        end\n\n        MD[metadata.is_chat_model]\n    end\n\n    subgraph External Service\n        OS[Ollama Server]\n\n        subgraph Endpoints\n            EC[\"/api/chat\"]\n            EG[\"/api/generate\"]\n        end\n    end\n\n    subgraph Response Models\n        CHR[ChatResponse]\n        COR[CompletionResponse]\n    end\n\n    %% Initialization Flow\n    UC --&gt;|1. Create parser| POP\n    POP --&gt;|output_cls| MT\n\n    UC --&gt;|2. Initialize| TCL\n    TCL --&gt;|Validate| VO\n    VO --&gt;|Extract| MT\n\n    TCL --&gt;|Validate| VL\n    VL --&gt;|Check/Fallback| OL\n\n    TCL --&gt;|Validate/Convert| VP\n    VP --&gt;|Create if string| PT\n\n    VO --&gt;|Store| SO\n    VO --&gt;|Store| SC\n    VL --&gt;|Store| SL\n    VP --&gt;|Store| SP\n\n    %% Execution Flow - Chat Path\n    UC --&gt;|3. Call with kwargs| TCL\n    TCL --&gt;|Check| MD\n\n    MD --&gt;|True| FM\n    FM --&gt;|Use kwargs| AV\n    AV --&gt;|Messages| GM\n    GM --&gt;|Add system| EM\n    EM --&gt;|Send| CH\n    CH --&gt;|HTTP POST| EC\n    EC --&gt;|Response| CHR\n    CHR --&gt;|message.content| EJ\n\n    %% Execution Flow - Completion Path\n    MD --&gt;|False| PF\n    PF --&gt;|Use kwargs| AV\n    AV --&gt;|String| GP\n    GP --&gt;|Add system| EP\n    EP --&gt;|Send| CO\n    CO --&gt;|HTTP POST| EG\n    EG --&gt;|Response| COR\n    COR --&gt;|text| EJ\n\n    %% Parsing Flow\n    EJ --&gt;|Extract| VJ\n    VJ --&gt;|Validate| MT\n    MT --&gt;|Instance| TCL\n    TCL --&gt;|Type check| UC\n\n    %% Styling\n    classDef userClass fill:#e1f5ff,stroke:#01579b\n    classDef validatorClass fill:#fff9c4,stroke:#f57f17\n    classDef stateClass fill:#f3e5f5,stroke:#4a148c\n    classDef parserClass fill:#e8f5e9,stroke:#1b5e20\n    classDef promptClass fill:#fce4ec,stroke:#880e4f\n    classDef llmClass fill:#e0f2f1,stroke:#004d40\n    classDef responseClass fill:#fff3e0,stroke:#e65100\n    classDef externalClass fill:#efebe9,stroke:#3e2723\n\n    class UC,MT userClass\n    class VP,VL,VO validatorClass\n    class SP,SL,SO,SC stateClass\n    class POP,EJ,VJ parserClass\n    class PT,CPT,PF,FM,AV promptClass\n    class OL,GM,GP,EM,EP,CH,CO,MD llmClass\n    class CHR,COR responseClass\n    class OS,EC,EG externalClass</code></pre>"},{"location":"reference/core/structured_tools/text_completion_llm/text_completion_llm_components/#component-responsibilities-matrix","title":"Component Responsibilities Matrix","text":"Component Initialization Execution Parsing TextCompletionLLM Validates &amp; stores all components Routes to chat/complete Type checks output PydanticParser Stores output_cls schema - Extracts JSON &amp; validates PromptTemplate Created/validated Formats with variables - Ollama (LLM) Validated/stored Executes requests - ModelTest Defines schema - Validates parsed data"},{"location":"reference/core/structured_tools/text_completion_llm/text_completion_llm_components/#interaction-patterns","title":"Interaction Patterns","text":""},{"location":"reference/core/structured_tools/text_completion_llm/text_completion_llm_components/#1-initialization-pattern-constructor","title":"1. Initialization Pattern (Constructor)","text":"<pre><code>User \u2192 TextCompletionLLM.__init__\n  \u251c\u2500\u2192 validate_output_parser_cls\n  \u2502   \u2514\u2500\u2192 Extract/Create parser &amp; output_cls\n  \u251c\u2500\u2192 validate_llm\n  \u2502   \u2514\u2500\u2192 Use provided or fallback to Configs.llm\n  \u2514\u2500\u2192 validate_prompt\n      \u2514\u2500\u2192 Convert string to PromptTemplate if needed\n</code></pre>"},{"location":"reference/core/structured_tools/text_completion_llm/text_completion_llm_components/#2-chat-model-execution-pattern","title":"2. Chat Model Execution Pattern","text":"<pre><code>User \u2192 TextCompletionLLM.__call__\n  \u251c\u2500\u2192 Check metadata.is_chat_model == True\n  \u251c\u2500\u2192 PromptTemplate.format_messages(kwargs)\n  \u251c\u2500\u2192 LLM._extend_messages\n  \u251c\u2500\u2192 Ollama.chat \u2192 HTTP \u2192 ChatResponse\n  \u251c\u2500\u2192 Extract message.content\n  \u2514\u2500\u2192 PydanticParser.parse \u2192 ModelTest\n</code></pre>"},{"location":"reference/core/structured_tools/text_completion_llm/text_completion_llm_components/#3-completion-model-execution-pattern","title":"3. Completion Model Execution Pattern","text":"<pre><code>User \u2192 TextCompletionLLM.__call__\n  \u251c\u2500\u2192 Check metadata.is_chat_model == False\n  \u251c\u2500\u2192 PromptTemplate.format(kwargs)\n  \u251c\u2500\u2192 LLM._extend_prompt\n  \u251c\u2500\u2192 Ollama.complete \u2192 HTTP \u2192 CompletionResponse\n  \u251c\u2500\u2192 Extract response.text\n  \u2514\u2500\u2192 PydanticParser.parse \u2192 ModelTest\n</code></pre>"},{"location":"reference/core/structured_tools/text_completion_llm/text_completion_llm_components/#state-management","title":"State Management","text":""},{"location":"reference/core/structured_tools/text_completion_llm/text_completion_llm_components/#immutable-state-post-initialization","title":"Immutable State (Post-Initialization)","text":"<ul> <li><code>_output_parser</code>: PydanticParser instance</li> <li><code>_output_cls</code>: Type[ModelTest]</li> <li><code>_llm</code>: Ollama instance</li> </ul>"},{"location":"reference/core/structured_tools/text_completion_llm/text_completion_llm_components/#mutable-state","title":"Mutable State","text":"<ul> <li><code>_prompt</code>: Can be updated via setter</li> <li><code>_verbose</code>: Controls logging output</li> </ul>"},{"location":"reference/core/structured_tools/text_completion_llm/text_completion_llm_components/#transient-state-per-call","title":"Transient State (Per Call)","text":"<ul> <li><code>llm_kwargs</code>: Forwarded to LLM methods</li> <li><code>**kwargs</code>: Template variables for prompt formatting</li> <li><code>raw_output</code>: Intermediate text from LLM</li> <li><code>parsed_output</code>: Final validated Pydantic model</li> </ul>"},{"location":"reference/core/structured_tools/text_completion_llm/text_completion_llm_dataflow/","title":"Data Transformations and Validation","text":"<p>This diagram shows how data flows through the <code>TextCompletionLLM</code> system.</p>  Hold \"Ctrl\" to enable pan &amp; zoom  <pre><code>flowchart TD\n    Start([User Code]) --&gt; Init{Initialize TextCompletionLLM}\n\n    Init --&gt; ValidateParser[Validate Output Parser]\n    ValidateParser --&gt; CheckParser{Parser Type?}\n    CheckParser --&gt;|PydanticParser| ExtractClass[Extract output_cls]\n    CheckParser --&gt;|None + output_cls| CreateParser[Create PydanticParser]\n    CheckParser --&gt;|Other| Error1[Raise ValueError]\n\n    ExtractClass --&gt; ValidateLLM[Validate LLM]\n    CreateParser --&gt; ValidateLLM\n\n    ValidateLLM --&gt; CheckLLM{LLM Provided?}\n    CheckLLM --&gt;|Yes| ValidatePrompt[Validate Prompt]\n    CheckLLM --&gt;|No| CheckConfigs{Configs.llm Set?}\n    CheckConfigs --&gt;|Yes| ValidatePrompt\n    CheckConfigs --&gt;|No| Error2[Raise AssertionError]\n\n    ValidatePrompt --&gt; CheckPrompt{Prompt Type?}\n    CheckPrompt --&gt;|BasePromptTemplate| StoreComponents[Store All Components]\n    CheckPrompt --&gt;|String| ConvertPrompt[Convert to PromptTemplate]\n    CheckPrompt --&gt;|Other| Error3[Raise ValueError]\n\n    ConvertPrompt --&gt; StoreComponents\n    StoreComponents --&gt; Ready([TextCompletionLLM Ready])\n\n    Ready --&gt; Call{User Calls text_llm}\n    Call --&gt;|With kwargs| MergeKwargs[Merge kwargs with defaults]\n\n    MergeKwargs --&gt; CheckModel{LLM.metadata.is_chat_model?}\n\n    CheckModel --&gt;|True - Chat Model| FormatMessages[Format Messages]\n    CheckModel --&gt;|False - Completion Model| FormatPrompt[Format Prompt String]\n\n    FormatMessages --&gt; ApplyVars1[Apply template variables]\n    ApplyVars1 --&gt; AddSchema1[Add JSON schema to messages]\n    AddSchema1 --&gt; ExtendMessages[Extend with system prompts]\n    ExtendMessages --&gt; ChatCall[Call LLM.chat]\n    ChatCall --&gt; ChatRequest[HTTP POST to /api/chat]\n    ChatRequest --&gt; ChatResponse[ChatResponse with message]\n    ChatResponse --&gt; ExtractContent1[Extract message.content]\n    ExtractContent1 --&gt; ParseOutput\n\n    FormatPrompt --&gt; ApplyVars2[Apply template variables]\n    ApplyVars2 --&gt; AddSchema2[Add JSON schema to prompt]\n    AddSchema2 --&gt; ExtendPrompt[Extend with system prompts]\n    ExtendPrompt --&gt; CompleteCall[Call LLM.complete]\n    CompleteCall --&gt; CompleteRequest[HTTP POST to /api/generate]\n    CompleteRequest --&gt; CompleteResponse[CompletionResponse with text]\n    CompleteResponse --&gt; ExtractContent2[Extract response.text]\n    ExtractContent2 --&gt; ParseOutput[Parse Output]\n\n    ParseOutput --&gt; ExtractJSON[Extract JSON string]\n    ExtractJSON --&gt; ValidateJSON{Valid JSON?}\n    ValidateJSON --&gt;|No| Error4[Raise ValueError]\n    ValidateJSON --&gt;|Yes| ValidateSchema[Validate against Pydantic schema]\n\n    ValidateSchema --&gt; CheckSchema{Schema Valid?}\n    CheckSchema --&gt;|No| Error5[Raise ValidationError]\n    CheckSchema --&gt;|Yes| CreateModel[Create Pydantic instance]\n\n    CreateModel --&gt; TypeCheck{isinstance check}\n    TypeCheck --&gt;|Pass| Return([Return ModelTest instance])\n    TypeCheck --&gt;|Fail| Error6[Raise ValueError]\n\n    style Start fill:#e1f5ff\n    style Ready fill:#e1f5ff\n    style Return fill:#c8e6c9\n    style Error1 fill:#ffcdd2\n    style Error2 fill:#ffcdd2\n    style Error3 fill:#ffcdd2\n    style Error4 fill:#ffcdd2\n    style Error5 fill:#ffcdd2\n    style Error6 fill:#ffcdd2\n    style ChatRequest fill:#fff9c4\n    style CompleteRequest fill:#fff9c4</code></pre>"},{"location":"reference/core/structured_tools/text_completion_llm/text_completion_llm_dataflow/#data-transformations","title":"Data Transformations","text":""},{"location":"reference/core/structured_tools/text_completion_llm/text_completion_llm_dataflow/#initialization-phase","title":"Initialization Phase","text":"<pre><code>Input:\n  - output_parser: PydanticParser(output_cls=ModelTest)\n  - prompt: \"This is a test prompt with a {test_input}.\"\n  - llm: Ollama(model=\"llama3.1\")\n\nOutput:\n  - TextCompletionLLM instance with validated components\n</code></pre>"},{"location":"reference/core/structured_tools/text_completion_llm/text_completion_llm_dataflow/#execution-phase-chat-model","title":"Execution Phase (Chat Model)","text":"<pre><code>Input:\n  text_llm(test_input=\"hello\")\n\nTransformations:\n  1. kwargs: {test_input: \"hello\"}\n  2. Formatted messages: [Message(role=USER, content=\"This is a test...\")]\n  3. Extended messages: [Message(role=SYSTEM, ...), Message(role=USER, ...)]\n  4. LLM request: {\"model\": \"llama3.1\", \"messages\": [...]}\n  5. Raw response: '{\"hello\": \"world\"}'\n  6. Extracted JSON: {\"hello\": \"world\"}\n  7. Validated model: ModelTest(hello=\"world\")\n\nOutput:\n  ModelTest(hello=\"world\")\n</code></pre>"},{"location":"reference/core/structured_tools/text_completion_llm/text_completion_llm_dataflow/#execution-phase-completion-model","title":"Execution Phase (Completion Model)","text":"<pre><code>Input:\n  text_llm(test_input=\"hello\")\n\nTransformations:\n  1. kwargs: {test_input: \"hello\"}\n  2. Formatted prompt: \"This is a test prompt with a hello.\"\n  3. Extended prompt: \"System: ...\\n\\nThis is a test prompt with a hello.\"\n  4. LLM request: {\"model\": \"llama3.1\", \"prompt\": \"...\"}\n  5. Raw response: '{\"hello\": \"world\"}'\n  6. Extracted JSON: {\"hello\": \"world\"}\n  7. Validated model: ModelTest(hello=\"world\")\n\nOutput:\n  ModelTest(hello=\"world\")\n</code></pre>"},{"location":"reference/core/structured_tools/text_completion_llm/text_completion_llm_dataflow/#error-handling-points","title":"Error Handling Points","text":"<ol> <li>Parser validation: Ensures PydanticParser is used</li> <li>LLM validation: Ensures LLM instance is available</li> <li>Prompt validation: Ensures prompt is convertible to template</li> <li>JSON extraction: Handles malformed JSON responses</li> <li>Schema validation: Catches Pydantic validation errors</li> <li>Type checking: Ensures output matches expected type</li> </ol>"},{"location":"reference/core/structured_tools/text_completion_llm/text_completion_llm_sequence/","title":"Execution Flow and Method Calls","text":"<p>This diagram shows the complete workflow from initialization to execution of <code>TextCompletionLLM</code>.</p>  Hold \"Ctrl\" to enable pan &amp; zoom  <pre><code>sequenceDiagram\n    participant User\n    participant PydanticParser\n    participant TextCompletionLLM\n    participant PromptTemplate\n    participant Ollama\n    participant LLMChat/Complete\n\n    Note over User: Initialization Phase\n    User-&gt;&gt;PydanticParser: Create parser with ModelTest\n    activate PydanticParser\n    PydanticParser--&gt;&gt;User: parser instance\n    deactivate PydanticParser\n\n    User-&gt;&gt;TextCompletionLLM: __init__(output_parser, prompt, llm)\n    activate TextCompletionLLM\n\n    TextCompletionLLM-&gt;&gt;TextCompletionLLM: validate_output_parser_cls(parser, None)\n    Note over TextCompletionLLM: Validates parser is PydanticParser&lt;br/&gt;Extracts output_cls (ModelTest)\n\n    TextCompletionLLM-&gt;&gt;TextCompletionLLM: validate_llm(llm)\n    Note over TextCompletionLLM: Ensures LLM instance is provided&lt;br/&gt;or falls back to Configs.llm\n\n    TextCompletionLLM-&gt;&gt;TextCompletionLLM: validate_prompt(prompt)\n    Note over TextCompletionLLM: Converts string to PromptTemplate&lt;br/&gt;if needed\n\n    TextCompletionLLM-&gt;&gt;PromptTemplate: Create/validate prompt template\n    activate PromptTemplate\n    PromptTemplate--&gt;&gt;TextCompletionLLM: template instance\n    deactivate PromptTemplate\n\n    TextCompletionLLM--&gt;&gt;User: text_llm instance\n    deactivate TextCompletionLLM\n\n    Note over User: Execution Phase\n    User-&gt;&gt;TextCompletionLLM: __call__(test_input=\"hello\")\n    activate TextCompletionLLM\n\n    TextCompletionLLM-&gt;&gt;Ollama: Check metadata.is_chat_model\n    activate Ollama\n    Ollama--&gt;&gt;TextCompletionLLM: True/False\n    deactivate Ollama\n\n    alt is_chat_model == True\n        TextCompletionLLM-&gt;&gt;PromptTemplate: format_messages(llm, test_input=\"hello\")\n        activate PromptTemplate\n        PromptTemplate--&gt;&gt;TextCompletionLLM: List[Message]\n        deactivate PromptTemplate\n\n        TextCompletionLLM-&gt;&gt;Ollama: _extend_messages(messages)\n        activate Ollama\n        Note over Ollama: Add system prompts if configured\n        Ollama--&gt;&gt;TextCompletionLLM: extended messages\n        deactivate Ollama\n\n        TextCompletionLLM-&gt;&gt;Ollama: chat(messages, **llm_kwargs)\n        activate Ollama\n        Ollama-&gt;&gt;LLMChat/Complete: HTTP request to Ollama server\n        activate LLMChat/Complete\n        LLMChat/Complete--&gt;&gt;Ollama: JSON response\n        deactivate LLMChat/Complete\n        Ollama--&gt;&gt;TextCompletionLLM: ChatResponse\n        deactivate Ollama\n\n        TextCompletionLLM-&gt;&gt;TextCompletionLLM: Extract message.content\n        Note over TextCompletionLLM: raw_output = chat_response.message.content\n    else is_chat_model == False\n        TextCompletionLLM-&gt;&gt;PromptTemplate: format(llm, test_input=\"hello\")\n        activate PromptTemplate\n        PromptTemplate--&gt;&gt;TextCompletionLLM: formatted prompt string\n        deactivate PromptTemplate\n\n        TextCompletionLLM-&gt;&gt;Ollama: complete(formatted_prompt, **llm_kwargs)\n        activate Ollama\n        Ollama-&gt;&gt;LLMChat/Complete: HTTP request to Ollama server\n        activate LLMChat/Complete\n        LLMChat/Complete--&gt;&gt;Ollama: JSON response\n        deactivate LLMChat/Complete\n        Ollama--&gt;&gt;TextCompletionLLM: CompletionResponse\n        deactivate Ollama\n\n        TextCompletionLLM-&gt;&gt;TextCompletionLLM: Extract text\n        Note over TextCompletionLLM: raw_output = response.text\n    end\n\n    TextCompletionLLM-&gt;&gt;PydanticParser: parse(raw_output)\n    activate PydanticParser\n    PydanticParser-&gt;&gt;PydanticParser: extract_json_str(raw_output)\n    Note over PydanticParser: Extracts JSON from text\n\n    PydanticParser-&gt;&gt;PydanticParser: model_validate_json(json_str)\n    Note over PydanticParser: Validates against ModelTest schema\n\n    PydanticParser--&gt;&gt;TextCompletionLLM: ModelTest instance\n    deactivate PydanticParser\n\n    TextCompletionLLM-&gt;&gt;TextCompletionLLM: Validate isinstance(output, ModelTest)\n    Note over TextCompletionLLM: Raises ValueError if type mismatch\n\n    TextCompletionLLM--&gt;&gt;User: ModelTest instance\n    deactivate TextCompletionLLM</code></pre>"},{"location":"reference/core/structured_tools/text_completion_llm/text_completion_llm_sequence/#key-points","title":"Key Points","text":"<ol> <li>Initialization validates all components before storing them</li> <li>Prompt formatting adapts based on whether the LLM is a chat model or completion model</li> <li>Output parsing extracts JSON and validates against the Pydantic schema</li> <li>Type checking ensures the parsed output matches the expected output class</li> </ol>"},{"location":"reference/core/structured_tools/text_completion_llm/text_completion_llm_state/","title":"Lifecycle States and Transitions","text":"<p>This diagram shows the lifecycle states of a <code>TextCompletionLLM</code> instance.</p>  Hold \"Ctrl\" to enable pan &amp; zoom  <pre><code>stateDiagram-v2\n    [*] --&gt; Uninitialized: User creates instance\n\n    Uninitialized --&gt; ValidatingParser: __init__ called\n    ValidatingParser --&gt; ValidatingLLM: Parser validated\n    ValidatingParser --&gt; InitError: Invalid parser type\n    ValidatingLLM --&gt; ValidatingPrompt: LLM validated\n    ValidatingLLM --&gt; InitError: No LLM available\n    ValidatingPrompt --&gt; Ready: Prompt validated\n    ValidatingPrompt --&gt; InitError: Invalid prompt type\n\n    Ready --&gt; ExecutingChat: __call__ invoked\\n(is_chat_model=True)\n    Ready --&gt; ExecutingComplete: __call__ invoked\\n(is_chat_model=False)\n    Ready --&gt; ExecutingAsync: acall invoked\n\n    ExecutingChat --&gt; FormattingMessages: Format prompt to messages\n    FormattingMessages --&gt; ExtendingMessages: Apply template vars\n    ExtendingMessages --&gt; CallingChatAPI: Add system prompts\n    CallingChatAPI --&gt; WaitingChatResponse: HTTP request sent\n    WaitingChatResponse --&gt; ParsingOutput: ChatResponse received\n    WaitingChatResponse --&gt; ExecutionError: Network/API error\n\n    ExecutingComplete --&gt; FormattingPrompt: Format prompt string\n    FormattingPrompt --&gt; ExtendingPrompt: Apply template vars\n    ExtendingPrompt --&gt; CallingCompleteAPI: Add system prompts\n    CallingCompleteAPI --&gt; WaitingCompleteResponse: HTTP request sent\n    WaitingCompleteResponse --&gt; ParsingOutput: CompletionResponse received\n    WaitingCompleteResponse --&gt; ExecutionError: Network/API error\n\n    ExecutingAsync --&gt; FormattingMessages: Similar to sync\n    ExecutingAsync --&gt; FormattingPrompt: but async\n\n    ParsingOutput --&gt; ExtractingJSON: Parse raw text\n    ExtractingJSON --&gt; ValidatingSchema: JSON extracted\n    ExtractingJSON --&gt; ParsingError: Malformed JSON\n    ValidatingSchema --&gt; TypeChecking: Pydantic validation passed\n    ValidatingSchema --&gt; ParsingError: Schema validation failed\n    TypeChecking --&gt; Completed: Type check passed\n    TypeChecking --&gt; ParsingError: Type mismatch\n\n    Completed --&gt; Ready: Return to caller\n    InitError --&gt; [*]: Exception raised\n    ExecutionError --&gt; [*]: Exception raised\n    ParsingError --&gt; [*]: Exception raised\n\n    note right of Ready\n        Instance is reusable\n        Can be called multiple times\n    end note\n\n    note right of ExecutingChat\n        Branch depends on\n        LLM.metadata.is_chat_model\n    end note\n\n    note right of ParsingOutput\n        Output parser extracts\n        structured data from\n        raw LLM response\n    end note\n\n    note right of Completed\n        Returns validated\n        Pydantic model instance\n        matching output_cls\n    end note</code></pre>"},{"location":"reference/core/structured_tools/text_completion_llm/text_completion_llm_state/#state-descriptions","title":"State Descriptions","text":""},{"location":"reference/core/structured_tools/text_completion_llm/text_completion_llm_state/#initialization-states","title":"Initialization States","text":"State Description Exit Conditions Uninitialized Object construction in progress <code>__init__</code> called ValidatingParser Checking output_parser and output_cls compatibility Valid: \u2192 ValidatingLLMInvalid: \u2192 InitError ValidatingLLM Ensuring LLM instance is available Valid: \u2192 ValidatingPromptMissing: \u2192 InitError ValidatingPrompt Converting/validating prompt template Valid: \u2192 ReadyInvalid: \u2192 InitError Ready Instance fully initialized and ready for calls User calls instance"},{"location":"reference/core/structured_tools/text_completion_llm/text_completion_llm_state/#execution-states-chat-path","title":"Execution States (Chat Path)","text":"State Description Exit Conditions ExecutingChat Entry point for chat model execution \u2192 FormattingMessages FormattingMessages Converting prompt to message list \u2192 ExtendingMessages ExtendingMessages Applying template variables to messages \u2192 CallingChatAPI CallingChatAPI Invoking Ollama.chat method \u2192 WaitingChatResponse WaitingChatResponse HTTP request in flight Success: \u2192 ParsingOutputError: \u2192 ExecutionError"},{"location":"reference/core/structured_tools/text_completion_llm/text_completion_llm_state/#execution-states-completion-path","title":"Execution States (Completion Path)","text":"State Description Exit Conditions ExecutingComplete Entry point for completion model execution \u2192 FormattingPrompt FormattingPrompt Converting prompt to string \u2192 ExtendingPrompt ExtendingPrompt Applying template variables to prompt \u2192 CallingCompleteAPI CallingCompleteAPI Invoking Ollama.complete method \u2192 WaitingCompleteResponse WaitingCompleteResponse HTTP request in flight Success: \u2192 ParsingOutputError: \u2192 ExecutionError"},{"location":"reference/core/structured_tools/text_completion_llm/text_completion_llm_state/#parsing-states","title":"Parsing States","text":"State Description Exit Conditions ParsingOutput Starting output parsing process \u2192 ExtractingJSON ExtractingJSON Extracting JSON from raw text Valid JSON: \u2192 ValidatingSchemaInvalid: \u2192 ParsingError ValidatingSchema Validating against Pydantic schema Valid: \u2192 TypeCheckingInvalid: \u2192 ParsingError TypeChecking Verifying output matches output_cls Match: \u2192 CompletedMismatch: \u2192 ParsingError Completed Successful execution with valid output \u2192 Ready (return result)"},{"location":"reference/core/structured_tools/text_completion_llm/text_completion_llm_state/#error-states","title":"Error States","text":"State Description Exceptions Raised InitError Initialization validation failed ValueError, AssertionError ExecutionError LLM execution failed ConnectionError, TimeoutError, HTTPError ParsingError Output parsing/validation failed ValueError, ValidationError"},{"location":"reference/core/structured_tools/text_completion_llm/text_completion_llm_state/#state-transitions-example","title":"State Transitions Example","text":""},{"location":"reference/core/structured_tools/text_completion_llm/text_completion_llm_state/#successful-execution-flow","title":"Successful Execution Flow","text":"<pre><code>[*]\n  \u2192 Uninitialized\n  \u2192 ValidatingParser\n  \u2192 ValidatingLLM\n  \u2192 ValidatingPrompt\n  \u2192 Ready\n  \u2192 ExecutingChat (or ExecutingComplete)\n  \u2192 FormattingMessages (or FormattingPrompt)\n  \u2192 ExtendingMessages (or ExtendingPrompt)\n  \u2192 CallingChatAPI (or CallingCompleteAPI)\n  \u2192 WaitingChatResponse (or WaitingCompleteResponse)\n  \u2192 ParsingOutput\n  \u2192 ExtractingJSON\n  \u2192 ValidatingSchema\n  \u2192 TypeChecking\n  \u2192 Completed\n  \u2192 Ready (reusable for next call)\n</code></pre>"},{"location":"reference/core/structured_tools/text_completion_llm/text_completion_llm_state/#error-during-initialization","title":"Error During Initialization","text":"<pre><code>[*]\n  \u2192 Uninitialized\n  \u2192 ValidatingParser\n  \u2192 ValidatingLLM\n  \u2192 InitError (no LLM provided and Configs.llm is None)\n  \u2192 [*] (AssertionError raised)\n</code></pre>"},{"location":"reference/core/structured_tools/text_completion_llm/text_completion_llm_state/#error-during-execution","title":"Error During Execution","text":"<pre><code>Ready\n  \u2192 ExecutingComplete\n  \u2192 FormattingPrompt\n  \u2192 ExtendingPrompt\n  \u2192 CallingCompleteAPI\n  \u2192 WaitingCompleteResponse\n  \u2192 ExecutionError (Ollama server not reachable)\n  \u2192 [*] (ConnectionError raised)\n</code></pre>"},{"location":"reference/core/structured_tools/text_completion_llm/text_completion_llm_state/#error-during-parsing","title":"Error During Parsing","text":"<pre><code>WaitingChatResponse\n  \u2192 ParsingOutput (raw_output: \"This is not JSON\")\n  \u2192 ExtractingJSON\n  \u2192 ParsingError (cannot find JSON in text)\n  \u2192 [*] (ValueError raised)\n</code></pre>"},{"location":"reference/core/structured_tools/text_completion_llm/text_completion_llm_state/#concurrency-notes","title":"Concurrency Notes","text":"<ul> <li>Thread-safe: The instance is reusable after reaching <code>Ready</code> state</li> <li>Stateless execution: Each <code>__call__</code> creates new transient state</li> <li>Immutable config: Parser, LLM, and output_cls don't change after initialization</li> <li>Async support: <code>acall</code> follows similar state transitions but with async operations</li> </ul>"},{"location":"reference/core/structured_tools/tool_orchestrating_llm/examples/","title":"ToolOrchestratingLLM Usage Examples","text":"<p>This guide provides comprehensive examples covering all possible ways to use <code>ToolOrchestratingLLM</code>.</p>"},{"location":"reference/core/structured_tools/tool_orchestrating_llm/examples/#table-of-contents","title":"Table of Contents","text":"<ol> <li>Basic Usage</li> <li>Initialization Patterns</li> <li>Prompt Formats</li> <li>Execution Modes</li> <li>Parallel Tool Calls</li> <li>Advanced Usage</li> <li>Error Handling</li> </ol>"},{"location":"reference/core/structured_tools/tool_orchestrating_llm/examples/#basic-usage","title":"Basic Usage","text":""},{"location":"reference/core/structured_tools/tool_orchestrating_llm/examples/#simple-string-prompt-with-variables","title":"Simple String Prompt with Variables","text":"<p>The most straightforward way to use <code>ToolOrchestratingLLM</code>:</p> <pre><code>from typing import List\nfrom pydantic import BaseModel\nfrom serapeum.core.structured_tools.tools_llm import ToolOrchestratingLLM\nfrom serapeum.llms.ollama import Ollama\n\n# Define your output schema\nclass Song(BaseModel):\n    title: str\n    duration: int  # in seconds\n\nclass Album(BaseModel):\n    title: str\n    artist: str\n    songs: List[Song]\n\n# Initialize the LLM with function calling support\nllm = Ollama(model=\"llama3.1\", request_timeout=80)\n\n# Create ToolOrchestratingLLM with string prompt\ntools_llm = ToolOrchestratingLLM(\n    output_cls=Album,\n    prompt=\"Create an album about {topic} with {num_songs} songs.\",\n    llm=llm,\n)\n\n# Execute with variables - LLM uses function calling to return structured data\nresult = tools_llm(topic=\"space exploration\", num_songs=3)\nprint(result.title)  # \"Journey Through the Cosmos\"\nprint(result.artist)  # \"The Astronomers\"\nprint(len(result.songs))  # 3\n</code></pre>"},{"location":"reference/core/structured_tools/tool_orchestrating_llm/examples/#initialization-patterns","title":"Initialization Patterns","text":""},{"location":"reference/core/structured_tools/tool_orchestrating_llm/examples/#1-with-explicit-llm","title":"1. With Explicit LLM","text":"<p>Provide a fully configured function-calling LLM:</p> <pre><code>from typing import List\nfrom pydantic import BaseModel, Field\nfrom serapeum.core.structured_tools.tools_llm import ToolOrchestratingLLM\nfrom serapeum.llms.ollama import Ollama\n\nclass Task(BaseModel):\n    id: str = Field(description=\"Task identifier\")\n    description: str = Field(description=\"Task description\")\n    priority: int = Field(description=\"Priority level 1-5\", ge=1, le=5)\n    subtasks: List[str] = Field(description=\"List of subtasks\")\n\n# Initialize Ollama with function calling support\nllm = Ollama(\n    model=\"llama3.1\",\n    request_timeout=80,\n    temperature=0.7,\n)\n\ntools_llm = ToolOrchestratingLLM(\n    output_cls=Task,\n    prompt=\"Break down this project into tasks: {project}\",\n    llm=llm,\n)\n\nresult = tools_llm(project=\"Build a web application\")\n# Returns: Task with properly structured data via function calling\n</code></pre>"},{"location":"reference/core/structured_tools/tool_orchestrating_llm/examples/#2-using-global-llm-from-configs","title":"2. Using Global LLM from Configs","text":"<p>Set a default function-calling LLM for the entire application:</p> <pre><code>from pydantic import BaseModel\nfrom serapeum.core.configs.configs import Configs\nfrom serapeum.core.structured_tools.tools_llm import ToolOrchestratingLLM\nfrom serapeum.llms.ollama import Ollama\n\n# Set global LLM\nConfigs.llm = Ollama(model=\"llama3.1\", request_timeout=80)\n\nclass Entity(BaseModel):\n    name: str\n    type: str\n    properties: dict\n\n# No need to pass llm parameter\ntools_llm = ToolOrchestratingLLM(\n    output_cls=Entity,\n    prompt=\"Extract entity from: {text}\",\n    # llm=None uses Configs.llm by default\n)\n\nresult = tools_llm(text=\"Apple Inc. is a technology company\")\n# Returns: Entity(name=\"Apple Inc.\", type=\"company\", ...)\n</code></pre>"},{"location":"reference/core/structured_tools/tool_orchestrating_llm/examples/#3-with-tool-choice-strategy","title":"3. With Tool Choice Strategy","text":"<p>Control which tool the LLM should use:</p> <pre><code>from pydantic import BaseModel\nfrom serapeum.core.structured_tools.tools_llm import ToolOrchestratingLLM\nfrom serapeum.llms.ollama import Ollama\n\nclass Response(BaseModel):\n    answer: str\n    confidence: float\n\nllm = Ollama(model=\"llama3.1\", request_timeout=80)\n\n# Force the LLM to use the tool\ntools_llm = ToolOrchestratingLLM(\n    output_cls=Response,\n    prompt=\"Answer: {question}\",\n    llm=llm,\n    tool_choice=\"auto\",  # or \"required\" to force tool use\n)\n\nresult = tools_llm(question=\"What is Python?\")\n</code></pre>"},{"location":"reference/core/structured_tools/tool_orchestrating_llm/examples/#prompt-formats","title":"Prompt Formats","text":""},{"location":"reference/core/structured_tools/tool_orchestrating_llm/examples/#1-string-prompt-auto-converted-to-prompttemplate","title":"1. String Prompt (Auto-converted to PromptTemplate)","text":"<p>Simple string prompts are automatically wrapped in <code>PromptTemplate</code>:</p> <pre><code>from typing import List\nfrom pydantic import BaseModel\nfrom serapeum.core.structured_tools.tools_llm import ToolOrchestratingLLM\nfrom serapeum.llms.ollama import Ollama\n\nclass Recipe(BaseModel):\n    name: str\n    ingredients: List[str]\n    steps: List[str]\n    prep_time: int\n\nllm = Ollama(model=\"llama3.1\", request_timeout=80)\n\ntools_llm = ToolOrchestratingLLM(\n    output_cls=Recipe,\n    prompt=\"Create a {cuisine} recipe for {dish}\",  # String prompt\n    llm=llm,\n)\n\nresult = tools_llm(cuisine=\"Italian\", dish=\"pasta\")\n</code></pre>"},{"location":"reference/core/structured_tools/tool_orchestrating_llm/examples/#2-prompttemplate-object","title":"2. PromptTemplate Object","text":"<p>Use <code>PromptTemplate</code> for more control:</p> <pre><code>from pydantic import BaseModel\nfrom serapeum.core.prompts.base import PromptTemplate\nfrom serapeum.core.structured_tools.tools_llm import ToolOrchestratingLLM\nfrom serapeum.llms.ollama import Ollama\n\nclass Analysis(BaseModel):\n    sentiment: str\n    topics: list[str]\n    summary: str\n\nllm = Ollama(model=\"llama3.1\", request_timeout=80)\n\n# Create explicit PromptTemplate\nprompt_template = PromptTemplate(\n    \"Analyze this text: {text}\\n\\nProvide sentiment, topics, and summary.\"\n)\n\ntools_llm = ToolOrchestratingLLM(\n    output_cls=Analysis,\n    prompt=prompt_template,\n    llm=llm,\n)\n\nresult = tools_llm(text=\"AI is transforming industries worldwide\")\n</code></pre>"},{"location":"reference/core/structured_tools/tool_orchestrating_llm/examples/#3-chatprompttemplate-with-messages","title":"3. ChatPromptTemplate with Messages","text":"<p>Use structured message templates for complex prompts:</p> <pre><code>from pydantic import BaseModel\nfrom serapeum.core.base.llms.models import Message, MessageRole\nfrom serapeum.core.prompts import ChatPromptTemplate\nfrom serapeum.core.structured_tools.tools_llm import ToolOrchestratingLLM\nfrom serapeum.llms.ollama import Ollama\n\nclass CodeReview(BaseModel):\n    issues: list[str]\n    suggestions: list[str]\n    rating: int  # 1-10\n\nllm = Ollama(model=\"llama3.1\", request_timeout=80)\n\n# Create message templates\nmessages = [\n    Message(\n        role=MessageRole.SYSTEM,\n        content=\"You are an expert code reviewer.\"\n    ),\n    Message(\n        role=MessageRole.USER,\n        content=\"Review this {language} code:\\n\\n{code}\"\n    ),\n]\n\nprompt = ChatPromptTemplate(message_templates=messages)\n\ntools_llm = ToolOrchestratingLLM(\n    output_cls=CodeReview,\n    prompt=prompt,\n    llm=llm,\n)\n\nresult = tools_llm(language=\"Python\", code=\"def foo(): pass\")\n</code></pre>"},{"location":"reference/core/structured_tools/tool_orchestrating_llm/examples/#execution-modes","title":"Execution Modes","text":""},{"location":"reference/core/structured_tools/tool_orchestrating_llm/examples/#1-synchronous-execution","title":"1. Synchronous Execution","text":"<p>Standard blocking execution:</p> <pre><code>from pydantic import BaseModel\nfrom serapeum.core.structured_tools.tools_llm import ToolOrchestratingLLM\nfrom serapeum.llms.ollama import Ollama\n\nclass Summary(BaseModel):\n    main_points: list[str]\n    conclusion: str\n\nllm = Ollama(model=\"llama3.1\", request_timeout=80)\n\ntools_llm = ToolOrchestratingLLM(\n    output_cls=Summary,\n    prompt=\"Summarize: {text}\",\n    llm=llm,\n)\n\n# Synchronous call using __call__\nresult = tools_llm(text=\"Long article text here...\")\nprint(result.main_points)\nprint(result.conclusion)\n</code></pre>"},{"location":"reference/core/structured_tools/tool_orchestrating_llm/examples/#2-asynchronous-execution","title":"2. Asynchronous Execution","text":"<p>Non-blocking async execution:</p> <pre><code>import asyncio\nfrom pydantic import BaseModel\nfrom serapeum.core.structured_tools.tools_llm import ToolOrchestratingLLM\nfrom serapeum.llms.ollama import Ollama\n\nclass Classification(BaseModel):\n    category: str\n    subcategory: str\n    confidence: float\n\nllm = Ollama(model=\"llama3.1\", request_timeout=80)\n\ntools_llm = ToolOrchestratingLLM(\n    output_cls=Classification,\n    prompt=\"Classify: {item}\",\n    llm=llm,\n)\n\nasync def classify_item(item: str) -&gt; Classification:\n    # Asynchronous call using acall\n    result = await tools_llm.acall(item=item)\n    return result\n\n# Run async function\nresult = asyncio.run(classify_item(\"Laptop computer\"))\nprint(f\"{result.category} &gt; {result.subcategory}\")\n</code></pre>"},{"location":"reference/core/structured_tools/tool_orchestrating_llm/examples/#3-batch-processing-with-async","title":"3. Batch Processing with Async","text":"<p>Process multiple inputs concurrently:</p> <pre><code>import asyncio\nfrom typing import List\nfrom pydantic import BaseModel\nfrom serapeum.core.structured_tools.tools_llm import ToolOrchestratingLLM\nfrom serapeum.llms.ollama import Ollama\n\nclass EntityExtraction(BaseModel):\n    entities: List[str]\n    entity_types: List[str]\n\nllm = Ollama(model=\"llama3.1\", request_timeout=80)\n\ntools_llm = ToolOrchestratingLLM(\n    output_cls=EntityExtraction,\n    prompt=\"Extract entities from: {text}\",\n    llm=llm,\n)\n\nasync def extract_batch(texts: List[str]) -&gt; List[EntityExtraction]:\n    tasks = [tools_llm.acall(text=text) for text in texts]\n    results = await asyncio.gather(*tasks)\n    return results\n\ntexts = [\n    \"Apple Inc. is in California\",\n    \"Microsoft was founded by Bill Gates\",\n    \"Paris is the capital of France\"\n]\nresults = asyncio.run(extract_batch(texts))\nfor text, result in zip(texts, results):\n    print(f\"{text}: {result.entities}\")\n</code></pre>"},{"location":"reference/core/structured_tools/tool_orchestrating_llm/examples/#4-streaming-execution","title":"4. Streaming Execution","text":"<p>Stream progressive updates:</p> <pre><code>from pydantic import BaseModel\nfrom serapeum.core.structured_tools.tools_llm import ToolOrchestratingLLM\nfrom serapeum.llms.ollama import Ollama\n\nclass Article(BaseModel):\n    title: str\n    sections: list[str]\n    word_count: int\n\nllm = Ollama(model=\"llama3.1\", request_timeout=80)\n\ntools_llm = ToolOrchestratingLLM(\n    output_cls=Article,\n    prompt=\"Write an article about {topic}\",\n    llm=llm,\n)\n\n# Stream results as they arrive\nfor partial_article in tools_llm.stream_call(topic=\"AI\"):\n    print(f\"Current title: {partial_article.title}\")\n    print(f\"Sections so far: {len(partial_article.sections)}\")\n    # Display progressive updates in UI\n</code></pre>"},{"location":"reference/core/structured_tools/tool_orchestrating_llm/examples/#5-async-streaming","title":"5. Async Streaming","text":"<p>Async version of streaming:</p> <pre><code>import asyncio\nfrom pydantic import BaseModel\nfrom serapeum.core.structured_tools.tools_llm import ToolOrchestratingLLM\nfrom serapeum.llms.ollama import Ollama\n\nclass Report(BaseModel):\n    title: str\n    findings: list[str]\n    recommendations: list[str]\n\nllm = Ollama(model=\"llama3.1\", request_timeout=80)\n\ntools_llm = ToolOrchestratingLLM(\n    output_cls=Report,\n    prompt=\"Generate report on {subject}\",\n    llm=llm,\n)\n\nasync def stream_report(subject: str):\n    stream = await tools_llm.astream_call(subject=subject)\n    async for partial_report in stream:\n        print(f\"Findings: {len(partial_report.findings)}\")\n        print(f\"Recommendations: {len(partial_report.recommendations)}\")\n\nasyncio.run(stream_report(\"Market analysis\"))\n</code></pre>"},{"location":"reference/core/structured_tools/tool_orchestrating_llm/examples/#6-passing-llm-specific-parameters","title":"6. Passing LLM-specific Parameters","text":"<p>Forward parameters directly to the LLM:</p> <pre><code>from pydantic import BaseModel\nfrom serapeum.core.structured_tools.tools_llm import ToolOrchestratingLLM\nfrom serapeum.llms.ollama import Ollama\n\nclass Story(BaseModel):\n    title: str\n    plot: str\n    characters: list[str]\n\nllm = Ollama(model=\"llama3.1\", request_timeout=80)\n\ntools_llm = ToolOrchestratingLLM(\n    output_cls=Story,\n    prompt=\"Write a {genre} story\",\n    llm=llm,\n)\n\n# Pass LLM-specific kwargs\nresult = tools_llm(\n    llm_kwargs={\n        \"temperature\": 0.9,  # Higher for creativity\n        \"top_p\": 0.95,\n        \"max_tokens\": 1000,\n    },\n    genre=\"science fiction\"\n)\n</code></pre>"},{"location":"reference/core/structured_tools/tool_orchestrating_llm/examples/#parallel-tool-calls","title":"Parallel Tool Calls","text":""},{"location":"reference/core/structured_tools/tool_orchestrating_llm/examples/#single-output-default","title":"Single Output (Default)","text":"<p>By default, only one tool call is expected:</p> <pre><code>from pydantic import BaseModel\nfrom serapeum.core.structured_tools.tools_llm import ToolOrchestratingLLM\nfrom serapeum.llms.ollama import Ollama\n\nclass Product(BaseModel):\n    name: str\n    price: float\n    description: str\n\nllm = Ollama(model=\"llama3.1\", request_timeout=80)\n\ntools_llm = ToolOrchestratingLLM(\n    output_cls=Product,\n    prompt=\"Extract product info: {text}\",\n    llm=llm,\n    allow_parallel_tool_calls=False,  # Default\n)\n\n# Returns single Product instance\nresult = tools_llm(text=\"iPhone 15 costs $999\")\nprint(type(result))  # &lt;class 'Product'&gt;\n</code></pre>"},{"location":"reference/core/structured_tools/tool_orchestrating_llm/examples/#multiple-outputs-parallel","title":"Multiple Outputs (Parallel)","text":"<p>Enable parallel tool calls to receive multiple objects:</p> <pre><code>from typing import List\nfrom pydantic import BaseModel\nfrom serapeum.core.structured_tools.tools_llm import ToolOrchestratingLLM\nfrom serapeum.llms.ollama import Ollama\n\nclass Item(BaseModel):\n    name: str\n    category: str\n\nllm = Ollama(model=\"llama3.1\", request_timeout=80)\n\ntools_llm = ToolOrchestratingLLM(\n    output_cls=Item,\n    prompt=\"Extract all items from this list: {text}\",\n    llm=llm,\n    allow_parallel_tool_calls=True,  # Enable parallel calls\n)\n\n# Returns List[Item]\nresults = tools_llm(text=\"apples, laptops, books, phones\")\nprint(type(results))  # &lt;class 'list'&gt;\nprint(len(results))   # 4\nfor item in results:\n    print(f\"{item.name}: {item.category}\")\n</code></pre>"},{"location":"reference/core/structured_tools/tool_orchestrating_llm/examples/#parallel-with-streaming","title":"Parallel with Streaming","text":"<p>Stream multiple objects as they're generated:</p> <pre><code>from pydantic import BaseModel\nfrom serapeum.core.structured_tools.tools_llm import ToolOrchestratingLLM\nfrom serapeum.llms.ollama import Ollama\n\nclass Question(BaseModel):\n    question: str\n    difficulty: str\n\nllm = Ollama(model=\"llama3.1\", request_timeout=80)\n\ntools_llm = ToolOrchestratingLLM(\n    output_cls=Question,\n    prompt=\"Generate 5 questions about {topic}\",\n    llm=llm,\n    allow_parallel_tool_calls=True,\n)\n\n# Stream list of questions as they arrive\nfor questions_so_far in tools_llm.stream_call(topic=\"Python\"):\n    if isinstance(questions_so_far, list):\n        print(f\"Questions generated: {len(questions_so_far)}\")\n        # Show latest question\n        if questions_so_far:\n            latest = questions_so_far[-1]\n            print(f\"  Latest: {latest.question}\")\n</code></pre>"},{"location":"reference/core/structured_tools/tool_orchestrating_llm/examples/#advanced-usage","title":"Advanced Usage","text":""},{"location":"reference/core/structured_tools/tool_orchestrating_llm/examples/#1-dynamic-prompt-updates","title":"1. Dynamic Prompt Updates","text":"<p>Change the prompt at runtime:</p> <pre><code>from pydantic import BaseModel\nfrom serapeum.core.prompts.base import PromptTemplate\nfrom serapeum.core.structured_tools.tools_llm import ToolOrchestratingLLM\nfrom serapeum.llms.ollama import Ollama\n\nclass Response(BaseModel):\n    answer: str\n    reasoning: str\n\nllm = Ollama(model=\"llama3.1\", request_timeout=80)\n\ntools_llm = ToolOrchestratingLLM(\n    output_cls=Response,\n    prompt=\"Answer briefly: {question}\",\n    llm=llm,\n)\n\n# Use with initial prompt\nresult1 = tools_llm(question=\"What is AI?\")\n\n# Update prompt dynamically\ntools_llm.prompt = PromptTemplate(\"Answer in detail: {question}\")\n\n# Use with new prompt\nresult2 = tools_llm(question=\"What is AI?\")\n</code></pre>"},{"location":"reference/core/structured_tools/tool_orchestrating_llm/examples/#2-reusable-instance-pattern","title":"2. Reusable Instance Pattern","text":"<p>Create once, use many times:</p> <pre><code>from pydantic import BaseModel\nfrom serapeum.core.structured_tools.tools_llm import ToolOrchestratingLLM\nfrom serapeum.llms.ollama import Ollama\n\nclass Sentiment(BaseModel):\n    sentiment: str  # positive, negative, neutral\n    confidence: float\n    keywords: list[str]\n\nllm = Ollama(model=\"llama3.1\", request_timeout=80)\n\n# Create reusable instance\nsentiment_analyzer = ToolOrchestratingLLM(\n    output_cls=Sentiment,\n    prompt=\"Analyze sentiment of: {text}\",\n    llm=llm,\n)\n\n# Reuse multiple times\nreviews = [\n    \"This product is amazing!\",\n    \"Terrible experience, very disappointed\",\n    \"It's okay, nothing special\",\n]\n\nfor review in reviews:\n    analysis = sentiment_analyzer(text=review)\n    print(f\"{review[:20]}... \u2192 {analysis.sentiment} ({analysis.confidence})\")\n</code></pre>"},{"location":"reference/core/structured_tools/tool_orchestrating_llm/examples/#3-complex-nested-models","title":"3. Complex Nested Models","text":"<p>Use deeply nested Pydantic models:</p> <pre><code>from typing import List\nfrom pydantic import BaseModel, Field\nfrom serapeum.core.structured_tools.tools_llm import ToolOrchestratingLLM\nfrom serapeum.llms.ollama import Ollama\n\nclass Author(BaseModel):\n    name: str\n    email: str\n\nclass Comment(BaseModel):\n    author: Author\n    text: str\n    upvotes: int\n\nclass Article(BaseModel):\n    title: str\n    content: str\n    author: Author\n    tags: List[str]\n    comments: List[Comment]\n\nllm = Ollama(model=\"llama3.1\", request_timeout=80)\n\ntools_llm = ToolOrchestratingLLM(\n    output_cls=Article,\n    prompt=\"Create a blog article about {topic} with comments\",\n    llm=llm,\n)\n\nresult = tools_llm(topic=\"Machine Learning\")\nprint(result.title)\nprint(result.author.name)\nprint(f\"Tags: {', '.join(result.tags)}\")\nprint(f\"Comments: {len(result.comments)}\")\nfor comment in result.comments:\n    print(f\"  - {comment.author.name}: {comment.text[:50]}...\")\n</code></pre>"},{"location":"reference/core/structured_tools/tool_orchestrating_llm/examples/#4-using-with-verbose-mode","title":"4. Using with Verbose Mode","text":"<p>Enable detailed logging:</p> <pre><code>from pydantic import BaseModel\nfrom serapeum.core.structured_tools.tools_llm import ToolOrchestratingLLM\nfrom serapeum.llms.ollama import Ollama\n\nclass Data(BaseModel):\n    result: str\n\nllm = Ollama(model=\"llama3.1\", request_timeout=80)\n\ntools_llm = ToolOrchestratingLLM(\n    output_cls=Data,\n    prompt=\"Process: {input}\",\n    llm=llm,\n    verbose=True,  # Enable verbose logging\n)\n\n# Will log detailed information about tool calls\nresult = tools_llm(input=\"test data\")\n</code></pre>"},{"location":"reference/core/structured_tools/tool_orchestrating_llm/examples/#5-custom-tool-choice","title":"5. Custom Tool Choice","text":"<p>Control tool selection:</p> <pre><code>from pydantic import BaseModel\nfrom serapeum.core.structured_tools.tools_llm import ToolOrchestratingLLM\nfrom serapeum.llms.ollama import Ollama\n\nclass Output(BaseModel):\n    data: str\n\nllm = Ollama(model=\"llama3.1\", request_timeout=80)\n\n# Force the LLM to always use the tool\ntools_llm = ToolOrchestratingLLM(\n    output_cls=Output,\n    prompt=\"Generate output for: {input}\",\n    llm=llm,\n    tool_choice=\"required\",  # Force tool use\n)\n\nresult = tools_llm(input=\"test\")\n</code></pre>"},{"location":"reference/core/structured_tools/tool_orchestrating_llm/examples/#using-regular-functions-with-toolorchestratingllm","title":"Using Regular Functions with ToolOrchestratingLLM","text":"<p><code>ToolOrchestratingLLM</code> now supports both Pydantic models and regular Python functions as <code>output_cls</code>. When you pass a function, the system automatically detects it and creates the appropriate tool.</p>"},{"location":"reference/core/structured_tools/tool_orchestrating_llm/examples/#1-using-regular-functions","title":"1. Using Regular Functions","text":"<p>Pass regular Python functions directly as <code>output_cls</code>:</p> <pre><code>from serapeum.core.structured_tools.tools_llm import ToolOrchestratingLLM\nfrom serapeum.llms.ollama import Ollama\n\ndef calculate_statistics(numbers: list[float], operation: str) -&gt; dict[str, float]:\n    \"\"\"Calculate statistics on a list of numbers.\n\n    Args:\n        numbers: List of numbers to analyze\n        operation: Type of operation (mean, sum, max, min)\n\n    Returns:\n        Dictionary with the result\n    \"\"\"\n    if operation == \"mean\":\n        result = sum(numbers) / len(numbers)\n    elif operation == \"sum\":\n        result = sum(numbers)\n    elif operation == \"max\":\n        result = max(numbers)\n    elif operation == \"min\":\n        result = min(numbers)\n    else:\n        raise ValueError(f\"Unknown operation: {operation}\")\n\n    return {\n        \"operation\": operation,\n        \"result\": result,\n        \"count\": len(numbers)\n    }\n\n# Use function directly with ToolOrchestratingLLM\nllm = Ollama(model=\"llama3.1\", request_timeout=80)\n\ntools_llm = ToolOrchestratingLLM(\n    output_cls=calculate_statistics,  # Pass function directly!\n    prompt=\"Calculate the mean of these numbers: {text}\",\n    llm=llm,\n)\n\n# Call with input\nresult = tools_llm(text=\"10, 20, 30, 40, 50\")\n\nprint(f\"Operation: {result['operation']}\")\nprint(f\"Result: {result['result']}\")\nprint(f\"Count: {result['count']}\")\n</code></pre>"},{"location":"reference/core/structured_tools/tool_orchestrating_llm/examples/#2-using-functions-with-regular-classes","title":"2. Using Functions with Regular Classes","text":"<p>Wrap regular Python classes in functions and use with ToolOrchestratingLLM:</p> <pre><code>from serapeum.core.structured_tools.tools_llm import ToolOrchestratingLLM\nfrom serapeum.llms.ollama import Ollama\n\nclass EmailValidator:\n    \"\"\"Regular Python class for email validation.\"\"\"\n\n    def __init__(self, email: str, check_mx: bool = False):\n        self.email = email\n        self.check_mx = check_mx\n        self.is_valid = self._validate()\n\n    def _validate(self) -&gt; bool:\n        \"\"\"Simple email validation.\"\"\"\n        return \"@\" in self.email and \".\" in self.email.split(\"@\")[1]\n\n    def to_dict(self) -&gt; dict:\n        return {\n            \"email\": self.email,\n            \"is_valid\": self.is_valid,\n            \"check_mx\": self.check_mx\n        }\n\ndef validate_email(email: str, check_mx: bool = False) -&gt; dict:\n    \"\"\"Validate an email address.\n\n    Args:\n        email: Email address to validate\n        check_mx: Whether to check MX records (not implemented)\n\n    Returns:\n        Validation result dictionary\n    \"\"\"\n    validator = EmailValidator(email, check_mx)\n    return validator.to_dict()\n\n# Use function with ToolOrchestratingLLM\nllm = Ollama(model=\"llama3.1\", request_timeout=80)\n\ntools_llm = ToolOrchestratingLLM(\n    output_cls=validate_email,  # Pass function that uses the class\n    prompt=\"Validate this email: {email_text}\",\n    llm=llm,\n)\n\nresult = tools_llm(email_text=\"user@example.com\")\n\nprint(f\"Email: {result['email']}\")\nprint(f\"Valid: {result['is_valid']}\")\n</code></pre>"},{"location":"reference/core/structured_tools/tool_orchestrating_llm/examples/#3-factory-functions-with-dataclasses","title":"3. Factory Functions with Dataclasses","text":"<p>Use factory functions that return dataclass instances:</p> <pre><code>from dataclasses import dataclass\nfrom serapeum.core.structured_tools.tools_llm import ToolOrchestratingLLM\nfrom serapeum.llms.ollama import Ollama\n\n@dataclass\nclass Product:\n    \"\"\"Regular dataclass (not Pydantic).\"\"\"\n    name: str\n    price: float\n    category: str\n    in_stock: bool\n\n    def to_dict(self) -&gt; dict:\n        return {\n            \"name\": self.name,\n            \"price\": self.price,\n            \"category\": self.category,\n            \"in_stock\": self.in_stock\n        }\n\ndef create_product(name: str, price: float, category: str, in_stock: bool = True) -&gt; dict:\n    \"\"\"Create a product from parameters.\n\n    Args:\n        name: Product name\n        price: Product price in USD\n        category: Product category\n        in_stock: Whether product is in stock\n\n    Returns:\n        Product data as dictionary\n    \"\"\"\n    product = Product(name, price, category, in_stock)\n    return product.to_dict()\n\n# Use factory function with ToolOrchestratingLLM\nllm = Ollama(model=\"llama3.1\", request_timeout=80)\n\ntools_llm = ToolOrchestratingLLM(\n    output_cls=create_product,  # Pass factory function\n    prompt=\"Create a product entry for: {product_info}\",\n    llm=llm,\n)\n\nresult = tools_llm(product_info=\"Laptop, $999, Electronics, available\")\n\nprint(f\"Product: {result['name']}\")\nprint(f\"Price: ${result['price']}\")\nprint(f\"Category: {result['category']}\")\n</code></pre>"},{"location":"reference/core/structured_tools/tool_orchestrating_llm/examples/#4-async-functions","title":"4. Async Functions","text":"<p>Use async functions directly with ToolOrchestratingLLM:</p> <pre><code>import asyncio\nfrom serapeum.core.structured_tools.tools_llm import ToolOrchestratingLLM\nfrom serapeum.llms.ollama import Ollama\n\nasync def fetch_user_data(user_id: int, include_posts: bool = False) -&gt; dict:\n    \"\"\"Asynchronously fetch user data.\n\n    Args:\n        user_id: User ID to fetch\n        include_posts: Whether to include user posts\n\n    Returns:\n        User data dictionary\n    \"\"\"\n    # Simulate async API call\n    await asyncio.sleep(0.1)\n\n    user_data = {\n        \"user_id\": user_id,\n        \"username\": f\"user_{user_id}\",\n        \"email\": f\"user{user_id}@example.com\"\n    }\n\n    if include_posts:\n        user_data[\"posts\"] = [\n            {\"id\": 1, \"title\": \"First post\"},\n            {\"id\": 2, \"title\": \"Second post\"}\n        ]\n\n    return user_data\n\n# Use async function with ToolOrchestratingLLM\nasync def main():\n    llm = Ollama(model=\"llama3.1\", request_timeout=80)\n\n    tools_llm = ToolOrchestratingLLM(\n        output_cls=fetch_user_data,  # Pass async function\n        prompt=\"Fetch data for user ID {user_id_text} with their posts\",\n        llm=llm,\n    )\n\n    # Use acall for async execution\n    result = await tools_llm.acall(user_id_text=\"42\")\n\n    print(f\"User: {result['username']}\")\n    print(f\"Email: {result['email']}\")\n    if \"posts\" in result:\n        print(f\"Posts: {len(result['posts'])}\")\n\n# Run async example\nasyncio.run(main())\n</code></pre>"},{"location":"reference/core/structured_tools/tool_orchestrating_llm/examples/#5-lambda-functions","title":"5. Lambda Functions","text":"<p>Use lambda functions for simple transformations:</p> <pre><code>from serapeum.core.structured_tools.tools_llm import ToolOrchestratingLLM\nfrom serapeum.llms.ollama import Ollama\n\n# Simple lambda function for temperature conversion\nconvert_temp = lambda celsius: {\n    \"celsius\": celsius,\n    \"fahrenheit\": (celsius * 9/5) + 32,\n    \"kelvin\": celsius + 273.15\n}\n\nllm = Ollama(model=\"llama3.1\", request_timeout=80)\n\ntools_llm = ToolOrchestratingLLM(\n    output_cls=convert_temp,  # Pass lambda function\n    prompt=\"Convert {temperature} degrees Celsius\",\n    llm=llm,\n)\n\nresult = tools_llm(temperature=\"25\")\n\nprint(f\"Celsius: {result['celsius']}\")\nprint(f\"Fahrenheit: {result['fahrenheit']}\")\nprint(f\"Kelvin: {result['kelvin']}\")\n</code></pre> <p>Note: Lambda functions work but have limitations: - No docstring for the LLM to understand the function - Parameters aren't well-documented - Better to use regular functions with proper documentation for complex cases</p>"},{"location":"reference/core/structured_tools/tool_orchestrating_llm/examples/#6-functions-with-complex-return-types","title":"6. Functions with Complex Return Types","text":"<p>Use functions that return complex data structures:</p> <pre><code>from serapeum.core.structured_tools.tools_llm import ToolOrchestratingLLM\nfrom serapeum.llms.ollama import Ollama\n\ndef analyze_text(text: str, language: str = \"en\") -&gt; dict:\n    \"\"\"Analyze text and return basic metrics.\n\n    Args:\n        text: Text to analyze\n        language: Language code\n\n    Returns:\n        Analysis metrics including word count, character count, and average word length\n    \"\"\"\n    words = text.split()\n    return {\n        \"word_count\": len(words),\n        \"char_count\": len(text),\n        \"language\": language,\n        \"avg_word_length\": sum(len(w) for w in words) / len(words) if words else 0\n    }\n\nllm = Ollama(model=\"llama3.1\", request_timeout=80)\n\ntools_llm = ToolOrchestratingLLM(\n    output_cls=analyze_text,  # Pass function with complex return\n    prompt=\"Analyze this text: {text_input}\",\n    llm=llm,\n)\n\nresult = tools_llm(text_input=\"Hello world, this is a test message.\")\n\nprint(f\"Word count: {result['word_count']}\")\nprint(f\"Character count: {result['char_count']}\")\nprint(f\"Average word length: {result['avg_word_length']:.2f}\")\n</code></pre>"},{"location":"reference/core/structured_tools/tool_orchestrating_llm/examples/#important-notes","title":"Important Notes","text":"<p>Advantages of using functions as output_cls: 1. Works with existing Python functions - no need to convert to Pydantic 2. Full access to all <code>ToolOrchestratingLLM</code> features (streaming, async, etc.) 3. Automatic tool creation and orchestration 4. Simple and direct - just pass your function</p> <p>When to use functions vs Pydantic models: - Use functions when:   - You have existing functions you want to reuse   - You need simple dict/list returns   - You're prototyping quickly   - Working with legacy code</p> <ul> <li>Use Pydantic models when:</li> <li>You need strict validation of outputs</li> <li>You want better type safety and IDE support</li> <li>Building production systems with clear schemas</li> <li>Need automatic documentation from models</li> </ul> <p>Both approaches work equally well with <code>ToolOrchestratingLLM</code> - choose based on your needs!</p>"},{"location":"reference/core/structured_tools/tool_orchestrating_llm/examples/#error-handling","title":"Error Handling","text":""},{"location":"reference/core/structured_tools/tool_orchestrating_llm/examples/#1-handling-llm-validation-errors","title":"1. Handling LLM Validation Errors","text":"<p>Catch initialization errors:</p> <pre><code>from pydantic import BaseModel\nfrom serapeum.core.structured_tools.tools_llm import ToolOrchestratingLLM\nfrom serapeum.llms.ollama import Ollama\n\nclass Data(BaseModel):\n    value: str\n\n# Create LLM that doesn't support function calling (hypothetically)\n# llm = SomeLLM(...)  # without function calling support\n\ntry:\n    tools_llm = ToolOrchestratingLLM(\n        output_cls=Data,\n        prompt=\"Process: {input}\",\n        llm=None,  # No LLM provided\n    )\nexcept AssertionError as e:\n    print(\"LLM must be provided or set in Configs\")\nexcept ValueError as e:\n    print(f\"LLM doesn't support function calling: {e}\")\n</code></pre>"},{"location":"reference/core/structured_tools/tool_orchestrating_llm/examples/#2-handling-tool-execution-errors","title":"2. Handling Tool Execution Errors","text":"<p>Handle runtime errors:</p> <pre><code>from pydantic import BaseModel, ValidationError\nfrom serapeum.core.structured_tools.tools_llm import ToolOrchestratingLLM\nfrom serapeum.llms.ollama import Ollama\n\nclass StrictData(BaseModel):\n    number: int  # Must be integer\n    ratio: float  # Must be float\n\nllm = Ollama(model=\"llama3.1\", request_timeout=80)\n\ntools_llm = ToolOrchestratingLLM(\n    output_cls=StrictData,\n    prompt=\"Extract numbers from: {text}\",\n    llm=llm,\n)\n\ntry:\n    result = tools_llm(text=\"Some text with invalid data\")\nexcept ValidationError as e:\n    print(f\"Validation failed: {e}\")\n    # LLM generated invalid tool arguments\nexcept ValueError as e:\n    print(f\"Error: {e}\")\n    # Other errors (network, timeout, etc.)\n</code></pre>"},{"location":"reference/core/structured_tools/tool_orchestrating_llm/examples/#3-retry-logic","title":"3. Retry Logic","text":"<p>Implement retry logic for robustness:</p> <pre><code>import asyncio\nfrom pydantic import BaseModel\nfrom serapeum.core.structured_tools.tools_llm import ToolOrchestratingLLM\nfrom serapeum.llms.ollama import Ollama\n\nclass Result(BaseModel):\n    data: str\n\nllm = Ollama(model=\"llama3.1\", request_timeout=80)\n\ntools_llm = ToolOrchestratingLLM(\n    output_cls=Result,\n    prompt=\"Process: {input}\",\n    llm=llm,\n)\n\nasync def call_with_retry(\n    tools_llm: ToolOrchestratingLLM,\n    max_retries: int = 3,\n    **kwargs\n) -&gt; Result:\n    \"\"\"Call ToolOrchestratingLLM with retry logic.\"\"\"\n    for attempt in range(max_retries):\n        try:\n            return await tools_llm.acall(**kwargs)\n        except Exception as e:\n            if attempt == max_retries - 1:\n                raise\n            print(f\"Attempt {attempt + 1} failed: {e}. Retrying...\")\n            await asyncio.sleep(2 ** attempt)  # Exponential backoff\n\n# Use with retry\nresult = asyncio.run(call_with_retry(tools_llm, input=\"test\"))\n</code></pre>"},{"location":"reference/core/structured_tools/tool_orchestrating_llm/examples/#4-handling-missing-tool-calls","title":"4. Handling Missing Tool Calls","text":"<p>Handle cases where LLM doesn't generate tool calls:</p> <pre><code>from pydantic import BaseModel\nfrom serapeum.core.structured_tools.tools_llm import ToolOrchestratingLLM\nfrom serapeum.llms.ollama import Ollama\n\nclass Output(BaseModel):\n    result: str\n\nllm = Ollama(model=\"llama3.1\", request_timeout=80)\n\ntools_llm = ToolOrchestratingLLM(\n    output_cls=Output,\n    prompt=\"Generate output\",\n    llm=llm,\n    tool_choice=\"required\",  # Force tool use to prevent this\n)\n\ntry:\n    result = tools_llm()\nexcept ValueError as e:\n    print(f\"No tool calls generated: {e}\")\n    # Try with different prompt or parameters\n</code></pre>"},{"location":"reference/core/structured_tools/tool_orchestrating_llm/examples/#best-practices","title":"Best Practices","text":""},{"location":"reference/core/structured_tools/tool_orchestrating_llm/examples/#1-clear-model-definitions","title":"1. Clear Model Definitions","text":"<p>Always define clear Pydantic models with descriptions:</p> <pre><code>from pydantic import BaseModel, Field\nfrom serapeum.core.structured_tools.tools_llm import ToolOrchestratingLLM\nfrom serapeum.llms.ollama import Ollama\n\nclass WellDefinedModel(BaseModel):\n    \"\"\"A well-documented model for structured output.\"\"\"\n\n    name: str = Field(description=\"The name of the entity\")\n    category: str = Field(description=\"Category classification\")\n    confidence: float = Field(\n        description=\"Confidence score\",\n        ge=0.0,\n        le=1.0\n    )\n    tags: list[str] = Field(\n        description=\"Relevant tags\",\n        default_factory=list\n    )\n\nllm = Ollama(model=\"llama3.1\", request_timeout=80)\n\ntools_llm = ToolOrchestratingLLM(\n    output_cls=WellDefinedModel,\n    prompt=\"Extract information from: {text}\",\n    llm=llm,\n)\n</code></pre>"},{"location":"reference/core/structured_tools/tool_orchestrating_llm/examples/#2-use-function-calling-compatible-models","title":"2. Use Function Calling Compatible Models","text":"<p>Ensure your LLM supports function calling:</p> <pre><code>from serapeum.llms.ollama import Ollama\n\n# Good: Models that support function calling\ngood_models = [\n    \"llama3.1\",\n    \"llama3.2\",\n    \"mistral\",\n    # Check Ollama docs for function calling support\n]\n\nllm = Ollama(model=\"llama3.1\", request_timeout=80)\n# llm.metadata.is_function_calling_model should be True\n</code></pre>"},{"location":"reference/core/structured_tools/tool_orchestrating_llm/examples/#3-instance-reuse","title":"3. Instance Reuse","text":"<p>Create instances once and reuse them:</p> <pre><code>from pydantic import BaseModel\nfrom serapeum.core.structured_tools.tools_llm import ToolOrchestratingLLM\nfrom serapeum.llms.ollama import Ollama\n\nclass Classification(BaseModel):\n    category: str\n    confidence: float\n\n# Create once\nllm = Ollama(model=\"llama3.1\", request_timeout=80)\nclassifier = ToolOrchestratingLLM(\n    output_cls=Classification,\n    prompt=\"Classify: {text}\",\n    llm=llm,\n)\n\n# Reuse many times - this is efficient!\ntexts = [\"text1\", \"text2\", \"text3\"]\nfor text in texts:\n    result = classifier(text=text)\n</code></pre>"},{"location":"reference/core/structured_tools/tool_orchestrating_llm/examples/#4-use-parallel-calls-for-lists","title":"4. Use Parallel Calls for Lists","text":"<p>When extracting multiple items, use parallel tool calls:</p> <pre><code>from pydantic import BaseModel\nfrom serapeum.core.structured_tools.tools_llm import ToolOrchestratingLLM\nfrom serapeum.llms.ollama import Ollama\n\nclass Item(BaseModel):\n    name: str\n    type: str\n\nllm = Ollama(model=\"llama3.1\", request_timeout=80)\n\n# Good: Enable parallel for extracting multiple items\ntools_llm = ToolOrchestratingLLM(\n    output_cls=Item,\n    prompt=\"Extract ALL items from: {text}\",\n    llm=llm,\n    allow_parallel_tool_calls=True,  # Enable for lists\n)\n\n# Returns List[Item] with all extracted items\nresults = tools_llm(text=\"apples, oranges, bananas\")\n</code></pre>"},{"location":"reference/core/structured_tools/tool_orchestrating_llm/examples/#see-also","title":"See Also","text":"<ul> <li>General Overview - Complete workflow explanation</li> <li>Execution Flow and Method Calls - Detailed sequence diagram</li> <li>Architecture and Class Relationships - Class structure</li> <li>Data Transformations and Validation - Data flow details</li> <li>Component Boundaries and Interactions - System components</li> <li>Lifecycle States and Transitions - State management</li> </ul>"},{"location":"reference/core/structured_tools/tool_orchestrating_llm/general/","title":"ToolOrchestratingLLM Workflow","text":"<p>This directory contains comprehensive documentation explaining the complete workflow of the <code>ToolOrchestratingLLM</code> class, from initialization to execution and structured output extraction.</p>"},{"location":"reference/core/structured_tools/tool_orchestrating_llm/general/#overview","title":"Overview","text":"<p>The <code>ToolOrchestratingLLM</code> is a function-calling orchestrator that: 1. Converts Pydantic models into callable tools with JSON schemas 2. Formats prompts with template variables 3. Executes function calling via LLM with tool schemas 4. Parses tool outputs into validated Pydantic model instances</p>"},{"location":"reference/core/structured_tools/tool_orchestrating_llm/general/#example-usage","title":"Example Usage","text":"<pre><code>from pydantic import BaseModel\nfrom serapeum.core.structured_tools.tools_llm import ToolOrchestratingLLM\nfrom serapeum.llms.ollama import Ollama\n\n# Define the output schema\nclass MockAlbum(BaseModel):\n    title: str\n    artist: str\n    songs: List[MockSong]\n\nclass MockSong(BaseModel):\n    title: str\n\n# Initialize LLM with function calling support\nllm = Ollama(model='llama3.1', request_timeout=80)\n\n# Create ToolOrchestratingLLM instance\ntools_llm = ToolOrchestratingLLM(\n    output_cls=MockAlbum,\n    prompt='This is a test album with {topic}',\n    llm=llm,\n)\n\n# Execute and get structured output via function calling\nobj_output = tools_llm(topic=\"songs\")\n# Returns: MockAlbum(title=\"hello\", artist=\"world\", songs=[...])\n</code></pre>"},{"location":"reference/core/structured_tools/tool_orchestrating_llm/general/#understanding-the-workflow","title":"Understanding the Workflow","text":""},{"location":"reference/core/structured_tools/tool_orchestrating_llm/general/#1-execution-flow-and-method-calls","title":"1. Execution Flow and Method Calls","text":"<p>Shows the chronological flow of method calls and interactions.</p> <p>Best for: - Understanding the order of operations - Seeing how function calling works - Debugging execution flow</p> <p>Key Sections: - Initialization phase (validation and component setup) - Tool creation (CallableTool.from_model) - Execution phase (predict_and_call with tools) - Tool execution and output parsing</p>"},{"location":"reference/core/structured_tools/tool_orchestrating_llm/general/#2-architecture-and-class-relationships","title":"2. Architecture and Class Relationships","text":"<p>Illustrates the static structure and relationships between classes.</p> <p>Best for: - Understanding the architecture - Seeing inheritance and composition - Identifying class responsibilities</p> <p>Key Classes: - <code>ToolOrchestratingLLM</code>: Main orchestrator - <code>CallableTool</code>: Pydantic-to-tool converter - <code>FunctionCallingLLM</code>: Abstract function-calling interface - <code>Ollama</code>: Concrete LLM implementation - <code>AgentChatResponse</code>: Response container with tool outputs</p>"},{"location":"reference/core/structured_tools/tool_orchestrating_llm/general/#3-data-transformations-and-validation","title":"3. Data Transformations and Validation","text":"<p>Tracks how data transforms through the system.</p> <p>Best for: - Understanding data transformations - Identifying validation points - Seeing error handling paths</p> <p>Key Flows: - Initialization validation pipeline - Tool schema generation - Function calling request preparation - Tool execution and validation - Single vs. parallel output extraction</p>"},{"location":"reference/core/structured_tools/tool_orchestrating_llm/general/#4-component-boundaries-and-interactions","title":"4. Component Boundaries and Interactions","text":"<p>Shows component boundaries and interaction patterns.</p> <p>Best for: - Understanding system architecture - Seeing component responsibilities - Identifying interaction patterns</p> <p>Key Components: - User space (application code) - ToolOrchestratingLLM layer (orchestration) - Tool layer (CallableTool) - Prompt layer (template formatting) - LLM layer (function calling execution) - Response layer (AgentChatResponse, ToolOutput)</p>"},{"location":"reference/core/structured_tools/tool_orchestrating_llm/general/#5-lifecycle-states-and-transitions","title":"5. Lifecycle States and Transitions","text":"<p>Depicts the lifecycle states and transitions.</p> <p>Best for: - Understanding instance lifecycle - Seeing state transitions - Identifying error states</p> <p>Key States: - Initialization states (validation) - Execution states (sync/async/streaming) - Tool execution states (single/parallel) - Parsing states (output extraction)</p>"},{"location":"reference/core/structured_tools/tool_orchestrating_llm/general/#workflow-summary","title":"Workflow Summary","text":""},{"location":"reference/core/structured_tools/tool_orchestrating_llm/general/#initialization-workflow","title":"Initialization Workflow","text":"<pre><code>1. Initialize ToolOrchestratingLLM with:\n   - output_cls: Pydantic model (MockAlbum)\n   - prompt: String or BasePromptTemplate\n   - llm: Function-calling LLM (Ollama)\n   - tool_choice: Optional tool selection strategy\n   - allow_parallel_tool_calls: Single vs. multiple outputs\n\n2. Validation occurs:\n   - LLM must support function calling (is_function_calling_model=True)\n   - Prompt conversion to template if needed\n\n3. Components stored in instance\n\n4. Instance ready for execution\n</code></pre>"},{"location":"reference/core/structured_tools/tool_orchestrating_llm/general/#execution-workflow","title":"Execution Workflow","text":"<pre><code>1. User calls tools_llm(topic=\"songs\")\n\n2. Convert Pydantic model to tool:\n   a. CallableTool.from_model(MockAlbum)\n   b. Extract JSON schema from model\n   c. Create callable tool with validation\n\n3. Format prompt:\n   a. Apply template variables: topic=\"songs\"\n   b. Create messages: [Message(role=USER, content=\"...\")]\n   c. Extend with system prompts\n\n4. Execute function calling:\n   a. Prepare request with tool schemas\n   b. Call predict_and_call([tool], messages, ...)\n   c. HTTP POST to Ollama server with tools parameter\n   d. LLM generates tool_calls in response\n\n5. Execute tools:\n   a. Parse tool_calls from response\n   b. Extract tool arguments\n   c. Validate arguments against Pydantic schema\n   d. Create MockAlbum instance(s)\n   e. Wrap in ToolOutput with raw_output\n\n6. Create AgentChatResponse:\n   a. Container with response text\n   b. sources: List[ToolOutput]\n\n7. Parse outputs:\n   a. Extract raw_output from ToolOutput(s)\n   b. Return MockAlbum or List[MockAlbum]\n\n8. Return validated Pydantic instance(s)\n</code></pre>"},{"location":"reference/core/structured_tools/tool_orchestrating_llm/general/#key-design-patterns","title":"Key Design Patterns","text":""},{"location":"reference/core/structured_tools/tool_orchestrating_llm/general/#1-function-calling-pattern","title":"1. Function Calling Pattern","text":"<p>Uses LLM's native function calling capabilities: - Pydantic models \u2192 Tool schemas - LLM generates tool_calls - Tools execute with validation - Structured outputs guaranteed</p>"},{"location":"reference/core/structured_tools/tool_orchestrating_llm/general/#2-tool-abstraction","title":"2. Tool Abstraction","text":"<p>Automatic conversion from Pydantic models to tools: - JSON schema extraction - Argument validation - Output wrapping</p>"},{"location":"reference/core/structured_tools/tool_orchestrating_llm/general/#3-flexible-output-modes","title":"3. Flexible Output Modes","text":"<p>Single or parallel tool calls: - <code>allow_parallel_tool_calls=False</code> \u2192 Single model - <code>allow_parallel_tool_calls=True</code> \u2192 List of models</p>"},{"location":"reference/core/structured_tools/tool_orchestrating_llm/general/#4-multi-modal-execution","title":"4. Multi-Modal Execution","text":"<p>Supports multiple execution modes: - Sync: <code>__call__</code> - Async: <code>acall</code> - Streaming: <code>stream_call</code> - Async streaming: <code>astream_call</code></p>"},{"location":"reference/core/structured_tools/tool_orchestrating_llm/general/#comparison-with-textcompletionllm","title":"Comparison with TextCompletionLLM","text":"Feature ToolOrchestratingLLM TextCompletionLLM Method Function calling Text completion with parsing LLM Requirement Must support function calling Any chat/completion model Schema Handling Native tool schemas JSON in prompt Validation Before execution (by LLM) After generation (by parser) Reliability Higher (structured by design) Depends on LLM output quality Parallel Outputs Native support Not supported Streaming Partial tool_calls Not applicable Use Case When function calling available Fallback for non-function-calling models"},{"location":"reference/core/structured_tools/tool_orchestrating_llm/general/#performance-considerations","title":"Performance Considerations","text":"<ol> <li>Reusable Instances: <code>ToolOrchestratingLLM</code> instances are reusable after initialization</li> <li>Async Support: <code>acall()</code> method provides async execution</li> <li>Streaming Support: <code>stream_call()</code> yields progressive updates</li> <li>Parallel Tool Calls: Enable with <code>allow_parallel_tool_calls=True</code></li> <li>Stateless Execution: Each call creates independent transient state</li> <li>Tool Schema Caching: Tool schemas are generated once per call</li> </ol>"},{"location":"reference/core/structured_tools/tool_orchestrating_llm/general/#error-handling","title":"Error Handling","text":""},{"location":"reference/core/structured_tools/tool_orchestrating_llm/general/#initialization-errors","title":"Initialization Errors","text":"<ul> <li><code>AssertionError</code>: No LLM provided</li> <li><code>ValueError</code>: LLM doesn't support function calling</li> </ul>"},{"location":"reference/core/structured_tools/tool_orchestrating_llm/general/#execution-errors","title":"Execution Errors","text":"<ul> <li>Network/timeout errors from LLM</li> <li>Missing tool_calls in response</li> <li>Invalid tool arguments</li> </ul>"},{"location":"reference/core/structured_tools/tool_orchestrating_llm/general/#validation-errors","title":"Validation Errors","text":"<ul> <li>Pydantic validation fails on tool arguments</li> <li>Type mismatch in output</li> </ul>"},{"location":"reference/core/structured_tools/tool_orchestrating_llm/general/#when-to-use-toolorchestratingllm","title":"When to Use ToolOrchestratingLLM","text":""},{"location":"reference/core/structured_tools/tool_orchestrating_llm/general/#use-when","title":"Use When:","text":"<p>\u2705 Your LLM supports function calling (OpenAI, Ollama, etc.) \u2705 You need guaranteed structured outputs \u2705 You want parallel tool calls \u2705 You need streaming with structured updates \u2705 Reliability is critical</p>"},{"location":"reference/core/structured_tools/tool_orchestrating_llm/general/#use-textcompletionllm-when","title":"Use TextCompletionLLM When:","text":"<p>\u274c LLM doesn't support function calling \u274c You need simple text-to-JSON parsing \u274c Function calling overhead is unnecessary</p>"},{"location":"reference/core/structured_tools/tool_orchestrating_llm/general/#next-steps","title":"Next Steps","text":"<ul> <li>View Examples - Comprehensive usage examples</li> <li>Sequence Diagram - Detailed flow</li> <li>Class Diagram - Architecture</li> <li>Data Flow - Transformations</li> <li>Components - Interactions</li> <li>State Machine - Lifecycle</li> </ul>"},{"location":"reference/core/structured_tools/tool_orchestrating_llm/tool_orchestrating_llm_class/","title":"Architecture and Class Relationships","text":"<p>This diagram shows the class relationships and structure for <code>ToolOrchestratingLLM</code>.</p>  Hold \"Ctrl\" to enable pan &amp; zoom  <pre><code>classDiagram\n    class BasePydanticLLM~BaseModel~ {\n        &lt;&lt;abstract&gt;&gt;\n        +output_cls: Type[BaseModel]\n        +__call__(**kwargs) BaseModel\n        +acall(**kwargs) BaseModel\n    }\n\n    class ToolOrchestratingLLM~Model~ {\n        +__init__(output_cls, prompt, llm, tool_choice, allow_parallel, verbose)\n        +output_cls: Type[Model]\n        +prompt: BasePromptTemplate\n        +__call__(llm_kwargs, **kwargs) Union[Model, List[Model]]\n        +acall(llm_kwargs, **kwargs) Union[Model, List[Model]]\n        +stream_call(llm_kwargs, **kwargs) Generator[Model, None, None]\n        +astream_call(llm_kwargs, **kwargs) AsyncGenerator[Model, None]\n        -_output_cls: Type[Model]\n        -_llm: FunctionCallingLLM\n        -_prompt: BasePromptTemplate\n        -_verbose: bool\n        -_allow_parallel_tool_calls: bool\n        -_tool_choice: Optional[Union[str, Dict]]\n        +_validate_prompt(prompt) BasePromptTemplate\n        +_validate_llm(llm) LLM\n    }\n\n    class BasePydanticLLM~BaseModel~ {\n        &lt;&lt;abstract&gt;&gt;\n        +output_cls: Type[BaseModel]\n    }\n\n    class BaseTool {\n        &lt;&lt;abstract&gt;&gt;\n        +metadata: ToolMetadata\n        +call(**kwargs) ToolOutput\n        +acall(**kwargs) ToolOutput\n    }\n\n    class CallableTool {\n        +__init__(fn, metadata)\n        +from_function(fn) CallableTool\n        +from_model(model_cls) CallableTool\n        +call(**kwargs) ToolOutput\n        +acall(**kwargs) ToolOutput\n        -_fn: Callable\n        -_async_fn: Optional[Callable]\n        -_metadata: ToolMetadata\n    }\n\n    class BasePromptTemplate {\n        &lt;&lt;abstract&gt;&gt;\n        +metadata: Dict[str, Any]\n        +template_vars: List[str]\n        +kwargs: Dict[str, str]\n        +output_parser: Optional[BaseParser]\n        +template_var_mappings: Optional[Dict]\n        +function_mappings: Optional[Dict]\n        +partial_format(**kwargs) BasePromptTemplate\n        +format(llm, **kwargs) str\n        +format_messages(llm, **kwargs) List[Message]\n        +get_template(llm) str\n    }\n\n    class PromptTemplate {\n        +template: str\n        +__init__(template, prompt_type, output_parser, metadata, ...)\n        +partial_format(**kwargs) PromptTemplate\n        +format(llm, completion_to_prompt, **kwargs) str\n        +format_messages(llm, **kwargs) List[Message]\n        +get_template(llm) str\n    }\n\n    class ChatPromptTemplate {\n        +message_templates: List[Message]\n        +__init__(message_templates, prompt_type, output_parser, ...)\n        +from_messages(message_templates, **kwargs) ChatPromptTemplate\n        +partial_format(**kwargs) ChatPromptTemplate\n        +format(llm, messages_to_prompt, **kwargs) str\n        +format_messages(llm, **kwargs) List[Message]\n        +get_template(llm) str\n    }\n\n    class BaseLLM {\n        &lt;&lt;abstract&gt;&gt;\n        +metadata: Metadata\n        +chat(messages, **kwargs) ChatResponse\n        +stream_chat(messages, **kwargs) ChatResponseGen\n        +achat(messages, **kwargs) ChatResponse\n        +astream_chat(messages, **kwargs) ChatResponseAsyncGen\n        +complete(prompt, **kwargs) CompletionResponse\n        +stream_complete(prompt, **kwargs) CompletionResponseGen\n        +acomplete(prompt, **kwargs) CompletionResponse\n        +astream_complete(prompt, **kwargs) CompletionResponseAsyncGen\n    }\n\n    class LLM {\n        +system_prompt: Optional[str]\n        +messages_to_prompt: MessagesToPromptCallable\n        +completion_to_prompt: CompletionToPromptCallable\n        +output_parser: Optional[BaseParser]\n        +pydantic_program_mode: StructuredLLMMode\n        +_get_prompt(prompt, **kwargs) str\n        +_get_messages(prompt, **kwargs) List[Message]\n        +_parse_output(output) str\n        +_extend_prompt(formatted_prompt) str\n        +_extend_messages(messages) List[Message]\n        +predict(prompt, **kwargs) str\n        +stream(prompt, **kwargs) TokenGen\n        +apredict(prompt, **kwargs) str\n        +astream(prompt, **kwargs) TokenAsyncGen\n        +structured_predict(output_cls, prompt, **kwargs) Model\n    }\n\n    class FunctionCallingLLM {\n        &lt;&lt;abstract&gt;&gt;\n        +predict_and_call(tools, chat_history, ...) AgentChatResponse\n        +apredict_and_call(tools, chat_history, ...) AgentChatResponse\n        +stream_chat_with_tools(tools, chat_history, ...) Generator\n        +astream_chat_with_tools(tools, chat_history, ...) AsyncGenerator\n        +chat_with_tools(tools, chat_history, ...) AgentChatResponse\n        +achat_with_tools(tools, chat_history, ...) AgentChatResponse\n        -_prepare_chat_with_tools(tools, chat_history, ...) Tuple\n        -_validate_chat_with_tools_response(...) AgentChatResponse\n    }\n\n    class Ollama {\n        +model: str\n        +base_url: str\n        +request_timeout: int\n        +temperature: float\n        +metadata: Metadata\n        +__init__(model, base_url, request_timeout, ...)\n        +chat(messages, **kwargs) ChatResponse\n        +achat(messages, **kwargs) ChatResponse\n        +complete(prompt, **kwargs) CompletionResponse\n        +acomplete(prompt, **kwargs) CompletionResponse\n        +predict_and_call(tools, ...) AgentChatResponse\n        +apredict_and_call(tools, ...) AgentChatResponse\n        +stream_chat_with_tools(tools, ...) Generator\n        +astream_chat_with_tools(tools, ...) AsyncGenerator\n        -_chat_request(messages, stream, **kwargs) dict\n        -_complete_request(prompt, stream, **kwargs) dict\n        -_prepare_tools_schema(tools) List[Dict]\n    }\n\n    class AgentChatResponse {\n        +response: str\n        +sources: List[ToolOutput]\n        +is_dummy_stream: bool\n        +metadata: Optional[Dict]\n        +__str__() str\n        +response_gen: Generator[str, None, None]\n        +async_response_gen: AsyncGenerator[str, None]\n        +parse_tool_outputs(allow_parallel) Union[Any, List[Any]]\n    }\n\n    class ToolOutput {\n        +content: str\n        +tool_name: str\n        +raw_input: Dict[str, Any]\n        +raw_output: Any\n        +is_error: bool\n    }\n\n    class StreamingObjectProcessor {\n        +__init__(output_cls, flexible_mode, allow_parallel, llm)\n        +process(partial_resp, cur_objects) Union[Model, List[Model]]\n        -_output_cls: Type[Model]\n        -_flexible_mode: bool\n        -_allow_parallel_tool_calls: bool\n        -_llm: FunctionCallingLLM\n    }\n\n    class BaseModel {\n        &lt;&lt;pydantic&gt;&gt;\n        +model_validate(data) BaseModel\n        +model_validate_json(json_data) BaseModel\n        +model_json_schema() dict\n    }\n\n    class MockAlbum {\n        +title: str\n        +artist: str\n        +songs: List[MockSong]\n    }\n\n    class MockSong {\n        +title: str\n    }\n\n    BasePydanticLLM &lt;|-- ToolOrchestratingLLM\n    BaseLLM &lt;|-- LLM\n    LLM &lt;|-- FunctionCallingLLM\n    FunctionCallingLLM &lt;|-- Ollama\n    BaseTool &lt;|-- CallableTool\n    BasePromptTemplate &lt;|-- PromptTemplate\n    BasePromptTemplate &lt;|-- ChatPromptTemplate\n    BaseModel &lt;|-- MockAlbum\n    BaseModel &lt;|-- MockSong\n\n    ToolOrchestratingLLM o-- FunctionCallingLLM : uses\n    ToolOrchestratingLLM o-- BasePromptTemplate : uses\n    ToolOrchestratingLLM ..&gt; CallableTool : creates from model\n    ToolOrchestratingLLM ..&gt; AgentChatResponse : receives\n    ToolOrchestratingLLM ..&gt; MockAlbum : produces\n    ToolOrchestratingLLM ..&gt; StreamingObjectProcessor : uses for streaming\n    CallableTool ..&gt; MockAlbum : validates against schema\n    CallableTool ..&gt; ToolOutput : produces\n    AgentChatResponse o-- ToolOutput : contains list\n    ToolOutput o-- MockAlbum : contains as raw_output\n    MockAlbum o-- MockSong : contains list\n\n    note for ToolOrchestratingLLM \"Main orchestrator that:\\n1. Converts Pydantic model to tool\\n2. Formats prompts with variables\\n3. Calls LLM with function calling\\n4. Parses tool outputs to models\\n5. Supports sync/async/streaming\"\n    note for CallableTool \"Converts Pydantic models\\nto callable tools with\\nJSON schema validation\"\n    note for AgentChatResponse \"Container for LLM response\\nand tool execution results\"\n    note for Ollama \"Concrete implementation\\nfor Ollama with\\nfunction calling support\"</code></pre>"},{"location":"reference/core/structured_tools/tool_orchestrating_llm/tool_orchestrating_llm_class/#class-responsibilities","title":"Class Responsibilities","text":""},{"location":"reference/core/structured_tools/tool_orchestrating_llm/tool_orchestrating_llm_class/#toolorchestratingllm","title":"ToolOrchestratingLLM","text":"<ul> <li>Orchestrates the complete function-calling workflow</li> <li>Validates LLM supports function calling during initialization</li> <li>Converts Pydantic models to callable tools</li> <li>Routes execution through predict_and_call</li> <li>Supports single or parallel tool calls</li> <li>Handles sync, async, and streaming modes</li> </ul>"},{"location":"reference/core/structured_tools/tool_orchestrating_llm/tool_orchestrating_llm_class/#callabletool","title":"CallableTool","text":"<ul> <li>Converts Pydantic models to function schemas</li> <li>Validates tool arguments against JSON schema</li> <li>Executes tool functions (sync/async)</li> <li>Wraps results in ToolOutput</li> </ul>"},{"location":"reference/core/structured_tools/tool_orchestrating_llm/tool_orchestrating_llm_class/#functioncallingllm-abstract","title":"FunctionCallingLLM (Abstract)","text":"<ul> <li>Defines interface for function-calling LLMs</li> <li>Provides predict_and_call abstraction</li> <li>Handles tool schema preparation</li> <li>Manages tool execution orchestration</li> </ul>"},{"location":"reference/core/structured_tools/tool_orchestrating_llm/tool_orchestrating_llm_class/#ollama","title":"Ollama","text":"<ul> <li>Implements FunctionCallingLLM for Ollama server</li> <li>Formats requests with tool schemas</li> <li>Parses tool_calls from responses</li> <li>Executes tools and aggregates results</li> </ul>"},{"location":"reference/core/structured_tools/tool_orchestrating_llm/tool_orchestrating_llm_class/#agentchatresponse","title":"AgentChatResponse","text":"<ul> <li>Contains LLM response text and tool outputs</li> <li>Parses tool outputs to extract structured models</li> <li>Supports single or list of outputs</li> <li>Provides streaming helpers</li> </ul>"},{"location":"reference/core/structured_tools/tool_orchestrating_llm/tool_orchestrating_llm_class/#streamingobjectprocessor","title":"StreamingObjectProcessor","text":"<ul> <li>Processes partial streaming responses</li> <li>Maintains state across chunks</li> <li>Yields progressively updated models</li> <li>Handles flexible/strict parsing modes</li> </ul>"},{"location":"reference/core/structured_tools/tool_orchestrating_llm/tool_orchestrating_llm_class/#key-design-patterns","title":"Key Design Patterns","text":"<ol> <li>Protocol-Based Interfaces: Uses abstract base classes for extensibility</li> <li>Pydantic Integration: First-class support for structured outputs</li> <li>Async-First: All operations support sync/async/streaming</li> <li>Tool Abstraction: Pydantic models become callable tools automatically</li> <li>Response Aggregation: AgentChatResponse unifies text and structured outputs</li> </ol>"},{"location":"reference/core/structured_tools/tool_orchestrating_llm/tool_orchestrating_llm_components/","title":"Component Boundaries and Interactions","text":"<p>This diagram shows how components interact during the complete lifecycle.</p>  Hold \"Ctrl\" to enable pan &amp; zoom  <pre><code>graph TB\n    subgraph User Space\n        UC[User Code]\n        MA[MockAlbum Schema]\n        MS[MockSong Schema]\n    end\n\n    subgraph ToolOrchestratingLLM Components\n        TOL[ToolOrchestratingLLM]\n\n        subgraph Validators\n            VL[validate_llm]\n            VP[validate_prompt]\n        end\n\n        subgraph Stored State\n            SL[\"_llm: FunctionCallingLLM\"]\n            SP[\"_prompt: BasePromptTemplate\"]\n            SO[\"_output_cls: Type[MockAlbum]\"]\n            SV[\"_verbose: bool\"]\n            SAP[\"_allow_parallel_tool_calls: bool\"]\n            STC[\"_tool_choice: Optional\"]\n        end\n\n        subgraph Execution Methods\n            CALL[\"__call__\"]\n            ACALL[\"acall\"]\n            STREAM[\"stream_call\"]\n            ASTREAM[\"astream_call\"]\n        end\n    end\n\n    subgraph Tool Layer\n        CT[CallableTool]\n\n        subgraph Tool Operations\n            FM[from_model]\n            ES[extract_schema]\n            TC[tool.call]\n        end\n    end\n\n    subgraph Prompt Layer\n        PT[PromptTemplate]\n        CPT[ChatPromptTemplate]\n\n        subgraph Prompt Operations\n            FMS[format_messages]\n            AV[Apply Variables]\n            GT[get_template]\n        end\n    end\n\n    subgraph LLM Layer - FunctionCallingLLM\n        OL[Ollama]\n\n        subgraph Orchestration Methods\n            PAC[predict_and_call]\n            APAC[apredict_and_call]\n            SCT[stream_chat_with_tools]\n            ASCT[astream_chat_with_tools]\n        end\n\n        subgraph Message Processing\n            EM[_extend_messages]\n            PM[_prepare_chat_with_tools]\n        end\n\n        subgraph Tool Schema\n            PTS[_prepare_tools_schema]\n        end\n\n        MD[metadata]\n    end\n\n    subgraph External Service\n        OS[Ollama Server]\n\n        subgraph Endpoints\n            EC[\"/api/chat\"]\n            EG[\"/api/generate\"]\n        end\n    end\n\n    subgraph Response Models\n        ACR[AgentChatResponse]\n\n        subgraph Response Components\n            RES[response: str]\n            SRC[\"sources: List[ToolOutput]\"]\n        end\n\n        subgraph Parsing Methods\n            PTO[parse_tool_outputs]\n        end\n    end\n\n    subgraph Tool Output\n        TOUT[ToolOutput]\n\n        subgraph Output Fields\n            TCONT[content]\n            TNAME[tool_name]\n            TINPUT[raw_input]\n            TOUTPUT[raw_output: MockAlbum]\n        end\n    end\n\n    subgraph Streaming Support\n        SOP[StreamingObjectProcessor]\n\n        subgraph Processor Methods\n            PROC[process]\n            PS[Parse partial JSON]\n            US[Update state]\n        end\n    end\n\n    %% Initialization Flow\n    UC --&gt;|1. Create instance| TOL\n    TOL --&gt;|Validate| VL\n    VL --&gt;|Check metadata| MD\n    MD --&gt;|is_function_calling_model| VL\n    VL --&gt;|Store| SL\n\n    TOL --&gt;|Validate/Convert| VP\n    VP --&gt;|Create if string| PT\n    VP --&gt;|Store| SP\n\n    TOL --&gt;|Store| SO\n    TOL --&gt;|Store| SV\n    TOL --&gt;|Store| SAP\n    TOL --&gt;|Store| STC\n\n    MA --&gt;|Defines schema| SO\n\n    %% Execution Flow - Standard Call\n    UC --&gt;|2. Call with kwargs| CALL\n    CALL --&gt;|Create tool| FM\n    FM --&gt;|Extract schema from| MA\n    FM --&gt;|Returns| CT\n\n    CALL --&gt;|Format with kwargs| FMS\n    FMS --&gt;|Uses template| PT\n    FMS --&gt;|Apply variables| AV\n    AV --&gt;|Returns| Messages\n\n    CALL --&gt;|Extend| EM\n    EM --&gt;|Add system prompts| Messages\n\n    CALL --&gt;|Invoke| PAC\n    PAC --&gt;|Prepare| PM\n    PM --&gt;|Add tools| PTS\n    PTS --&gt;|From| CT\n\n    PAC --&gt;|HTTP POST| EC\n    EC --&gt;|In| OS\n    OS --&gt;|Returns| Response\n\n    PAC --&gt;|Parse tool_calls| Response\n    PAC --&gt;|Execute| TC\n    TC --&gt;|Validate against| MA\n    TC --&gt;|Create instance| MA\n    TC --&gt;|Wrap in| TOUT\n\n    PAC --&gt;|Create| ACR\n    ACR --&gt;|Contains| TOUT\n    ACR --&gt;|In| SRC\n\n    CALL --&gt;|Parse| PTO\n    PTO --&gt;|Extract from| SRC\n    PTO --&gt;|Get| TOUTPUT\n    TOUTPUT --&gt;|Returns| MA\n\n    MA --&gt;|Instance to| UC\n\n    %% Async Flow\n    UC --&gt;|3. Async call| ACALL\n    ACALL --&gt;|Invoke| APAC\n    APAC --&gt;|Similar flow| PAC\n\n    %% Streaming Flow\n    UC --&gt;|4. Stream call| STREAM\n    STREAM --&gt;|Invoke| SCT\n    SCT --&gt;|Yields chunks| Response\n\n    STREAM --&gt;|Process| PROC\n    PROC --&gt;|Uses| SOP\n    SOP --&gt;|Parse| PS\n    PS --&gt;|Update| US\n    US --&gt;|Yields partial| MA\n\n    %% Async Streaming Flow\n    UC --&gt;|5. Async stream| ASTREAM\n    ASTREAM --&gt;|Invoke| ASCT\n    ASCT --&gt;|Similar flow| SCT\n\n    %% Parallel Tool Calls\n    SAP --&gt;|If True| PTO\n    PTO --&gt;|Returns List| MA\n\n    %% Styling\n    classDef userClass fill:#e1f5ff,stroke:#01579b\n    classDef validatorClass fill:#fff9c4,stroke:#f57f17\n    classDef stateClass fill:#f3e5f5,stroke:#4a148c\n    classDef toolClass fill:#e8f5e9,stroke:#1b5e20\n    classDef promptClass fill:#fce4ec,stroke:#880e4f\n    classDef llmClass fill:#e0f2f1,stroke:#004d40\n    classDef responseClass fill:#fff3e0,stroke:#e65100\n    classDef externalClass fill:#efebe9,stroke:#3e2723\n    classDef streamClass fill:#e3f2fd,stroke:#0d47a1\n\n    class UC,MA,MS userClass\n    class VL,VP validatorClass\n    class SL,SP,SO,SV,SAP,STC stateClass\n    class CT,FM,ES,TC toolClass\n    class PT,CPT,FMS,AV,GT promptClass\n    class OL,PAC,APAC,SCT,ASCT,EM,PM,PTS,MD llmClass\n    class ACR,RES,SRC,PTO responseClass\n    class TOUT,TCONT,TNAME,TINPUT,TOUTPUT responseClass\n    class OS,EC,EG externalClass\n    class SOP,PROC,PS,US streamClass</code></pre>"},{"location":"reference/core/structured_tools/tool_orchestrating_llm/tool_orchestrating_llm_components/#component-responsibilities-matrix","title":"Component Responsibilities Matrix","text":"Component Initialization Execution Parsing Streaming ToolOrchestratingLLM Validates &amp; stores components Routes to predict_and_call Extracts from AgentChatResponse Processes with StreamingObjectProcessor CallableTool - Created from Pydantic model Validates tool arguments - PromptTemplate Created/validated Formats with variables - - Ollama Validated for function calling Executes predict_and_call - Streams chat_with_tools AgentChatResponse - Created by LLM Contains ToolOutputs - ToolOutput - Created by tool execution Contains raw_output (Pydantic) - MockAlbum Defines schema - Validates parsed data Progressively built StreamingObjectProcessor - - - Parses partial responses"},{"location":"reference/core/structured_tools/tool_orchestrating_llm/tool_orchestrating_llm_components/#interaction-patterns","title":"Interaction Patterns","text":""},{"location":"reference/core/structured_tools/tool_orchestrating_llm/tool_orchestrating_llm_components/#1-initialization-pattern-constructor","title":"1. Initialization Pattern (Constructor)","text":"<pre><code>User \u2192 ToolOrchestratingLLM.__init__\n  \u251c\u2500\u2192 validate_llm\n  \u2502   \u251c\u2500\u2192 Use provided or fallback to Configs.llm\n  \u2502   \u2514\u2500\u2192 Check metadata.is_function_calling_model == True\n  \u251c\u2500\u2192 validate_prompt\n  \u2502   \u2514\u2500\u2192 Convert string to PromptTemplate if needed\n  \u2514\u2500\u2192 Store all components:\n      \u251c\u2500\u2192 _output_cls (MockAlbum)\n      \u251c\u2500\u2192 _llm (Ollama)\n      \u251c\u2500\u2192 _prompt (PromptTemplate)\n      \u251c\u2500\u2192 _verbose\n      \u251c\u2500\u2192 _allow_parallel_tool_calls\n      \u2514\u2500\u2192 _tool_choice\n</code></pre>"},{"location":"reference/core/structured_tools/tool_orchestrating_llm/tool_orchestrating_llm_components/#2-standard-execution-pattern-sync","title":"2. Standard Execution Pattern (Sync)","text":"<pre><code>User \u2192 ToolOrchestratingLLM.__call__(topic=\"songs\")\n  \u251c\u2500\u2192 CallableTool.from_model(MockAlbum)\n  \u2502   \u251c\u2500\u2192 Extract JSON schema from MockAlbum\n  \u2502   \u2514\u2500\u2192 Create callable tool with validation\n  \u251c\u2500\u2192 PromptTemplate.format_messages(topic=\"songs\")\n  \u2502   \u251c\u2500\u2192 Apply template variables\n  \u2502   \u2514\u2500\u2192 Return List[Message]\n  \u251c\u2500\u2192 Ollama._extend_messages(messages)\n  \u2502   \u2514\u2500\u2192 Add system prompts if configured\n  \u251c\u2500\u2192 Ollama.predict_and_call([tool], messages, ...)\n  \u2502   \u251c\u2500\u2192 Prepare chat request with tool schemas\n  \u2502   \u251c\u2500\u2192 HTTP POST to /api/chat\n  \u2502   \u251c\u2500\u2192 Parse tool_calls from response\n  \u2502   \u251c\u2500\u2192 Execute tool.call(args)\n  \u2502   \u2502   \u251c\u2500\u2192 Validate args against MockAlbum schema\n  \u2502   \u2502   \u251c\u2500\u2192 Create MockAlbum instance\n  \u2502   \u2502   \u2514\u2500\u2192 Wrap in ToolOutput\n  \u2502   \u2514\u2500\u2192 Create AgentChatResponse with sources\n  \u2514\u2500\u2192 AgentChatResponse.parse_tool_outputs(allow_parallel=False)\n      \u251c\u2500\u2192 Extract sources[0].raw_output\n      \u2514\u2500\u2192 Return MockAlbum instance\n</code></pre>"},{"location":"reference/core/structured_tools/tool_orchestrating_llm/tool_orchestrating_llm_components/#3-parallel-execution-pattern","title":"3. Parallel Execution Pattern","text":"<pre><code>User \u2192 ToolOrchestratingLLM.__call__(..., allow_parallel_tool_calls=True)\n  \u251c\u2500\u2192 [Same tool creation and message formatting]\n  \u251c\u2500\u2192 Ollama.predict_and_call([tool], ..., allow_parallel=True)\n  \u2502   \u251c\u2500\u2192 LLM generates multiple tool_calls\n  \u2502   \u251c\u2500\u2192 Execute each tool.call(args)\n  \u2502   \u2502   \u251c\u2500\u2192 ToolOutput 1: MockAlbum(title=\"hello\", ...)\n  \u2502   \u2502   \u2514\u2500\u2192 ToolOutput 2: MockAlbum(title=\"hello2\", ...)\n  \u2502   \u2514\u2500\u2192 Create AgentChatResponse with multiple sources\n  \u2514\u2500\u2192 AgentChatResponse.parse_tool_outputs(allow_parallel=True)\n      \u251c\u2500\u2192 Extract all sources[i].raw_output\n      \u2514\u2500\u2192 Return List[MockAlbum]\n</code></pre>"},{"location":"reference/core/structured_tools/tool_orchestrating_llm/tool_orchestrating_llm_components/#4-async-execution-pattern","title":"4. Async Execution Pattern","text":"<pre><code>User \u2192 await ToolOrchestratingLLM.acall(...)\n  \u251c\u2500\u2192 [Same tool creation and message formatting]\n  \u251c\u2500\u2192 await Ollama.apredict_and_call([tool], messages, ...)\n  \u2502   \u251c\u2500\u2192 Async HTTP request\n  \u2502   \u251c\u2500\u2192 Async tool execution\n  \u2502   \u2514\u2500\u2192 Return AgentChatResponse\n  \u2514\u2500\u2192 AgentChatResponse.parse_tool_outputs(...)\n      \u2514\u2500\u2192 Return MockAlbum or List[MockAlbum]\n</code></pre>"},{"location":"reference/core/structured_tools/tool_orchestrating_llm/tool_orchestrating_llm_components/#5-streaming-execution-pattern-sync","title":"5. Streaming Execution Pattern (Sync)","text":"<pre><code>User \u2192 for obj in ToolOrchestratingLLM.stream_call(...):\n  \u251c\u2500\u2192 [Same tool creation and message formatting]\n  \u251c\u2500\u2192 Ollama.stream_chat_with_tools([tool], messages, ...)\n  \u2502   \u2514\u2500\u2192 Yields partial ChatResponse chunks\n  \u2514\u2500\u2192 For each chunk:\n      \u251c\u2500\u2192 StreamingObjectProcessor.process(chunk, cur_objects)\n      \u2502   \u251c\u2500\u2192 Parse partial tool_calls JSON\n      \u2502   \u251c\u2500\u2192 Validate against MockAlbum schema (flexible mode)\n      \u2502   \u251c\u2500\u2192 Update cur_objects state\n      \u2502   \u2514\u2500\u2192 Return progressively updated MockAlbum\n      \u2514\u2500\u2192 Yield MockAlbum (partial or complete)\n</code></pre>"},{"location":"reference/core/structured_tools/tool_orchestrating_llm/tool_orchestrating_llm_components/#6-async-streaming-pattern","title":"6. Async Streaming Pattern","text":"<pre><code>User \u2192 async for obj in await ToolOrchestratingLLM.astream_call(...):\n  \u251c\u2500\u2192 [Same tool creation and message formatting]\n  \u251c\u2500\u2192 await Ollama.astream_chat_with_tools([tool], messages, ...)\n  \u2502   \u2514\u2500\u2192 Async yields partial ChatResponse chunks\n  \u2514\u2500\u2192 For each chunk:\n      \u251c\u2500\u2192 StreamingObjectProcessor.process(chunk, cur_objects)\n      \u2514\u2500\u2192 Yield MockAlbum (partial or complete)\n</code></pre>"},{"location":"reference/core/structured_tools/tool_orchestrating_llm/tool_orchestrating_llm_components/#state-management","title":"State Management","text":""},{"location":"reference/core/structured_tools/tool_orchestrating_llm/tool_orchestrating_llm_components/#immutable-state-post-initialization","title":"Immutable State (Post-Initialization)","text":"<ul> <li><code>_output_cls</code>: Type[MockAlbum] - Schema for structured output</li> <li><code>_llm</code>: FunctionCallingLLM - Language model instance</li> <li><code>_verbose</code>: bool - Logging control</li> <li><code>_allow_parallel_tool_calls</code>: bool - Single vs. multiple outputs</li> <li><code>_tool_choice</code>: Optional - Tool selection strategy</li> </ul>"},{"location":"reference/core/structured_tools/tool_orchestrating_llm/tool_orchestrating_llm_components/#mutable-state","title":"Mutable State","text":"<ul> <li><code>_prompt</code>: BasePromptTemplate - Can be updated via setter</li> </ul>"},{"location":"reference/core/structured_tools/tool_orchestrating_llm/tool_orchestrating_llm_components/#transient-state-per-call","title":"Transient State (Per Call)","text":"<ul> <li><code>llm_kwargs</code>: Forwarded to LLM methods (temperature, max_tokens, etc.)</li> <li><code>**kwargs</code>: Template variables for prompt formatting</li> <li><code>tool</code>: CallableTool instance created from output_cls</li> <li><code>messages</code>: Formatted and extended message list</li> <li><code>agent_response</code>: AgentChatResponse from LLM</li> <li><code>parsed_output</code>: Final MockAlbum or List[MockAlbum]</li> </ul>"},{"location":"reference/core/structured_tools/tool_orchestrating_llm/tool_orchestrating_llm_components/#streaming-state-per-stream","title":"Streaming State (Per Stream)","text":"<ul> <li><code>cur_objects</code>: List of partial/complete objects maintained across chunks</li> <li><code>partial_resp</code>: Current chunk being processed</li> <li><code>objects</code>: Progressively updated Pydantic instances</li> </ul>"},{"location":"reference/core/structured_tools/tool_orchestrating_llm/tool_orchestrating_llm_components/#data-flow-between-components","title":"Data Flow Between Components","text":"<pre><code>User Input (kwargs)\n  \u2193\nToolOrchestratingLLM\n  \u2193\nCallableTool (from MockAlbum schema)\n  \u2193\nPromptTemplate (formatted with kwargs)\n  \u2193\nMessages (List[Message])\n  \u2193\nOllama (extended with system prompts)\n  \u2193\nHTTP Request (with tool schemas)\n  \u2193\nOllama Server\n  \u2193\nHTTP Response (with tool_calls)\n  \u2193\nOllama (parse and execute tools)\n  \u2193\nToolOutput (with raw_output=MockAlbum)\n  \u2193\nAgentChatResponse (with sources=[ToolOutput])\n  \u2193\nparse_tool_outputs (extract raw_output)\n  \u2193\nMockAlbum instance or List[MockAlbum]\n  \u2193\nUser Output\n</code></pre>"},{"location":"reference/core/structured_tools/tool_orchestrating_llm/tool_orchestrating_llm_components/#error-boundaries","title":"Error Boundaries","text":"<ol> <li>Initialization: validate_llm, validate_prompt</li> <li>Tool Creation: CallableTool.from_model - schema extraction</li> <li>Prompt Formatting: format_messages - template variable errors</li> <li>LLM Execution: predict_and_call - network errors, timeout</li> <li>Tool Parsing: Parse tool_calls - missing/malformed data</li> <li>Tool Execution: Validate args - Pydantic ValidationError</li> <li>Output Extraction: parse_tool_outputs - missing raw_output</li> </ol>"},{"location":"reference/core/structured_tools/tool_orchestrating_llm/tool_orchestrating_llm_dataflow/","title":"Data Transformations and Validation","text":"<p>This diagram shows how data flows through the <code>ToolOrchestratingLLM</code> system.</p>  Hold \"Ctrl\" to enable pan &amp; zoom  <pre><code>flowchart TD\n    Start([User Code]) --&gt; Init{Initialize ToolOrchestratingLLM}\n\n    Init --&gt; ValidateLLM[Validate LLM]\n    ValidateLLM --&gt; CheckLLM{LLM Provided?}\n    CheckLLM --&gt;|Yes| CheckFunctionCalling{is_function_calling_model?}\n    CheckLLM --&gt;|No| CheckConfigs{Configs.llm Set?}\n    CheckConfigs --&gt;|Yes| CheckFunctionCalling\n    CheckConfigs --&gt;|No| Error1[Raise AssertionError]\n    CheckFunctionCalling --&gt;|True| ValidatePrompt\n    CheckFunctionCalling --&gt;|False| Error2[Raise ValueError:&lt;br/&gt;Model does not support function calling]\n\n    ValidatePrompt --&gt; CheckPrompt{Prompt Type?}\n    CheckPrompt --&gt;|BasePromptTemplate| StoreComponents\n    CheckPrompt --&gt;|String| ConvertPrompt[Convert to PromptTemplate]\n    CheckPrompt --&gt;|Other| Error3[Raise ValueError]\n\n    ConvertPrompt --&gt; StoreComponents[Store All Components]\n    StoreComponents --&gt; Ready([ToolOrchestratingLLM Ready])\n\n    Ready --&gt; Call{User Calls tools_llm}\n    Call --&gt;|With kwargs| CreateTool[CallableTool.from_model]\n\n    CreateTool --&gt; ExtractSchema[Extract Pydantic schema]\n    ExtractSchema --&gt; BuildToolSchema[Build tool JSON schema]\n    BuildToolSchema --&gt; FormatMessages[Format Messages]\n\n    FormatMessages --&gt; ApplyVars[Apply template variables]\n    ApplyVars --&gt; CreateMessages[Create Message list]\n    CreateMessages --&gt; ExtendMessages[Extend with system prompts]\n\n    ExtendMessages --&gt; PredictAndCall[Call predict_and_call]\n    PredictAndCall --&gt; PrepareRequest[Prepare chat request with tools]\n\n    PrepareRequest --&gt; AddToolSchemas[Add tool schemas to request]\n    AddToolSchemas --&gt; SendRequest[HTTP POST to Ollama server]\n    SendRequest --&gt; ReceiveResponse[Receive response with tool_calls]\n\n    ReceiveResponse --&gt; ParseToolCalls{tool_calls present?}\n    ParseToolCalls --&gt;|No| Error4[No tool calls generated]\n    ParseToolCalls --&gt;|Yes| ExtractArgs[Extract tool arguments]\n\n    ExtractArgs --&gt; CheckParallel{allow_parallel_tool_calls?}\n    CheckParallel --&gt;|False| ExecuteSingle[Execute single tool call]\n    CheckParallel --&gt;|True| ExecuteMultiple[Execute multiple tool calls]\n\n    ExecuteSingle --&gt; ValidateArgs1[Validate args against schema]\n    ValidateArgs1 --&gt; CreateModel1[Create Pydantic instance]\n    CreateModel1 --&gt; WrapOutput1[Wrap in ToolOutput]\n    WrapOutput1 --&gt; CreateResponse1[Create AgentChatResponse]\n    CreateResponse1 --&gt; ParseSingle\n\n    ExecuteMultiple --&gt; ValidateArgsN[Validate each args against schema]\n    ValidateArgsN --&gt; CreateModelN[Create multiple Pydantic instances]\n    CreateModelN --&gt; WrapOutputN[Wrap each in ToolOutput]\n    WrapOutputN --&gt; CreateResponseN[Create AgentChatResponse]\n    CreateResponseN --&gt; ParseMultiple\n\n    ParseSingle[parse_tool_outputs&lt;br/&gt;allow_parallel=False]\n    ParseSingle --&gt; ExtractFirst[Extract first source.raw_output]\n    ExtractFirst --&gt; ReturnSingle([Return MockAlbum instance])\n\n    ParseMultiple[parse_tool_outputs&lt;br/&gt;allow_parallel=True]\n    ParseMultiple --&gt; ExtractAll[Extract all sources.raw_output]\n    ExtractAll --&gt; ReturnList([Return List of MockAlbum])\n\n    style Start fill:#e1f5ff\n    style Ready fill:#e1f5ff\n    style ReturnSingle fill:#c8e6c9\n    style ReturnList fill:#c8e6c9\n    style Error1 fill:#ffcdd2\n    style Error2 fill:#ffcdd2\n    style Error3 fill:#ffcdd2\n    style Error4 fill:#ffcdd2\n    style SendRequest fill:#fff9c4\n    style ReceiveResponse fill:#fff9c4</code></pre>"},{"location":"reference/core/structured_tools/tool_orchestrating_llm/tool_orchestrating_llm_dataflow/#data-transformations","title":"Data Transformations","text":""},{"location":"reference/core/structured_tools/tool_orchestrating_llm/tool_orchestrating_llm_dataflow/#initialization-phase","title":"Initialization Phase","text":"<pre><code>Input:\n  - output_cls: MockAlbum (Pydantic model class)\n  - prompt: \"This is a test album with {topic}\"\n  - llm: Ollama(model=\"llama3.1\")\n\nValidations:\n  1. Check llm.metadata.is_function_calling_model == True\n  2. Convert prompt string to PromptTemplate if needed\n  3. Store all validated components\n\nOutput:\n  - ToolOrchestratingLLM instance ready for execution\n</code></pre>"},{"location":"reference/core/structured_tools/tool_orchestrating_llm/tool_orchestrating_llm_dataflow/#execution-phase-single-output","title":"Execution Phase - Single Output","text":"<pre><code>Input:\n  tools_llm(topic=\"songs\")\n\nTransformations:\n  1. kwargs: {topic: \"songs\"}\n\n  2. CallableTool.from_model(MockAlbum)\n     \u2192 tool with JSON schema:\n     {\n       \"type\": \"function\",\n       \"function\": {\n         \"name\": \"MockAlbum\",\n         \"parameters\": {\n           \"type\": \"object\",\n           \"properties\": {\n             \"title\": {\"type\": \"string\"},\n             \"artist\": {\"type\": \"string\"},\n             \"songs\": {\"type\": \"array\", \"items\": {...}}\n           }\n         }\n       }\n     }\n\n  3. format_messages(topic=\"songs\")\n     \u2192 [Message(role=USER, content=\"This is a test album with songs\")]\n\n  4. _extend_messages(messages)\n     \u2192 [Message(role=SYSTEM, ...), Message(role=USER, ...)]\n\n  5. predict_and_call([tool], messages, ...)\n     \u2192 HTTP request:\n     {\n       \"model\": \"llama3.1\",\n       \"messages\": [...],\n       \"tools\": [{\"type\": \"function\", \"function\": {...}}]\n     }\n\n  6. LLM response:\n     {\n       \"message\": {\n         \"role\": \"assistant\",\n         \"tool_calls\": [{\n           \"id\": \"call_123\",\n           \"type\": \"function\",\n           \"function\": {\n             \"name\": \"MockAlbum\",\n             \"arguments\": '{\"title\":\"hello\",\"artist\":\"world\",\"songs\":[...]}'\n           }\n         }]\n       }\n     }\n\n  7. Parse arguments \u2192 Pydantic validation\n     \u2192 MockAlbum(title=\"hello\", artist=\"world\", songs=[...])\n\n  8. Wrap in ToolOutput\n     \u2192 ToolOutput(\n         content=\"...\",\n         tool_name=\"MockAlbum\",\n         raw_output=MockAlbum(...)\n       )\n\n  9. Create AgentChatResponse\n     \u2192 AgentChatResponse(response=\"...\", sources=[ToolOutput(...)])\n\n  10. parse_tool_outputs(allow_parallel=False)\n      \u2192 Extract sources[0].raw_output\n      \u2192 MockAlbum(title=\"hello\", artist=\"world\", songs=[...])\n\nOutput:\n  MockAlbum(title=\"hello\", artist=\"world\", songs=[...])\n</code></pre>"},{"location":"reference/core/structured_tools/tool_orchestrating_llm/tool_orchestrating_llm_dataflow/#execution-phase-parallel-outputs","title":"Execution Phase - Parallel Outputs","text":"<pre><code>Input:\n  tools_llm(topic=\"songs\")  # with allow_parallel_tool_calls=True\n\nTransformations:\n  1-6. [Same as single output]\n\n  7. LLM response with multiple tool_calls:\n     {\n       \"message\": {\n         \"tool_calls\": [\n           {\"function\": {\"arguments\": '{\"title\":\"hello\",\"artist\":\"world\",...}'}},\n           {\"function\": {\"arguments\": '{\"title\":\"hello2\",\"artist\":\"world2\",...}'}}\n         ]\n       }\n     }\n\n  8. Parse each \u2192 Multiple Pydantic instances\n     \u2192 [MockAlbum(...), MockAlbum(...)]\n\n  9. Wrap each in ToolOutput\n     \u2192 [ToolOutput(raw_output=MockAlbum(...)), ToolOutput(raw_output=MockAlbum(...))]\n\n  10. Create AgentChatResponse\n      \u2192 AgentChatResponse(sources=[ToolOutput(...), ToolOutput(...)])\n\n  11. parse_tool_outputs(allow_parallel=True)\n      \u2192 Extract all sources[i].raw_output\n      \u2192 [MockAlbum(...), MockAlbum(...)]\n\nOutput:\n  [MockAlbum(title=\"hello\", ...), MockAlbum(title=\"hello2\", ...)]\n</code></pre>"},{"location":"reference/core/structured_tools/tool_orchestrating_llm/tool_orchestrating_llm_dataflow/#error-handling-points","title":"Error Handling Points","text":"<ol> <li>LLM validation:</li> <li>Ensures LLM instance is provided or available in Configs</li> <li> <p>Validates <code>is_function_calling_model == True</code></p> </li> <li> <p>Prompt validation:</p> </li> <li> <p>Ensures prompt is BasePromptTemplate or string</p> </li> <li> <p>Tool schema generation:</p> </li> <li>Extracts JSON schema from Pydantic model</li> <li> <p>Handles complex nested models</p> </li> <li> <p>Tool call parsing:</p> </li> <li>Validates LLM generated tool_calls</li> <li> <p>Handles missing or malformed tool calls</p> </li> <li> <p>Argument validation:</p> </li> <li>Validates tool arguments against Pydantic schema</li> <li> <p>Catches ValidationError and reports issues</p> </li> <li> <p>Output extraction:</p> </li> <li>Ensures raw_output exists in ToolOutput</li> <li>Handles single vs. list outputs based on allow_parallel_tool_calls</li> </ol>"},{"location":"reference/core/structured_tools/tool_orchestrating_llm/tool_orchestrating_llm_dataflow/#streaming-data-flow","title":"Streaming Data Flow","text":"Hold \"Ctrl\" to enable pan &amp; zoom  <pre><code>flowchart TD\n    StreamStart([stream_call called]) --&gt; CreateTool[Create CallableTool]\n    CreateTool --&gt; FormatMsgs[Format messages]\n    FormatMsgs --&gt; StreamChat[stream_chat_with_tools]\n\n    StreamChat --&gt; ReceiveChunk{Receive chunk}\n    ReceiveChunk --&gt; ProcessChunk[StreamingObjectProcessor.process]\n\n    ProcessChunk --&gt; ParsePartial[Parse partial tool_calls]\n    ParsePartial --&gt; UpdateState[Update cur_objects state]\n    UpdateState --&gt; YieldPartial[Yield partial/complete model]\n\n    YieldPartial --&gt; MoreChunks{More chunks?}\n    MoreChunks --&gt;|Yes| ReceiveChunk\n    MoreChunks --&gt;|No| StreamEnd([End streaming])\n\n    style StreamStart fill:#e1f5ff\n    style YieldPartial fill:#c8e6c9\n    style StreamEnd fill:#c8e6c9</code></pre> <p>In streaming mode: 1. Each chunk may contain partial JSON for tool arguments 2. <code>StreamingObjectProcessor</code> maintains state across chunks 3. Yields progressively updated Pydantic instances 4. Handles flexible parsing for incomplete JSON</p>"},{"location":"reference/core/structured_tools/tool_orchestrating_llm/tool_orchestrating_llm_sequence/","title":"Execution Flow and Method Calls","text":"<p>This diagram shows the complete workflow from initialization to execution of <code>ToolOrchestratingLLM</code>.</p>  Hold \"Ctrl\" to enable pan &amp; zoom  <pre><code>sequenceDiagram\n    participant User\n    participant ToolOrchestratingLLM\n    participant CallableTool\n    participant PromptTemplate\n    participant Ollama\n    participant LLMServer\n    participant AgentChatResponse\n\n    Note over User: Initialization Phase\n    User-&gt;&gt;ToolOrchestratingLLM: __init__(output_cls=MockAlbum, prompt, llm)\n    activate ToolOrchestratingLLM\n\n    ToolOrchestratingLLM-&gt;&gt;ToolOrchestratingLLM: validate_llm(llm)\n    Note over ToolOrchestratingLLM: Checks if llm is provided or uses Configs.llm&lt;br/&gt;Validates is_function_calling_model=True\n\n    ToolOrchestratingLLM-&gt;&gt;ToolOrchestratingLLM: validate_prompt(prompt)\n    Note over ToolOrchestratingLLM: Converts string to PromptTemplate&lt;br/&gt;if not already BasePromptTemplate\n\n    ToolOrchestratingLLM-&gt;&gt;PromptTemplate: Create/validate prompt template\n    activate PromptTemplate\n    PromptTemplate--&gt;&gt;ToolOrchestratingLLM: template instance\n    deactivate PromptTemplate\n\n    Note over ToolOrchestratingLLM: Store: _output_cls, _llm, _prompt,&lt;br/&gt;_verbose, _allow_parallel_tool_calls,&lt;br/&gt;_tool_choice\n\n    ToolOrchestratingLLM--&gt;&gt;User: tools_llm instance\n    deactivate ToolOrchestratingLLM\n\n    Note over User: Execution Phase\n    User-&gt;&gt;ToolOrchestratingLLM: __call__(topic=\"songs\")\n    activate ToolOrchestratingLLM\n\n    ToolOrchestratingLLM-&gt;&gt;CallableTool: from_model(MockAlbum)\n    activate CallableTool\n    Note over CallableTool: Convert Pydantic model to tool&lt;br/&gt;with JSON schema\n    CallableTool--&gt;&gt;ToolOrchestratingLLM: tool instance\n    deactivate CallableTool\n\n    ToolOrchestratingLLM-&gt;&gt;PromptTemplate: format_messages(topic=\"songs\")\n    activate PromptTemplate\n    Note over PromptTemplate: Apply template variables:&lt;br/&gt;\"This is a test album with songs\"\n    PromptTemplate--&gt;&gt;ToolOrchestratingLLM: List[Message]\n    deactivate PromptTemplate\n\n    ToolOrchestratingLLM-&gt;&gt;Ollama: _extend_messages(messages)\n    activate Ollama\n    Note over Ollama: Add system prompts if configured\n    Ollama--&gt;&gt;ToolOrchestratingLLM: extended messages\n    deactivate Ollama\n\n    ToolOrchestratingLLM-&gt;&gt;Ollama: predict_and_call(tools=[tool], messages, ...)\n    activate Ollama\n    Note over Ollama: Prepare function calling request&lt;br/&gt;with tool schemas\n\n    Ollama-&gt;&gt;LLMServer: HTTP POST /api/chat\n    activate LLMServer\n    Note over LLMServer: Process chat with tools&lt;br/&gt;Generate tool calls\n    LLMServer--&gt;&gt;Ollama: Response with tool_calls\n    deactivate LLMServer\n\n    Ollama-&gt;&gt;Ollama: Parse tool_calls from response\n    Note over Ollama: Extract tool arguments\n\n    Ollama-&gt;&gt;CallableTool: Execute tool with parsed arguments\n    activate CallableTool\n    CallableTool-&gt;&gt;CallableTool: Validate args against schema\n    CallableTool-&gt;&gt;CallableTool: Create MockAlbum instance\n    CallableTool--&gt;&gt;Ollama: ToolOutput with raw_output=MockAlbum\n    deactivate CallableTool\n\n    Ollama-&gt;&gt;AgentChatResponse: Create response with sources\n    activate AgentChatResponse\n    AgentChatResponse--&gt;&gt;Ollama: response instance\n    deactivate AgentChatResponse\n\n    Ollama--&gt;&gt;ToolOrchestratingLLM: AgentChatResponse\n    deactivate Ollama\n\n    ToolOrchestratingLLM-&gt;&gt;AgentChatResponse: parse_tool_outputs(allow_parallel=False)\n    activate AgentChatResponse\n    Note over AgentChatResponse: Extract raw_output from&lt;br/&gt;first ToolOutput in sources\n    AgentChatResponse--&gt;&gt;ToolOrchestratingLLM: MockAlbum instance\n    deactivate AgentChatResponse\n\n    ToolOrchestratingLLM--&gt;&gt;User: MockAlbum(title=\"hello\", artist=\"world\", ...)\n    deactivate ToolOrchestratingLLM</code></pre>"},{"location":"reference/core/structured_tools/tool_orchestrating_llm/tool_orchestrating_llm_sequence/#key-points","title":"Key Points","text":"<ol> <li>Initialization validates all components before storing them - LLM must support function calling</li> <li>Tool creation converts Pydantic model to CallableTool with JSON schema</li> <li>Prompt formatting applies template variables to create messages</li> <li>predict_and_call orchestrates the function calling flow with the LLM</li> <li>Tool execution happens automatically after LLM generates tool calls</li> <li>Response parsing extracts structured Pydantic instances from ToolOutput</li> </ol>"},{"location":"reference/core/structured_tools/tool_orchestrating_llm/tool_orchestrating_llm_sequence/#parallel-tool-calls","title":"Parallel Tool Calls","text":"<p>When <code>allow_parallel_tool_calls=True</code>:</p>  Hold \"Ctrl\" to enable pan &amp; zoom  <pre><code>sequenceDiagram\n    participant User\n    participant ToolOrchestratingLLM\n    participant Ollama\n    participant AgentChatResponse\n\n    User-&gt;&gt;ToolOrchestratingLLM: __call__(allow_parallel_tool_calls=True)\n    activate ToolOrchestratingLLM\n\n    ToolOrchestratingLLM-&gt;&gt;Ollama: predict_and_call(..., allow_parallel=True)\n    activate Ollama\n    Note over Ollama: LLM generates multiple tool calls\n\n    Ollama-&gt;&gt;Ollama: Execute tool 1 \u2192 ToolOutput 1\n    Ollama-&gt;&gt;Ollama: Execute tool 2 \u2192 ToolOutput 2\n\n    Ollama--&gt;&gt;ToolOrchestratingLLM: AgentChatResponse with multiple sources\n    deactivate Ollama\n\n    ToolOrchestratingLLM-&gt;&gt;AgentChatResponse: parse_tool_outputs(allow_parallel=True)\n    activate AgentChatResponse\n    Note over AgentChatResponse: Extract all raw_outputs&lt;br/&gt;from sources list\n    AgentChatResponse--&gt;&gt;ToolOrchestratingLLM: List[MockAlbum]\n    deactivate AgentChatResponse\n\n    ToolOrchestratingLLM--&gt;&gt;User: [MockAlbum(...), MockAlbum(...)]\n    deactivate ToolOrchestratingLLM</code></pre>"},{"location":"reference/core/structured_tools/tool_orchestrating_llm/tool_orchestrating_llm_sequence/#async-execution-flow","title":"Async Execution Flow","text":"<p>The async flow (<code>acall</code>) follows the same pattern but uses: - <code>apredict_and_call</code> instead of <code>predict_and_call</code> - Async tool execution - All operations are awaited</p>"},{"location":"reference/core/structured_tools/tool_orchestrating_llm/tool_orchestrating_llm_sequence/#streaming-execution-flow","title":"Streaming Execution Flow","text":"<p>For <code>stream_call</code>: 1. Uses <code>stream_chat_with_tools</code> instead of <code>predict_and_call</code> 2. Yields partial responses as <code>StreamingObjectProcessor</code> parses incremental tool calls 3. Maintains <code>cur_objects</code> state across chunks 4. Each yield contains progressively updated Pydantic instances</p>"},{"location":"reference/core/structured_tools/tool_orchestrating_llm/tool_orchestrating_llm_state/","title":"State Transitions and Lifecycle","text":"<p>This diagram shows the state transitions and lifecycle of <code>ToolOrchestratingLLM</code>.</p>  Hold \"Ctrl\" to enable pan &amp; zoom  <pre><code>stateDiagram-v2\n    [*] --&gt; Uninitialized: User creates instance\n\n    Uninitialized --&gt; Validating: __init__ called\n\n    state Validating {\n        [*] --&gt; ValidatingLLM\n        ValidatingLLM --&gt; ValidatingPrompt: LLM valid\n        ValidatingPrompt --&gt; StoringState: Prompt valid\n        StoringState --&gt; [*]\n\n        ValidatingLLM --&gt; Error: No LLM or not function-calling\n        ValidatingPrompt --&gt; Error: Invalid prompt type\n    }\n\n    Validating --&gt; Ready: All validations pass\n    Validating --&gt; [*]: Validation error\n\n    Ready --&gt; ExecutingSync: __call__ invoked\n    Ready --&gt; ExecutingAsync: acall invoked\n    Ready --&gt; ExecutingStream: stream_call invoked\n    Ready --&gt; ExecutingAsyncStream: astream_call invoked\n    Ready --&gt; UpdatingPrompt: prompt.setter called\n\n    UpdatingPrompt --&gt; Ready: Prompt updated\n\n    state ExecutingSync {\n        [*] --&gt; CreatingTool\n        CreatingTool --&gt; FormattingPrompt\n        FormattingPrompt --&gt; ExtendingMessages\n        ExtendingMessages --&gt; CallingLLM\n        CallingLLM --&gt; ParsingToolCalls\n        ParsingToolCalls --&gt; ExecutingTools\n        ExecutingTools --&gt; CreatingResponse\n        CreatingResponse --&gt; ExtractingOutput\n        ExtractingOutput --&gt; [*]\n\n        CallingLLM --&gt; SyncError: Network/timeout error\n        ParsingToolCalls --&gt; SyncError: No tool_calls\n        ExecutingTools --&gt; SyncError: Validation error\n    }\n\n    state ExecutingAsync {\n        [*] --&gt; CreatingToolAsync\n        CreatingToolAsync --&gt; FormattingPromptAsync\n        FormattingPromptAsync --&gt; ExtendingMessagesAsync\n        ExtendingMessagesAsync --&gt; CallingLLMAsync\n        CallingLLMAsync --&gt; ParsingToolCallsAsync\n        ParsingToolCallsAsync --&gt; ExecutingToolsAsync\n        ExecutingToolsAsync --&gt; CreatingResponseAsync\n        CreatingResponseAsync --&gt; ExtractingOutputAsync\n        ExtractingOutputAsync --&gt; [*]\n\n        CallingLLMAsync --&gt; AsyncError: Network/timeout error\n        ParsingToolCallsAsync --&gt; AsyncError: No tool_calls\n        ExecutingToolsAsync --&gt; AsyncError: Validation error\n    }\n\n    state ExecutingStream {\n        [*] --&gt; CreatingToolStream\n        CreatingToolStream --&gt; FormattingPromptStream\n        FormattingPromptStream --&gt; InitiatingStream\n        InitiatingStream --&gt; StreamingChunks\n\n        state StreamingChunks {\n            [*] --&gt; ReceivingChunk\n            ReceivingChunk --&gt; ProcessingChunk\n            ProcessingChunk --&gt; ParsingPartialToolCalls\n            ParsingPartialToolCalls --&gt; UpdatingObjects\n            UpdatingObjects --&gt; YieldingPartial\n            YieldingPartial --&gt; MoreChunks: Has more chunks\n            MoreChunks --&gt; ReceivingChunk\n            YieldingPartial --&gt; [*]: Stream complete\n\n            ProcessingChunk --&gt; SkippingChunk: Parse error (logged)\n            SkippingChunk --&gt; MoreChunks\n        }\n\n        StreamingChunks --&gt; [*]\n    }\n\n    state ExecutingAsyncStream {\n        [*] --&gt; CreatingToolAsyncStream\n        CreatingToolAsyncStream --&gt; FormattingPromptAsyncStream\n        FormattingPromptAsyncStream --&gt; InitiatingAsyncStream\n        InitiatingAsyncStream --&gt; StreamingChunksAsync\n\n        state StreamingChunksAsync {\n            [*] --&gt; ReceivingChunkAsync\n            ReceivingChunkAsync --&gt; ProcessingChunkAsync\n            ProcessingChunkAsync --&gt; ParsingPartialAsync\n            ParsingPartialAsync --&gt; UpdatingObjectsAsync\n            UpdatingObjectsAsync --&gt; YieldingPartialAsync\n            YieldingPartialAsync --&gt; MoreChunksAsync: Has more chunks\n            MoreChunksAsync --&gt; ReceivingChunkAsync\n            YieldingPartialAsync --&gt; [*]: Stream complete\n\n            ProcessingChunkAsync --&gt; SkippingChunkAsync: Parse error (logged)\n            SkippingChunkAsync --&gt; MoreChunksAsync\n        }\n\n        StreamingChunksAsync --&gt; [*]\n    }\n\n    ExecutingSync --&gt; Ready: Returns output\n    ExecutingSync --&gt; [*]: Error raised\n\n    ExecutingAsync --&gt; Ready: Returns output\n    ExecutingAsync --&gt; [*]: Error raised\n\n    ExecutingStream --&gt; Ready: Stream complete\n    ExecutingStream --&gt; [*]: Error raised\n\n    ExecutingAsyncStream --&gt; Ready: Stream complete\n    ExecutingAsyncStream --&gt; [*]: Error raised\n\n    Ready --&gt; [*]: Instance destroyed</code></pre>"},{"location":"reference/core/structured_tools/tool_orchestrating_llm/tool_orchestrating_llm_state/#state-descriptions","title":"State Descriptions","text":""},{"location":"reference/core/structured_tools/tool_orchestrating_llm/tool_orchestrating_llm_state/#uninitialized","title":"Uninitialized","text":"<ul> <li>Entry: Instance creation started</li> <li>Actions: None</li> <li>Exit: When <code>__init__</code> is called</li> <li>Data: Constructor arguments available</li> </ul>"},{"location":"reference/core/structured_tools/tool_orchestrating_llm/tool_orchestrating_llm_state/#validating","title":"Validating","text":"<ul> <li>Entry: <code>__init__</code> method executing</li> <li>Substates:</li> <li><code>ValidatingLLM</code>: Check LLM exists and supports function calling</li> <li><code>ValidatingPrompt</code>: Convert/validate prompt template</li> <li><code>StoringState</code>: Store all validated components</li> <li>Exit: All validations pass or error raised</li> <li>Data: Validated components stored in instance attributes</li> </ul>"},{"location":"reference/core/structured_tools/tool_orchestrating_llm/tool_orchestrating_llm_state/#ready","title":"Ready","text":"<ul> <li>Entry: Instance fully initialized</li> <li>Actions: Waiting for execution call</li> <li>Transitions:</li> <li><code>__call__</code> \u2192 ExecutingSync</li> <li><code>acall</code> \u2192 ExecutingAsync</li> <li><code>stream_call</code> \u2192 ExecutingStream</li> <li><code>astream_call</code> \u2192 ExecutingAsyncStream</li> <li><code>prompt.setter</code> \u2192 UpdatingPrompt</li> <li>Data: All instance state available and immutable (except prompt)</li> </ul>"},{"location":"reference/core/structured_tools/tool_orchestrating_llm/tool_orchestrating_llm_state/#executingsync-synchronous-execution","title":"ExecutingSync (Synchronous Execution)","text":"<ul> <li>Entry: <code>__call__</code> invoked</li> <li>Substates:</li> <li><code>CreatingTool</code>: CallableTool.from_model(output_cls)</li> <li><code>FormattingPrompt</code>: format_messages(**kwargs)</li> <li><code>ExtendingMessages</code>: _extend_messages(messages)</li> <li><code>CallingLLM</code>: predict_and_call(tools, messages, ...)</li> <li><code>ParsingToolCalls</code>: Extract tool_calls from response</li> <li><code>ExecutingTools</code>: Validate args and create Pydantic instances</li> <li><code>CreatingResponse</code>: Build AgentChatResponse</li> <li><code>ExtractingOutput</code>: parse_tool_outputs(allow_parallel)</li> <li>Exit: Returns Pydantic instance(s) or raises error</li> <li>Data: Transient execution state (tool, messages, response)</li> </ul>"},{"location":"reference/core/structured_tools/tool_orchestrating_llm/tool_orchestrating_llm_state/#executingasync-asynchronous-execution","title":"ExecutingAsync (Asynchronous Execution)","text":"<ul> <li>Entry: <code>acall</code> invoked</li> <li>Substates: Same as ExecutingSync but async</li> <li>Exit: Returns Pydantic instance(s) or raises error</li> <li>Data: Same as ExecutingSync</li> </ul>"},{"location":"reference/core/structured_tools/tool_orchestrating_llm/tool_orchestrating_llm_state/#executingstream-synchronous-streaming","title":"ExecutingStream (Synchronous Streaming)","text":"<ul> <li>Entry: <code>stream_call</code> invoked</li> <li>Substates:</li> <li><code>CreatingToolStream</code>: Create CallableTool</li> <li><code>FormattingPromptStream</code>: Format messages</li> <li><code>InitiatingStream</code>: Start stream_chat_with_tools</li> <li><code>StreamingChunks</code>: Process chunks loop<ul> <li><code>ReceivingChunk</code>: Get next chunk from generator</li> <li><code>ProcessingChunk</code>: Initialize StreamingObjectProcessor</li> <li><code>ParsingPartialToolCalls</code>: Parse partial JSON</li> <li><code>UpdatingObjects</code>: Update cur_objects state</li> <li><code>YieldingPartial</code>: Yield partial/complete model</li> <li><code>MoreChunks</code>: Check if more chunks available</li> <li><code>SkippingChunk</code>: Handle parse errors gracefully</li> </ul> </li> <li>Exit: Generator exhausted or error raised</li> <li>Data: Streaming state (cur_objects, partial_resp)</li> </ul>"},{"location":"reference/core/structured_tools/tool_orchestrating_llm/tool_orchestrating_llm_state/#executingasyncstream-asynchronous-streaming","title":"ExecutingAsyncStream (Asynchronous Streaming)","text":"<ul> <li>Entry: <code>astream_call</code> invoked</li> <li>Substates: Same as ExecutingStream but async</li> <li>Exit: Async generator exhausted or error raised</li> <li>Data: Same as ExecutingStream</li> </ul>"},{"location":"reference/core/structured_tools/tool_orchestrating_llm/tool_orchestrating_llm_state/#updatingprompt","title":"UpdatingPrompt","text":"<ul> <li>Entry: <code>prompt.setter</code> called</li> <li>Actions: Update <code>_prompt</code> attribute</li> <li>Exit: Immediately returns to Ready</li> <li>Data: New prompt template stored</li> </ul>"},{"location":"reference/core/structured_tools/tool_orchestrating_llm/tool_orchestrating_llm_state/#state-data-by-phase","title":"State Data by Phase","text":""},{"location":"reference/core/structured_tools/tool_orchestrating_llm/tool_orchestrating_llm_state/#initialization-phase","title":"Initialization Phase","text":"<pre><code># Uninitialized \u2192 Validating \u2192 Ready\n{\n    \"_output_cls\": Type[MockAlbum],\n    \"_llm\": Ollama,\n    \"_prompt\": PromptTemplate,\n    \"_verbose\": False,\n    \"_allow_parallel_tool_calls\": False,\n    \"_tool_choice\": None\n}\n</code></pre>"},{"location":"reference/core/structured_tools/tool_orchestrating_llm/tool_orchestrating_llm_state/#execution-phase-sync","title":"Execution Phase (Sync)","text":"<pre><code># Ready \u2192 ExecutingSync \u2192 Ready\n{\n    # Instance state (immutable during execution)\n    \"_output_cls\": Type[MockAlbum],\n    \"_llm\": Ollama,\n    \"_prompt\": PromptTemplate,\n\n    # Transient state (created during execution)\n    \"kwargs\": {\"topic\": \"songs\"},\n    \"tool\": CallableTool,\n    \"messages\": List[Message],\n    \"agent_response\": AgentChatResponse,\n    \"output\": MockAlbum\n}\n</code></pre>"},{"location":"reference/core/structured_tools/tool_orchestrating_llm/tool_orchestrating_llm_state/#streaming-phase","title":"Streaming Phase","text":"<pre><code># Ready \u2192 ExecutingStream \u2192 Ready\n{\n    # Instance state\n    \"_output_cls\": Type[MockAlbum],\n    \"_llm\": Ollama,\n    \"_prompt\": PromptTemplate,\n\n    # Streaming state (maintained across chunks)\n    \"tool\": CallableTool,\n    \"messages\": List[Message],\n    \"chat_response_gen\": Generator,\n    \"cur_objects\": List[MockAlbum],  # Progressive state\n    \"partial_resp\": StreamingChatResponse\n}\n</code></pre>"},{"location":"reference/core/structured_tools/tool_orchestrating_llm/tool_orchestrating_llm_state/#transition-triggers","title":"Transition Triggers","text":""},{"location":"reference/core/structured_tools/tool_orchestrating_llm/tool_orchestrating_llm_state/#user-triggered-transitions","title":"User-Triggered Transitions","text":"<ol> <li><code>__init__</code> \u2192 Starts validation</li> <li><code>__call__</code> \u2192 Starts sync execution</li> <li><code>acall</code> \u2192 Starts async execution</li> <li><code>stream_call</code> \u2192 Starts streaming</li> <li><code>astream_call</code> \u2192 Starts async streaming</li> <li><code>prompt.setter</code> \u2192 Updates prompt</li> </ol>"},{"location":"reference/core/structured_tools/tool_orchestrating_llm/tool_orchestrating_llm_state/#system-triggered-transitions","title":"System-Triggered Transitions","text":"<ol> <li>Validation success \u2192 Ready state</li> <li>Validation failure \u2192 Error state</li> <li>Execution complete \u2192 Return to Ready</li> <li>Execution error \u2192 Error state</li> <li>Stream chunk received \u2192 Process and yield</li> <li>Stream exhausted \u2192 Return to Ready</li> </ol>"},{"location":"reference/core/structured_tools/tool_orchestrating_llm/tool_orchestrating_llm_state/#error-states","title":"Error States","text":""},{"location":"reference/core/structured_tools/tool_orchestrating_llm/tool_orchestrating_llm_state/#validation-errors-initialization","title":"Validation Errors (Initialization)","text":"<ul> <li><code>AssertionError</code>: No LLM provided and Configs.llm not set</li> <li><code>ValueError</code>: LLM does not support function calling</li> <li><code>ValueError</code>: Invalid prompt type</li> </ul>"},{"location":"reference/core/structured_tools/tool_orchestrating_llm/tool_orchestrating_llm_state/#execution-errors-runtime","title":"Execution Errors (Runtime)","text":"<ul> <li><code>ValueError</code>: LLM errors (network, timeout, invalid args)</li> <li><code>ValidationError</code>: Tool argument validation fails</li> <li><code>AttributeError</code>: Missing raw_output in ToolOutput</li> </ul>"},{"location":"reference/core/structured_tools/tool_orchestrating_llm/tool_orchestrating_llm_state/#streaming-errors-runtime","title":"Streaming Errors (Runtime)","text":"<ul> <li>Parse errors are logged as warnings and skipped</li> <li>Critical errors raised and propagated</li> </ul>"},{"location":"reference/core/structured_tools/tool_orchestrating_llm/tool_orchestrating_llm_state/#state-persistence","title":"State Persistence","text":""},{"location":"reference/core/structured_tools/tool_orchestrating_llm/tool_orchestrating_llm_state/#persistent-across-calls","title":"Persistent Across Calls","text":"<ul> <li><code>_output_cls</code>: Never changes</li> <li><code>_llm</code>: Never changes</li> <li><code>_prompt</code>: Can be updated via setter</li> <li><code>_verbose</code>: Never changes</li> <li><code>_allow_parallel_tool_calls</code>: Never changes</li> <li><code>_tool_choice</code>: Never changes</li> </ul>"},{"location":"reference/core/structured_tools/tool_orchestrating_llm/tool_orchestrating_llm_state/#transient-per-call","title":"Transient Per Call","text":"<ul> <li>Tool instance</li> <li>Formatted messages</li> <li>LLM response</li> <li>Parsed output</li> </ul>"},{"location":"reference/core/structured_tools/tool_orchestrating_llm/tool_orchestrating_llm_state/#transient-per-stream","title":"Transient Per Stream","text":"<ul> <li><code>cur_objects</code>: Maintains progressive parsing state</li> <li>Each new stream_call creates fresh state</li> </ul>"},{"location":"reference/core/structured_tools/tool_orchestrating_llm/tool_orchestrating_llm_state/#parallel-execution-states","title":"Parallel Execution States","text":"<p>When <code>allow_parallel_tool_calls=True</code>:</p>  Hold \"Ctrl\" to enable pan &amp; zoom  <pre><code>stateDiagram-v2\n    [*] --&gt; ExecutingTools: Multiple tool_calls\n\n    state ExecutingTools {\n        [*] --&gt; ExecuteTool1\n        [*] --&gt; ExecuteTool2\n\n        ExecuteTool1 --&gt; ValidateTool1\n        ExecuteTool2 --&gt; ValidateTool2\n\n        ValidateTool1 --&gt; CreateModel1\n        ValidateTool2 --&gt; CreateModel2\n\n        CreateModel1 --&gt; WrapOutput1\n        CreateModel2 --&gt; WrapOutput2\n\n        WrapOutput1 --&gt; CollectOutputs\n        WrapOutput2 --&gt; CollectOutputs\n\n        CollectOutputs --&gt; [*]: All complete\n    }\n\n    ExecutingTools --&gt; [*]: List[MockAlbum]</code></pre>"},{"location":"reference/core/structured_tools/tool_orchestrating_llm/tool_orchestrating_llm_state/#lifecycle-summary","title":"Lifecycle Summary","text":"<ol> <li>Birth: Instance created \u2192 Validation \u2192 Ready</li> <li>Active: Ready \u2192 Execute \u2192 Ready (repeatable)</li> <li>Mutable: Prompt can be updated at any time when Ready</li> <li>Death: Instance destroyed when no longer referenced</li> </ol> <p>Each execution is independent and returns the instance to Ready state, allowing multiple calls with different arguments.</p>"},{"location":"reference/core/tools/callable_tools/","title":"Callable Tools","text":"Hold \"Ctrl\" to enable pan &amp; zoom"},{"location":"reference/core/tools/tools/","title":"serapeum.core.tools \u2014 Developer Documentation","text":"<p>This documentation provides a complete, visual, and navigable overview of the <code>serapeum.core.tools</code> submodule: purpose, structure, classes and functions, relationships, diagrams, and usage examples.</p>"},{"location":"reference/core/tools/tools/#high-level-overview","title":"High-level overview","text":"<p>The <code>tools</code> submodule turns ordinary Python callables into LLM-callable \"tools\" and provides a unified runtime to execute them synchronously or asynchronously. It standardizes:</p> <ul> <li>Tool metadata (name, description, input schema) for function-calling providers.</li> <li>Tool outputs (text and multimodal chunks) via a common <code>ToolOutput</code> container.</li> <li>A consistent calling surface for both sync and async functions/classes.</li> <li>A robust executor with optional single-argument auto-unpacking and standardized error handling.</li> </ul> <p>Main capabilities: - Derive Pydantic input schemas from Python function signatures and docstrings. - Wrap a function into a <code>CallableTool</code> that returns <code>ToolOutput</code> and carries <code>ToolMetadata</code>. - Execute tools with <code>ToolExecutor</code> (sync/async) or adapt a synchronous tool to async flows.</p>"},{"location":"reference/core/tools/tools/#package-hierarchy","title":"Package hierarchy","text":"<p>See the package and file layout.</p>  Hold \"Ctrl\" to enable pan &amp; zoom  <pre><code>%% Mermaid Package Hierarchy for serapeum.core.tools\n%% Save as docs/tools/diagrams/package-hierarchy.mmd\n\nflowchart TD\n    A[serapeum] --&gt; B[core]\n    B --&gt; C[tools]\n    C --&gt; C1[__init__.py]\n    C --&gt; C2[callable_tool.py]\n    C --&gt; C3[models.py]\n    C --&gt; C4[utils.py]\n\n    %% Context (other top-level packages, optional)\n    A -.-&gt; LLMS[llms]\n    A -.-&gt; CORE_BASE[core/base]</code></pre>"},{"location":"reference/core/tools/tools/#module-dependency-diagram","title":"Module dependency diagram","text":"<p>This shows how the modules in <code>serapeum.core.tools</code> depend on one another and on external packages inside the project.</p> <ul> <li>Draw.io:  diagrams/module-deps.drawio</li> </ul>  Hold \"Ctrl\" to enable pan &amp; zoom  <pre><code>%% Mermaid Module Dependency Diagram for serapeum.core.tools\n%% Save as docs/tools/diagrams/module-deps.mmd\n\ngraph LR\n    subgraph \"serapeum.core.tools\"\n        tools_init[\"tools.__init__\"]\n        callable_tool[\"callable_tool.py\"]\n        models[\"models.py\"]\n        utils[\"utils.py\"]\n    end\n\n    tools_init --&gt; callable_tool\n    tools_init --&gt; models\n    tools_init --&gt; utils\n\n    callable_tool --&gt; models\n    callable_tool --&gt; utils\n    callable_tool --&gt; async_utils[\"serapeum.core.utils.async_utils\"]\n    callable_tool --&gt; core_llms_models[\"serapeum.core.base.llms.models\"]\n\n    models --&gt; core_llms_models\n    models --&gt; pydantic[\"pydantic\"]\n\n    utils --&gt; models\n    utils --&gt; pydantic\n    utils --&gt; inspect[\"inspect\"]\n    utils --&gt; datetime[\"datetime\"]</code></pre>"},{"location":"reference/core/tools/tools/#module-by-module-breakdown","title":"Module-by-module breakdown","text":"<ul> <li>init.py</li> <li> <p>Re-exports key public types for convenience: <code>CallableTool</code>, <code>ToolOutput</code>, <code>ToolCallArguments</code>.</p> </li> <li> <p>models.py</p> </li> <li>MinimalToolSchema: Default args schema when no custom schema is provided (<code>{\"input\": str}</code>).</li> <li>ToolMetadata: Name/description/schema of a tool; exports OpenAI-style function tool specs.</li> <li>ToolOutput: Standard output holder containing text/image/audio chunks, raw input/output, errors.</li> <li>BaseTool / AsyncBaseTool: Base interfaces for tools (sync/async contracting).</li> <li>BaseToolAsyncAdapter / adapt_to_async_tool: Adapts sync tools to the async interface.</li> <li> <p>ToolCallArguments: Selected tool name/id and kwargs to pass at runtime.</p> </li> <li> <p>callable_tool.py</p> </li> <li>SyncAsyncConverter: Bridges sync&lt;-&gt;async callables in both directions.</li> <li> <p>CallableTool: Wraps a Python callable (sync or async), infers metadata and schema when needed, and returns <code>ToolOutput</code>.</p> </li> <li> <p>utils.py</p> </li> <li>Docstring: Parses Google/Sphinx/Javadoc-style parameter descriptions from function docstrings.</li> <li>FunctionArgument: Converts <code>inspect.Parameter</code> to <code>(type, FieldInfo)</code> with sensible defaults.</li> <li>FunctionConverter: Builds a Pydantic model from a function signature (+Annotated, +datetime formats).</li> <li>ExecutionConfig: Flags for executor behavior.</li> <li>ToolExecutor: Safe execution harness (sync/async) with optional single-arg auto-unpack and error standardization.</li> </ul>"},{"location":"reference/core/tools/tools/#uml-class-diagram","title":"UML class diagram","text":"<p>Quick preview (key relationships only; see the full file for details):</p>  Hold \"Ctrl\" to enable pan &amp; zoom  <pre><code>classDiagram\n  BaseTool &lt;|-- AsyncBaseTool\n  AsyncBaseTool &lt;|-- BaseToolAsyncAdapter\n  AsyncBaseTool &lt;|-- CallableTool\n  CallableTool ..&gt; ToolMetadata\n  CallableTool ..&gt; ToolOutput\n  ToolExecutor --&gt; ExecutionConfig\n  ToolExecutor ..&gt; ToolOutput\n  ToolCallArguments ..&gt; ToolExecutor\n  ToolMetadata ..&gt; MinimalToolSchema</code></pre> <ul> <li>Detailed class diagram</li> </ul>  Hold \"Ctrl\" to enable pan &amp; zoom  <pre><code>%% Mermaid Class Diagram for serapeum.core.tools\n%% Save as docs/tools/diagrams/uml-classes.mmd\n\nclassDiagram\n    %% Core models\n    class MinimalToolSchema {\n        +input: str\n    }\n\n    class ToolMetadata {\n        +name: str?\n        +description: str\n        +tool_schema: BaseModel?\n        +return_direct: bool\n        +get_schema() dict\n        +tool_schema_str: str\n        +get_name() str\n        +to_openai_tool(skip_length_check=False) Dict\n    }\n\n    class ToolOutput {\n        +chunks: List~ChunkType~\n        +tool_name: str\n        +raw_input: Dict\n        +raw_output: Any\n        +is_error: bool\n        +content: str\n        +__str__() str\n    }\n\n    class ToolCallArguments {\n        +tool_id: str\n        +tool_name: str\n        +tool_kwargs: Dict~str, Any~\n    }\n\n    %% Tool interfaces and adapters\n    class BaseTool {\n        &lt;&lt;abstract&gt;&gt;\n        +metadata: ToolMetadata\n        +__call__(input_values) ToolOutput\n    }\n\n    class AsyncBaseTool {\n        &lt;&lt;abstract&gt;&gt;\n        +call(input_values) ToolOutput\n        +acall(input_values) ToolOutput\n    }\n\n    class BaseToolAsyncAdapter {\n        +base_tool: BaseTool\n        +call(input_values) ToolOutput\n        +acall(input_values) ToolOutput\n    }\n\n    %% Callable adapter\n    class CallableTool {\n        +metadata: ToolMetadata\n        +default_arguments: Dict\n        +sync_func(*args, **kwargs) Any\n        +async_func(*args, **kwargs) Awaitable\n        +call(*args, **kwargs) ToolOutput\n        +acall(*args, **kwargs) ToolOutput\n        +from_function(...)\n    }\n\n    class SyncAsyncConverter {\n        +is_async(func) bool\n        +to_async(fn) AsyncCallable\n        +async_to_sync(func_async) Callable\n        -sync_func\n        -async_func\n    }\n\n    %% Schema utilities\n    class Docstring {\n        +signature\n        +extract_param_docs() (dict, set)\n        +get_short_summary_line() str\n    }\n\n    class FunctionArgument {\n        +to_field() (Type, FieldInfo)\n    }\n\n    class FunctionConverter {\n        +to_schema() Type~BaseModel~\n    }\n\n    %% Execution utilities\n    class ExecutionConfig {\n        +verbose: bool\n        +single_arg_auto_unpack: bool\n        +raise_on_error: bool\n    }\n\n    class ToolExecutor {\n        +execute(tool, arguments) ToolOutput\n        +execute_async(tool, arguments) ToolOutput\n        +execute_with_selection(sel, tools) ToolOutput\n        +execute_async_with_selection(sel, tools) ToolOutput\n    }\n\n    %% External content chunk types\n    class TextChunk\n    class Image\n    class Audio\n\n    %% Inheritance\n    BaseTool &lt;|-- AsyncBaseTool\n    AsyncBaseTool &lt;|-- BaseToolAsyncAdapter\n    AsyncBaseTool &lt;|-- CallableTool\n\n    %% Associations/uses\n    MinimalToolSchema &lt;.. ToolMetadata : default\n    ToolMetadata ..&gt; MinimalToolSchema : fallback\n    ToolOutput ..&gt; TextChunk : contains\n    CallableTool ..&gt; ToolMetadata : uses\n    CallableTool ..&gt; ToolOutput : produces\n    CallableTool ..&gt; Docstring : parses\n    CallableTool ..&gt; FunctionConverter : builds schema\n    CallableTool ..&gt; SyncAsyncConverter : wraps\n    CallableTool ..&gt; TextChunk\n    CallableTool ..&gt; Image\n    CallableTool ..&gt; Audio\n    FunctionConverter ..&gt; FunctionArgument : converts\n    BaseToolAsyncAdapter --&gt; BaseTool : wraps\n    ToolExecutor --&gt; ExecutionConfig : configured by\n    ToolExecutor ..&gt; ToolOutput : returns\n    ToolExecutor ..&gt; AsyncBaseTool : executes\n    ToolCallArguments ..&gt; ToolExecutor : inputs to</code></pre>"},{"location":"reference/core/tools/tools/#call-graph-key-flows","title":"Call graph (key flows)","text":"<ul> <li>Draw.io:  diagrams/call-graph.drawio</li> </ul>  Hold \"Ctrl\" to enable pan &amp; zoom  <pre><code>%% Mermaid Call Graphs for serapeum.core.tools\n%% Save as docs/tools/diagrams/call-graph.mmd\n\nflowchart TD\n    %% CallableTool flow\n    subgraph CallableTool\n        CT_call[\"__call__(*args, **kwargs)\"] --&gt; CT_merge[\"merge default_arguments\"]\n        CT_merge --&gt; CT_call2[\"call(*args, **kwargs)\"]\n        CT_call2 --&gt; CT_sync[\"_sync_func(*args, **kwargs)\"]\n        CT_sync --&gt; CT_parse[\"_parse_tool_output(raw)\"]\n        CT_parse --&gt; CT_out[\"ToolOutput(... )&lt;br/&gt;(chunks, tool_name, raw_input, raw_output)\"]\n\n        CT_acall[\"acall(*args, **kwargs)\"] --&gt; CT_async[\"_async_func(*args, **kwargs)\"]\n        CT_async --&gt; CT_parse\n    end\n\n    %% ToolExecutor flow\n    subgraph ToolExecutor\n        EX_exec[\"execute(tool, arguments)\"] --&gt; EX_logStart[\"_log_execution_start (optional)\"]\n        EX_logStart --&gt; EX_invoke{\"_should_unpack_single_arg?\"}\n        EX_invoke -- yes --&gt; EX_try1[\"_try_single_arg_then_kwargs\"]\n        EX_invoke -- no  --&gt; EX_direct[\"tool(**arguments)\"]\n        EX_try1 --&gt; EX_result[\"ToolOutput\"]\n        EX_direct --&gt; EX_result\n        EX_exec --&gt;|exception| EX_err[\"_create_error_output\"]\n        EX_result --&gt; EX_logRes[\"_log_execution_result (optional)\"]\n\n        EX_async[\"execute_async(tool, arguments)\"] --&gt; EX_logStart2[\"_log_execution_start (optional)\"]\n        EX_logStart2 --&gt; EX_adapt[\"adapt_to_async_tool(tool)\"]\n        EX_adapt --&gt; EX_invokeA{\"_should_unpack_single_arg?\"}\n        EX_invokeA -- yes --&gt; EX_try1A[\"_try_single_arg_then_kwargs_async\"]\n        EX_invokeA -- no  --&gt; EX_directA[\"async_tool.acall(**arguments)\"]\n        EX_try1A --&gt; EX_resultA[\"ToolOutput\"]\n        EX_directA --&gt; EX_resultA\n        EX_async --&gt;|exception| EX_errA[\"_create_error_output\"]\n        EX_resultA --&gt; EX_logRes2[\"_log_execution_result (optional)\"]\n    end</code></pre>"},{"location":"reference/core/tools/tools/#summary-of-main-classes-and-functions","title":"Summary of main classes and functions","text":"Name Kind Purpose Key methods/properties MinimalToolSchema Pydantic model Default function args schema <code>{input: str}</code> when no custom schema is provided <code>input: str</code> ToolMetadata dataclass Describes tool (name, description, schema, return_direct) and exports to provider formats <code>get_schema()</code>, <code>tool_schema_str</code>, <code>get_name()</code>, <code>to_openai_tool()</code> ToolOutput Pydantic model Standard tool response: chunks, <code>content</code> view, raw input/output, error flag <code>content</code> property, <code>__str__</code> BaseTool abstract class Sync tool interface <code>metadata</code>, <code>__call__</code> AsyncBaseTool abstract class Async-capable tool interface <code>call()</code>, <code>acall()</code> BaseToolAsyncAdapter class Wraps <code>BaseTool</code> to provide async interface <code>metadata</code>, <code>call()</code>, <code>acall()</code> adapt_to_async_tool function Returns <code>AsyncBaseTool</code> (adapts if needed) \u2014 ToolCallArguments Pydantic model Tool selection and kwargs with forgiving validation validator: ignore non-dict <code>tool_kwargs</code> SyncAsyncConverter helper class Converts sync-&gt;async and async-&gt;sync wrappers <code>to_async()</code>, <code>async_to_sync()</code> CallableTool class Turns a Python callable into a Tool with metadata/schema and standardized output <code>from_function()</code>, <code>metadata</code>, <code>sync_func</code>, <code>async_func</code>, <code>call()</code>, <code>acall()</code> Docstring helper class Extracts param docs and brief summary from docstrings <code>extract_param_docs()</code>, <code>get_short_summary_line()</code> FunctionArgument helper class Converts <code>inspect.Parameter</code> to <code>(type, FieldInfo)</code>, supports Annotated <code>to_field()</code> FunctionConverter helper class Creates Pydantic model from function signature (+extra fields) <code>to_schema()</code> ExecutionConfig dataclass Executor behavior flags <code>verbose</code>, <code>single_arg_auto_unpack</code>, <code>raise_on_error</code> ToolExecutor class Orchestrates tool execution with error handling <code>execute()</code>, <code>execute_async()</code>, <code>execute_with_selection()</code>, <code>execute_async_with_selection()</code>"},{"location":"reference/core/tools/tools/#usage-examples","title":"Usage examples","text":"<p>Wrap a synchronous function and call it</p> <pre><code>from serapeum.core.tools.callable_tool import CallableTool\nfrom serapeum.core.tools.models import ToolMetadata\n\ndef greet(name: str) -&gt; str:\n    \"\"\"Greet a user by name.\"\"\"\n    return f\"Hello, {name}!\"\n\ntool = CallableTool(func=greet, metadata=ToolMetadata(name=\"greet\", description=\"Greets by name\"))\nresult = tool(\"Ada\")  # or tool.call(\"Ada\")\nprint(result.content)  # \"Hello, Ada!\"\n</code></pre> <p>Wrap an async function and await it</p> <pre><code>import asyncio\nfrom serapeum.core.tools.callable_tool import CallableTool\nfrom serapeum.core.tools.models import ToolMetadata\n\nasync def add(a: int, b: int) -&gt; int:\n    return a + b\n\ntool = CallableTool(func=add, metadata=ToolMetadata(name=\"add\", description=\"Add two ints\"))\nresult = asyncio.run(tool.acall(2, 3))\nprint(result.content)  # \"5\"\n</code></pre> <p>Infer schema and metadata with from_function</p> <pre><code>from serapeum.core.tools.callable_tool import CallableTool\n\n# from_function inspects signature and docstring to infer schema and description\n# (first docstring line + signature)\n\ndef power(base: int, exp: int = 2) -&gt; int:\n    \"\"\"Exponentiation.\"\"\"\n    return base ** exp\n\ntool = CallableTool.from_function(power)\nprint(tool.metadata.get_name())  # \"power\"\nprint(tool(\"3\").content)       # \"9\" (exp defaults to 2 unless overridden)\n</code></pre> <p>Export as an OpenAI function tool</p> <pre><code>from pydantic import BaseModel\nfrom serapeum.core.tools.models import ToolMetadata\n\nclass SearchArgs(BaseModel):\n    query: str\n    limit: int | None = None\n\nmeta = ToolMetadata(name=\"search\", description=\"Search items\", tool_schema=SearchArgs)\nprint(meta.to_openai_tool())\n</code></pre> <p>Execute with ToolExecutor (selection-based)</p> <pre><code>from serapeum.core.tools.models import ToolCallArguments\nfrom serapeum.core.tools.utils import ToolExecutor\nfrom serapeum.core.tools.callable_tool import CallableTool\nfrom serapeum.core.tools.models import ToolMetadata\n\n# Prepare two tools\nsay = CallableTool.from_function(lambda text: text, name=\"say\")\ninc = CallableTool(func=lambda x: x + 1, metadata=ToolMetadata(name=\"inc\", description=\"Increment\"))\n\n# Simulate a selection coming from an LLM\nselection = ToolCallArguments(tool_id=\"1\", tool_name=\"say\", tool_kwargs={\"text\": \"hi\"})\n\nexecutor = ToolExecutor()\nout = executor.execute_with_selection(selection, [say, inc])\nprint(out.content)  # \"hi\"\n</code></pre> <p>Single-argument auto-unpack</p> <pre><code>from serapeum.core.tools.utils import ToolExecutor, ExecutionConfig\nfrom serapeum.core.tools.models import ToolMetadata\nfrom serapeum.core.tools.callable_tool import CallableTool\n\ndef echo_list(lst: list[int]):\n    return \",\".join(map(str, lst))\n\ntool = CallableTool(func=echo_list, metadata=ToolMetadata(name=\"echo_list\", description=\"Echo list\"))\nexecutor = ToolExecutor(ExecutionConfig(single_arg_auto_unpack=True))\nprint(executor.execute(tool, {\"lst\": [1, 2, 3]}).content)  # \"1,2,3\"\n</code></pre>"},{"location":"reference/integrations/llms/ollama/examples/","title":"Ollama Usage Examples","text":"<p>This guide provides comprehensive examples covering all possible ways to use the <code>Ollama</code> LLM class based on real test cases from the codebase.</p>"},{"location":"reference/integrations/llms/ollama/examples/#table-of-contents","title":"Table of Contents","text":"<ol> <li>Basic Usage</li> <li>Initialization Patterns</li> <li>Chat Operations</li> <li>Completion Operations</li> <li>Streaming Operations</li> <li>Tool/Function Calling</li> <li>Integration with Orchestrators</li> <li>Async Operations</li> </ol>"},{"location":"reference/integrations/llms/ollama/examples/#basic-usage","title":"Basic Usage","text":""},{"location":"reference/integrations/llms/ollama/examples/#simple-chat","title":"Simple Chat","text":"<p>The most straightforward way to use <code>Ollama</code>:</p> <pre><code>from serapeum.core.base.llms.models import Message, MessageRole\nfrom serapeum.llms.ollama import Ollama\n\n# Initialize Ollama LLM\nllm = Ollama(\n    model=\"llama3.1\",\n    request_timeout=180,\n)\n\n# Create a message\nmessages = [Message(role=MessageRole.USER, content=\"Say 'pong'.\")]\n\n# Send chat request\nresponse = llm.chat(messages)\nprint(response.message.content)  # \"Pong!\"\n</code></pre>"},{"location":"reference/integrations/llms/ollama/examples/#simple-completion","title":"Simple Completion","text":"<p>Using the completion API:</p> <pre><code>from serapeum.llms.ollama import Ollama\n\n# Initialize Ollama LLM\nllm = Ollama(\n    model=\"llama3.1\",\n    request_timeout=180,\n)\n\n# Send completion request\nresponse = llm.complete(\"Say 'pong'.\")\nprint(response.text)  # \"Pong!\"\n</code></pre>"},{"location":"reference/integrations/llms/ollama/examples/#initialization-patterns","title":"Initialization Patterns","text":""},{"location":"reference/integrations/llms/ollama/examples/#1-basic-initialization","title":"1. Basic Initialization","text":"<p>Minimal configuration:</p> <pre><code>from serapeum.llms.ollama import Ollama\n\nllm = Ollama(model=\"llama3.1\")\n</code></pre>"},{"location":"reference/integrations/llms/ollama/examples/#2-full-configuration","title":"2. Full Configuration","text":"<p>With all common parameters:</p> <pre><code>from serapeum.llms.ollama import Ollama\n\nllm = Ollama(\n    model=\"llama3.1\",\n    base_url=\"http://localhost:11434\",\n    temperature=0.8,\n    context_window=4096,\n    request_timeout=180.0,\n    json_mode=True,\n    keep_alive=\"5m\",\n    additional_kwargs={\"top_p\": 0.9, \"top_k\": 40}\n)\n</code></pre>"},{"location":"reference/integrations/llms/ollama/examples/#3-with-custom-client","title":"3. With Custom Client","text":"<p>Pre-configured Ollama client:</p> <pre><code>from ollama import Client\nfrom serapeum.llms.ollama import Ollama\n\n# Create custom client\nclient = Client(host=\"http://localhost:11434\", timeout=300)\n\n# Pass to Ollama\nllm = Ollama(\n    model=\"llama3.1\",\n    client=client,\n)\n</code></pre>"},{"location":"reference/integrations/llms/ollama/examples/#4-json-mode-for-structured-outputs","title":"4. JSON Mode for Structured Outputs","text":"<p>Enable JSON formatting:</p> <pre><code>from serapeum.llms.ollama import Ollama\n\nllm = Ollama(\n    model=\"llama3.1\",\n    json_mode=True,  # Forces JSON output\n    request_timeout=180,\n)\n</code></pre>"},{"location":"reference/integrations/llms/ollama/examples/#chat-operations","title":"Chat Operations","text":""},{"location":"reference/integrations/llms/ollama/examples/#1-single-turn-chat","title":"1. Single Turn Chat","text":"<p>Basic conversation:</p> <pre><code>from serapeum.core.base.llms.models import Message, MessageRole\nfrom serapeum.llms.ollama import Ollama\n\nllm = Ollama(model=\"llama3.1\", request_timeout=180)\n\nmessages = [\n    Message(role=MessageRole.USER, content=\"What is 2+2?\")\n]\n\nresponse = llm.chat(messages)\nprint(response.message.content)  # \"4\"\n</code></pre>"},{"location":"reference/integrations/llms/ollama/examples/#2-multi-turn-conversation","title":"2. Multi-turn Conversation","text":"<p>With conversation history:</p> <pre><code>from serapeum.core.base.llms.models import Message, MessageRole\nfrom serapeum.llms.ollama import Ollama\n\nllm = Ollama(model=\"llama3.1\", request_timeout=180)\n\nmessages = [\n    Message(role=MessageRole.SYSTEM, content=\"You are a helpful math tutor.\"),\n    Message(role=MessageRole.USER, content=\"What is 2+2?\"),\n    Message(role=MessageRole.ASSISTANT, content=\"2+2 equals 4.\"),\n    Message(role=MessageRole.USER, content=\"What about 3+3?\"),\n]\n\nresponse = llm.chat(messages)\nprint(response.message.content)  # \"3+3 equals 6.\"\n</code></pre>"},{"location":"reference/integrations/llms/ollama/examples/#3-chat-with-parameters","title":"3. Chat with Parameters","text":"<p>Passing custom parameters:</p> <pre><code>from serapeum.core.base.llms.models import Message, MessageRole\nfrom serapeum.llms.ollama import Ollama\n\nllm = Ollama(model=\"llama3.1\", request_timeout=180)\n\nmessages = [Message(role=MessageRole.USER, content=\"Write a creative story.\")]\n\n# Override default settings\nresponse = llm.chat(\n    messages,\n    temperature=0.9,      # Higher for creativity\n    top_p=0.95,\n    max_tokens=500,\n)\n</code></pre>"},{"location":"reference/integrations/llms/ollama/examples/#4-chat-with-images","title":"4. Chat with Images","text":"<p>Multi-modal input (if supported by model):</p> <pre><code>from serapeum.core.base.llms.models import Message, MessageRole, Image\nfrom serapeum.llms.ollama import Ollama\n\nllm = Ollama(model=\"llama4\", request_timeout=180)  # Vision model\n\n# Create message with image\nimage = Image(path=\"docs/reference/integrations/llms/ollama/images/baharia-oasis.jpg\")\nmessages = [\n    Message(\n        role=MessageRole.USER,\n        content=\"What's in this image?\",\n        images=[image]\n    )\n]\n\nresponse = llm.chat(messages)\nprint(response.message.content)\n</code></pre>"},{"location":"reference/integrations/llms/ollama/examples/#completion-operations","title":"Completion Operations","text":""},{"location":"reference/integrations/llms/ollama/examples/#1-basic-completion","title":"1. Basic Completion","text":"<p>Simple text completion:</p> <pre><code>from serapeum.llms.ollama import Ollama\n\nllm = Ollama(model=\"llama3.1\", request_timeout=180)\n\nprompt = \"The capital of France is\"\nresponse = llm.complete(prompt)\nprint(response.text)  # \"Paris\"\n</code></pre>"},{"location":"reference/integrations/llms/ollama/examples/#2-completion-with-parameters","title":"2. Completion with Parameters","text":"<p>Custom generation settings:</p> <pre><code>from serapeum.llms.ollama import Ollama\n\nllm = Ollama(model=\"llama3.1\", request_timeout=180)\n\nresponse = llm.complete(\n    \"Once upon a time\",\n    temperature=0.8,\n    max_tokens=200,\n)\nprint(response.text)\n</code></pre>"},{"location":"reference/integrations/llms/ollama/examples/#3-json-completion","title":"3. JSON Completion","text":"<p>Force JSON output:</p> <pre><code>from serapeum.llms.ollama import Ollama\n\nllm = Ollama(\n    model=\"llama3.1\",\n    json_mode=True,\n    request_timeout=180,\n)\n\nprompt = 'Return {\"name\": \"John\", \"age\": 30} as JSON'\nresponse = llm.complete(prompt)\nprint(response.text)  # {\"name\": \"John\", \"age\": 30}\n</code></pre>"},{"location":"reference/integrations/llms/ollama/examples/#streaming-operations","title":"Streaming Operations","text":""},{"location":"reference/integrations/llms/ollama/examples/#1-stream-chat","title":"1. Stream Chat","text":"<p>Real-time streaming chat:</p> <pre><code>from serapeum.core.base.llms.models import Message, MessageRole\nfrom serapeum.llms.ollama import Ollama\n\nllm = Ollama(model=\"llama3.1\", request_timeout=180)\n\nmessages = [Message(role=MessageRole.USER, content=\"Count from 1 to 5.\")]\n\n# Stream responses\nfor chunk in llm.stream_chat(messages):\n    print(chunk.message.content, end=\"\", flush=True)\n    # Outputs: \"1\" \" 2\" \" 3\" \" 4\" \" 5\"\n</code></pre>"},{"location":"reference/integrations/llms/ollama/examples/#2-stream-completion","title":"2. Stream Completion","text":"<p>Real-time streaming completion:</p> <pre><code>from serapeum.llms.ollama import Ollama\n\nllm = Ollama(model=\"llama3.1\", request_timeout=180)\n\nprompt = \"Write a haiku about coding:\"\n\n# Stream completion\nfor chunk in llm.stream_complete(prompt):\n    print(chunk.text, end=\"\", flush=True)\n</code></pre>"},{"location":"reference/integrations/llms/ollama/examples/#3-processing-stream-with-delta","title":"3. Processing Stream with Delta","text":"<p>Access incremental content:</p> <pre><code>from serapeum.core.base.llms.models import Message, MessageRole\nfrom serapeum.llms.ollama import Ollama\n\nllm = Ollama(model=\"llama3.1\", request_timeout=180)\n\nmessages = [Message(role=MessageRole.USER, content=\"Tell me a joke.\")]\n\nfull_response = \"\"\nfor chunk in llm.stream_chat(messages):\n    delta = chunk.delta  # Incremental content\n    if delta:\n        full_response += delta\n        print(delta, end=\"\", flush=True)\n\nprint(f\"\\n\\nFull response: {full_response}\")\n</code></pre>"},{"location":"reference/integrations/llms/ollama/examples/#toolfunction-calling","title":"Tool/Function Calling","text":""},{"location":"reference/integrations/llms/ollama/examples/#1-basic-tool-calling","title":"1. Basic Tool Calling","text":"<p>Using tools with Ollama:</p> <pre><code>from pydantic import BaseModel\nfrom serapeum.core.base.llms.models import Message, MessageRole\nfrom serapeum.core.tools import CallableTool\nfrom serapeum.llms.ollama import Ollama\n\n\nclass Album(BaseModel):\n    title: str\n    artist: str\n    songs: list[str]\n\n\ndef create_album(title: str, artist: str, songs: list[str]) -&gt; Album:\n    \"\"\"Create an album with the given information.\"\"\"\n    return Album(title=title, artist=artist, songs=songs)\n\n\nllm = Ollama(model=\"llama3.1\", request_timeout=180)\n\n# Create tool from function\ntool = CallableTool.from_function(create_album)\n\nmessage =  Message(\n    role=MessageRole.USER,\n    content=\"Create a rock album with two songs\"\n)\n\n# Call with tools\nresponse = llm.chat_with_tools(tools=[tool], user_msg=message)\n\n# Extract tool calls\ntool_calls = llm.get_tool_calls_from_response(response)\nprint(tool_calls)\n</code></pre>"},{"location":"reference/integrations/llms/ollama/examples/#2-tool-calling-from-pydantic-model","title":"2. Tool Calling from Pydantic Model","text":"<p>Create tools from Pydantic models:</p> <pre><code>from pydantic import BaseModel, Field\nfrom serapeum.core.base.llms.models import Message, MessageRole\nfrom serapeum.core.tools import CallableTool\nfrom serapeum.llms.ollama import Ollama\n\n\nclass Album(BaseModel):\n    \"\"\"An music album.\"\"\"\n    title: str = Field(description=\"Album title\")\n    artist: str = Field(description=\"Artist name\")\n    songs: list[str] = Field(description=\"List of song titles\")\n\n\nllm = Ollama(model=\"llama3.1\", request_timeout=180)\n\n# Create tool from Pydantic model\ntool = CallableTool.from_model(Album)\n\nmessage = Message(\n    role=MessageRole.USER,\n    content=\"Create a jazz album with title 'Blue Notes' by Miles Davis with 3 songs\"\n)\n\nresponse = llm.chat_with_tools(tools=[tool], user_msg=message)\n\n# Extract and execute tool call\ntool_calls = llm.get_tool_calls_from_response(response)\nfor tool_call in tool_calls:\n    # Execute tool\n    result = tool.call(**tool_call.tool_kwargs)\n    print(result)  # Album instance\n</code></pre>"},{"location":"reference/integrations/llms/ollama/examples/#3-single-tool-call-mode","title":"3. Single Tool Call Mode","text":"<p>Force single tool call:</p> <pre><code>from pydantic import BaseModel\nfrom serapeum.core.base.llms.models import Message, MessageRole\nfrom serapeum.core.tools import CallableTool\nfrom serapeum.llms.ollama import Ollama\n\n\nclass Album(BaseModel):\n    title: str\n    artist: str\n    songs: list[str]\n\n\nllm = Ollama(model=\"llama3.1\", request_timeout=180)\n\ntool = CallableTool.from_model(Album)\n\nmessage = Message(\n    role=MessageRole.USER,\n    content=\"Create two albums\"\n)\n\n# Force single tool call\nresponse = llm.chat_with_tools(\n    tools=[tool],\n    user_msg=message,\n    allow_parallel_tool_calls=False,  # Only one tool call allowed\n)\n\ntool_calls = llm.get_tool_calls_from_response(response)\nprint(len(tool_calls))  # 1 (even if model tried to return multiple)\n</code></pre>"},{"location":"reference/integrations/llms/ollama/examples/#4-parallel-tool-calls","title":"4. Parallel Tool Calls","text":"<p>Allow multiple tool calls:</p> <pre><code>from pydantic import BaseModel\nfrom serapeum.core.base.llms.models import Message, MessageRole\nfrom serapeum.core.tools import CallableTool\nfrom serapeum.llms.ollama import Ollama\n\n\nclass Album(BaseModel):\n    title: str\n    artist: str\n    songs: list[str]\n\n\nllm = Ollama(model=\"llama3.1\", request_timeout=180)\n\ntool = CallableTool.from_model(Album)\n\nmessage = Message(\n    role=MessageRole.USER,\n    content=\"Create two albums: one rock album and one jazz album\"\n)\n\n# Allow parallel tool calls\nresponse = llm.chat_with_tools(\n    tools=[tool],\n    user_msg=message,\n    allow_parallel_tool_calls=True,\n)\n\ntool_calls = llm.get_tool_calls_from_response(response)\nprint(len(tool_calls))  # 2 (if model returns multiple)\n\nfor tool_call in tool_calls:\n    result = tool.call(**tool_call.tool_kwargs)\n    print(result)\n</code></pre>"},{"location":"reference/integrations/llms/ollama/examples/#5-streaming-with-tools","title":"5. Streaming with Tools","text":"<p>Stream tool calls:</p> <pre><code>from pydantic import BaseModel\nfrom serapeum.core.base.llms.models import Message, MessageRole\nfrom serapeum.core.tools import CallableTool\nfrom serapeum.llms.ollama import Ollama\n\n\nclass Album(BaseModel):\n    title: str\n    artist: str\n    songs: list[str]\n\n\nllm = Ollama(model=\"llama3.1\", request_timeout=180)\n\ntool = CallableTool.from_model(Album)\n\nmessage = Message(\n    role=MessageRole.USER,\n    content=\"Create a pop album\"\n)\n\n# Stream with tools\nfor chunk in llm.stream_chat_with_tools(tools=[tool], user_msg=message):\n    # Process streaming tool calls\n    if chunk.message.additional_kwargs.get(\"tool_calls\"):\n        print(f\"Tool call chunk: {chunk.message.additional_kwargs['tool_calls']}\")\n</code></pre>"},{"location":"reference/integrations/llms/ollama/examples/#integration-with-orchestrators","title":"Integration with Orchestrators","text":""},{"location":"reference/integrations/llms/ollama/examples/#1-with-textcompletionllm","title":"1. With TextCompletionLLM","text":"<p>Use Ollama with <code>TextCompletionLLM</code> for structured outputs:</p> <pre><code>from pydantic import BaseModel\nfrom serapeum.core.output_parsers import PydanticParser\nfrom serapeum.core.structured_tools.text_completion_llm import TextCompletionLLM\nfrom serapeum.llms.ollama import Ollama\n\n\nclass DummyModel(BaseModel):\n    value: str\n\n\n# Initialize Ollama\nllm = Ollama(model=\"llama3.1\", request_timeout=180)\n\n# Create parser\nparser = PydanticParser(output_cls=DummyModel)\n\n# Create TextCompletionLLM\ntext_llm = TextCompletionLLM(\n    output_parser=parser,\n    prompt=\"Value: {value}\",\n    llm=llm,\n)\n\n# Execute\nresult = text_llm(value=\"input\")\nprint(result.value)  # \"input\"\n</code></pre>"},{"location":"reference/integrations/llms/ollama/examples/#2-with-toolorchestratingllm","title":"2. With ToolOrchestratingLLM","text":"<p>Use Ollama with <code>ToolOrchestratingLLM</code> for tool-based workflows:</p> <pre><code>from pydantic import BaseModel\nfrom serapeum.core.structured_tools.tools_llm import ToolOrchestratingLLM\nfrom serapeum.llms.ollama import Ollama\n\n\nclass Album(BaseModel):\n    title: str\n    artist: str\n    songs: list[str]\n\n\n# Initialize Ollama\nllm = Ollama(model=\"llama3.1\", request_timeout=180)\n\n# Create ToolOrchestratingLLM\ntools_llm = ToolOrchestratingLLM(\n    output_cls=Album,\n    prompt=\"Create an album about {topic} with two random songs\",\n    llm=llm,\n)\n\n# Execute - returns Album instance\nresult = tools_llm(topic=\"rock\")\nprint(result.title)\nprint(result.artist)\nprint(result.songs)\n</code></pre>"},{"location":"reference/integrations/llms/ollama/examples/#3-parallel-tool-execution","title":"3. Parallel Tool Execution","text":"<p>Using <code>ToolOrchestratingLLM</code> with parallel tools:</p> <pre><code>from pydantic import BaseModel\nfrom serapeum.core.structured_tools.tools_llm import ToolOrchestratingLLM\nfrom serapeum.llms.ollama import Ollama\n\n\nclass Album(BaseModel):\n    title: str\n    artist: str\n    songs: list[str]\n\n\nllm = Ollama(model=\"llama3.1\", request_timeout=180)\n\n# Enable parallel tool calls\ntools_llm = ToolOrchestratingLLM(\n    output_cls=Album,\n    prompt=\"Create albums about {topic}\",\n    llm=llm,\n    allow_parallel_tool_calls=True,\n)\n\n# Returns list of Album instances\nresults = tools_llm(topic=\"jazz\")\nprint(len(results))  # Potentially multiple albums\nfor album in results:\n    print(f\"{album.title} by {album.artist}\")\n</code></pre>"},{"location":"reference/integrations/llms/ollama/examples/#4-streaming-with-toolorchestratingllm","title":"4. Streaming with ToolOrchestratingLLM","text":"<p>Stream tool execution results:</p> <pre><code>from pydantic import BaseModel\nfrom serapeum.core.structured_tools.tools_llm import ToolOrchestratingLLM\nfrom serapeum.llms.ollama import Ollama\n\n\nclass Album(BaseModel):\n    title: str\n    artist: str\n    songs: list[str]\n\n\nllm = Ollama(model=\"llama3.1\", request_timeout=180)\n\ntools_llm = ToolOrchestratingLLM(\n    output_cls=Album,\n    prompt=\"Create albums about {topic}\",\n    llm=llm,\n    allow_parallel_tool_calls=False,\n)\n\n# Stream results\nfor album in tools_llm.stream_call(topic=\"rock\"):\n    print(f\"Received: {album.title}\")\n</code></pre>"},{"location":"reference/integrations/llms/ollama/examples/#async-operations","title":"Async Operations","text":""},{"location":"reference/integrations/llms/ollama/examples/#1-async-chat","title":"1. Async Chat","text":"<p>Non-blocking chat:</p> <pre><code>import asyncio\nfrom serapeum.core.base.llms.models import Message, MessageRole\nfrom serapeum.llms.ollama import Ollama\n\n\nasync def async_chat_example():\n    llm = Ollama(model=\"llama3.1\", request_timeout=180)\n\n    messages = [Message(role=MessageRole.USER, content=\"Hello!\")]\n\n    response = await llm.achat(messages)\n    print(response.message.content)\n\n\nasyncio.run(async_chat_example())\n</code></pre>"},{"location":"reference/integrations/llms/ollama/examples/#2-async-completion","title":"2. Async Completion","text":"<p>Non-blocking completion:</p> <pre><code>import asyncio\nfrom serapeum.llms.ollama import Ollama\n\n\nasync def async_complete_example():\n    llm = Ollama(model=\"llama3.1\", request_timeout=180)\n\n    response = await llm.acomplete(\"Say hello\")\n    print(response.text)\n\n\nasyncio.run(async_complete_example())\n</code></pre>"},{"location":"reference/integrations/llms/ollama/examples/#3-async-streaming-chat","title":"3. Async Streaming Chat","text":"<p>Non-blocking streaming:</p> <pre><code>import asyncio\nfrom serapeum.core.base.llms.models import Message, MessageRole\nfrom serapeum.llms.ollama import Ollama\n\n\nasync def async_stream_example():\n    llm = Ollama(model=\"llama3.1\", request_timeout=180)\n\n    messages = [Message(role=MessageRole.USER, content=\"Count to 5\")]\n\n    async for chunk in await llm.astream_chat(messages):\n        print(chunk.message.content, end=\"\", flush=True)\n\n\nasyncio.run(async_stream_example())\n</code></pre>"},{"location":"reference/integrations/llms/ollama/examples/#4-concurrent-async-requests","title":"4. Concurrent Async Requests","text":"<p>Process multiple requests concurrently:</p> <pre><code>import asyncio\nfrom serapeum.core.base.llms.models import Message, MessageRole\nfrom serapeum.llms.ollama import Ollama\n\n\nasync def process_multiple():\n    llm = Ollama(model=\"llama3.1\", request_timeout=180)\n\n    prompts = [\"What is 2+2?\", \"What is 3+3?\", \"What is 4+4?\"]\n\n    # Create tasks\n    tasks = [\n        llm.achat([Message(role=MessageRole.USER, content=prompt)])\n        for prompt in prompts\n    ]\n\n    # Execute concurrently\n    responses = await asyncio.gather(*tasks)\n\n    for prompt, response in zip(prompts, responses):\n        print(f\"{prompt} -&gt; {response.message.content}\")\n\n\nasyncio.run(process_multiple())\n</code></pre>"},{"location":"reference/integrations/llms/ollama/examples/#5-async-with-toolorchestratingllm","title":"5. Async with ToolOrchestratingLLM","text":"<p>Async tool orchestration:</p> <pre><code>import asyncio\nfrom pydantic import BaseModel\nfrom serapeum.core.structured_tools.tools_llm import ToolOrchestratingLLM\nfrom serapeum.llms.ollama import Ollama\n\n\nclass Album(BaseModel):\n    title: str\n    artist: str\n    songs: list[str]\n\n\nasync def async_tool_example():\n    llm = Ollama(model=\"llama3.1\", request_timeout=180)\n\n    tools_llm = ToolOrchestratingLLM(\n        output_cls=Album,\n        prompt=\"Create an album about {topic}\",\n        llm=llm,\n    )\n\n    result = await tools_llm.acall(topic=\"pop\")\n    print(result.title)\n\n\nasyncio.run(async_tool_example())\n</code></pre>"},{"location":"reference/integrations/llms/ollama/examples/#6-async-streaming-with-tools","title":"6. Async Streaming with Tools","text":"<p>Async streaming tool execution:</p> <pre><code>import asyncio\nfrom pydantic import BaseModel\nfrom serapeum.core.structured_tools.tools_llm import ToolOrchestratingLLM\nfrom serapeum.llms.ollama import Ollama\n\n\nclass Album(BaseModel):\n    title: str\n    artist: str\n    songs: list[str]\n\n\nasync def async_stream_tool_example():\n    llm = Ollama(model=\"llama3.1\", request_timeout=180)\n\n    tools_llm = ToolOrchestratingLLM(\n        output_cls=Album,\n        prompt=\"Create albums about {topic}\",\n        llm=llm,\n        allow_parallel_tool_calls=False,\n    )\n\n    stream = await tools_llm.astream_call(topic=\"rock\")\n    async for album in stream:\n        print(f\"Received: {album.title}\")\n\n\nasyncio.run(async_stream_tool_example())\n</code></pre>"},{"location":"reference/integrations/llms/ollama/examples/#best-practices","title":"Best Practices","text":""},{"location":"reference/integrations/llms/ollama/examples/#1-reuse-llm-instances","title":"1. Reuse LLM Instances","text":"<p>Create once, use many times:</p> <pre><code>from serapeum.llms.ollama import Ollama\n\n# \u2713 Good: Create once\nllm = Ollama(model=\"llama3.1\", request_timeout=180)\n\n# Reuse for multiple calls\nresponse1 = llm.chat(messages1)\nresponse2 = llm.chat(messages2)\n\n# \u2717 Bad: Don't recreate for each call\ndef process(messages):\n    llm = Ollama(model=\"llama3.1\")  # Inefficient\n    return llm.chat(messages)\n</code></pre>"},{"location":"reference/integrations/llms/ollama/examples/#2-use-appropriate-timeout","title":"2. Use Appropriate Timeout","text":"<p>Set timeout based on expected response time:</p> <pre><code>from serapeum.llms.ollama import Ollama\n\n# Short timeout for simple queries\nquick_llm = Ollama(model=\"llama3.1\", request_timeout=30)\n\n# Longer timeout for complex queries\ncomplex_llm = Ollama(model=\"llama3.1\", request_timeout=300)\n</code></pre>"},{"location":"reference/integrations/llms/ollama/examples/#3-handle-errors-gracefully","title":"3. Handle Errors Gracefully","text":"<p>Always handle potential errors:</p> <pre><code>from serapeum.core.base.llms.models import Message, MessageRole\nfrom serapeum.llms.ollama import Ollama\n\nllm = Ollama(model=\"llama3.1\", request_timeout=180)\n\ntry:\n    response = llm.chat([Message(role=MessageRole.USER, content=\"Hello\")])\nexcept TimeoutError:\n    print(\"Request timed out\")\nexcept ConnectionError:\n    print(\"Could not connect to Ollama server\")\nexcept Exception as e:\n    print(f\"Unexpected error: {e}\")\n</code></pre>"},{"location":"reference/integrations/llms/ollama/examples/#4-use-json-mode-for-structured-outputs","title":"4. Use JSON Mode for Structured Outputs","text":"<p>Enable when expecting JSON:</p> <pre><code>from serapeum.llms.ollama import Ollama\n\n# Enable JSON mode\nllm = Ollama(\n    model=\"llama3.1\",\n    json_mode=True,\n    request_timeout=180,\n)\n\n# LLM will always return valid JSON\n</code></pre>"},{"location":"reference/integrations/llms/ollama/examples/#5-monitor-response-metadata","title":"5. Monitor Response Metadata","text":"<p>Use metadata for monitoring:</p> <pre><code>from serapeum.core.base.llms.models import Message, MessageRole\nfrom serapeum.llms.ollama import Ollama\n\nllm = Ollama(model=\"llama3.1\", request_timeout=180)\n\nresponse = llm.chat([Message(role=MessageRole.USER, content=\"Hello\")])\n\n# Access metadata\nprint(f\"Model: {response.additional_kwargs.get('model')}\")\nprint(f\"Tokens: {response.additional_kwargs.get('eval_count')}\")\nprint(f\"Duration: {response.additional_kwargs.get('total_duration')}\")\n</code></pre>"},{"location":"reference/integrations/llms/ollama/examples/#see-also","title":"See Also","text":"<ul> <li>Execution Flow and Method Calls - Detailed sequence diagrams</li> <li>Architecture and Class Relationships - Class structure</li> <li>Data Transformations and Validation - Data flow details</li> <li>Component Boundaries and Interactions - System components</li> <li>Lifecycle and State Management - State management</li> </ul>"},{"location":"reference/integrations/llms/ollama/general/","title":"Ollama LLM Integration","text":"<p>This directory contains comprehensive documentation explaining the complete workflow of the <code>Ollama</code> class, from initialization to execution across various modes (chat, completion, streaming, tool calling, async).</p>"},{"location":"reference/integrations/llms/ollama/general/#overview","title":"Overview","text":"<p>The <code>Ollama</code> class is a production-ready LLM integration that provides: 1. Connection to Ollama server (local or remote) 2. Chat and completion APIs with sync/async support 3. Streaming responses for real-time output 4. Tool/function calling for structured interactions 5. Integration with orchestrators (TextCompletionLLM, ToolOrchestratingLLM)</p>"},{"location":"reference/integrations/llms/ollama/general/#example-usage","title":"Example Usage","text":""},{"location":"reference/integrations/llms/ollama/general/#basic-chat","title":"Basic Chat","text":"<pre><code>from serapeum.core.base.llms.models import Message, MessageRole\nfrom serapeum.llms.ollama import Ollama\n\n# Initialize Ollama\nllm = Ollama(\n    model=\"llama3.1\",\n    request_timeout=180,\n)\n\n# Send chat request\nmessages = [Message(role=MessageRole.USER, content=\"Say 'pong'.\")]\nresponse = llm.chat(messages)\nprint(response.message.content)  # \"Pong!\"\n</code></pre>"},{"location":"reference/integrations/llms/ollama/general/#with-textcompletionllm","title":"With TextCompletionLLM","text":"<pre><code>from pydantic import BaseModel\nfrom serapeum.core.output_parsers import PydanticParser\nfrom serapeum.core.structured_tools.text_completion_llm import TextCompletionLLM\nfrom serapeum.llms.ollama import Ollama\n\n\nclass DummyModel(BaseModel):\n    value: str\n\n\n# Initialize Ollama\nllm = Ollama(model=\"llama3.1\", request_timeout=180)\n\n# Create structured completion runner\ntext_llm = TextCompletionLLM(\n    output_parser=PydanticParser(output_cls=DummyModel),\n    prompt=\"Value: {value}\",\n    llm=llm,\n)\n\n# Execute and get structured output\nresult = text_llm(value=\"input\")\n# Returns: DummyModel(value=\"input\")\n</code></pre>"},{"location":"reference/integrations/llms/ollama/general/#with-toolorchestratingllm","title":"With ToolOrchestratingLLM","text":"<pre><code>from pydantic import BaseModel\nfrom serapeum.core.structured_tools.tools_llm import ToolOrchestratingLLM\nfrom serapeum.llms.ollama import Ollama\n\n\nclass Album(BaseModel):\n    title: str\n    artist: str\n    songs: list[str]\n\n\n# Initialize Ollama\nllm = Ollama(model=\"llama3.1\", request_timeout=180)\n\n# Create tool orchestrator\ntools_llm = ToolOrchestratingLLM(\n    output_cls=Album,\n    prompt=\"Create an album about {topic} with two random songs\",\n    llm=llm,\n)\n\n# Execute and get structured output via tool calling\nresult = tools_llm(topic=\"rock\")\n# Returns: Album(title=\"...\", artist=\"...\", songs=[...])\n</code></pre>"},{"location":"reference/integrations/llms/ollama/general/#understanding-the-workflow","title":"Understanding the Workflow","text":""},{"location":"reference/integrations/llms/ollama/general/#1-execution-flow-and-method-calls","title":"1. Execution Flow and Method Calls","text":"<p>Shows the chronological flow of method calls and interactions across all usage patterns.</p> <p>Best for: - Understanding the order of operations - Seeing how Ollama communicates with the server - Debugging execution flow - Understanding integration patterns</p> <p>Key Flows: - Initialization phase (lazy client creation) - Direct chat/completion calls - Tool calling with schema conversion - Streaming execution - Integration with TextCompletionLLM and ToolOrchestratingLLM - Async operations</p>"},{"location":"reference/integrations/llms/ollama/general/#2-architecture-and-class-relationships","title":"2. Architecture and Class Relationships","text":"<p>Illustrates the static structure, inheritance hierarchy, and relationships.</p> <p>Best for: - Understanding the architecture - Seeing inheritance chain (BaseLLM \u2192 LLM \u2192 FunctionCallingLLM \u2192 Ollama) - Identifying class responsibilities - Understanding integration points</p> <p>Key Classes: - <code>Ollama</code>: Main LLM implementation - <code>FunctionCallingLLM</code>: Tool calling abstraction - <code>LLM</code>: High-level orchestration - <code>BaseLLM</code>: Core interface - <code>Client</code>/<code>AsyncClient</code>: HTTP communication - Response models: <code>ChatResponse</code>, <code>CompletionResponse</code>, <code>Message</code></p>"},{"location":"reference/integrations/llms/ollama/general/#3-data-transformations-and-validation","title":"3. Data Transformations and Validation","text":"<p>Tracks how data transforms through the system across different operation modes.</p> <p>Best for: - Understanding data transformations - Identifying validation points - Seeing error handling paths - Understanding request/response formats</p> <p>Key Flows: - Initialization and configuration - Chat request building and response parsing - Completion via decorator pattern - Tool schema conversion - Streaming chunk processing - Error handling pipelines</p>"},{"location":"reference/integrations/llms/ollama/general/#4-component-boundaries-and-interactions","title":"4. Component Boundaries and Interactions","text":"<p>Shows component boundaries, responsibilities, and interaction patterns.</p> <p>Best for: - Understanding system architecture - Seeing component responsibilities - Identifying interaction patterns - Understanding integration layers</p> <p>Key Components: - User space (application code) - Ollama core (request building, response parsing, tool handling) - Client layer (HTTP communication) - Ollama server (model runtime, inference) - Orchestrator layer (TextCompletionLLM, ToolOrchestratingLLM)</p>"},{"location":"reference/integrations/llms/ollama/general/#5-lifecycle-and-state-management","title":"5. Lifecycle and State Management","text":"<p>Depicts the lifecycle states, transitions, and state variables.</p> <p>Best for: - Understanding instance lifecycle - Seeing state transitions - Identifying error states - Understanding concurrency considerations</p> <p>Key States: - Uninitialized \u2192 Configured (initialization) - Configured \u2192 ClientInitialized (lazy client creation) - Idle \u2194 Processing* (request handling) - Processing \u2192 Error \u2192 Idle (error handling)</p>"},{"location":"reference/integrations/llms/ollama/general/#6-usage-examples","title":"6. Usage Examples","text":"<p>Comprehensive examples from real test cases.</p> <p>Best for: - Learning by example - Understanding practical usage - Seeing all API variants - Integration patterns</p> <p>Key Examples: - Basic chat and completion - Streaming operations - Tool/function calling - Integration with orchestrators - Async operations - Error handling</p>"},{"location":"reference/integrations/llms/ollama/general/#core-capabilities","title":"Core Capabilities","text":""},{"location":"reference/integrations/llms/ollama/general/#1-chat-api","title":"1. Chat API","text":"<pre><code>Direct conversation with the model:\n- Single and multi-turn conversations\n- System messages for context\n- Image inputs (if model supports)\n- Custom parameters (temperature, top_p, etc.)\n</code></pre>"},{"location":"reference/integrations/llms/ollama/general/#2-completion-api","title":"2. Completion API","text":"<pre><code>Text completion via decorator pattern:\n- Converts prompt to chat message\n- Delegates to chat API\n- Extracts text from response\n</code></pre>"},{"location":"reference/integrations/llms/ollama/general/#3-streaming","title":"3. Streaming","text":"<pre><code>Real-time response generation:\n- Stream chat responses\n- Stream completion responses\n- Chunk-by-chunk processing\n- Delta content access\n</code></pre>"},{"location":"reference/integrations/llms/ollama/general/#4-toolfunction-calling","title":"4. Tool/Function Calling","text":"<pre><code>Structured interactions with tools:\n- Automatic schema conversion\n- Single or parallel tool calls\n- Tool call validation\n- Streaming tool calls\n</code></pre>"},{"location":"reference/integrations/llms/ollama/general/#5-async-operations","title":"5. Async Operations","text":"<pre><code>Non-blocking execution:\n- Async chat and completion\n- Async streaming\n- Concurrent request handling\n- Separate async client per event loop\n</code></pre>"},{"location":"reference/integrations/llms/ollama/general/#key-design-patterns","title":"Key Design Patterns","text":""},{"location":"reference/integrations/llms/ollama/general/#1-lazy-initialization","title":"1. Lazy Initialization","text":"<p>Clients are created on first use, not during <code>__init__</code>: <pre><code>@property\ndef client(self) -&gt; Client:\n    if self._client is None:\n        self._client = Client(host=self.base_url, timeout=self.request_timeout)\n    return self._client\n</code></pre></p>"},{"location":"reference/integrations/llms/ollama/general/#2-decorator-pattern","title":"2. Decorator Pattern","text":"<p>Completion API wraps chat API for code reuse: <pre><code>@chat_to_completion_decorator\ndef complete(self, prompt: str, **kwargs) -&gt; CompletionResponse:\n    # Decorator handles conversion\n    pass\n</code></pre></p>"},{"location":"reference/integrations/llms/ollama/general/#3-template-method-pattern","title":"3. Template Method Pattern","text":"<p>FunctionCallingLLM defines workflow, Ollama implements specifics: <pre><code>def chat_with_tools(self, messages, tools, **kwargs):\n    prepared = self._prepare_chat_with_tools(messages, tools, **kwargs)  # Subclass\n    response = self.chat(prepared)\n    validated = self._validate_chat_with_tools_response(response, tools)  # Subclass\n    return validated\n</code></pre></p>"},{"location":"reference/integrations/llms/ollama/general/#4-adapter-pattern","title":"4. Adapter Pattern","text":"<p>Ollama adapts between internal types and Ollama server format: - <code>Message</code> \u2192 Ollama message dict - <code>BaseTool</code> \u2192 Ollama tool schema - Raw response dict \u2192 <code>ChatResponse</code>/<code>CompletionResponse</code></p>"},{"location":"reference/integrations/llms/ollama/general/#integration-architecture","title":"Integration Architecture","text":"<pre><code>User Application\n    \u2193\nToolOrchestratingLLM / TextCompletionLLM\n    \u2193\nOllama\n    \u2193\nClient / AsyncClient\n    \u2193\nOllama Server (HTTP)\n    \u2193\nModel Runtime (llama3.1, etc.)\n</code></pre>"},{"location":"reference/integrations/llms/ollama/general/#textcompletionllm-integration","title":"TextCompletionLLM Integration","text":"<pre><code>1. Formats prompt with variables\n2. Checks is_chat_model \u2192 True\n3. Calls Ollama.chat()\n4. Parses response with PydanticParser\n5. Returns validated model instance\n</code></pre>"},{"location":"reference/integrations/llms/ollama/general/#toolorchestratingllm-integration","title":"ToolOrchestratingLLM Integration","text":"<pre><code>1. Converts output_cls to CallableTool\n2. Formats prompt with variables\n3. Calls Ollama.chat_with_tools()\n4. Ollama converts tool to schema\n5. Server returns tool_calls\n6. Executes tool to create instance\n7. Returns model instance(s)\n</code></pre>"},{"location":"reference/integrations/llms/ollama/general/#performance-considerations","title":"Performance Considerations","text":"<ol> <li>Client Reuse: Client created once and reused for all requests</li> <li>Async Support: Separate async client for concurrent operations</li> <li>Streaming: Reduces latency for long responses</li> <li>Connection Pooling: HTTP client handles connection reuse</li> <li>Lazy Initialization: Only create clients when needed</li> </ol>"},{"location":"reference/integrations/llms/ollama/general/#configuration-options","title":"Configuration Options","text":""},{"location":"reference/integrations/llms/ollama/general/#essential","title":"Essential","text":"<ul> <li><code>model</code>: Model name (e.g., \"llama3.1\")</li> <li><code>base_url</code>: Ollama server URL (default: \"http://localhost:11434\")</li> <li><code>request_timeout</code>: Timeout in seconds (default: 60.0)</li> </ul>"},{"location":"reference/integrations/llms/ollama/general/#generation","title":"Generation","text":"<ul> <li><code>temperature</code>: Sampling temperature (0.0-1.0, default: 0.75)</li> <li><code>context_window</code>: Maximum context tokens (default: 3900)</li> <li><code>json_mode</code>: Force JSON output (default: False)</li> </ul>"},{"location":"reference/integrations/llms/ollama/general/#advanced","title":"Advanced","text":"<ul> <li><code>keep_alive</code>: Model keep-alive duration (default: None)</li> <li><code>additional_kwargs</code>: Additional Ollama options</li> <li><code>client</code>: Pre-configured client (default: None, lazy-created)</li> <li><code>async_client</code>: Pre-configured async client (default: None, lazy-created)</li> </ul>"},{"location":"reference/integrations/llms/ollama/general/#error-handling","title":"Error Handling","text":""},{"location":"reference/integrations/llms/ollama/general/#network-errors","title":"Network Errors","text":"<pre><code>TimeoutError: Request timeout exceeded\nConnectionError: Cannot reach Ollama server\nHTTPError: Server returned error status\n</code></pre>"},{"location":"reference/integrations/llms/ollama/general/#parsing-errors","title":"Parsing Errors","text":"<pre><code>JSONDecodeError: Invalid JSON response\nKeyError: Missing required field in response\nValueError: Invalid response format\n</code></pre>"},{"location":"reference/integrations/llms/ollama/general/#configuration-errors","title":"Configuration Errors","text":"<pre><code>ValueError: Invalid model or URL\nTypeError: Missing required field\nAssertionError: Invalid parameter value\n</code></pre>"},{"location":"reference/integrations/llms/ollama/general/#prerequisites","title":"Prerequisites","text":""},{"location":"reference/integrations/llms/ollama/general/#server-requirements","title":"Server Requirements","text":"<pre><code># Install Ollama\ncurl -fsSL https://ollama.com/install.sh | sh\n\n# Pull model\nollama pull llama3.1\n\n# Start server (runs on port 11434 by default)\nollama serve\n</code></pre>"},{"location":"reference/integrations/llms/ollama/general/#python-requirements","title":"Python Requirements","text":"<pre><code># Install serapeum-ollama\nuv pip install serapeum-ollama\n\n# Or install from source\nuv pip install -e serapeum-integrations/llms/serapeum-ollama\n</code></pre>"},{"location":"reference/integrations/llms/ollama/general/#common-patterns","title":"Common Patterns","text":""},{"location":"reference/integrations/llms/ollama/general/#pattern-1-reusable-instance","title":"Pattern 1: Reusable Instance","text":"<pre><code># Create once\nllm = Ollama(model=\"llama3.1\", request_timeout=180)\n\n# Reuse many times\nresponse1 = llm.chat(messages1)\nresponse2 = llm.chat(messages2)\n</code></pre>"},{"location":"reference/integrations/llms/ollama/general/#pattern-2-streaming-for-long-responses","title":"Pattern 2: Streaming for Long Responses","text":"<pre><code>llm = Ollama(model=\"llama3.1\", request_timeout=180)\n\nfor chunk in llm.stream_chat(messages):\n    print(chunk.message.content, end=\"\", flush=True)\n</code></pre>"},{"location":"reference/integrations/llms/ollama/general/#pattern-3-tool-calling-for-structured-outputs","title":"Pattern 3: Tool Calling for Structured Outputs","text":"<pre><code>llm = Ollama(model=\"llama3.1\", request_timeout=180)\n\n# Define tool from Pydantic model\ntool = CallableTool.from_model(MyModel)\n\n# Get structured output via tool calling\nresponse = llm.chat_with_tools(messages, tools=[tool])\n</code></pre>"},{"location":"reference/integrations/llms/ollama/general/#pattern-4-async-for-concurrency","title":"Pattern 4: Async for Concurrency","text":"<pre><code>llm = Ollama(model=\"llama3.1\", request_timeout=180)\n\n# Process multiple requests concurrently\ntasks = [llm.achat(messages) for messages in message_list]\nresponses = await asyncio.gather(*tasks)\n</code></pre>"},{"location":"reference/integrations/llms/ollama/general/#troubleshooting","title":"Troubleshooting","text":""},{"location":"reference/integrations/llms/ollama/general/#issue-connection-refused","title":"Issue: Connection Refused","text":"<pre><code>Solution: Ensure Ollama server is running\n  $ ollama serve\n</code></pre>"},{"location":"reference/integrations/llms/ollama/general/#issue-model-not-found","title":"Issue: Model Not Found","text":"<pre><code>Solution: Pull the model first\n  $ ollama pull llama3.1\n</code></pre>"},{"location":"reference/integrations/llms/ollama/general/#issue-timeout","title":"Issue: Timeout","text":"<pre><code>Solution: Increase request_timeout\n  llm = Ollama(model=\"llama3.1\", request_timeout=300)\n</code></pre>"},{"location":"reference/integrations/llms/ollama/general/#issue-invalid-json-response","title":"Issue: Invalid JSON Response","text":"<pre><code>Solution: Enable json_mode\n  llm = Ollama(model=\"llama3.1\", json_mode=True)\n</code></pre>"},{"location":"reference/integrations/llms/ollama/general/#next-steps","title":"Next Steps","text":"<ol> <li>Start with Examples for practical usage patterns</li> <li>Review Sequence Diagrams to understand execution flow</li> <li>Study Class Diagram to understand architecture</li> <li>Explore Data Flow to understand transformations</li> <li>Check State Management for lifecycle details</li> </ol>"},{"location":"reference/integrations/llms/ollama/general/#see-also","title":"See Also","text":"<ul> <li>TextCompletionLLM - Structured completion orchestrator</li> <li>ToolOrchestratingLLM - Tool-based orchestrator</li> <li>Ollama Official Documentation - Ollama server documentation</li> </ul>"},{"location":"reference/integrations/llms/ollama/ollama_class/","title":"Architecture and Class Relationships","text":"<p>This diagram shows the class relationships and inheritance hierarchy for the <code>Ollama</code> LLM implementation.</p>  Hold \"Ctrl\" to enable pan &amp; zoom  <pre><code>classDiagram\n    class BaseLLM {\n        &lt;&lt;abstract&gt;&gt;\n        +metadata: Metadata\n        +chat(messages, **kwargs) ChatResponse\n        +stream_chat(messages, **kwargs) ChatResponseGen\n        +achat(messages, **kwargs) ChatResponse\n        +astream_chat(messages, **kwargs) ChatResponseAsyncGen\n        +complete(prompt, **kwargs) CompletionResponse\n        +stream_complete(prompt, **kwargs) CompletionResponseGen\n        +acomplete(prompt, **kwargs) CompletionResponse\n        +astream_complete(prompt, **kwargs) CompletionResponseAsyncGen\n    }\n\n    class LLM {\n        +system_prompt: Optional[str]\n        +messages_to_prompt: Callable\n        +completion_to_prompt: Callable\n        +output_parser: Optional[BaseParser]\n        +pydantic_program_mode: StructuredLLMMode\n        +_get_prompt(prompt, **kwargs) str\n        +_get_messages(prompt, **kwargs) List[Message]\n        +_parse_output(output) str\n        +_extend_prompt(formatted_prompt) str\n        +_extend_messages(messages) List[Message]\n        +predict(prompt, **kwargs) str\n        +stream(prompt, **kwargs) TokenGen\n        +apredict(prompt, **kwargs) str\n        +astream(prompt, **kwargs) TokenAsyncGen\n        +structured_predict(output_cls, prompt, **kwargs) Model\n    }\n\n    class FunctionCallingLLM {\n        &lt;&lt;abstract&gt;&gt;\n        +chat_with_tools(messages, tools, **kwargs) ChatResponse\n        +achat_with_tools(messages, tools, **kwargs) ChatResponse\n        +stream_chat_with_tools(messages, tools, **kwargs) ChatResponseGen\n        +astream_chat_with_tools(messages, tools, **kwargs) ChatResponseAsyncGen\n        +get_tool_calls_from_response(response, error_on_no_tool_call) List[ToolSelection]\n        #_prepare_chat_with_tools(messages, tools, **kwargs) dict\n        #_validate_chat_with_tools_response(response, tools, **kwargs) ChatResponse\n    }\n\n    class Ollama {\n        +model: str\n        +base_url: str\n        +temperature: float\n        +context_window: int\n        +request_timeout: float\n        +prompt_key: str\n        +json_mode: bool\n        +additional_kwargs: dict\n        +keep_alive: Optional[str]\n        -_client: Optional[Client]\n        -_async_client: Optional[AsyncClient]\n        -_is_function_calling_model: bool\n        +__init__(model, base_url, temperature, ...)\n        +metadata: Metadata\n        +client: Client\n        +async_client: AsyncClient\n        +chat(messages, **kwargs) ChatResponse\n        +stream_chat(messages, **kwargs) ChatResponseGen\n        +achat(messages, **kwargs) ChatResponse\n        +astream_chat(messages, **kwargs) ChatResponseAsyncGen\n        +complete(prompt, **kwargs) CompletionResponse\n        +stream_complete(prompt, **kwargs) CompletionResponseGen\n        +acomplete(prompt, **kwargs) CompletionResponse\n        +astream_complete(prompt, **kwargs) CompletionResponseAsyncGen\n        +chat_with_tools(messages, tools, **kwargs) ChatResponse\n        +stream_chat_with_tools(messages, tools, **kwargs) ChatResponseGen\n        +achat_with_tools(messages, tools, **kwargs) ChatResponse\n        +astream_chat_with_tools(messages, tools, **kwargs) ChatResponseAsyncGen\n        -_chat(messages, stream, **kwargs) ChatResponse\n        -_achat(messages, stream, **kwargs) ChatResponse\n        -_prepare_chat_with_tools(messages, tools, **kwargs) dict\n        -_validate_chat_with_tools_response(response, tools, **kwargs) ChatResponse\n        -_chat_from_response(response) ChatResponse\n        -_chat_stream_from_response(response) ChatResponse\n        #_get_model_kwargs(**kwargs) dict\n    }\n\n    class Client {\n        &lt;&lt;ollama.Client&gt;&gt;\n        +chat(**kwargs) dict\n        +generate(**kwargs) dict\n        +__init__(host, timeout)\n    }\n\n    class AsyncClient {\n        &lt;&lt;ollama.AsyncClient&gt;&gt;\n        +chat(**kwargs) dict\n        +generate(**kwargs) dict\n        +__init__(host, timeout)\n    }\n\n    class Metadata {\n        +model_name: str\n        +context_window: int\n        +num_output: int\n        +is_chat_model: bool\n        +is_function_calling_model: bool\n        +system_role: MessageRole\n    }\n\n    class Message {\n        +role: MessageRole\n        +content: str\n        +additional_kwargs: dict\n        +images: Optional[List[Image]]\n    }\n\n    class MessageRole {\n        &lt;&lt;enumeration&gt;&gt;\n        SYSTEM\n        USER\n        ASSISTANT\n        TOOL\n    }\n\n    class ChatResponse {\n        +message: Message\n        +raw: Optional[dict]\n        +delta: Optional[str]\n        +logprobs: Optional[List]\n        +additional_kwargs: dict\n    }\n\n    class CompletionResponse {\n        +text: str\n        +raw: Optional[dict]\n        +delta: Optional[str]\n        +logprobs: Optional[List]\n        +additional_kwargs: dict\n    }\n\n    class BaseTool {\n        &lt;&lt;protocol&gt;&gt;\n        +metadata: ToolMetadata\n        +call(**kwargs) ToolOutput\n        +acall(**kwargs) ToolOutput\n    }\n\n    class CallableTool {\n        +metadata: ToolMetadata\n        -_fn: Callable\n        +__init__(fn, metadata)\n        +call(**kwargs) ToolOutput\n        +acall(**kwargs) ToolOutput\n        +from_function(fn) CallableTool\n        +from_model(model_cls) CallableTool\n    }\n\n    class ToolMetadata {\n        +name: str\n        +description: str\n        +fn_schema: dict\n    }\n\n    class TextCompletionLLM {\n        -_llm: LLM\n        -_prompt: BasePromptTemplate\n        -_output_parser: PydanticParser\n        -_output_cls: Type[BaseModel]\n        +__call__(**kwargs) BaseModel\n        +acall(**kwargs) BaseModel\n    }\n\n    class ToolOrchestratingLLM {\n        -_llm: FunctionCallingLLM\n        -_prompt: BasePromptTemplate\n        -_output_cls: Type[BaseModel]\n        -_tools: List[BaseTool]\n        -_allow_parallel_tool_calls: bool\n        +__call__(**kwargs) BaseModel | List[BaseModel]\n        +acall(**kwargs) BaseModel | List[BaseModel]\n        +stream_call(**kwargs) Generator[BaseModel]\n        +astream_call(**kwargs) AsyncGenerator[BaseModel]\n    }\n\n    class BaseModel {\n        &lt;&lt;pydantic&gt;&gt;\n        +model_validate_json(json_data) BaseModel\n        +model_json_schema() dict\n    }\n\n    class DummyModel {\n        +value: str\n    }\n\n    class Album {\n        +title: str\n        +artist: str\n        +songs: List[str]\n    }\n\n    %% Inheritance relationships\n    BaseLLM &lt;|-- LLM\n    LLM &lt;|-- FunctionCallingLLM\n    FunctionCallingLLM &lt;|-- Ollama\n    BaseTool &lt;|.. CallableTool\n    BaseModel &lt;|-- DummyModel\n    BaseModel &lt;|-- Album\n\n    %% Composition relationships\n    Ollama o-- Client : uses (lazy init)\n    Ollama o-- AsyncClient : uses (lazy init)\n    Ollama ..&gt; Metadata : provides\n    Ollama ..&gt; ChatResponse : produces\n    Ollama ..&gt; CompletionResponse : produces\n    Ollama ..&gt; Message : consumes/produces\n\n    %% Message relationships\n    Message o-- MessageRole : has\n    ChatResponse o-- Message : contains\n    Message ..&gt; Image : may contain\n\n    %% Tool relationships\n    BaseTool o-- ToolMetadata : has\n    CallableTool ..&gt; ToolMetadata : creates\n    BaseTool ..&gt; BaseModel : may wrap\n\n    %% Orchestrator relationships\n    TextCompletionLLM o-- Ollama : uses\n    TextCompletionLLM ..&gt; DummyModel : produces\n    ToolOrchestratingLLM o-- Ollama : uses\n    ToolOrchestratingLLM o-- CallableTool : uses\n    ToolOrchestratingLLM ..&gt; Album : produces\n\n    note for Ollama \"Main LLM implementation that:\\n1. Connects to Ollama server\\n2. Supports chat and completion\\n3. Handles tool/function calling\\n4. Manages streaming responses\\n5. Provides sync/async interfaces\"\n    note for FunctionCallingLLM \"Abstract class providing:\\n- Tool calling interface\\n- Tool response validation\\n- Tool preparation helpers\"\n    note for Client \"Synchronous Ollama client\\nfrom ollama package\"\n    note for AsyncClient \"Asynchronous Ollama client\\nfrom ollama package\"</code></pre>"},{"location":"reference/integrations/llms/ollama/ollama_class/#class-hierarchy","title":"Class Hierarchy","text":""},{"location":"reference/integrations/llms/ollama/ollama_class/#inheritance-chain","title":"Inheritance Chain","text":"<pre><code>BaseLLM (abstract)\n  \u2514\u2500\u2192 LLM (adds prompting and structured outputs)\n      \u2514\u2500\u2192 FunctionCallingLLM (abstract, adds tool calling)\n          \u2514\u2500\u2192 Ollama (concrete implementation)\n</code></pre>"},{"location":"reference/integrations/llms/ollama/ollama_class/#component-responsibilities","title":"Component Responsibilities","text":""},{"location":"reference/integrations/llms/ollama/ollama_class/#ollama","title":"Ollama","text":"<p>Core LLM Implementation - Connection Management: Manages sync/async clients for Ollama server - Request Handling: Builds and executes chat/completion requests - Response Parsing: Converts raw responses to typed models - Tool Integration: Prepares tools in Ollama format, validates responses - Streaming Support: Handles incremental response chunks - Configuration: Manages model settings, temperature, context window, etc.</p>"},{"location":"reference/integrations/llms/ollama/ollama_class/#functioncallingllm-parent-class","title":"FunctionCallingLLM (Parent Class)","text":"<p>Tool Calling Abstraction - Tool Interface: Defines standard methods for tool-calling interactions - Tool Preparation: Abstract method for preparing tools in provider format - Response Validation: Ensures tool calls are properly structured - Tool Extraction: Gets tool calls from chat responses</p>"},{"location":"reference/integrations/llms/ollama/ollama_class/#llm-grandparent-class","title":"LLM (Grandparent Class)","text":"<p>High-Level Orchestration - Prompt Management: Extends prompts with system messages - Message Formatting: Converts between formats - Structured Outputs: Forces Pydantic model outputs via <code>structured_predict</code> - Parser Integration: Applies output parsers to responses</p>"},{"location":"reference/integrations/llms/ollama/ollama_class/#basellm-root-class","title":"BaseLLM (Root Class)","text":"<p>Core Interface - Standard Methods: Defines chat, complete, and their variants - Sync/Async: Requires both synchronous and asynchronous implementations - Streaming: Requires streaming variants of all methods - Metadata: Requires metadata property for capabilities</p>"},{"location":"reference/integrations/llms/ollama/ollama_class/#clientasyncclient","title":"Client/AsyncClient","text":"<p>HTTP Communication - API Requests: Handles HTTP communication with Ollama server - Streaming: Supports streaming responses - Configuration: Manages host, timeout, and connection settings</p>"},{"location":"reference/integrations/llms/ollama/ollama_class/#messagechatresponsecompletionresponse","title":"Message/ChatResponse/CompletionResponse","text":"<p>Data Models - Message: Represents a single chat message with role and content - ChatResponse: Wraps assistant response with metadata - CompletionResponse: Wraps text completion with metadata</p>"},{"location":"reference/integrations/llms/ollama/ollama_class/#tool-classes","title":"Tool Classes","text":"<p>Function Calling - BaseTool: Protocol defining tool interface - CallableTool: Concrete implementation wrapping Python functions or Pydantic models - ToolMetadata: Describes tool name, description, and schema</p>"},{"location":"reference/integrations/llms/ollama/ollama_class/#orchestration-classes","title":"Orchestration Classes","text":"<p>High-Level Patterns - TextCompletionLLM: Formats prompts \u2192 calls LLM \u2192 parses to Pydantic - ToolOrchestratingLLM: Formats prompts \u2192 calls LLM with tools \u2192 executes tools \u2192 returns instances</p>"},{"location":"reference/integrations/llms/ollama/ollama_class/#design-patterns","title":"Design Patterns","text":""},{"location":"reference/integrations/llms/ollama/ollama_class/#1-lazy-initialization","title":"1. Lazy Initialization","text":"<pre><code>@property\ndef client(self) -&gt; Client:\n    if self._client is None:\n        self._client = Client(host=self.base_url, timeout=self.request_timeout)\n    return self._client\n</code></pre>"},{"location":"reference/integrations/llms/ollama/ollama_class/#2-decorator-pattern-completion-via-chat","title":"2. Decorator Pattern (Completion via Chat)","text":"<pre><code>@chat_to_completion_decorator\ndef complete(self, prompt: str, **kwargs) -&gt; CompletionResponse:\n    # Decorator handles conversion\n    pass\n</code></pre>"},{"location":"reference/integrations/llms/ollama/ollama_class/#3-template-method-pattern","title":"3. Template Method Pattern","text":"<pre><code># FunctionCallingLLM defines workflow\ndef chat_with_tools(self, messages, tools, **kwargs):\n    prepared = self._prepare_chat_with_tools(messages, tools, **kwargs)  # Subclass implements\n    response = self.chat(prepared)\n    validated = self._validate_chat_with_tools_response(response, tools)  # Subclass implements\n    return validated\n</code></pre>"},{"location":"reference/integrations/llms/ollama/ollama_class/#4-protocol-based-tools","title":"4. Protocol-Based Tools","text":"<pre><code># BaseTool is a protocol, not a base class\nclass BaseTool(Protocol):\n    def call(self, **kwargs) -&gt; ToolOutput: ...\n</code></pre>"},{"location":"reference/integrations/llms/ollama/ollama_class/#integration-points","title":"Integration Points","text":""},{"location":"reference/integrations/llms/ollama/ollama_class/#with-textcompletionllm","title":"With TextCompletionLLM","text":"<pre><code>TextCompletionLLM uses Ollama for:\n  - Checking is_chat_model via metadata\n  - Calling chat() or complete()\n  - Getting raw text responses for parsing\n</code></pre>"},{"location":"reference/integrations/llms/ollama/ollama_class/#with-toolorchestratingllm","title":"With ToolOrchestratingLLM","text":"<pre><code>ToolOrchestratingLLM uses Ollama for:\n  - Tool-calling capabilities\n  - chat_with_tools() method\n  - Tool call extraction from responses\n</code></pre>"},{"location":"reference/integrations/llms/ollama/ollama_class/#with-external-packages","title":"With External Packages","text":"<pre><code>Ollama depends on:\n  - ollama package (Client, AsyncClient)\n  - pydantic (for configuration and models)\n  - serapeum.core (for base classes and types)\n</code></pre>"},{"location":"reference/integrations/llms/ollama/ollama_components/","title":"Component Boundaries and Interactions","text":"<p>This diagram shows how components interact during the complete lifecycle of the Ollama LLM.</p>  Hold \"Ctrl\" to enable pan &amp; zoom  <pre><code>graph TB\n    subgraph User Space\n        UC[User Code]\n        PM[Pydantic Models: DummyModel, Album]\n    end\n\n    subgraph Ollama Core\n        OL[Ollama Instance]\n\n        subgraph Configuration\n            CFG[Configuration Fields]\n            MD[Metadata]\n        end\n\n        subgraph Client Management\n            CL[Client Property]\n            ACL[AsyncClient Property]\n            LINIT[Lazy Initialization]\n        end\n\n        subgraph Request Building\n            BCRQ[_chat Request Builder]\n            BCRQ_MSG[Message Converter]\n            BCRQ_OPT[Options Builder]\n            BCRQ_TOOL[Tool Converter]\n        end\n\n        subgraph Response Parsing\n            PRSP[_chat_from_response]\n            PRSP_MSG[Message Parser]\n            PRSP_TOOL[Tool Calls Parser]\n            PRSP_META[Metadata Extractor]\n        end\n\n        subgraph Stream Handling\n            STRM[_chat_stream_from_response]\n            ACC[Accumulator]\n        end\n\n        subgraph Tool Handling\n            PREP[_prepare_chat_with_tools]\n            VAL[_validate_chat_with_tools_response]\n            FORCE[force_single_tool_call]\n        end\n\n        subgraph Decorators\n            C2C[chat_to_completion_decorator]\n            SC2C[stream_chat_to_completion]\n            AC2C[achat_to_completion]\n        end\n    end\n\n    subgraph External Client Layer\n        CLI[ollama.Client]\n        ACLI[ollama.AsyncClient]\n\n        subgraph Client Operations\n            CHAT_OP[chat method]\n            GEN_OP[generate method]\n        end\n    end\n\n    subgraph Ollama Server\n        SRV[Ollama Server Process]\n\n        subgraph API Endpoints\n            EP_CHAT[\"/api/chat\"]\n            EP_GEN[\"/api/generate\"]\n        end\n\n        subgraph Model Runtime\n            MDL[Loaded Model: llama3.1]\n            CTX[Context Window]\n            INF[Inference Engine]\n        end\n    end\n\n    subgraph Orchestrator Layer\n        TCL[TextCompletionLLM]\n        TOL[ToolOrchestratingLLM]\n\n        subgraph Orchestrator Components\n            PRS[PydanticParser]\n            PTMP[PromptTemplate]\n            CTOOL[CallableTool]\n        end\n    end\n\n    subgraph Response Models\n        CRESP[ChatResponse]\n        CORESP[CompletionResponse]\n        MSG[Message]\n    end\n\n    %% Initialization Flow\n    UC --&gt;|1. Initialize| OL\n    OL --&gt;|Store config| CFG\n    OL --&gt;|Create| MD\n    MD --&gt;|is_chat_model=True| OL\n    MD --&gt;|is_function_calling_model=True| OL\n\n    %% Client lazy init\n    OL --&gt;|On first use| LINIT\n    LINIT --&gt;|Create if None| CL\n    LINIT --&gt;|Create if None| ACL\n    CL -.-&gt;|Wraps| CLI\n    ACL -.-&gt;|Wraps| ACLI\n\n    %% Chat Flow\n    UC --&gt;|2a. chat(messages)| OL\n    OL --&gt;|Check tools| PREP\n    PREP --&gt;|Convert tools| BCRQ_TOOL\n    BCRQ_TOOL --&gt;|Merge| BCRQ\n\n    OL --&gt;|Build request| BCRQ\n    BCRQ --&gt;|Convert messages| BCRQ_MSG\n    BCRQ_MSG --&gt;|Add options| BCRQ_OPT\n    BCRQ_OPT --&gt;|Final payload| CL\n\n    CL --&gt;|chat(**request)| CLI\n    CLI --&gt;|HTTP POST| EP_CHAT\n    EP_CHAT --&gt;|Route to| MDL\n    MDL --&gt;|Use| CTX\n    MDL --&gt;|Run| INF\n    INF --&gt;|Generate| EP_CHAT\n    EP_CHAT --&gt;|Response dict| CLI\n    CLI --&gt;|Return| CL\n\n    CL --&gt;|Raw response| PRSP\n    PRSP --&gt;|Extract message| PRSP_MSG\n    PRSP --&gt;|Extract tool_calls| PRSP_TOOL\n    PRSP --&gt;|Extract metadata| PRSP_META\n    PRSP_META --&gt;|Create| CRESP\n    CRESP --&gt;|Contains| MSG\n    CRESP --&gt;|Return| UC\n\n    %% Complete Flow (via decorator)\n    UC --&gt;|2b. complete(prompt)| C2C\n    C2C --&gt;|Wrap to Message| OL\n    OL --&gt;|Delegate chat| CL\n    CL --&gt;|ChatResponse| C2C\n    C2C --&gt;|Extract text| CORESP\n    CORESP --&gt;|Return| UC\n\n    %% Stream Flow\n    UC --&gt;|2c. stream_chat(messages)| OL\n    OL --&gt;|stream=True| CLI\n    CLI --&gt;|Streaming POST| EP_CHAT\n    EP_CHAT -.-&gt;|Chunk 1| CLI\n    CLI -.-&gt;|Chunk 1| STRM\n    STRM -.-&gt;|Accumulate| ACC\n    ACC -.-&gt;|Yield| CRESP\n    CRESP -.-&gt;|Yield| UC\n    EP_CHAT -.-&gt;|Chunk N| CLI\n    CLI -.-&gt;|Chunk N| STRM\n\n    %% Tool calling flow\n    UC --&gt;|2d. chat_with_tools(messages, tools)| OL\n    OL --&gt;|Prepare| PREP\n    PREP --&gt;|Convert to schema| BCRQ_TOOL\n    OL --&gt;|Call chat| CLI\n    CLI --&gt;|Response with tool_calls| PRSP\n    PRSP --&gt;|Parse| PRSP_TOOL\n    PRSP_TOOL --&gt;|Return to| VAL\n    VAL --&gt;|Check parallel| FORCE\n    FORCE --&gt;|Trim if needed| CRESP\n    CRESP --&gt;|Return| UC\n\n    %% TextCompletionLLM Integration\n    UC --&gt;|3a. TextCompletionLLM(llm=Ollama)| TCL\n    TCL --&gt;|Store| OL\n    TCL --&gt;|Use| PTMP\n    TCL --&gt;|Use| PRS\n    UC --&gt;|Call| TCL\n    TCL --&gt;|Format prompt| PTMP\n    PTMP --&gt;|Messages| OL\n    OL --&gt;|chat| CRESP\n    CRESP --&gt;|message.content| PRS\n    PRS --&gt;|Parse JSON| PM\n    PM --&gt;|Return| UC\n\n    %% ToolOrchestratingLLM Integration\n    UC --&gt;|3b. ToolOrchestratingLLM(llm=Ollama)| TOL\n    TOL --&gt;|Store| OL\n    TOL --&gt;|Create tool from| PM\n    PM --&gt;|Schema| CTOOL\n    TOL --&gt;|Use| PTMP\n    UC --&gt;|Call| TOL\n    TOL --&gt;|Format prompt| PTMP\n    PTMP --&gt;|Messages| OL\n    TOL --&gt;|Pass tools| CTOOL\n    CTOOL --&gt;|Convert| OL\n    OL --&gt;|chat_with_tools| CRESP\n    CRESP --&gt;|tool_calls| TOL\n    TOL --&gt;|Execute tool| CTOOL\n    CTOOL --&gt;|Create| PM\n    PM --&gt;|Return| UC\n\n    %% Styling\n    classDef userClass fill:#e1f5ff,stroke:#01579b\n    classDef ollamaClass fill:#e0f2f1,stroke:#004d40\n    classDef configClass fill:#fff9c4,stroke:#f57f17\n    classDef clientClass fill:#f3e5f5,stroke:#4a148c\n    classDef parserClass fill:#e8f5e9,stroke:#1b5e20\n    classDef serverClass fill:#efebe9,stroke:#3e2723\n    classDef orchestratorClass fill:#fce4ec,stroke:#880e4f\n    classDef responseClass fill:#fff3e0,stroke:#e65100\n\n    class UC,PM userClass\n    class OL ollamaClass\n    class CFG,MD,BCRQ,BCRQ_MSG,BCRQ_OPT,BCRQ_TOOL,PRSP,PRSP_MSG,PRSP_TOOL,PRSP_META configClass\n    class CL,ACL,LINIT,CLI,ACLI,CHAT_OP,GEN_OP clientClass\n    class PREP,VAL,FORCE parserClass\n    class SRV,EP_CHAT,EP_GEN,MDL,CTX,INF serverClass\n    class TCL,TOL,PRS,PTMP,CTOOL orchestratorClass\n    class CRESP,CORESP,MSG responseClass</code></pre>"},{"location":"reference/integrations/llms/ollama/ollama_components/#component-interaction-patterns","title":"Component Interaction Patterns","text":""},{"location":"reference/integrations/llms/ollama/ollama_components/#1-initialization-pattern","title":"1. Initialization Pattern","text":"<pre><code>User Code\n  \u2514\u2500\u2192 Ollama.__init__\n      \u251c\u2500\u2192 Store: model, base_url, temperature, request_timeout, json_mode, additional_kwargs\n      \u251c\u2500\u2192 Create Metadata: is_chat_model=True, is_function_calling_model=True\n      \u2514\u2500\u2192 Set _client=None, _async_client=None (lazy init)\n</code></pre>"},{"location":"reference/integrations/llms/ollama/ollama_components/#2-client-lazy-initialization-pattern","title":"2. Client Lazy Initialization Pattern","text":"<pre><code>User \u2192 Ollama.chat\n  \u2514\u2500\u2192 Access self.client property\n      \u2514\u2500\u2192 Check if self._client is None\n          \u251c\u2500\u2192 If None: Create Client(host=base_url, timeout=request_timeout)\n          \u2514\u2500\u2192 Return self._client\n</code></pre>"},{"location":"reference/integrations/llms/ollama/ollama_components/#3-chat-request-pattern","title":"3. Chat Request Pattern","text":"<pre><code>User \u2192 Ollama.chat(messages, **kwargs)\n  \u251c\u2500\u2192 _prepare_chat_with_tools (if tools present)\n  \u2502   \u2514\u2500\u2192 For each tool:\n  \u2502       \u251c\u2500\u2192 Extract tool.metadata\n  \u2502       \u251c\u2500\u2192 Get fn_schema from metadata\n  \u2502       \u2514\u2500\u2192 Build Ollama tool dict\n  \u251c\u2500\u2192 _chat(messages, stream=False, **kwargs)\n  \u2502   \u251c\u2500\u2192 Build request dict:\n  \u2502   \u2502   \u251c\u2500\u2192 model: self.model\n  \u2502   \u2502   \u251c\u2500\u2192 messages: [msg.dict() for msg in messages]\n  \u2502   \u2502   \u251c\u2500\u2192 options: {temperature, ...}\n  \u2502   \u2502   \u251c\u2500\u2192 format: \"json\" if json_mode\n  \u2502   \u2502   \u2514\u2500\u2192 tools: converted tool dicts\n  \u2502   \u251c\u2500\u2192 Ensure client initialized\n  \u2502   \u251c\u2500\u2192 client.chat(**request)\n  \u2502   \u2514\u2500\u2192 _chat_from_response(raw_response)\n  \u2502       \u251c\u2500\u2192 Extract message dict\n  \u2502       \u251c\u2500\u2192 Parse role, content, tool_calls\n  \u2502       \u251c\u2500\u2192 Create Message object\n  \u2502       \u251c\u2500\u2192 Extract metadata: model, times, tokens\n  \u2502       \u2514\u2500\u2192 Create ChatResponse\n  \u2514\u2500\u2192 Return ChatResponse\n</code></pre>"},{"location":"reference/integrations/llms/ollama/ollama_components/#4-completion-via-decorator-pattern","title":"4. Completion via Decorator Pattern","text":"<pre><code>User \u2192 Ollama.complete(prompt, **kwargs)\n  \u2514\u2500\u2192 @chat_to_completion_decorator wrapper\n      \u251c\u2500\u2192 Convert prompt to Message(role=USER, content=prompt)\n      \u251c\u2500\u2192 Call self.chat([message], **kwargs)\n      \u251c\u2500\u2192 Receive ChatResponse\n      \u251c\u2500\u2192 Extract text = response.message.content\n      \u2514\u2500\u2192 Return CompletionResponse(text=text, raw=response.raw, ...)\n</code></pre>"},{"location":"reference/integrations/llms/ollama/ollama_components/#5-streaming-pattern","title":"5. Streaming Pattern","text":"<pre><code>User \u2192 Ollama.stream_chat(messages, **kwargs)\n  \u2514\u2500\u2192 _chat(messages, stream=True, **kwargs)\n      \u251c\u2500\u2192 Build request with stream=True\n      \u251c\u2500\u2192 client.chat(**request) returns iterator\n      \u2514\u2500\u2192 For each chunk:\n          \u251c\u2500\u2192 _chat_stream_from_response(chunk)\n          \u2502   \u251c\u2500\u2192 Extract delta content\n          \u2502   \u251c\u2500\u2192 Accumulate tool_calls\n          \u2502   \u2514\u2500\u2192 Create ChatResponse with delta\n          \u2514\u2500\u2192 Yield ChatResponse\n</code></pre>"},{"location":"reference/integrations/llms/ollama/ollama_components/#6-tool-calling-pattern","title":"6. Tool Calling Pattern","text":"<pre><code>User \u2192 Ollama.chat_with_tools(messages, tools, **kwargs)\n  \u251c\u2500\u2192 _prepare_chat_with_tools(messages, tools, **kwargs)\n  \u2502   \u2514\u2500\u2192 For each tool in tools:\n  \u2502       \u251c\u2500\u2192 Get tool.metadata.fn_schema\n  \u2502       \u2514\u2500\u2192 Build: {\"type\": \"function\", \"function\": {\"name\": ..., \"parameters\": schema}}\n  \u251c\u2500\u2192 Merge tools into kwargs\n  \u251c\u2500\u2192 Call self.chat(messages, **kwargs)\n  \u251c\u2500\u2192 Receive ChatResponse with tool_calls\n  \u251c\u2500\u2192 _validate_chat_with_tools_response(response, tools, **kwargs)\n  \u2502   \u2514\u2500\u2192 If not allow_parallel_tool_calls:\n  \u2502       \u2514\u2500\u2192 force_single_tool_call(response)\n  \u2514\u2500\u2192 Return ChatResponse\n</code></pre>"},{"location":"reference/integrations/llms/ollama/ollama_components/#7-textcompletionllm-integration-pattern","title":"7. TextCompletionLLM Integration Pattern","text":"<pre><code>User \u2192 TextCompletionLLM(output_parser=parser, prompt=prompt, llm=Ollama(...))\n  \u2514\u2500\u2192 TextCompletionLLM stores Ollama instance\n\nUser \u2192 text_llm(key=\"value\")\n  \u251c\u2500\u2192 Check llm.metadata.is_chat_model \u2192 True\n  \u251c\u2500\u2192 Format prompt with variables \u2192 List[Message]\n  \u251c\u2500\u2192 Ollama.chat(messages) \u2192 ChatResponse\n  \u251c\u2500\u2192 Extract response.message.content\n  \u251c\u2500\u2192 PydanticParser.parse(content) \u2192 DummyModel\n  \u2514\u2500\u2192 Return DummyModel instance\n</code></pre>"},{"location":"reference/integrations/llms/ollama/ollama_components/#8-toolorchestratingllm-integration-pattern","title":"8. ToolOrchestratingLLM Integration Pattern","text":"<pre><code>User \u2192 ToolOrchestratingLLM(output_cls=Album, prompt=prompt, llm=Ollama(...))\n  \u251c\u2500\u2192 Convert Album Pydantic model to CallableTool\n  \u2514\u2500\u2192 Store Ollama instance\n\nUser \u2192 tools_llm(topic=\"rock\")\n  \u251c\u2500\u2192 Format prompt with topic \u2192 List[Message]\n  \u251c\u2500\u2192 CallableTool.metadata.fn_schema \u2192 Album JSON schema\n  \u251c\u2500\u2192 Ollama.chat_with_tools(messages, tools=[album_tool])\n  \u2502   \u251c\u2500\u2192 _prepare_chat_with_tools converts tool to Ollama format\n  \u2502   \u251c\u2500\u2192 Server returns tool_calls with arguments\n  \u2502   \u2514\u2500\u2192 Return ChatResponse with tool_calls\n  \u251c\u2500\u2192 Extract tool_calls from response\n  \u251c\u2500\u2192 For each tool_call:\n  \u2502   \u251c\u2500\u2192 Get function name and arguments\n  \u2502   \u251c\u2500\u2192 Execute CallableTool.call(**arguments)\n  \u2502   \u2514\u2500\u2192 Creates Album instance from arguments\n  \u2514\u2500\u2192 Return Album instance (or list if parallel)\n</code></pre>"},{"location":"reference/integrations/llms/ollama/ollama_components/#component-state-management","title":"Component State Management","text":""},{"location":"reference/integrations/llms/ollama/ollama_components/#ollama-instance-state","title":"Ollama Instance State","text":"<pre><code>Initialization:\n  - model: str (immutable after init)\n  - base_url: str (immutable after init)\n  - request_timeout: float (immutable after init)\n  - temperature: float (immutable after init)\n  - json_mode: bool (immutable after init)\n  - additional_kwargs: dict (immutable after init)\n  - _client: Optional[Client] (mutable, lazy-initialized)\n  - _async_client: Optional[AsyncClient] (mutable, lazy-initialized)\n\nRuntime:\n  - _client: None \u2192 Client instance (on first use)\n  - _async_client: None \u2192 AsyncClient instance (on first async use)\n</code></pre>"},{"location":"reference/integrations/llms/ollama/ollama_components/#request-state-per-call","title":"Request State (Per Call)","text":"<pre><code>Input:\n  - messages: List[Message]\n  - tools: Optional[List[BaseTool]]\n  - stream: bool\n  - **kwargs: Additional options\n\nProcessing:\n  - request_dict: Built from inputs\n  - raw_response: dict from server\n  - parsed_response: ChatResponse/CompletionResponse\n\nOutput:\n  - ChatResponse or CompletionResponse\n</code></pre>"},{"location":"reference/integrations/llms/ollama/ollama_components/#streaming-state-per-stream","title":"Streaming State (Per Stream)","text":"<pre><code>Initialization:\n  - iterator: From client.chat(stream=True)\n\nPer Chunk:\n  - chunk_dict: Raw chunk from server\n  - accumulated_content: Growing string\n  - accumulated_tool_calls: Growing list\n  - delta: New content in this chunk\n\nOutput:\n  - Generator yielding ChatResponse objects\n</code></pre>"},{"location":"reference/integrations/llms/ollama/ollama_components/#error-boundaries","title":"Error Boundaries","text":""},{"location":"reference/integrations/llms/ollama/ollama_components/#1-configuration-errors-initialization","title":"1. Configuration Errors (Initialization)","text":"<pre><code>Ollama.__init__\n  \u2514\u2500\u2192 Validate inputs\n      \u251c\u2500\u2192 model: must be non-empty string\n      \u251c\u2500\u2192 base_url: must be valid URL\n      \u251c\u2500\u2192 temperature: must be in [0.0, 1.0]\n      \u2514\u2500\u2192 request_timeout: must be positive\n</code></pre>"},{"location":"reference/integrations/llms/ollama/ollama_components/#2-client-creation-errors-first-use","title":"2. Client Creation Errors (First Use)","text":"<pre><code>client property\n  \u2514\u2500\u2192 Create Client(host, timeout)\n      \u2514\u2500\u2192 Catch: ValueError, ConnectionError\n</code></pre>"},{"location":"reference/integrations/llms/ollama/ollama_components/#3-request-errors-during-call","title":"3. Request Errors (During Call)","text":"<pre><code>client.chat(**request)\n  \u2514\u2500\u2192 Catch: TimeoutError, ConnectionError, HTTPError\n      \u2514\u2500\u2192 Wrap and re-raise with context\n</code></pre>"},{"location":"reference/integrations/llms/ollama/ollama_components/#4-parsing-errors-response-processing","title":"4. Parsing Errors (Response Processing)","text":"<pre><code>_chat_from_response(raw_response)\n  \u2514\u2500\u2192 Extract required fields\n      \u2514\u2500\u2192 Catch: KeyError, TypeError\n          \u2514\u2500\u2192 Log warning and return default\n</code></pre>"},{"location":"reference/integrations/llms/ollama/ollama_components/#5-tool-validation-errors","title":"5. Tool Validation Errors","text":"<pre><code>_prepare_chat_with_tools(messages, tools)\n  \u2514\u2500\u2192 For each tool:\n      \u2514\u2500\u2192 Validate metadata.fn_schema exists\n          \u2514\u2500\u2192 Raise ValueError if missing\n</code></pre>"},{"location":"reference/integrations/llms/ollama/ollama_components/#component-dependencies","title":"Component Dependencies","text":""},{"location":"reference/integrations/llms/ollama/ollama_components/#ollama-depends-on","title":"Ollama Depends On:","text":"<ul> <li><code>ollama.Client</code> and <code>ollama.AsyncClient</code> (external package)</li> <li><code>serapeum.core.base.llms.models</code> (Message, ChatResponse, CompletionResponse, Metadata)</li> <li><code>serapeum.core.llm.function_calling.FunctionCallingLLM</code> (base class)</li> <li><code>serapeum.core.base.llms.utils</code> (decorators)</li> <li><code>pydantic</code> (for configuration)</li> </ul>"},{"location":"reference/integrations/llms/ollama/ollama_components/#ollama-is-used-by","title":"Ollama Is Used By:","text":"<ul> <li><code>TextCompletionLLM</code> (as the LLM engine)</li> <li><code>ToolOrchestratingLLM</code> (as the function-calling LLM)</li> <li>Direct user code (standalone usage)</li> </ul>"},{"location":"reference/integrations/llms/ollama/ollama_components/#external-dependencies","title":"External Dependencies:","text":"<ul> <li>Ollama Server: Must be running and accessible at base_url</li> <li>Model: Must be pulled and available on the server</li> </ul>"},{"location":"reference/integrations/llms/ollama/ollama_dataflow/","title":"Data Transformations and Validation","text":"<p>This diagram shows how data flows and transforms through the Ollama LLM system.</p>  Hold \"Ctrl\" to enable pan &amp; zoom  <pre><code>flowchart TD\n    Start([User Code]) --&gt; Init{Initialize Ollama}\n\n    Init --&gt; SetConfig[Set Configuration]\n    SetConfig --&gt; StoreModel[Store model name]\n    StoreModel --&gt; StoreURL[Store base_url]\n    StoreURL --&gt; StoreTimeout[Store request_timeout]\n    StoreTimeout --&gt; StoreTemp[Store temperature]\n    StoreTemp --&gt; StoreJSON[Store json_mode flag]\n    StoreJSON --&gt; StoreKwargs[Store additional_kwargs]\n\n    StoreKwargs --&gt; CreateMetadata[Create Metadata]\n    CreateMetadata --&gt; SetChatFlag[Set is_chat_model=True]\n    SetChatFlag --&gt; SetFnCallFlag[Set is_function_calling_model]\n    SetFnCallFlag --&gt; SetContext[Set context_window]\n    SetContext --&gt; Ready([Ollama Instance Ready])\n\n    Ready --&gt; CallType{Call Type?}\n\n    CallType --&gt;|chat| ChatPath[Chat Path]\n    CallType --&gt;|complete| CompletePath[Complete Path]\n    CallType --&gt;|chat_with_tools| ToolsPath[Tools Path]\n    CallType --&gt;|stream_chat| StreamPath[Stream Path]\n\n    %% Chat Path\n    ChatPath --&gt; ValidateMessages{Messages Valid?}\n    ValidateMessages --&gt;|No| Error1[Raise ValueError]\n    ValidateMessages --&gt;|Yes| BuildChatReq[Build Chat Request]\n\n    BuildChatReq --&gt; AddModel[Add model name]\n    AddModel --&gt; ConvertMessages[Convert Messages to dicts]\n    ConvertMessages --&gt; AddOptions[Add options: temperature, etc.]\n    AddOptions --&gt; CheckJSON{json_mode?}\n    CheckJSON --&gt;|True| AddFormat[Add format: json]\n    CheckJSON --&gt;|False| AddKeepAlive\n    AddFormat --&gt; AddKeepAlive[Add keep_alive]\n\n    AddKeepAlive --&gt; EnsureClient[Ensure client initialized]\n    EnsureClient --&gt; CheckClient{Client exists?}\n    CheckClient --&gt;|No| CreateClient[Create Client with base_url, timeout]\n    CheckClient --&gt;|Yes| SendRequest\n    CreateClient --&gt; SendRequest[Send HTTP POST /api/chat]\n\n    SendRequest --&gt; ReceiveRaw[Receive raw dict response]\n    ReceiveRaw --&gt; ParseResponse[_chat_from_response]\n\n    ParseResponse --&gt; ExtractMsg[Extract message dict]\n    ExtractMsg --&gt; ParseRole[Parse role: assistant]\n    ParseRole --&gt; ParseContent[Parse content: str]\n    ParseContent --&gt; CheckTools{tool_calls present?}\n    CheckTools --&gt;|Yes| ParseToolCalls[Parse tool_calls array]\n    CheckTools --&gt;|No| CreateMessage1\n    ParseToolCalls --&gt; CreateMessage1[Create Message object]\n\n    CreateMessage1 --&gt; ExtractMeta[Extract metadata: model, times, tokens]\n    ExtractMeta --&gt; CreateChatResp[Create ChatResponse]\n    CreateChatResp --&gt; ReturnChat([Return ChatResponse])\n\n    %% Complete Path\n    CompletePath --&gt; Decorator[@chat_to_completion_decorator]\n    Decorator --&gt; WrapPrompt[Wrap prompt in Message]\n    WrapPrompt --&gt; SetRole[role=USER, content=prompt]\n    SetRole --&gt; CallChat[Delegate to chat method]\n    CallChat --&gt; ChatPath\n    ReturnChat --&gt; UnwrapDecorator[Decorator unwraps response]\n    UnwrapDecorator --&gt; ExtractText[Extract message.content as text]\n    ExtractText --&gt; CreateCompleteResp[Create CompletionResponse]\n    CreateCompleteResp --&gt; ReturnComplete([Return CompletionResponse])\n\n    %% Tools Path\n    ToolsPath --&gt; PrepareTools[_prepare_chat_with_tools]\n    PrepareTools --&gt; ConvertToolsLoop[For each tool in tools]\n    ConvertToolsLoop --&gt; ExtractToolMeta[Extract tool.metadata]\n    ExtractToolMeta --&gt; GetSchema[Get fn_schema from metadata]\n    GetSchema --&gt; BuildToolDict[Build Ollama tool dict]\n    BuildToolDict --&gt; AddToolType[Add type: function]\n    AddToolType --&gt; AddToolFunc[Add function: name, description, parameters]\n\n    AddToolFunc --&gt; MergeKwargs[Merge tools into kwargs]\n    MergeKwargs --&gt; CallChatWithTools[Call chat with tools kwarg]\n    CallChatWithTools --&gt; SendRequestTools[HTTP POST with tools array]\n    SendRequestTools --&gt; ReceiveToolResp[Receive response with tool_calls]\n    ReceiveToolResp --&gt; ValidateTools[_validate_chat_with_tools_response]\n    ValidateTools --&gt; CheckParallel{allow_parallel?}\n    CheckParallel --&gt;|No| ForceSingle[force_single_tool_call]\n    CheckParallel --&gt;|Yes| ReturnToolResp\n    ForceSingle --&gt; ReturnToolResp([Return ChatResponse with tools])\n\n    %% Stream Path\n    StreamPath --&gt; BuildStreamReq[Build chat request with stream=True]\n    BuildStreamReq --&gt; SendStreamReq[HTTP POST /api/chat streaming]\n    SendStreamReq --&gt; StreamLoop{For each chunk}\n\n    StreamLoop --&gt; ReceiveChunk[Receive chunk dict]\n    ReceiveChunk --&gt; ParseChunk[_chat_stream_from_response]\n    ParseChunk --&gt; ExtractDelta[Extract message delta]\n    ExtractDelta --&gt; AccumContent[Accumulate content]\n    AccumContent --&gt; CheckToolChunk{tool_calls in chunk?}\n    CheckToolChunk --&gt;|Yes| AccumTools[Accumulate tool_calls]\n    CheckToolChunk --&gt;|No| CreateStreamResp\n    AccumTools --&gt; CreateStreamResp[Create ChatResponse with delta]\n    CreateStreamResp --&gt; YieldResp[Yield ChatResponse]\n    YieldResp --&gt; CheckDone{done=True?}\n    CheckDone --&gt;|No| StreamLoop\n    CheckDone --&gt;|Yes| EndStream([Stream Complete])\n\n    %% Error paths\n    Error1 --&gt; ErrorEnd([Raise Exception])\n\n    %% Styling\n    style Start fill:#e1f5ff\n    style Ready fill:#e1f5ff\n    style ReturnChat fill:#c8e6c9\n    style ReturnComplete fill:#c8e6c9\n    style ReturnToolResp fill:#c8e6c9\n    style EndStream fill:#c8e6c9\n    style Error1 fill:#ffcdd2\n    style ErrorEnd fill:#ffcdd2\n    style SendRequest fill:#fff9c4\n    style SendRequestTools fill:#fff9c4\n    style SendStreamReq fill:#fff9c4</code></pre>"},{"location":"reference/integrations/llms/ollama/ollama_dataflow/#data-transformation-examples","title":"Data Transformation Examples","text":""},{"location":"reference/integrations/llms/ollama/ollama_dataflow/#1-initialization","title":"1. Initialization","text":"<pre><code>Input:\n  Ollama(model=\"llama3.1\", base_url=\"http://localhost:11434\", request_timeout=180)\n\nTransformations:\n  1. Store configuration:\n     - model = \"llama3.1\"\n     - base_url = \"http://localhost:11434\"\n     - request_timeout = 180\n     - temperature = 0.75 (default)\n     - json_mode = False (default)\n\n  2. Create metadata:\n     - model_name = \"llama3.1\"\n     - is_chat_model = True\n     - is_function_calling_model = True\n     - context_window = 3900\n     - num_output = 256\n\nOutput:\n  Ollama instance with lazy-initialized client\n</code></pre>"},{"location":"reference/integrations/llms/ollama/ollama_dataflow/#2-chat-request","title":"2. Chat Request","text":"<pre><code>Input:\n  messages = [Message(role=MessageRole.USER, content=\"Say 'pong'.\")]\n  kwargs = {\"temperature\": 0.2}\n\nTransformations:\n  1. Convert messages to dicts:\n     [{\"role\": \"user\", \"content\": \"Say 'pong'.\"}]\n\n  2. Build request payload:\n     {\n       \"model\": \"llama3.1\",\n       \"messages\": [{\"role\": \"user\", \"content\": \"Say 'pong'.\"}],\n       \"options\": {\"temperature\": 0.2},\n       \"stream\": False,\n       \"keep_alive\": None\n     }\n\n  3. HTTP POST to /api/chat\n\n  4. Raw response:\n     {\n       \"model\": \"llama3.1\",\n       \"created_at\": \"2025-01-22T...\",\n       \"message\": {\n         \"role\": \"assistant\",\n         \"content\": \"Pong!\"\n       },\n       \"done\": True,\n       \"total_duration\": 1234567890,\n       \"prompt_eval_count\": 10,\n       \"eval_count\": 2\n     }\n\n  5. Parse to ChatResponse:\n     ChatResponse(\n       message=Message(\n         role=MessageRole.ASSISTANT,\n         content=\"Pong!\",\n         additional_kwargs={}\n       ),\n       raw={...},\n       additional_kwargs={\n         \"model\": \"llama3.1\",\n         \"created_at\": \"...\",\n         \"total_duration\": 1234567890,\n         \"prompt_eval_count\": 10,\n         \"eval_count\": 2\n       }\n     )\n\nOutput:\n  ChatResponse with assistant message\n</code></pre>"},{"location":"reference/integrations/llms/ollama/ollama_dataflow/#3-complete-request-via-decorator","title":"3. Complete Request (via Decorator)","text":"<pre><code>Input:\n  prompt = \"Say 'pong'.\"\n  kwargs = {}\n\nTransformations:\n  1. Decorator wraps prompt:\n     Message(role=MessageRole.USER, content=\"Say 'pong'.\")\n\n  2. Delegates to chat([message], **kwargs)\n     [Follows Chat Request flow above]\n\n  3. Decorator extracts text:\n     text = chat_response.message.content  # \"Pong!\"\n\n  4. Creates CompletionResponse:\n     CompletionResponse(\n       text=\"Pong!\",\n       raw={...},\n       additional_kwargs={...}\n     )\n\nOutput:\n  CompletionResponse with text\n</code></pre>"},{"location":"reference/integrations/llms/ollama/ollama_dataflow/#4-chat-with-tools","title":"4. Chat with Tools","text":"<pre><code>Input:\n  messages = [Message(role=MessageRole.USER, content=\"Create album about rock\")]\n  tools = [CallableTool(fn=create_album, metadata=ToolMetadata(...))]\n  kwargs = {}\n\nTransformations:\n  1. Convert each tool to Ollama format:\n     {\n       \"type\": \"function\",\n       \"function\": {\n         \"name\": \"create_album\",\n         \"description\": \"Create an album with title and songs\",\n         \"parameters\": {\n           \"type\": \"object\",\n           \"properties\": {\n             \"title\": {\"type\": \"string\"},\n             \"artist\": {\"type\": \"string\"},\n             \"songs\": {\"type\": \"array\", \"items\": {\"type\": \"string\"}}\n           },\n           \"required\": [\"title\", \"artist\", \"songs\"]\n         }\n       }\n     }\n\n  2. Build request with tools:\n     {\n       \"model\": \"llama3.1\",\n       \"messages\": [...],\n       \"tools\": [&lt;converted tool dicts&gt;],\n       \"stream\": False\n     }\n\n  3. HTTP POST to /api/chat\n\n  4. Raw response with tool_calls:\n     {\n       \"message\": {\n         \"role\": \"assistant\",\n         \"content\": \"\",\n         \"tool_calls\": [\n           {\n             \"function\": {\n               \"name\": \"create_album\",\n               \"arguments\": {\n                 \"title\": \"Rock Legends\",\n                 \"artist\": \"Various Artists\",\n                 \"songs\": [\"Song 1\", \"Song 2\"]\n               }\n             }\n           }\n         ]\n       },\n       ...\n     }\n\n  5. Parse tool_calls in message:\n     Message(\n       role=MessageRole.ASSISTANT,\n       content=\"\",\n       additional_kwargs={\n         \"tool_calls\": [\n           {\n             \"function\": {\n               \"name\": \"create_album\",\n               \"arguments\": {...}\n             }\n           }\n         ]\n       }\n     )\n\n  6. If not allow_parallel, force_single_tool_call:\n     Keep only first tool call\n\nOutput:\n  ChatResponse with tool_calls in message.additional_kwargs\n</code></pre>"},{"location":"reference/integrations/llms/ollama/ollama_dataflow/#5-streaming-chat","title":"5. Streaming Chat","text":"<pre><code>Input:\n  messages = [Message(role=MessageRole.USER, content=\"Count to 3\")]\n  stream = True\n\nTransformations:\n  1. Build request with stream=True\n\n  2. HTTP POST returns chunk iterator\n\n  3. For each chunk:\n     Chunk 1: {\"message\": {\"content\": \"1\"}, \"done\": False}\n       \u2192 ChatResponse(message=Message(content=\"1\"), delta=\"1\")\n       \u2192 Yield\n\n     Chunk 2: {\"message\": {\"content\": \", 2\"}, \"done\": False}\n       \u2192 ChatResponse(message=Message(content=\", 2\"), delta=\", 2\")\n       \u2192 Yield\n\n     Chunk 3: {\"message\": {\"content\": \", 3\"}, \"done\": True}\n       \u2192 ChatResponse(message=Message(content=\", 3\"), delta=\", 3\")\n       \u2192 Yield\n\nOutput:\n  Generator yielding ChatResponse objects\n</code></pre>"},{"location":"reference/integrations/llms/ollama/ollama_dataflow/#validation-points","title":"Validation Points","text":""},{"location":"reference/integrations/llms/ollama/ollama_dataflow/#1-configuration-validation","title":"1. Configuration Validation","text":"<ul> <li>model: Must be non-empty string</li> <li>base_url: Must be valid URL format</li> <li>temperature: Must be float in range [0.0, 1.0]</li> <li>request_timeout: Must be positive float</li> </ul>"},{"location":"reference/integrations/llms/ollama/ollama_dataflow/#2-message-validation","title":"2. Message Validation","text":"<ul> <li>messages: Must be non-empty list</li> <li>role: Must be valid MessageRole enum</li> <li>content: Must be string (can be empty for tool calls)</li> </ul>"},{"location":"reference/integrations/llms/ollama/ollama_dataflow/#3-tool-validation","title":"3. Tool Validation","text":"<ul> <li>tools: Must be list of BaseTool</li> <li>tool.metadata: Must have name, description, fn_schema</li> <li>fn_schema: Must be valid JSON schema dict</li> </ul>"},{"location":"reference/integrations/llms/ollama/ollama_dataflow/#4-response-validation","title":"4. Response Validation","text":"<ul> <li>HTTP status: Must be 200, else raise error</li> <li>JSON parsing: Must be valid JSON</li> <li>Required fields: Must have message/text in response</li> <li>tool_calls format: Must match expected structure</li> </ul>"},{"location":"reference/integrations/llms/ollama/ollama_dataflow/#error-handling","title":"Error Handling","text":""},{"location":"reference/integrations/llms/ollama/ollama_dataflow/#network-errors","title":"Network Errors","text":"<pre><code>Request \u2192 Timeout \u2192 Raise RequestException with timeout info\nRequest \u2192 Connection Error \u2192 Raise ConnectionError with URL\nRequest \u2192 HTTP Error \u2192 Raise HTTPError with status code\n</code></pre>"},{"location":"reference/integrations/llms/ollama/ollama_dataflow/#parsing-errors","title":"Parsing Errors","text":"<pre><code>Response \u2192 Invalid JSON \u2192 Raise JSONDecodeError\nResponse \u2192 Missing fields \u2192 Raise KeyError\nResponse \u2192 Invalid tool_calls \u2192 Log warning, return empty list\n</code></pre>"},{"location":"reference/integrations/llms/ollama/ollama_dataflow/#configuration-errors","title":"Configuration Errors","text":"<pre><code>Invalid model \u2192 Raise ValueError\nInvalid URL \u2192 Raise ValueError\nMissing required field \u2192 Raise TypeError\n</code></pre>"},{"location":"reference/integrations/llms/ollama/ollama_dataflow/#data-flow-summary","title":"Data Flow Summary","text":"<pre><code>User Input\n  \u2193\nConfiguration/Validation\n  \u2193\nRequest Building (convert to Ollama format)\n  \u2193\nClient Initialization (lazy)\n  \u2193\nHTTP Request (sync/async)\n  \u2193\nRaw Response (dict)\n  \u2193\nResponse Parsing (to typed models)\n  \u2193\nValidation/Post-processing\n  \u2193\nTyped Response (ChatResponse/CompletionResponse)\n  \u2193\nUser Output\n</code></pre>"},{"location":"reference/integrations/llms/ollama/ollama_sequence/","title":"Execution Flow and Method Calls","text":"<p>This diagram shows the complete workflow from initialization to execution of the <code>Ollama</code> class.</p>  Hold \"Ctrl\" to enable pan &amp; zoom  <pre><code>sequenceDiagram\n    participant User\n    participant Ollama\n    participant Client/AsyncClient\n    participant OllamaServer\n    participant TextCompletionLLM\n    participant ToolOrchestratingLLM\n    participant PydanticParser\n\n    Note over User: Initialization Phase\n    User-&gt;&gt;Ollama: __init__(model, base_url, request_timeout, ...)\n    activate Ollama\n\n    Ollama-&gt;&gt;Ollama: Set model configuration\n    Note over Ollama: Store: model, base_url, temperature,&lt;br/&gt;request_timeout, json_mode, etc.\n\n    Ollama-&gt;&gt;Ollama: Initialize metadata\n    Note over Ollama: Set is_chat_model=True&lt;br/&gt;is_function_calling_model=True&lt;br/&gt;context_window, num_output\n\n    alt client not provided\n        Ollama-&gt;&gt;Client/AsyncClient: Create lazy client\n        Note over Client/AsyncClient: Client created on first use&lt;br/&gt;with base_url and timeout\n    else client provided\n        Ollama-&gt;&gt;Ollama: Store provided client\n    end\n\n    Ollama--&gt;&gt;User: Ollama instance\n    deactivate Ollama\n\n    Note over User: Direct Usage - Chat Method\n    User-&gt;&gt;Ollama: chat(messages, **kwargs)\n    activate Ollama\n\n    Ollama-&gt;&gt;Ollama: _prepare_chat_with_tools(messages, tools)\n    Note over Ollama: Convert tools to Ollama format&lt;br/&gt;if tools provided\n\n    Ollama-&gt;&gt;Ollama: _chat(messages, stream=False, **kwargs)\n    Ollama-&gt;&gt;Client/AsyncClient: Ensure client initialized\n    activate Client/AsyncClient\n    Client/AsyncClient--&gt;&gt;Ollama: client instance\n    deactivate Client/AsyncClient\n\n    Ollama-&gt;&gt;Ollama: Build request payload\n    Note over Ollama: Combine: model, messages,&lt;br/&gt;options (temp, etc.), format, tools\n\n    Ollama-&gt;&gt;Client/AsyncClient: client.chat(**request)\n    activate Client/AsyncClient\n    Client/AsyncClient-&gt;&gt;OllamaServer: HTTP POST /api/chat\n    activate OllamaServer\n    OllamaServer--&gt;&gt;Client/AsyncClient: JSON response\n    deactivate OllamaServer\n    Client/AsyncClient--&gt;&gt;Ollama: raw response dict\n    deactivate Client/AsyncClient\n\n    Ollama-&gt;&gt;Ollama: _chat_from_response(response)\n    Note over Ollama: Parse message, tool_calls,&lt;br/&gt;additional_kwargs\n\n    Ollama--&gt;&gt;User: ChatResponse\n    deactivate Ollama\n\n    Note over User: Direct Usage - Complete Method\n    User-&gt;&gt;Ollama: complete(prompt, **kwargs)\n    activate Ollama\n\n    Ollama-&gt;&gt;Ollama: @chat_to_completion_decorator\n    Note over Ollama: Converts prompt to Message&lt;br/&gt;and delegates to chat()\n\n    Ollama-&gt;&gt;Ollama: chat([Message(USER, prompt)], **kwargs)\n    Note over Ollama: Follows chat flow above\n\n    Ollama-&gt;&gt;Ollama: Extract text from ChatResponse\n    Ollama--&gt;&gt;User: CompletionResponse\n    deactivate Ollama\n\n    Note over User: Usage with TextCompletionLLM\n    User-&gt;&gt;PydanticParser: Create with output_cls\n    activate PydanticParser\n    PydanticParser--&gt;&gt;User: parser\n    deactivate PydanticParser\n\n    User-&gt;&gt;TextCompletionLLM: __init__(parser, prompt, llm=Ollama)\n    activate TextCompletionLLM\n    TextCompletionLLM-&gt;&gt;TextCompletionLLM: Validate components\n    TextCompletionLLM--&gt;&gt;User: text_llm instance\n    deactivate TextCompletionLLM\n\n    User-&gt;&gt;TextCompletionLLM: __call__(value=\"input\")\n    activate TextCompletionLLM\n\n    TextCompletionLLM-&gt;&gt;Ollama: Check metadata.is_chat_model\n    activate Ollama\n    Ollama--&gt;&gt;TextCompletionLLM: True\n    deactivate Ollama\n\n    TextCompletionLLM-&gt;&gt;TextCompletionLLM: Format prompt with variables\n    TextCompletionLLM-&gt;&gt;Ollama: chat(formatted_messages)\n    activate Ollama\n    Ollama-&gt;&gt;OllamaServer: HTTP POST /api/chat\n    activate OllamaServer\n    OllamaServer--&gt;&gt;Ollama: JSON response\n    deactivate OllamaServer\n    Ollama--&gt;&gt;TextCompletionLLM: ChatResponse\n    deactivate Ollama\n\n    TextCompletionLLM-&gt;&gt;PydanticParser: parse(response.message.content)\n    activate PydanticParser\n    PydanticParser--&gt;&gt;TextCompletionLLM: Parsed model instance\n    deactivate PydanticParser\n\n    TextCompletionLLM--&gt;&gt;User: DummyModel instance\n    deactivate TextCompletionLLM\n\n    Note over User: Usage with ToolOrchestratingLLM\n    User-&gt;&gt;ToolOrchestratingLLM: __init__(output_cls=Album, prompt, llm=Ollama)\n    activate ToolOrchestratingLLM\n    ToolOrchestratingLLM-&gt;&gt;ToolOrchestratingLLM: Create tool from output_cls\n    Note over ToolOrchestratingLLM: Convert Album Pydantic model&lt;br/&gt;to CallableTool\n    ToolOrchestratingLLM--&gt;&gt;User: tools_llm instance\n    deactivate ToolOrchestratingLLM\n\n    User-&gt;&gt;ToolOrchestratingLLM: __call__(topic=\"rock\")\n    activate ToolOrchestratingLLM\n\n    ToolOrchestratingLLM-&gt;&gt;ToolOrchestratingLLM: Format prompt with topic\n    ToolOrchestratingLLM-&gt;&gt;Ollama: chat(messages, tools=[Album tool])\n    activate Ollama\n\n    Ollama-&gt;&gt;Ollama: _prepare_chat_with_tools(messages, tools)\n    Note over Ollama: Convert CallableTool to&lt;br/&gt;Ollama tool format with schema\n\n    Ollama-&gt;&gt;OllamaServer: HTTP POST /api/chat with tools\n    activate OllamaServer\n    OllamaServer--&gt;&gt;Ollama: Response with tool_calls\n    deactivate OllamaServer\n\n    Ollama--&gt;&gt;ToolOrchestratingLLM: ChatResponse(tool_calls=[...])\n    deactivate Ollama\n\n    ToolOrchestratingLLM-&gt;&gt;ToolOrchestratingLLM: Extract tool calls\n    ToolOrchestratingLLM-&gt;&gt;ToolOrchestratingLLM: Execute tool with arguments\n    Note over ToolOrchestratingLLM: Create Album instance&lt;br/&gt;from tool arguments\n\n    ToolOrchestratingLLM--&gt;&gt;User: Album instance\n    deactivate ToolOrchestratingLLM\n\n    Note over User: Streaming Usage\n    User-&gt;&gt;Ollama: stream_chat(messages)\n    activate Ollama\n\n    Ollama-&gt;&gt;Ollama: _chat(messages, stream=True)\n    Ollama-&gt;&gt;Client/AsyncClient: client.chat(stream=True)\n    activate Client/AsyncClient\n    Client/AsyncClient-&gt;&gt;OllamaServer: HTTP POST /api/chat (streaming)\n    activate OllamaServer\n\n    loop For each chunk\n        OllamaServer--&gt;&gt;Client/AsyncClient: Stream chunk\n        Client/AsyncClient--&gt;&gt;Ollama: Raw chunk dict\n        Ollama-&gt;&gt;Ollama: _chat_stream_from_response(chunk)\n        Note over Ollama: Parse incremental message,&lt;br/&gt;accumulate tool_calls\n        Ollama--&gt;&gt;User: Yield ChatResponse\n    end\n\n    deactivate OllamaServer\n    deactivate Client/AsyncClient\n    deactivate Ollama\n\n    Note over User: Async Usage\n    User-&gt;&gt;Ollama: await achat(messages)\n    activate Ollama\n\n    Ollama-&gt;&gt;Ollama: _achat(messages, stream=False)\n    Ollama-&gt;&gt;Client/AsyncClient: await async_client.chat()\n    activate Client/AsyncClient\n    Client/AsyncClient-&gt;&gt;OllamaServer: HTTP POST /api/chat (async)\n    activate OllamaServer\n    OllamaServer--&gt;&gt;Client/AsyncClient: JSON response\n    deactivate OllamaServer\n    Client/AsyncClient--&gt;&gt;Ollama: raw response dict\n    deactivate Client/AsyncClient\n\n    Ollama-&gt;&gt;Ollama: _chat_from_response(response)\n    Ollama--&gt;&gt;User: ChatResponse\n    deactivate Ollama</code></pre>"},{"location":"reference/integrations/llms/ollama/ollama_sequence/#key-execution-paths","title":"Key Execution Paths","text":""},{"location":"reference/integrations/llms/ollama/ollama_sequence/#1-direct-chat-call","title":"1. Direct Chat Call","text":"<pre><code>User \u2192 Ollama.chat\n  \u251c\u2500\u2192 _prepare_chat_with_tools (if tools provided)\n  \u251c\u2500\u2192 _chat (build request)\n  \u251c\u2500\u2192 Client.chat \u2192 HTTP POST /api/chat\n  \u251c\u2500\u2192 _chat_from_response (parse response)\n  \u2514\u2500\u2192 Return ChatResponse\n</code></pre>"},{"location":"reference/integrations/llms/ollama/ollama_sequence/#2-complete-call-via-decorator","title":"2. Complete Call (via Decorator)","text":"<pre><code>User \u2192 Ollama.complete\n  \u251c\u2500\u2192 @chat_to_completion_decorator\n  \u251c\u2500\u2192 Convert prompt to Message\n  \u251c\u2500\u2192 Delegate to chat()\n  \u2514\u2500\u2192 Return CompletionResponse\n</code></pre>"},{"location":"reference/integrations/llms/ollama/ollama_sequence/#3-with-textcompletionllm","title":"3. With TextCompletionLLM","text":"<pre><code>User \u2192 TextCompletionLLM(llm=Ollama)\n  \u251c\u2500\u2192 Format prompt with variables\n  \u251c\u2500\u2192 Ollama.chat (get raw response)\n  \u251c\u2500\u2192 PydanticParser.parse\n  \u2514\u2500\u2192 Return validated model instance\n</code></pre>"},{"location":"reference/integrations/llms/ollama/ollama_sequence/#4-with-toolorchestratingllm","title":"4. With ToolOrchestratingLLM","text":"<pre><code>User \u2192 ToolOrchestratingLLM(llm=Ollama)\n  \u251c\u2500\u2192 Convert output_cls to CallableTool\n  \u251c\u2500\u2192 Format prompt with variables\n  \u251c\u2500\u2192 Ollama.chat with tools parameter\n  \u251c\u2500\u2192 Ollama converts tools to schema\n  \u251c\u2500\u2192 Server returns tool_calls\n  \u251c\u2500\u2192 Execute tool to create instance\n  \u2514\u2500\u2192 Return model instance\n</code></pre>"},{"location":"reference/integrations/llms/ollama/ollama_sequence/#5-streaming","title":"5. Streaming","text":"<pre><code>User \u2192 Ollama.stream_chat\n  \u251c\u2500\u2192 _chat(stream=True)\n  \u251c\u2500\u2192 Client.chat(stream=True)\n  \u2514\u2500\u2192 For each chunk:\n      \u251c\u2500\u2192 _chat_stream_from_response\n      \u2514\u2500\u2192 Yield ChatResponse\n</code></pre>"},{"location":"reference/integrations/llms/ollama/ollama_sequence/#important-implementation-details","title":"Important Implementation Details","text":"<ol> <li>Client Lazy Initialization: The Ollama client is created lazily on first use, not during <code>__init__</code></li> <li>Tool Conversion: When tools are provided, they're converted from <code>BaseTool</code> to Ollama's tool schema format</li> <li>Decorator Pattern: The <code>complete</code> method uses decorators to wrap the <code>chat</code> method for consistency</li> <li>Streaming Accumulation: In streaming mode, tool calls are accumulated across chunks</li> <li>Metadata Handling: Response metadata includes model info, timing, and token counts</li> <li>Error Handling: Network errors, timeout errors, and parsing errors are handled at each stage</li> </ol>"},{"location":"reference/integrations/llms/ollama/ollama_state/","title":"Lifecycle and State Management","text":"<p>This diagram shows the complete lifecycle and state transitions of the Ollama LLM.</p>  Hold \"Ctrl\" to enable pan &amp; zoom  <pre><code>stateDiagram-v2\n    [*] --&gt; Uninitialized\n\n    Uninitialized --&gt; Configured: __init__(model, base_url, ...)\n\n    state Configured {\n        [*] --&gt; ClientNotCreated\n\n        note right of ClientNotCreated\n            State: Configuration stored\n            - model: str\n            - base_url: str\n            - request_timeout: float\n            - temperature: float\n            - json_mode: bool\n            - _client: None\n            - _async_client: None\n        end note\n\n        ClientNotCreated --&gt; ClientInitialized: First chat/complete call\n\n        state ClientInitialized {\n            [*] --&gt; Idle\n\n            note right of Idle\n                State: Ready for requests\n                - _client: Client instance\n                - _async_client: None or AsyncClient\n            end note\n\n            Idle --&gt; ProcessingChat: chat(messages)\n            Idle --&gt; ProcessingComplete: complete(prompt)\n            Idle --&gt; ProcessingStream: stream_chat(messages)\n            Idle --&gt; ProcessingAsync: achat(messages)\n            Idle --&gt; ProcessingTools: chat_with_tools(messages, tools)\n\n            state ProcessingChat {\n                [*] --&gt; BuildingRequest\n                BuildingRequest --&gt; ConvertingMessages: Convert Message objects\n                ConvertingMessages --&gt; AddingOptions: Add temperature, etc.\n                AddingOptions --&gt; AddingFormat: Add json format if enabled\n                AddingFormat --&gt; SendingRequest: HTTP POST to server\n                SendingRequest --&gt; WaitingResponse: Awaiting response\n                WaitingResponse --&gt; ParsingResponse: Parse raw dict\n                ParsingResponse --&gt; CreatingChatResponse: Create ChatResponse\n                CreatingChatResponse --&gt; [*]\n            }\n\n            state ProcessingComplete {\n                [*] --&gt; DecoratorWrap\n                DecoratorWrap --&gt; ConvertToMessage: Wrap prompt as Message\n                ConvertToMessage --&gt; DelegateToChat: Call chat([message])\n                DelegateToChat --&gt; ProcessingChat\n                ProcessingChat --&gt; DecoratorUnwrap: Extract text\n                DecoratorUnwrap --&gt; CreateCompletionResponse: Create CompletionResponse\n                CreateCompletionResponse --&gt; [*]\n            }\n\n            state ProcessingStream {\n                [*] --&gt; BuildingStreamRequest\n                BuildingStreamRequest --&gt; SendingStreamRequest: stream=True\n                SendingStreamRequest --&gt; StreamLoop\n\n                state StreamLoop {\n                    [*] --&gt; WaitingChunk\n                    WaitingChunk --&gt; ReceivingChunk: Chunk arrives\n                    ReceivingChunk --&gt; ParsingChunk: Parse chunk dict\n                    ParsingChunk --&gt; AccumulatingContent: Accumulate content\n                    AccumulatingContent --&gt; AccumulatingTools: Accumulate tool_calls\n                    AccumulatingTools --&gt; YieldingResponse: Yield ChatResponse\n                    YieldingResponse --&gt; CheckDone: Check done flag\n                    CheckDone --&gt; WaitingChunk: done=False\n                    CheckDone --&gt; [*]: done=True\n                }\n\n                StreamLoop --&gt; [*]\n            }\n\n            state ProcessingAsync {\n                [*] --&gt; EnsureAsyncClient\n                EnsureAsyncClient --&gt; BuildingAsyncRequest: Create AsyncClient if needed\n                BuildingAsyncRequest --&gt; SendingAsyncRequest: await async_client.chat()\n                SendingAsyncRequest --&gt; WaitingAsyncResponse: Awaiting\n                WaitingAsyncResponse --&gt; ParsingAsyncResponse: Parse response\n                ParsingAsyncResponse --&gt; CreatingAsyncChatResponse: Create ChatResponse\n                CreatingAsyncChatResponse --&gt; [*]\n            }\n\n            state ProcessingTools {\n                [*] --&gt; PreparingTools\n                PreparingTools --&gt; ConvertingTools: Convert to Ollama format\n                ConvertingTools --&gt; ExtractingSchemas: Extract fn_schema from metadata\n                ExtractingSchemas --&gt; BuildingToolDicts: Build tool dicts\n                BuildingToolDicts --&gt; MergingKwargs: Add tools to kwargs\n                MergingKwargs --&gt; CallingChat: Call chat(messages, tools=...)\n                CallingChat --&gt; ProcessingChat\n                ProcessingChat --&gt; ValidatingToolResponse: Check tool_calls present\n                ValidatingToolResponse --&gt; CheckParallel: Check allow_parallel_tool_calls\n                CheckParallel --&gt; ForcingSingle: False - trim to 1\n                CheckParallel --&gt; ReturningMultiple: True - keep all\n                ForcingSingle --&gt; [*]\n                ReturningMultiple --&gt; [*]\n            }\n\n            ProcessingChat --&gt; Idle: Return ChatResponse\n            ProcessingComplete --&gt; Idle: Return CompletionResponse\n            ProcessingStream --&gt; Idle: Stream complete\n            ProcessingAsync --&gt; Idle: Return ChatResponse\n            ProcessingTools --&gt; Idle: Return ChatResponse with tools\n\n            Idle --&gt; Error: Exception occurs\n            ProcessingChat --&gt; Error: Network/Parse error\n            ProcessingComplete --&gt; Error: Network/Parse error\n            ProcessingStream --&gt; Error: Network/Parse error\n            ProcessingAsync --&gt; Error: Network/Parse error\n            ProcessingTools --&gt; Error: Tool validation error\n\n            state Error {\n                [*] --&gt; LoggingError\n                LoggingError --&gt; RaisingException: Raise appropriate exception\n                RaisingException --&gt; [*]\n            }\n\n            Error --&gt; Idle: Error handled by caller\n        }\n    }\n\n    Configured --&gt; [*]: Delete instance\n\n    note left of Configured\n        Lifecycle Phases:\n        1. Uninitialized: Before __init__\n        2. Configured: After __init__, clients lazy\n        3. ClientInitialized: After first use\n        4. Idle: Ready for requests\n        5. Processing*: Handling request\n        6. Error: Exception state\n    end note</code></pre>"},{"location":"reference/integrations/llms/ollama/ollama_state/#state-transitions","title":"State Transitions","text":""},{"location":"reference/integrations/llms/ollama/ollama_state/#1-initialization-configured","title":"1. Initialization \u2192 Configured","text":"<pre><code>llm = Ollama(\n    model=\"llama3.1\",\n    base_url=\"http://localhost:11434\",\n    request_timeout=180\n)\n\n# State: Configured\n# - Configuration fields populated\n# - Metadata created (is_chat_model=True, is_function_calling_model=True)\n# - _client = None (not yet created)\n# - _async_client = None (not yet created)\n</code></pre>"},{"location":"reference/integrations/llms/ollama/ollama_state/#2-configured-clientinitialized-lazy","title":"2. Configured \u2192 ClientInitialized (Lazy)","text":"<pre><code># First call triggers client creation\nresponse = llm.chat([Message(role=MessageRole.USER, content=\"Hello\")])\n\n# Transition:\n# - Access self.client property\n# - Check if self._client is None \u2192 True\n# - Create Client(host=self.base_url, timeout=self.request_timeout)\n# - Store in self._client\n\n# State: ClientInitialized \u2192 Idle\n# - _client = Client instance\n# - Ready to process requests\n</code></pre>"},{"location":"reference/integrations/llms/ollama/ollama_state/#3-idle-processingchat-idle","title":"3. Idle \u2192 ProcessingChat \u2192 Idle","text":"<pre><code># Idle state: Ready for requests\nresponse = llm.chat(messages)\n\n# Transition to ProcessingChat:\n# 1. BuildingRequest: Create request dict\n# 2. ConvertingMessages: Message objects to dicts\n# 3. AddingOptions: Merge temperature, etc.\n# 4. AddingFormat: Add json format if enabled\n# 5. SendingRequest: client.chat(**request)\n# 6. WaitingResponse: Block until response\n# 7. ParsingResponse: _chat_from_response(raw)\n# 8. CreatingChatResponse: Build ChatResponse object\n\n# Transition back to Idle:\n# - Return ChatResponse to caller\n</code></pre>"},{"location":"reference/integrations/llms/ollama/ollama_state/#4-idle-processingcomplete-idle","title":"4. Idle \u2192 ProcessingComplete \u2192 Idle","text":"<pre><code># Complete uses decorator pattern\nresponse = llm.complete(prompt)\n\n# Transition to ProcessingComplete:\n# 1. DecoratorWrap: @chat_to_completion_decorator intercepts\n# 2. ConvertToMessage: prompt \u2192 Message(role=USER, content=prompt)\n# 3. DelegateToChat: Call self.chat([message])\n#    [Enters ProcessingChat state]\n# 4. DecoratorUnwrap: Extract message.content\n# 5. CreateCompletionResponse: Wrap in CompletionResponse\n\n# Transition back to Idle:\n# - Return CompletionResponse to caller\n</code></pre>"},{"location":"reference/integrations/llms/ollama/ollama_state/#5-idle-processingstream-idle","title":"5. Idle \u2192 ProcessingStream \u2192 Idle","text":"<pre><code># Streaming maintains state across multiple yields\nfor chunk in llm.stream_chat(messages):\n    print(chunk.message.content)\n\n# Transition to ProcessingStream:\n# 1. BuildingStreamRequest: Create request with stream=True\n# 2. SendingStreamRequest: client.chat(stream=True)\n# 3. StreamLoop - for each chunk:\n#    a. WaitingChunk: Block for next chunk\n#    b. ReceivingChunk: Chunk dict arrives\n#    c. ParsingChunk: _chat_stream_from_response(chunk)\n#    d. AccumulatingContent: Append to content buffer\n#    e. AccumulatingTools: Append to tool_calls buffer\n#    f. YieldingResponse: Create and yield ChatResponse with delta\n#    g. CheckDone: If done=True, exit loop\n# 4. StreamLoop exits when done=True\n\n# Transition back to Idle:\n# - Generator exhausted\n</code></pre>"},{"location":"reference/integrations/llms/ollama/ollama_state/#6-idle-processingasync-idle","title":"6. Idle \u2192 ProcessingAsync \u2192 Idle","text":"<pre><code># Async uses separate client and event loop\nresponse = await llm.achat(messages)\n\n# Transition to ProcessingAsync:\n# 1. EnsureAsyncClient: Check self._async_client\n#    - If None, create AsyncClient(host=base_url, timeout=timeout)\n# 2. BuildingAsyncRequest: Create request dict\n# 3. SendingAsyncRequest: await async_client.chat(**request)\n# 4. WaitingAsyncResponse: Coroutine awaits response\n# 5. ParsingAsyncResponse: _chat_from_response(raw)\n# 6. CreatingAsyncChatResponse: Build ChatResponse\n\n# Transition back to Idle:\n# - Return ChatResponse to caller\n</code></pre>"},{"location":"reference/integrations/llms/ollama/ollama_state/#7-idle-processingtools-idle","title":"7. Idle \u2192 ProcessingTools \u2192 Idle","text":"<pre><code># Tool calling adds preparation and validation steps\nresponse = llm.chat_with_tools(messages, tools)\n\n# Transition to ProcessingTools:\n# 1. PreparingTools: _prepare_chat_with_tools(messages, tools)\n# 2. ConvertingTools: For each tool:\n#    a. ExtractingSchemas: Get tool.metadata.fn_schema\n#    b. BuildingToolDicts: Create Ollama tool dict format\n# 3. MergingKwargs: Add tools to kwargs\n# 4. CallingChat: Delegate to chat(messages, **kwargs)\n#    [Enters ProcessingChat state]\n# 5. ValidatingToolResponse: _validate_chat_with_tools_response\n# 6. CheckParallel: Check allow_parallel_tool_calls flag\n#    - If False: ForcingSingle \u2192 trim to first tool call\n#    - If True: ReturningMultiple \u2192 keep all tool calls\n\n# Transition back to Idle:\n# - Return ChatResponse with tool_calls\n</code></pre>"},{"location":"reference/integrations/llms/ollama/ollama_state/#8-any-state-error-idle","title":"8. Any State \u2192 Error \u2192 Idle","text":"<pre><code>try:\n    response = llm.chat(messages)\nexcept Exception as e:\n    # Handle error\n\n# Error transition can occur from:\n# - ProcessingChat: Network timeout, invalid response\n# - ProcessingComplete: Any chat error propagates\n# - ProcessingStream: Chunk parsing error\n# - ProcessingAsync: Async operation failure\n# - ProcessingTools: Tool schema validation error\n\n# Error state:\n# 1. LoggingError: Log exception details\n# 2. RaisingException: Raise appropriate exception type\n#    - TimeoutError: request_timeout exceeded\n#    - ConnectionError: Cannot reach server\n#    - ValueError: Invalid response format\n#    - KeyError: Missing required field in response\n\n# Transition back to Idle:\n# - Exception handled by caller\n# - Instance still usable for next call\n</code></pre>"},{"location":"reference/integrations/llms/ollama/ollama_state/#state-variables","title":"State Variables","text":""},{"location":"reference/integrations/llms/ollama/ollama_state/#configuration-state-immutable-after-init","title":"Configuration State (Immutable after init)","text":"<pre><code># Set during __init__, never change\nself.model: str = \"llama3.1\"\nself.base_url: str = \"http://localhost:11434\"\nself.request_timeout: float = 180.0\nself.temperature: float = 0.75\nself.context_window: int = 3900\nself.prompt_key: str = \"prompt\"\nself.json_mode: bool = False\nself.additional_kwargs: dict[str, Any] = {}\nself.keep_alive: Optional[str] = None\nself._is_function_calling_model: bool = True\n</code></pre>"},{"location":"reference/integrations/llms/ollama/ollama_state/#client-state-mutable-lazy-initialized","title":"Client State (Mutable, lazy-initialized)","text":"<pre><code># None until first use\nself._client: Optional[Client] = None\nself._async_client: Optional[AsyncClient] = None\n\n# After first sync call\nself._client: Client = Client(host=self.base_url, timeout=self.request_timeout)\n\n# After first async call\nself._async_client: AsyncClient = AsyncClient(host=self.base_url, timeout=self.request_timeout)\n</code></pre>"},{"location":"reference/integrations/llms/ollama/ollama_state/#request-state-per-call-transient","title":"Request State (Per-call, transient)","text":"<pre><code># Created fresh for each call, not stored\nrequest_dict = {\n    \"model\": self.model,\n    \"messages\": [...],\n    \"options\": {\"temperature\": self.temperature, ...},\n    \"stream\": False,\n    \"format\": \"json\" if self.json_mode else None,\n    \"tools\": [...] if tools else None,\n    \"keep_alive\": self.keep_alive\n}\n</code></pre>"},{"location":"reference/integrations/llms/ollama/ollama_state/#streaming-state-per-stream-transient","title":"Streaming State (Per-stream, transient)","text":"<pre><code># Maintained during stream, not stored on instance\naccumulated_content: str = \"\"\naccumulated_tool_calls: list[dict] = []\ncurrent_chunk: dict = {}\ndone: bool = False\n</code></pre>"},{"location":"reference/integrations/llms/ollama/ollama_state/#response-state-per-call-returned","title":"Response State (Per-call, returned)","text":"<pre><code># Created and returned, not stored\nchat_response = ChatResponse(\n    message=Message(\n        role=MessageRole.ASSISTANT,\n        content=\"...\",\n        additional_kwargs={\"tool_calls\": [...]}\n    ),\n    raw={...},\n    additional_kwargs={...}\n)\n</code></pre>"},{"location":"reference/integrations/llms/ollama/ollama_state/#lifecycle-diagram","title":"Lifecycle Diagram","text":"Hold \"Ctrl\" to enable pan &amp; zoom  <pre><code>graph LR\n    A[Create Instance] --&gt; B[Configured State]\n    B --&gt; C{First Call?}\n    C --&gt;|Yes| D[Initialize Client]\n    C --&gt;|No| E[Use Existing Client]\n    D --&gt; E\n    E --&gt; F[Process Request]\n    F --&gt; G{Success?}\n    G --&gt;|Yes| H[Return Response]\n    G --&gt;|No| I[Handle Error]\n    H --&gt; J{More Calls?}\n    I --&gt; J\n    J --&gt;|Yes| E\n    J --&gt;|No| K[Delete Instance]\n\n    style A fill:#e1f5ff\n    style B fill:#fff9c4\n    style D fill:#f3e5f5\n    style F fill:#e0f2f1\n    style H fill:#c8e6c9\n    style I fill:#ffcdd2\n    style K fill:#efebe9</code></pre>"},{"location":"reference/integrations/llms/ollama/ollama_state/#concurrency-considerations","title":"Concurrency Considerations","text":""},{"location":"reference/integrations/llms/ollama/ollama_state/#thread-safety","title":"Thread Safety","text":"<pre><code>Ollama instance is NOT thread-safe by default:\n  - _client and _async_client are shared state\n  - Lazy initialization is not synchronized\n\nRecommendation:\n  - Use separate Ollama instance per thread\n  - Or use locks around lazy initialization\n</code></pre>"},{"location":"reference/integrations/llms/ollama/ollama_state/#async-safety","title":"Async Safety","text":"<pre><code>Ollama async methods are event-loop safe:\n  - Uses separate _async_client per event loop\n  - No shared mutable state in async methods\n\nSafe to use:\n  - Multiple concurrent achat() calls in same loop\n  - Multiple event loops with same instance (separate clients)\n</code></pre>"},{"location":"reference/integrations/llms/ollama/ollama_state/#streaming-state","title":"Streaming State","text":"<pre><code>Each stream maintains its own state:\n  - Generator/async generator has local variables\n  - No shared state between streams\n\nSafe to have:\n  - Multiple concurrent streams from same instance\n</code></pre>"},{"location":"reference/integrations/llms/ollama/ollama_state/#state-management-best-practices","title":"State Management Best Practices","text":""},{"location":"reference/integrations/llms/ollama/ollama_state/#1-initialization","title":"1. Initialization","text":"<pre><code># \u2713 Good: Initialize once, reuse\nllm = Ollama(model=\"llama3.1\", request_timeout=180)\n\n# \u2717 Bad: Create new instance per call\ndef get_response(prompt):\n    llm = Ollama(model=\"llama3.1\")  # Inefficient\n    return llm.complete(prompt)\n</code></pre>"},{"location":"reference/integrations/llms/ollama/ollama_state/#2-client-reuse","title":"2. Client Reuse","text":"<pre><code># \u2713 Good: Client automatically reused\nllm = Ollama(model=\"llama3.1\")\nresponse1 = llm.chat(messages1)  # Creates client\nresponse2 = llm.chat(messages2)  # Reuses client\n\n# \u2717 Bad: Don't access _client directly\nllm._client = None  # Don't do this\n</code></pre>"},{"location":"reference/integrations/llms/ollama/ollama_state/#3-configuration","title":"3. Configuration","text":"<pre><code># \u2713 Good: Set configuration at init\nllm = Ollama(model=\"llama3.1\", temperature=0.8, json_mode=True)\n\n# \u2717 Bad: Don't modify config after init\nllm.temperature = 0.5  # Config is immutable\n</code></pre>"},{"location":"reference/integrations/llms/ollama/ollama_state/#4-error-handling","title":"4. Error Handling","text":"<pre><code># \u2713 Good: Instance remains usable after error\nllm = Ollama(model=\"llama3.1\")\ntry:\n    response = llm.chat(messages)\nexcept TimeoutError:\n    # Can still use llm for next call\n    response = llm.chat(messages, temperature=0.2)\n\n# \u2713 Good: Instance is reusable\n</code></pre>"},{"location":"reference/integrations/llms/ollama/ollama_state/#5-streaming","title":"5. Streaming","text":"<pre><code># \u2713 Good: Complete stream before next call\nfor chunk in llm.stream_chat(messages1):\n    process(chunk)\nresponse = llm.chat(messages2)  # Safe\n\n# \u26a0 Warning: Interleaving streams\nstream1 = llm.stream_chat(messages1)\nstream2 = llm.stream_chat(messages2)  # Both use same client\n</code></pre>"}]}