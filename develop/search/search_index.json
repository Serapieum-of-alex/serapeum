{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"AUTHORS/","title":"Credits","text":""},{"location":"AUTHORS/#development-lead","title":"Development Lead","text":"<ul> <li>Mostafa Farrag moah.farag@gmail.com</li> </ul>"},{"location":"LICENSE/","title":"License","text":""},{"location":"LICENSE/#gnu-general-public-license","title":"GNU GENERAL PUBLIC LICENSE","text":"<p>Version 3, 29 June 2007</p> <p>Copyright (C) 2007 Free Software Foundation, Inc. https://fsf.org/</p> <p>Everyone is permitted to copy and distribute verbatim copies of this license document, but changing it is not allowed.</p>"},{"location":"LICENSE/#preamble","title":"Preamble","text":"<p>The GNU General Public License is a free, copyleft license for software and other kinds of works.</p> <p>The licenses for most software and other practical works are designed to take away your freedom to share and change the works. By contrast, the GNU General Public License is intended to guarantee your freedom to share and change all versions of a program--to make sure it remains free software for all its users. We, the Free Software Foundation, use the GNU General Public License for most of our software; it applies also to any other work released this way by its authors. You can apply it to your programs, too.</p> <p>When we speak of free software, we are referring to freedom, not price. Our General Public Licenses are designed to make sure that you have the freedom to distribute copies of free software (and charge for them if you wish), that you receive source code or can get it if you want it, that you can change the software or use pieces of it in new free programs, and that you know you can do these things.</p> <p>To protect your rights, we need to prevent others from denying you these rights or asking you to surrender the rights. Therefore, you have certain responsibilities if you distribute copies of the software, or if you modify it: responsibilities to respect the freedom of others.</p> <p>For example, if you distribute copies of such a program, whether gratis or for a fee, you must pass on to the recipients the same freedoms that you received. You must make sure that they, too, receive or can get the source code. And you must show them these terms so they know their rights.</p> <p>Developers that use the GNU GPL protect your rights with two steps: (1) assert copyright on the software, and (2) offer you this License giving you legal permission to copy, distribute and/or modify it.</p> <p>For the developers' and authors' protection, the GPL clearly explains that there is no warranty for this free software. For both users' and authors' sake, the GPL requires that modified versions be marked as changed, so that their problems will not be attributed erroneously to authors of previous versions.</p> <p>Some devices are designed to deny users access to install or run modified versions of the software inside them, although the manufacturer can do so. This is fundamentally incompatible with the aim of protecting users' freedom to change the software. The systematic pattern of such abuse occurs in the area of products for individuals to use, which is precisely where it is most unacceptable. Therefore, we have designed this version of the GPL to prohibit the practice for those products. If such problems arise substantially in other domains, we stand ready to extend this provision to those domains in future versions of the GPL, as needed to protect the freedom of users.</p> <p>Finally, every program is threatened constantly by software patents. States should not allow patents to restrict development and use of software on general-purpose computers, but in those that do, we wish to avoid the special danger that patents applied to a free program could make it effectively proprietary. To prevent this, the GPL assures that patents cannot be used to render the program non-free.</p> <p>The precise terms and conditions for copying, distribution and modification follow.</p>"},{"location":"LICENSE/#terms-and-conditions","title":"TERMS AND CONDITIONS","text":""},{"location":"LICENSE/#0-definitions","title":"0. Definitions.","text":"<p>\"This License\" refers to version 3 of the GNU General Public License.</p> <p>\"Copyright\" also means copyright-like laws that apply to other kinds of works, such as semiconductor masks.</p> <p>\"The Program\" refers to any copyrightable work licensed under this License. Each licensee is addressed as \"you\". \"Licensees\" and \"recipients\" may be individuals or organizations.</p> <p>To \"modify\" a work means to copy from or adapt all or part of the work in a fashion requiring copyright permission, other than the making of an exact copy. The resulting work is called a \"modified version\" of the earlier work or a work \"based on\" the earlier work.</p> <p>A \"covered work\" means either the unmodified Program or a work based on the Program.</p> <p>To \"propagate\" a work means to do anything with it that, without permission, would make you directly or secondarily liable for infringement under applicable copyright law, except executing it on a computer or modifying a private copy. Propagation includes copying, distribution (with or without modification), making available to the public, and in some countries other activities as well.</p> <p>To \"convey\" a work means any kind of propagation that enables other parties to make or receive copies. Mere interaction with a user through a computer network, with no transfer of a copy, is not conveying.</p> <p>An interactive user interface displays \"Appropriate Legal Notices\" to the extent that it includes a convenient and prominently visible feature that (1) displays an appropriate copyright notice, and (2) tells the user that there is no warranty for the work (except to the extent that warranties are provided), that licensees may convey the work under this License, and how to view a copy of this License. If the interface presents a list of user commands or options, such as a menu, a prominent item in the list meets this criterion.</p>"},{"location":"LICENSE/#1-source-code","title":"1. Source Code.","text":"<p>The \"source code\" for a work means the preferred form of the work for making modifications to it. \"Object code\" means any non-source form of a work.</p> <p>A \"Standard Interface\" means an interface that either is an official standard defined by a recognized standards body, or, in the case of interfaces specified for a particular programming language, one that is widely used among developers working in that language.</p> <p>The \"System Libraries\" of an executable work include anything, other than the work as a whole, that (a) is included in the normal form of packaging a Major Component, but which is not part of that Major Component, and (b) serves only to enable use of the work with that Major Component, or to implement a Standard Interface for which an implementation is available to the public in source code form. A \"Major Component\", in this context, means a major essential component (kernel, window system, and so on) of the specific operating system (if any) on which the executable work runs, or a compiler used to produce the work, or an object code interpreter used to run it.</p> <p>The \"Corresponding Source\" for a work in object code form means all the source code needed to generate, install, and (for an executable work) run the object code and to modify the work, including scripts to control those activities. However, it does not include the work's System Libraries, or general-purpose tools or generally available free programs which are used unmodified in performing those activities but which are not part of the work. For example, Corresponding Source includes interface definition files associated with source files for the work, and the source code for shared libraries and dynamically linked subprograms that the work is specifically designed to require, such as by intimate data communication or control flow between those subprograms and other parts of the work.</p> <p>The Corresponding Source need not include anything that users can regenerate automatically from other parts of the Corresponding Source.</p> <p>The Corresponding Source for a work in source code form is that same work.</p>"},{"location":"LICENSE/#2-basic-permissions","title":"2. Basic Permissions.","text":"<p>All rights granted under this License are granted for the term of copyright on the Program, and are irrevocable provided the stated conditions are met. This License explicitly affirms your unlimited permission to run the unmodified Program. The output from running a covered work is covered by this License only if the output, given its content, constitutes a covered work. This License acknowledges your rights of fair use or other equivalent, as provided by copyright law.</p> <p>You may make, run and propagate covered works that you do not convey, without conditions so long as your license otherwise remains in force. You may convey covered works to others for the sole purpose of having them make modifications exclusively for you, or provide you with facilities for running those works, provided that you comply with the terms of this License in conveying all material for which you do not control copyright. Those thus making or running the covered works for you must do so exclusively on your behalf, under your direction and control, on terms that prohibit them from making any copies of your copyrighted material outside their relationship with you.</p> <p>Conveying under any other circumstances is permitted solely under the conditions stated below. Sublicensing is not allowed; section 10 makes it unnecessary.</p>"},{"location":"LICENSE/#3-protecting-users-legal-rights-from-anti-circumvention-law","title":"3. Protecting Users' Legal Rights From Anti-Circumvention Law.","text":"<p>No covered work shall be deemed part of an effective technological measure under any applicable law fulfilling obligations under article 11 of the WIPO copyright treaty adopted on 20 December 1996, or similar laws prohibiting or restricting circumvention of such measures.</p> <p>When you convey a covered work, you waive any legal power to forbid circumvention of technological measures to the extent such circumvention is effected by exercising rights under this License with respect to the covered work, and you disclaim any intention to limit operation or modification of the work as a means of enforcing, against the work's users, your or third parties' legal rights to forbid circumvention of technological measures.</p>"},{"location":"LICENSE/#4-conveying-verbatim-copies","title":"4. Conveying Verbatim Copies.","text":"<p>You may convey verbatim copies of the Program's source code as you receive it, in any medium, provided that you conspicuously and appropriately publish on each copy an appropriate copyright notice; keep intact all notices stating that this License and any non-permissive terms added in accord with section 7 apply to the code; keep intact all notices of the absence of any warranty; and give all recipients a copy of this License along with the Program.</p> <p>You may charge any price or no price for each copy that you convey, and you may offer support or warranty protection for a fee.</p>"},{"location":"LICENSE/#5-conveying-modified-source-versions","title":"5. Conveying Modified Source Versions.","text":"<p>You may convey a work based on the Program, or the modifications to produce it from the Program, in the form of source code under the terms of section 4, provided that you also meet all of these conditions:</p> <ul> <li>a) The work must carry prominent notices stating that you modified     it, and giving a relevant date.</li> <li>b) The work must carry prominent notices stating that it is     released under this License and any conditions added under     section 7. This requirement modifies the requirement in section 4     to \"keep intact all notices\".</li> <li>c) You must license the entire work, as a whole, under this     License to anyone who comes into possession of a copy. This     License will therefore apply, along with any applicable section 7     additional terms, to the whole of the work, and all its parts,     regardless of how they are packaged. This License gives no     permission to license the work in any other way, but it does not     invalidate such permission if you have separately received it.</li> <li>d) If the work has interactive user interfaces, each must display     Appropriate Legal Notices; however, if the Program has interactive     interfaces that do not display Appropriate Legal Notices, your     work need not make them do so.</li> </ul> <p>A compilation of a covered work with other separate and independent works, which are not by their nature extensions of the covered work, and which are not combined with it such as to form a larger program, in or on a volume of a storage or distribution medium, is called an \"aggregate\" if the compilation and its resulting copyright are not used to limit the access or legal rights of the compilation's users beyond what the individual works permit. Inclusion of a covered work in an aggregate does not cause this License to apply to the other parts of the aggregate.</p>"},{"location":"LICENSE/#6-conveying-non-source-forms","title":"6. Conveying Non-Source Forms.","text":"<p>You may convey a covered work in object code form under the terms of sections 4 and 5, provided that you also convey the machine-readable Corresponding Source under the terms of this License, in one of these ways:</p> <ul> <li>a) Convey the object code in, or embodied in, a physical product     (including a physical distribution medium), accompanied by the     Corresponding Source fixed on a durable physical medium     customarily used for software interchange.</li> <li>b) Convey the object code in, or embodied in, a physical product     (including a physical distribution medium), accompanied by a     written offer, valid for at least three years and valid for as     long as you offer spare parts or customer support for that product     model, to give anyone who possesses the object code either (1) a     copy of the Corresponding Source for all the software in the     product that is covered by this License, on a durable physical     medium customarily used for software interchange, for a price no     more than your reasonable cost of physically performing this     conveying of source, or (2) access to copy the Corresponding     Source from a network server at no charge.</li> <li>c) Convey individual copies of the object code with a copy of the     written offer to provide the Corresponding Source. This     alternative is allowed only occasionally and noncommercially, and     only if you received the object code with such an offer, in accord     with subsection 6b.</li> <li>d) Convey the object code by offering access from a designated     place (gratis or for a charge), and offer equivalent access to the     Corresponding Source in the same way through the same place at no     further charge. You need not require recipients to copy the     Corresponding Source along with the object code. If the place to     copy the object code is a network server, the Corresponding Source     may be on a different server (operated by you or a third party)     that supports equivalent copying facilities, provided you maintain     clear directions next to the object code saying where to find the     Corresponding Source. Regardless of what server hosts the     Corresponding Source, you remain obligated to ensure that it is     available for as long as needed to satisfy these requirements.</li> <li>e) Convey the object code using peer-to-peer transmission,     provided you inform other peers where the object code and     Corresponding Source of the work are being offered to the general     public at no charge under subsection 6d.</li> </ul> <p>A separable portion of the object code, whose source code is excluded from the Corresponding Source as a System Library, need not be included in conveying the object code work.</p> <p>A \"User Product\" is either (1) a \"consumer product\", which means any tangible personal property which is normally used for personal, family, or household purposes, or (2) anything designed or sold for incorporation into a dwelling. In determining whether a product is a consumer product, doubtful cases shall be resolved in favor of coverage. For a particular product received by a particular user, \"normally used\" refers to a typical or common use of that class of product, regardless of the status of the particular user or of the way in which the particular user actually uses, or expects or is expected to use, the product. A product is a consumer product regardless of whether the product has substantial commercial, industrial or non-consumer uses, unless such uses represent the only significant mode of use of the product.</p> <p>\"Installation Information\" for a User Product means any methods, procedures, authorization keys, or other information required to install and execute modified versions of a covered work in that User Product from a modified version of its Corresponding Source. The information must suffice to ensure that the continued functioning of the modified object code is in no case prevented or interfered with solely because modification has been made.</p> <p>If you convey an object code work under this section in, or with, or specifically for use in, a User Product, and the conveying occurs as part of a transaction in which the right of possession and use of the User Product is transferred to the recipient in perpetuity or for a fixed term (regardless of how the transaction is characterized), the Corresponding Source conveyed under this section must be accompanied by the Installation Information. But this requirement does not apply if neither you nor any third party retains the ability to install modified object code on the User Product (for example, the work has been installed in ROM).</p> <p>The requirement to provide Installation Information does not include a requirement to continue to provide support service, warranty, or updates for a work that has been modified or installed by the recipient, or for the User Product in which it has been modified or installed. Access to a network may be denied when the modification itself materially and adversely affects the operation of the network or violates the rules and protocols for communication across the network.</p> <p>Corresponding Source conveyed, and Installation Information provided, in accord with this section must be in a format that is publicly documented (and with an implementation available to the public in source code form), and must require no special password or key for unpacking, reading or copying.</p>"},{"location":"LICENSE/#7-additional-terms","title":"7. Additional Terms.","text":"<p>\"Additional permissions\" are terms that supplement the terms of this License by making exceptions from one or more of its conditions. Additional permissions that are applicable to the entire Program shall be treated as though they were included in this License, to the extent that they are valid under applicable law. If additional permissions apply only to part of the Program, that part may be used separately under those permissions, but the entire Program remains governed by this License without regard to the additional permissions.</p> <p>When you convey a copy of a covered work, you may at your option remove any additional permissions from that copy, or from any part of it. (Additional permissions may be written to require their own removal in certain cases when you modify the work.) You may place additional permissions on material, added by you to a covered work, for which you have or can give appropriate copyright permission.</p> <p>Notwithstanding any other provision of this License, for material you add to a covered work, you may (if authorized by the copyright holders of that material) supplement the terms of this License with terms:</p> <ul> <li>a) Disclaiming warranty or limiting liability differently from the     terms of sections 15 and 16 of this License; or</li> <li>b) Requiring preservation of specified reasonable legal notices or     author attributions in that material or in the Appropriate Legal     Notices displayed by works containing it; or</li> <li>c) Prohibiting misrepresentation of the origin of that material,     or requiring that modified versions of such material be marked in     reasonable ways as different from the original version; or</li> <li>d) Limiting the use for publicity purposes of names of licensors     or authors of the material; or</li> <li>e) Declining to grant rights under trademark law for use of some     trade names, trademarks, or service marks; or</li> <li>f) Requiring indemnification of licensors and authors of that     material by anyone who conveys the material (or modified versions     of it) with contractual assumptions of liability to the recipient,     for any liability that these contractual assumptions directly     impose on those licensors and authors.</li> </ul> <p>All other non-permissive additional terms are considered \"further restrictions\" within the meaning of section 10. If the Program as you received it, or any part of it, contains a notice stating that it is governed by this License along with a term that is a further restriction, you may remove that term. If a license document contains a further restriction but permits relicensing or conveying under this License, you may add to a covered work material governed by the terms of that license document, provided that the further restriction does not survive such relicensing or conveying.</p> <p>If you add terms to a covered work in accord with this section, you must place, in the relevant source files, a statement of the additional terms that apply to those files, or a notice indicating where to find the applicable terms.</p> <p>Additional terms, permissive or non-permissive, may be stated in the form of a separately written license, or stated as exceptions; the above requirements apply either way.</p>"},{"location":"LICENSE/#8-termination","title":"8. Termination.","text":"<p>You may not propagate or modify a covered work except as expressly provided under this License. Any attempt otherwise to propagate or modify it is void, and will automatically terminate your rights under this License (including any patent licenses granted under the third paragraph of section 11).</p> <p>However, if you cease all violation of this License, then your license from a particular copyright holder is reinstated (a) provisionally, unless and until the copyright holder explicitly and finally terminates your license, and (b) permanently, if the copyright holder fails to notify you of the violation by some reasonable means prior to 60 days after the cessation.</p> <p>Moreover, your license from a particular copyright holder is reinstated permanently if the copyright holder notifies you of the violation by some reasonable means, this is the first time you have received notice of violation of this License (for any work) from that copyright holder, and you cure the violation prior to 30 days after your receipt of the notice.</p> <p>Termination of your rights under this section does not terminate the licenses of parties who have received copies or rights from you under this License. If your rights have been terminated and not permanently reinstated, you do not qualify to receive new licenses for the same material under section 10.</p>"},{"location":"LICENSE/#9-acceptance-not-required-for-having-copies","title":"9. Acceptance Not Required for Having Copies.","text":"<p>You are not required to accept this License in order to receive or run a copy of the Program. Ancillary propagation of a covered work occurring solely as a consequence of using peer-to-peer transmission to receive a copy likewise does not require acceptance. However, nothing other than this License grants you permission to propagate or modify any covered work. These actions infringe copyright if you do not accept this License. Therefore, by modifying or propagating a covered work, you indicate your acceptance of this License to do so.</p>"},{"location":"LICENSE/#10-automatic-licensing-of-downstream-recipients","title":"10. Automatic Licensing of Downstream Recipients.","text":"<p>Each time you convey a covered work, the recipient automatically receives a license from the original licensors, to run, modify and propagate that work, subject to this License. You are not responsible for enforcing compliance by third parties with this License.</p> <p>An \"entity transaction\" is a transaction transferring control of an organization, or substantially all assets of one, or subdividing an organization, or merging organizations. If propagation of a covered work results from an entity transaction, each party to that transaction who receives a copy of the work also receives whatever licenses to the work the party's predecessor in interest had or could give under the previous paragraph, plus a right to possession of the Corresponding Source of the work from the predecessor in interest, if the predecessor has it or can get it with reasonable efforts.</p> <p>You may not impose any further restrictions on the exercise of the rights granted or affirmed under this License. For example, you may not impose a license fee, royalty, or other charge for exercise of rights granted under this License, and you may not initiate litigation (including a cross-claim or counterclaim in a lawsuit) alleging that any patent claim is infringed by making, using, selling, offering for sale, or importing the Program or any portion of it.</p>"},{"location":"LICENSE/#11-patents","title":"11. Patents.","text":"<p>A \"contributor\" is a copyright holder who authorizes use under this License of the Program or a work on which the Program is based. The work thus licensed is called the contributor's \"contributor version\".</p> <p>A contributor's \"essential patent claims\" are all patent claims owned or controlled by the contributor, whether already acquired or hereafter acquired, that would be infringed by some manner, permitted by this License, of making, using, or selling its contributor version, but do not include claims that would be infringed only as a consequence of further modification of the contributor version. For purposes of this definition, \"control\" includes the right to grant patent sublicenses in a manner consistent with the requirements of this License.</p> <p>Each contributor grants you a non-exclusive, worldwide, royalty-free patent license under the contributor's essential patent claims, to make, use, sell, offer for sale, import and otherwise run, modify and propagate the contents of its contributor version.</p> <p>In the following three paragraphs, a \"patent license\" is any express agreement or commitment, however denominated, not to enforce a patent (such as an express permission to practice a patent or covenant not to sue for patent infringement). To \"grant\" such a patent license to a party means to make such an agreement or commitment not to enforce a patent against the party.</p> <p>If you convey a covered work, knowingly relying on a patent license, and the Corresponding Source of the work is not available for anyone to copy, free of charge and under the terms of this License, through a publicly available network server or other readily accessible means, then you must either (1) cause the Corresponding Source to be so available, or (2) arrange to deprive yourself of the benefit of the patent license for this particular work, or (3) arrange, in a manner consistent with the requirements of this License, to extend the patent license to downstream recipients. \"Knowingly relying\" means you have actual knowledge that, but for the patent license, your conveying the covered work in a country, or your recipient's use of the covered work in a country, would infringe one or more identifiable patents in that country that you have reason to believe are valid.</p> <p>If, pursuant to or in connection with a single transaction or arrangement, you convey, or propagate by procuring conveyance of, a covered work, and grant a patent license to some of the parties receiving the covered work authorizing them to use, propagate, modify or convey a specific copy of the covered work, then the patent license you grant is automatically extended to all recipients of the covered work and works based on it.</p> <p>A patent license is \"discriminatory\" if it does not include within the scope of its coverage, prohibits the exercise of, or is conditioned on the non-exercise of one or more of the rights that are specifically granted under this License. You may not convey a covered work if you are a party to an arrangement with a third party that is in the business of distributing software, under which you make payment to the third party based on the extent of your activity of conveying the work, and under which the third party grants, to any of the parties who would receive the covered work from you, a discriminatory patent license (a) in connection with copies of the covered work conveyed by you (or copies made from those copies), or (b) primarily for and in connection with specific products or compilations that contain the covered work, unless you entered into that arrangement, or that patent license was granted, prior to 28 March 2007.</p> <p>Nothing in this License shall be construed as excluding or limiting any implied license or other defenses to infringement that may otherwise be available to you under applicable patent law.</p>"},{"location":"LICENSE/#12-no-surrender-of-others-freedom","title":"12. No Surrender of Others' Freedom.","text":"<p>If conditions are imposed on you (whether by court order, agreement or otherwise) that contradict the conditions of this License, they do not excuse you from the conditions of this License. If you cannot convey a covered work so as to satisfy simultaneously your obligations under this License and any other pertinent obligations, then as a consequence you may not convey it at all. For example, if you agree to terms that obligate you to collect a royalty for further conveying from those to whom you convey the Program, the only way you could satisfy both those terms and this License would be to refrain entirely from conveying the Program.</p>"},{"location":"LICENSE/#13-use-with-the-gnu-affero-general-public-license","title":"13. Use with the GNU Affero General Public License.","text":"<p>Notwithstanding any other provision of this License, you have permission to link or combine any covered work with a work licensed under version 3 of the GNU Affero General Public License into a single combined work, and to convey the resulting work. The terms of this License will continue to apply to the part which is the covered work, but the special requirements of the GNU Affero General Public License, section 13, concerning interaction through a network will apply to the combination as such.</p>"},{"location":"LICENSE/#14-revised-versions-of-this-license","title":"14. Revised Versions of this License.","text":"<p>The Free Software Foundation may publish revised and/or new versions of the GNU General Public License from time to time. Such new versions will be similar in spirit to the present version, but may differ in detail to address new problems or concerns.</p> <p>Each version is given a distinguishing version number. If the Program specifies that a certain numbered version of the GNU General Public License \"or any later version\" applies to it, you have the option of following the terms and conditions either of that numbered version or of any later version published by the Free Software Foundation. If the Program does not specify a version number of the GNU General Public License, you may choose any version ever published by the Free Software Foundation.</p> <p>If the Program specifies that a proxy can decide which future versions of the GNU General Public License can be used, that proxy's public statement of acceptance of a version permanently authorizes you to choose that version for the Program.</p> <p>Later license versions may give you additional or different permissions. However, no additional obligations are imposed on any author or copyright holder as a result of your choosing to follow a later version.</p>"},{"location":"LICENSE/#15-disclaimer-of-warranty","title":"15. Disclaimer of Warranty.","text":"<p>THERE IS NO WARRANTY FOR THE PROGRAM, TO THE EXTENT PERMITTED BY APPLICABLE LAW. EXCEPT WHEN OTHERWISE STATED IN WRITING THE COPYRIGHT HOLDERS AND/OR OTHER PARTIES PROVIDE THE PROGRAM \"AS IS\" WITHOUT WARRANTY OF ANY KIND, EITHER EXPRESSED OR IMPLIED, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE. THE ENTIRE RISK AS TO THE QUALITY AND PERFORMANCE OF THE PROGRAM IS WITH YOU. SHOULD THE PROGRAM PROVE DEFECTIVE, YOU ASSUME THE COST OF ALL NECESSARY SERVICING, REPAIR OR CORRECTION.</p>"},{"location":"LICENSE/#16-limitation-of-liability","title":"16. Limitation of Liability.","text":"<p>IN NO EVENT UNLESS REQUIRED BY APPLICABLE LAW OR AGREED TO IN WRITING WILL ANY COPYRIGHT HOLDER, OR ANY OTHER PARTY WHO MODIFIES AND/OR CONVEYS THE PROGRAM AS PERMITTED ABOVE, BE LIABLE TO YOU FOR DAMAGES, INCLUDING ANY GENERAL, SPECIAL, INCIDENTAL OR CONSEQUENTIAL DAMAGES ARISING OUT OF THE USE OR INABILITY TO USE THE PROGRAM (INCLUDING BUT NOT LIMITED TO LOSS OF DATA OR DATA BEING RENDERED INACCURATE OR LOSSES SUSTAINED BY YOU OR THIRD PARTIES OR A FAILURE OF THE PROGRAM TO OPERATE WITH ANY OTHER PROGRAMS), EVEN IF SUCH HOLDER OR OTHER PARTY HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES.</p>"},{"location":"LICENSE/#17-interpretation-of-sections-15-and-16","title":"17. Interpretation of Sections 15 and 16.","text":"<p>If the disclaimer of warranty and limitation of liability provided above cannot be given local legal effect according to their terms, reviewing courts shall apply local law that most closely approximates an absolute waiver of all civil liability in connection with the Program, unless a warranty or assumption of liability accompanies a copy of the Program in return for a fee.</p> <p>END OF TERMS AND CONDITIONS</p>"},{"location":"LICENSE/#how-to-apply-these-terms-to-your-new-programs","title":"How to Apply These Terms to Your New Programs","text":"<p>If you develop a new program, and you want it to be of the greatest possible use to the public, the best way to achieve this is to make it free software which everyone can redistribute and change under these terms.</p> <p>To do so, attach the following notices to the program. It is safest to attach them to the start of each source file to most effectively state the exclusion of warranty; and each file should have at least the \"copyright\" line and a pointer to where the full notice is found.</p> <pre><code>    &lt;one line to give the program's name and a brief idea of what it does.&gt;\n    Copyright (C) &lt;year&gt;  &lt;name of author&gt;\n\n    This program is free software: you can redistribute it and/or modify\n    it under the terms of the GNU General Public License as published by\n    the Free Software Foundation, either version 3 of the License, or\n    (at your option) any later version.\n\n    This program is distributed in the hope that it will be useful,\n    but WITHOUT ANY WARRANTY; without even the implied warranty of\n    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n    GNU General Public License for more details.\n\n    You should have received a copy of the GNU General Public License\n    along with this program.  If not, see &lt;https://www.gnu.org/licenses/&gt;.\n</code></pre> <p>Also add information on how to contact you by electronic and paper mail.</p> <p>If the program does terminal interaction, make it output a short notice like this when it starts in an interactive mode:</p> <pre><code>    &lt;program&gt;  Copyright (C) &lt;year&gt;  &lt;name of author&gt;\n    This program comes with ABSOLUTELY NO WARRANTY; for details type `show w'.\n    This is free software, and you are welcome to redistribute it\n    under certain conditions; type `show c' for details.\n</code></pre> <p>The hypothetical commands `show w' and `show c' should show the appropriate parts of the General Public License. Of course, your program's commands might be different; for a GUI interface, you would use an \"about box\".</p> <p>You should also get your employer (if you work as a programmer) or school, if any, to sign a \"copyright disclaimer\" for the program, if necessary. For more information on this, and how to apply and follow the GNU GPL, see https://www.gnu.org/licenses/.</p> <p>The GNU General Public License does not permit incorporating your program into proprietary programs. If your program is a subroutine library, you may consider it more useful to permit linking proprietary applications with the library. If this is what you want to do, use the GNU Lesser General Public License instead of this License. But first, please read https://www.gnu.org/licenses/why-not-lgpl.html.</p>"},{"location":"change-log/","title":"Changelog","text":""},{"location":"developer-guide/SECURITY/","title":"Security Policy","text":""},{"location":"developer-guide/SECURITY/#supported-versions","title":"Supported Versions","text":"<p>Use this section to tell people about which versions of your project are currently being supported with security updates.</p> Version Supported 5.1.x 5.0.x 4.0.x &lt; 4.0"},{"location":"developer-guide/SECURITY/#reporting-a-vulnerability","title":"Reporting a Vulnerability","text":"<p>Use this section to tell people how to report a vulnerability.</p> <p>Tell them where to go, how often they can expect to get an update on a reported vulnerability, what to expect if the vulnerability is accepted or declined, etc.</p>"},{"location":"developer-guide/contributing/","title":"Contributing","text":"<p>When contributing to this repository, please first discuss the change you wish to make via issue, email, or any other method with the owners of this repository before making a change.</p> <p>Please note we have a code of conduct, please follow it in all your interactions with the project.</p>"},{"location":"developer-guide/contributing/#pull-request-process","title":"Pull Request Process","text":"<ol> <li>Ensure any install or build dependencies are removed before the end of the layer when doing a    build.</li> <li>Update the README.md with details of changes to the interface, this includes new environment    variables, exposed ports, useful file locations and container parameters.</li> <li>Increase the version numbers in any examples files and the README.md to the new version that this    Pull Request would represent. The versioning scheme we use is SemVer.</li> <li>You may merge the Pull Request in once you have the sign-off of two other developers, or if you    do not have permission to do that, you may request the second reviewer to merge it for you.</li> </ol>"},{"location":"developer-guide/contributing/#code-of-conduct","title":"Code of Conduct","text":""},{"location":"developer-guide/contributing/#our-pledge","title":"Our Pledge","text":"<p>In the interest of fostering an open and welcoming environment, we as contributors and maintainers pledge to making participation in our project and our community a harassment-free experience for everyone, regardless of age, body size, disability, ethnicity, gender identity and expression, level of experience, nationality, personal appearance, race, religion, or sexual identity and orientation.</p>"},{"location":"developer-guide/contributing/#our-standards","title":"Our Standards","text":"<p>Examples of behavior that contributes to creating a positive environment include:</p> <ul> <li>Using welcoming and inclusive language</li> <li>Being respectful of differing viewpoints and experiences</li> <li>Gracefully accepting constructive criticism</li> <li>Focusing on what is best for the community</li> <li>Showing empathy towards other community members</li> </ul> <p>Examples of unacceptable behavior by participants include:</p> <ul> <li>The use of sexualized language or imagery and unwelcome sexual attention or advances</li> <li>Trolling, insulting/derogatory comments, and personal or political attacks</li> <li>Public or private harassment</li> <li>Publishing others' private information, such as a physical or electronic   address, without explicit permission</li> <li>Other conduct which could reasonably be considered inappropriate in a   professional setting</li> </ul>"},{"location":"developer-guide/contributing/#our-responsibilities","title":"Our Responsibilities","text":"<p>Project maintainers are responsible for clarifying the standards of acceptable behavior and are expected to take appropriate and fair corrective action in response to any instances of unacceptable behavior.</p> <p>Project maintainers have the right and responsibility to remove, edit, or reject comments, commits, code, wiki edits, issues, and other contributions that are not aligned to this Code of Conduct, or to ban temporarily or permanently any contributor for other behaviors that they deem inappropriate, threatening, offensive, or harmful.</p>"},{"location":"developer-guide/contributing/#scope","title":"Scope","text":"<p>This Code of Conduct applies both within project spaces and in public spaces when an individual is representing the project or its community. Examples of representing a project or community include using an official project e-mail address, posting via an official social media account, or acting as an appointed representative at an online or offline event. Representation of a project may be further defined and clarified by project maintainers.</p>"},{"location":"developer-guide/contributing/#enforcement","title":"Enforcement","text":"<p>Instances of abusive, harassing, or otherwise unacceptable behavior may be reported by contacting the project team at [INSERT EMAIL ADDRESS]. All complaints will be reviewed and investigated and will result in a response that is deemed necessary and appropriate to the circumstances. The project team is obligated to maintain confidentiality with regard to the reporter of an incident. Further details of specific enforcement policies may be posted separately.</p> <p>Project maintainers who do not follow or enforce the Code of Conduct in good faith may face temporary or permanent repercussions as determined by other members of the project's leadership.</p>"},{"location":"developer-guide/contributing/#attribution","title":"Attribution","text":"<p>This Code of Conduct is adapted from the Contributor Covenant, version 1.4, available at http://contributor-covenant.org/version/1/4</p>"},{"location":"developer-guide/installation/","title":"Installation and Usage Guide","text":"<p>This page explains how to install Serapeum and its dependencies, using the recommended uv package manager, or plain pip. It also provides troubleshooting tips and common commands for development.</p>"},{"location":"developer-guide/installation/#project-information","title":"Project Information","text":"<ul> <li>Package name: serapeum</li> <li>Current version: 0.1.0</li> <li>Supported Python versions: 3.11\u20133.12 (requires Python &gt;=3.11,&lt;4.0)</li> </ul>"},{"location":"developer-guide/installation/#dependencies","title":"Dependencies","text":"<ul> <li>Core runtime: ollama &gt;= 0.5.4, numpy, filetype &gt;= 1.2.0, requests &gt;= 2.32.5</li> <li>Development group: pytest, pytest-cov, pre-commit, pre-commit-hooks, pytest-asyncio, nest-asyncio, nbval</li> <li>Docs group: mkdocs, mkdocs-material, mkdocstrings, mkdocstrings-python, mike, mkdocs-jupyter, mkdocs-autorefs, mkdocs-macros-plugin, mkdocs-table-reader-plugin, mkdocs-mermaid2-plugin, jupyter, notebook&lt;7, commitizen, mkdocs-panzoom-plugin</li> </ul>"},{"location":"developer-guide/installation/#installing-uv","title":"Installing uv","text":"<pre><code># On macOS and Linux\ncurl -LsSf https://astral.sh/uv/install.sh | sh\n\n# On Windows\npowershell -c \"irm https://astral.sh/uv/install.ps1 | iex\"\n\n# Using pip (alternative)\npip install uv\n</code></pre>"},{"location":"developer-guide/installation/#setting-up-the-project-recommended-uv","title":"Setting Up the Project (Recommended: uv)","text":"<ol> <li> <p>Create a virtual environment <pre><code>uv venv\n</code></pre>    This creates a <code>.venv</code> directory in your project root.</p> </li> <li> <p>Activate the virtual environment <pre><code># On macOS/Linux\nsource .venv/bin/activate\n# On Windows\n.venv\\Scripts\\activate\n</code></pre></p> </li> <li> <p>Install the package (editable/local) <pre><code>uv pip install -e .\n</code></pre></p> </li> <li> <p>Optionally add development or docs tools <pre><code>uv pip install --group dev\nuv pip install --group docs\n</code></pre></p> </li> <li> <p>Run tests <pre><code>uv run pytest -q\n</code></pre></p> </li> <li> <p>Sync dependencies from <code>pyproject.toml</code> <pre><code>uv sync --active\n</code></pre></p> </li> </ol>"},{"location":"developer-guide/installation/#install-from-github","title":"Install from GitHub","text":"<pre><code>uv pip install \"git+https://github.com/Serapieum-of-alex/serapeum.git\"\n# or a specific tag (example: v0.1.0)\nuv pip install \"git+https://github.com/Serapieum-of-alex/serapeum.git@v0.1.0\"\n</code></pre>"},{"location":"developer-guide/installation/#alternative-using-pip","title":"Alternative: Using pip","text":"<ol> <li> <p>Create and activate a venv <pre><code>python -m venv .venv\n# macOS/Linux\nsource .venv/bin/activate\n# Windows\n.venv\\Scripts\\activate\n</code></pre></p> </li> <li> <p>Install the package (editable/local) <pre><code>pip install -e .\n</code></pre></p> </li> <li> <p>(Optional) Install dev/docs tools</p> </li> <li>Note: dev/docs are defined as groups, not pip extras. With pip, install the needed packages manually according to <code>pyproject.toml</code>.</li> <li> <p>Example (partial):      <pre><code>pip install pytest pytest-cov pre-commit pytest-asyncio nbval\n</code></pre></p> </li> <li> <p>Install from GitHub with pip <pre><code>pip install \"git+https://github.com/Serapieum-of-alex/serapeum.git\"\n# or a specific tag\npip install \"git+https://github.com/Serapieum-of-alex/serapeum.git@v0.1.0\"\n</code></pre></p> </li> </ol>"},{"location":"developer-guide/installation/#common-uv-commands","title":"Common uv Commands","text":"<ul> <li>Install a new package: <pre><code>uv pip install &lt;package-name&gt;\nuv pip install &lt;package-name&gt; --group dev  # as dev dependency\n</code></pre></li> <li>Update a package: <pre><code>uv pip install --upgrade &lt;package-name&gt;\n</code></pre></li> <li>Run Python scripts: <pre><code>uv run python script.py\nuv run pytest\n</code></pre></li> </ul>"},{"location":"developer-guide/installation/#troubleshooting","title":"Troubleshooting","text":"<ul> <li>Virtual environment not activating:   Make sure you've created the virtual environment first:   <pre><code>uv venv\n</code></pre></li> <li>Package installation fails:   Try clearing the cache:   <pre><code>uv cache clean\n</code></pre></li> <li>ImportError after installation:   Ensure you've installed the package in editable mode:   <pre><code>uv pip install -e .\n</code></pre></li> </ul>"},{"location":"developer-guide/installation/#quick-check","title":"Quick Check","text":"<p>After installation, open Python and run: <pre><code>import serapeum\nprint(serapeum.__version__)\n</code></pre></p>"},{"location":"developer-guide/installation/#additional-resources","title":"Additional Resources","text":"<ul> <li>uv Documentation</li> <li>PEP 621 - Storing project metadata in pyproject.toml</li> <li>Project homepage: https://github.com/Serapieum-of-alex/serapeum</li> <li>Documentation: https://serapeum.readthedocs.io/</li> </ul>"},{"location":"developer-guide/taskfile/","title":"Taskfile usage","text":"<p>This project uses a Taskfile (<code>Taskfile.yml</code>) to standardize build and install steps for the core package and libs. Tasks are run with the <code>task</code> CLI.</p>"},{"location":"developer-guide/taskfile/#prerequisites","title":"Prerequisites","text":"<ul> <li>Install Task (go-task): https://taskfile.dev/installation/</li> <li>Install <code>uv</code> (used by build and install tasks).</li> <li>A working Python environment you want to target.</li> </ul>"},{"location":"developer-guide/taskfile/#quick-start","title":"Quick start","text":"<p>List available tasks:</p> <pre><code>task -l\n</code></pre> <p>Build everything:</p> <pre><code>task build:all\n</code></pre> <p>Build and install all wheels into the default Python:</p> <pre><code>task install:all\n</code></pre>"},{"location":"developer-guide/taskfile/#task-variables","title":"Task variables","text":"<p>The Taskfile defines a few variables you can override at runtime:</p> <ul> <li><code>PYTHON</code>: Optional path to a Python executable, used for <code>uv pip</code> commands.</li> <li><code>DIST_DIR</code>: Output directory for built wheels (default: <code>dist</code>).</li> <li><code>CORE_LIB_DIR</code>: Path to the core library package.</li> <li><code>PLUGIN_OLLAMA_DIR</code>: Path to the Ollama plugin package.</li> </ul> <p>Override variables on the command line:</p> <pre><code>task install:all PYTHON=C:\\path\\to\\python.exe\ntask build:all DIST_DIR=dist-artifacts\n</code></pre>"},{"location":"developer-guide/taskfile/#common-workflows","title":"Common workflows","text":"<p>Build only the core packages:</p> <pre><code>task build:core-lib\ntask build:core\n</code></pre> <p>Build and install just the Ollama plugin:</p> <pre><code>task build:plugin:ollama\ntask install:plugin:ollama\n</code></pre> <p>Uninstall all packages from the target environment:</p> <pre><code>task uninstall:all\n</code></pre> <p>Remove built wheels:</p> <pre><code>task clean:dist\n</code></pre>"},{"location":"developer-guide/taskfile/#task-reference","title":"Task reference","text":"Task Description <code>build:core</code> Build wheel for the root project (<code>serapeum</code>). <code>build:core-lib</code> Build wheel for <code>serapeum-core</code>. <code>build:plugin:ollama</code> Build wheel for the Ollama plugin. <code>build:all</code> Build wheels for core and libs. <code>install:core</code> Install <code>serapeum-core</code> wheel into the target env. <code>install:plugin:ollama</code> Install the Ollama plugin wheel into the target env. <code>install:all</code> Build and install all wheels into the target env. <code>uninstall:all</code> Uninstall <code>serapeum</code>, <code>serapeum-core</code>, and the plugin. <code>clean:dist</code> Remove the wheel output directory."},{"location":"developer-guide/taskfile/#notes","title":"Notes","text":"<ul> <li>Install tasks are non-editable installs from the built wheels in <code>DIST_DIR</code>.</li> <li>If you change <code>DIST_DIR</code>, make sure the install tasks point to the same   directory you built into.</li> </ul>"},{"location":"overview/codebase-map/","title":"Codebase Map","text":"<p>This page summarizes the main modules, key classes, and the public API surface of the <code>serapeum</code> package.</p>"},{"location":"overview/codebase-map/#packages-and-modules","title":"Packages and Modules","text":""},{"location":"overview/codebase-map/#core-framework-serapeumcore","title":"Core Framework (<code>serapeum.core</code>)","text":""},{"location":"overview/codebase-map/#base-abstractions","title":"Base Abstractions","text":"<ul> <li>serapeum.core.base.llms</li> <li><code>BaseLLM</code>: Foundation protocol for all LLM backends with sync/async chat and completion, streaming endpoints, and message conversion helpers</li> <li>Core data models: <code>Message</code>, <code>MessageList</code>, <code>ChatResponse</code>, <code>CompletionResponse</code>, <code>Metadata</code>, <code>MessageRole</code></li> <li>Multimodal support: <code>TextChunk</code>, <code>Image</code>, <code>Audio</code></li> <li> <p>Utilities for adapting chat endpoints to completion-style calls</p> </li> <li> <p>serapeum.core.base.embeddings</p> </li> <li><code>BaseEmbedding</code>: Foundation protocol for embedding models</li> <li>Core embedding types: <code>NodeType</code>, <code>BaseNode</code>, <code>LinkedNodes</code>, <code>NodeInfo</code>, <code>MetadataMode</code></li> <li>Utilities for working with document nodes and embeddings</li> </ul>"},{"location":"overview/codebase-map/#llm-layer","title":"LLM Layer","text":"<ul> <li>serapeum.core.llms</li> <li><code>LLM</code>: High-level LLM orchestration built on <code>BaseLLM</code> with prompt/message formatting and structured prediction to Pydantic models</li> <li><code>FunctionCallingLLM</code>: Tool-calling specialization with chat with tools, tool call extraction/validation, and predict-and-call helpers</li> <li><code>StructuredOutputLLM</code>: Wrapper that forces structured outputs (<code>BaseModel</code>) from any LLM while keeping chat/completion interfaces</li> <li><code>ChatToCompletionMixin</code>: Adapter for using chat models in completion mode</li> <li> <p>Sync/async/streaming support across all abstractions</p> </li> <li> <p>serapeum.core.llms.orchestrators</p> </li> <li><code>ToolOrchestratingLLM</code>: Composes prompts, an LLM, and a toolset to drive structured tool-calling conversations</li> <li><code>TextCompletionLLM</code>: Text-completion style orchestration utilities</li> <li><code>StreamingObjectProcessor</code>: Handles streaming structured outputs</li> <li>Support for sync/async operations with streaming</li> </ul>"},{"location":"overview/codebase-map/#embeddings-layer","title":"Embeddings Layer","text":"<ul> <li>serapeum.core.embeddings</li> <li><code>MockEmbedding</code>: Testing/development embedding implementation</li> <li>Embedding utilities and helpers</li> <li>Integration with node types for document processing</li> </ul>"},{"location":"overview/codebase-map/#tools-system","title":"Tools System","text":"<ul> <li>serapeum.core.tools</li> <li><code>BaseTool</code> / <code>AsyncBaseTool</code>: Core tool protocols</li> <li><code>CallableTool</code>: Create tools from Python functions or Pydantic models with automatic schema generation</li> <li><code>ToolMetadata</code>: Tool metadata and JSON schema utilities</li> <li><code>ToolOutput</code> / <code>ToolCallArguments</code>: Tool execution types</li> <li>Automatic sync/async bridging and output parsing</li> </ul>"},{"location":"overview/codebase-map/#prompts","title":"Prompts","text":"<ul> <li>serapeum.core.prompts</li> <li><code>PromptTemplate</code>: String-based prompts with variable/function mappings</li> <li><code>ChatPromptTemplate</code>: Message-based prompts for chat interfaces</li> <li>Prompt-related utilities and type definitions</li> </ul>"},{"location":"overview/codebase-map/#output-parsing","title":"Output Parsing","text":"<ul> <li>serapeum.core.output_parsers</li> <li><code>PydanticParser</code>: Parse LLM outputs into Pydantic models</li> <li>Output parser protocols and base classes</li> <li>Error handling and retry mechanisms for robust parsing</li> </ul>"},{"location":"overview/codebase-map/#chat-support","title":"Chat Support","text":"<ul> <li>serapeum.core.chat</li> <li><code>AgentChatResponse</code>: Aggregates model/tool outputs with sync/async streaming</li> <li>Utilities for managing conversation state</li> <li>Tool output parsing and aggregation</li> </ul>"},{"location":"overview/codebase-map/#configuration-types","title":"Configuration &amp; Types","text":"<ul> <li>serapeum.core.configs</li> <li><code>Configs</code>: Global configuration object</li> <li> <p>Default values and settings used across the framework</p> </li> <li> <p>serapeum.core.types</p> </li> <li><code>SerializableModel</code>: Base model with JSON/pickle serialization helpers</li> <li><code>Model</code>: Pydantic model base</li> <li> <p><code>StructuredLLMMode</code>: Enum for structured output modes</p> </li> <li> <p>serapeum.core.utils</p> </li> <li>Common utilities: sync/async helpers, base utilities</li> <li>Shared functionality across modules</li> </ul>"},{"location":"overview/codebase-map/#provider-integrations","title":"Provider Integrations","text":"<ul> <li>serapeum.ollama</li> <li>Complete Ollama integration package</li> <li><code>Ollama</code>: <code>FunctionCallingLLM</code> implementation for Ollama server (chat, completion, tool calling, structured outputs)</li> <li><code>OllamaEmbedding</code>: Local embedding generation using Ollama models</li> <li>Full sync/async/streaming support for both LLM and embeddings</li> <li>See Ollama Provider Documentation for detailed documentation</li> </ul>"},{"location":"overview/codebase-map/#key-public-classes","title":"Key Public Classes","text":""},{"location":"overview/codebase-map/#llm-abstractions","title":"LLM Abstractions","text":"<ul> <li><code>serapeum.core.base.llms.BaseLLM</code> - Base LLM protocol</li> <li><code>serapeum.core.llms.LLM</code> - High-level LLM orchestration</li> <li><code>serapeum.core.llms.FunctionCallingLLM</code> - Tool-calling LLM specialization</li> <li><code>serapeum.core.llms.StructuredOutputLLM</code> - Structured output wrapper</li> </ul>"},{"location":"overview/codebase-map/#orchestration","title":"Orchestration","text":"<ul> <li><code>serapeum.core.llms.orchestrators.ToolOrchestratingLLM</code> - Tool-calling orchestrator</li> <li><code>serapeum.core.llms.orchestrators.TextCompletionLLM</code> - Text completion orchestrator</li> </ul>"},{"location":"overview/codebase-map/#tools","title":"Tools","text":"<ul> <li><code>serapeum.core.tools.CallableTool</code> - Function/model-based tools</li> <li><code>serapeum.core.tools.BaseTool</code> - Base tool protocol</li> <li><code>serapeum.core.tools.AsyncBaseTool</code> - Async tool protocol</li> </ul>"},{"location":"overview/codebase-map/#data-types","title":"Data Types","text":"<ul> <li><code>serapeum.core.base.llms.types.Message</code> - Individual messages</li> <li><code>serapeum.core.base.llms.types.MessageList</code> - Message sequences</li> <li><code>serapeum.core.base.llms.types.ChatResponse</code> - Chat responses</li> <li><code>serapeum.core.base.llms.types.CompletionResponse</code> - Completion responses</li> <li><code>serapeum.core.base.llms.types.Metadata</code> - LLM metadata</li> </ul>"},{"location":"overview/codebase-map/#prompts_1","title":"Prompts","text":"<ul> <li><code>serapeum.core.prompts.PromptTemplate</code> - String templates</li> <li><code>serapeum.core.prompts.ChatPromptTemplate</code> - Chat templates</li> </ul>"},{"location":"overview/codebase-map/#base-models","title":"Base Models","text":"<ul> <li><code>serapeum.core.types.SerializableModel</code> - Serialization base</li> <li><code>serapeum.core.types.Model</code> - Pydantic model base</li> </ul>"},{"location":"overview/codebase-map/#embeddings","title":"Embeddings","text":"<ul> <li><code>serapeum.core.base.embeddings.BaseEmbedding</code> - Base embedding protocol</li> <li><code>serapeum.core.embeddings.MockEmbedding</code> - Mock implementation</li> </ul>"},{"location":"overview/codebase-map/#provider-implementations","title":"Provider Implementations","text":"<ul> <li><code>serapeum.ollama.Ollama</code> - Ollama LLM implementation</li> <li><code>serapeum.ollama.OllamaEmbedding</code> - Ollama embeddings implementation</li> </ul>"},{"location":"overview/codebase-map/#representative-public-methods","title":"Representative Public Methods","text":""},{"location":"overview/codebase-map/#basellm","title":"BaseLLM","text":"<ul> <li><code>chat(messages, **kwargs) \u2192 ChatResponse</code></li> <li><code>complete(prompt, formatted=False, **kwargs) \u2192 CompletionResponse</code></li> <li><code>stream_chat(...) \u2192 ChatResponseGen</code></li> <li><code>stream_complete(...) \u2192 CompletionResponseGen</code></li> <li><code>achat(...) \u2192 ChatResponse</code> (async)</li> <li><code>acomplete(...) \u2192 CompletionResponse</code> (async)</li> <li><code>astream_chat(...) \u2192 ChatResponseAsyncGen</code> (async)</li> <li><code>astream_complete(...) \u2192 CompletionResponseAsyncGen</code> (async)</li> </ul>"},{"location":"overview/codebase-map/#llm","title":"LLM","text":"<ul> <li><code>predict(prompt: PromptTemplate, **kwargs) \u2192 str</code></li> <li><code>stream(prompt, **kwargs) \u2192 CompletionResponseGen</code></li> <li><code>apredict(...) \u2192 str</code> (async)</li> <li><code>astream(...) \u2192 CompletionResponseAsyncGen</code> (async)</li> <li><code>structured_predict(output_cls: type[BaseModel], prompt, **kwargs) \u2192 BaseModel</code></li> <li><code>stream_structured_predict(...) \u2192 Generator[BaseModel, None, None]</code></li> <li><code>astructured_predict(...) \u2192 BaseModel</code> (async)</li> <li><code>astream_structured_predict(...) \u2192 AsyncGenerator[BaseModel, None]</code> (async)</li> </ul>"},{"location":"overview/codebase-map/#functioncallingllm","title":"FunctionCallingLLM","text":"<ul> <li><code>chat_with_tools(tools, user_msg=None, chat_history=None, **kwargs) \u2192 ChatResponse</code></li> <li><code>predict_and_call(tools, user_msg=None, chat_history=None, **kwargs) \u2192 AgentChatResponse</code></li> <li><code>get_tool_calls_from_response(response, error_on_no_tool_call=True) \u2192 list[ToolCallArguments]</code></li> <li><code>stream_chat_with_tools(...) \u2192 ChatResponseGen</code></li> <li><code>astream_chat_with_tools(...) \u2192 ChatResponseAsyncGen</code> (async)</li> </ul>"},{"location":"overview/codebase-map/#callabletool","title":"CallableTool","text":"<ul> <li><code>from_function(func, name=None, description=None, **kwargs) \u2192 CallableTool</code> (class method)</li> <li><code>from_model(model_cls, fn, name=None, description=None, **kwargs) \u2192 CallableTool</code> (class method)</li> <li><code>call(input, **kwargs) \u2192 ToolOutput</code></li> <li><code>acall(input, **kwargs) \u2192 ToolOutput</code> (async)</li> </ul>"},{"location":"overview/codebase-map/#toolorchestratingllm","title":"ToolOrchestratingLLM","text":"<ul> <li><code>__call__(**prompt_args, llm_kwargs=None) \u2192 BaseModel | Any</code></li> <li><code>acall(**prompt_args, llm_kwargs=None) \u2192 BaseModel | Any</code> (async)</li> <li><code>stream_call(**prompt_args, llm_kwargs=None) \u2192 Generator</code></li> <li><code>astream_call(**prompt_args, llm_kwargs=None) \u2192 AsyncGenerator</code> (async)</li> </ul>"},{"location":"overview/codebase-map/#baseembedding","title":"BaseEmbedding","text":"<ul> <li><code>get_text_embedding(text: str) \u2192 list[float]</code></li> <li><code>get_query_embedding(query: str) \u2192 list[float]</code></li> <li><code>get_text_embeddings(texts: list[str]) \u2192 list[list[float]]</code></li> <li><code>aget_text_embedding(text: str) \u2192 list[float]</code> (async)</li> <li><code>aget_query_embedding(query: str) \u2192 list[float]</code> (async)</li> <li><code>aget_text_embeddings(texts: list[str]) \u2192 list[list[float]]</code> (async)</li> </ul>"},{"location":"overview/codebase-map/#data-flow-high-level","title":"Data Flow (High Level)","text":""},{"location":"overview/codebase-map/#basic-llm-flow","title":"Basic LLM Flow","text":"<pre><code>User input/messages\n  \u2192 Prompt building (PromptTemplate / ChatPromptTemplate)\n  \u2192 LLM (LLM or concrete provider like Ollama)\n  \u2192 ChatResponse / CompletionResponse\n</code></pre>"},{"location":"overview/codebase-map/#structured-output-flow","title":"Structured Output Flow","text":"<pre><code>User input\n  \u2192 PromptTemplate\n  \u2192 LLM.structured_predict(output_cls=MyModel, ...)\n  \u2192 Pydantic BaseModel instance\n</code></pre>"},{"location":"overview/codebase-map/#tool-calling-flow","title":"Tool-Calling Flow","text":"<pre><code>User message\n  \u2192 FunctionCallingLLM.predict_and_call(tools=[...])\n  \u2192 LLM predicts tool calls\n  \u2192 Tools executed (BaseTool/CallableTool)\n  \u2192 ToolOutput aggregated\n  \u2192 AgentChatResponse\n</code></pre>"},{"location":"overview/codebase-map/#orchestrated-tool-flow","title":"Orchestrated Tool Flow","text":"<pre><code>User input\n  \u2192 ToolOrchestratingLLM(llm=..., tools=[...], prompt=...)\n  \u2192 Automatic tool selection and execution\n  \u2192 Structured output (if output_cls specified)\n</code></pre>"},{"location":"overview/codebase-map/#embedding-flow","title":"Embedding Flow","text":"<pre><code>Documents/queries\n  \u2192 BaseEmbedding.get_text_embeddings(texts)\n  \u2192 Vector embeddings (list[list[float]])\n  \u2192 Use for similarity search, RAG, etc.\n</code></pre> <p>See the Architecture section for diagrams and deeper internals, and the API Reference for exhaustive signatures.</p>"},{"location":"overview/codebase-map/#class-dependency-graph","title":"Class &amp; Dependency Graph","text":"<p>Below is a high-level Mermaid class/dependency diagram showing the main modules and their primary classes, plus key relationships between them.</p>  Hold \"Ctrl\" to enable pan &amp; zoom  <pre><code>classDiagram\n  class SerializableModel { &lt;&lt;base&gt;&gt; }\n  class BaseLLM { &lt;&lt;abstract&gt;&gt; }\n  class BaseEmbedding { &lt;&lt;abstract&gt;&gt; }\n  class LLM { }\n  class FunctionCallingLLM { }\n  class StructuredOutputLLM { }\n  class ToolOrchestratingLLM { }\n  class BaseTool { }\n  class AsyncBaseTool { }\n  class CallableTool { }\n  class PromptTemplate { }\n  class ChatPromptTemplate { }\n  class Message { }\n  class MessageList { }\n  class ChatResponse { }\n  class CompletionResponse { }\n  class Ollama { }\n  class OllamaEmbedding { }\n\n  SerializableModel &lt;|-- BaseLLM\n  SerializableModel &lt;|-- BaseEmbedding\n  BaseLLM &lt;|-- LLM\n  LLM &lt;|-- FunctionCallingLLM\n  LLM &lt;|-- StructuredOutputLLM\n  FunctionCallingLLM &lt;|-- Ollama\n  BaseEmbedding &lt;|-- OllamaEmbedding\n  ToolOrchestratingLLM ..&gt; FunctionCallingLLM : uses\n  ToolOrchestratingLLM ..&gt; PromptTemplate : composes\n  ToolOrchestratingLLM ..&gt; ChatPromptTemplate : composes\n  AsyncBaseTool &lt;|-- CallableTool\n  BaseTool &lt;|-- AsyncBaseTool\n  ToolOrchestratingLLM ..&gt; BaseTool : orchestrates\n  LLM ..&gt; MessageList : formats\n  LLM ..&gt; ChatResponse : returns\n  LLM ..&gt; CompletionResponse : returns</code></pre>"},{"location":"overview/codebase-map/#adding-new-providers","title":"Adding New Providers","text":"<p>To add a new provider (e.g., for OpenAI, Anthropic, etc.):</p> <ol> <li>Create a new package in <code>libs/providers/{provider}/</code></li> <li>Implement <code>FunctionCallingLLM</code> for chat/completion models</li> <li>Implement <code>BaseEmbedding</code> for embedding models (if applicable)</li> <li>Follow the provider-based organization pattern (all features in one package)</li> <li>Add comprehensive examples and documentation</li> </ol> <p>See the Ollama provider as a reference implementation.</p> <p>Notes: - The diagram abstracts many modules for clarity; see source files for full method signatures and additional classes - <code>Ollama</code> and <code>OllamaEmbedding</code> are concrete implementations; additional providers follow the same pattern - All async methods use the <code>a</code> prefix convention (e.g., <code>achat</code>, <code>astream_chat</code>) - Streaming methods return generators (sync) or async generators (async)</p>"},{"location":"reference/core/embeddings/module/","title":"Core Embeddings","text":""},{"location":"reference/core/embeddings/module/#core-embedding-module","title":"Core Embedding module","text":""},{"location":"reference/core/embeddings/module/#serapeum.core.embeddings","title":"<code>serapeum.core.embeddings</code>","text":"<p>Embedding module.</p>"},{"location":"reference/core/embeddings/module/#serapeum.core.embeddings.BaseEmbedding","title":"<code>BaseEmbedding</code>","text":"<p>               Bases: <code>SerializableModel</code>, <code>CallMixin</code>, <code>ABC</code></p> <p>Abstract base class for all embedding model implementations.</p> <p>This class provides the core interface and shared functionality for converting text into dense vector embeddings. It supports both query and document embedding, with optional caching, batching, and async operations.</p> <p>Subclasses must implement the abstract methods for generating embeddings from text and queries. The class handles caching, batching, and progress tracking automatically.</p> <p>Attributes:</p> Name Type Description <code>model_name</code> <code>str</code> <p>Name of the embedding model. Defaults to \"unknown\".</p> <code>batch_size</code> <code>int</code> <p>Number of texts to process in each batch. Must be between 1 and 2048. Defaults to 10.</p> <code>num_workers</code> <code>int | None</code> <p>Number of worker threads for async operations. If None, uses default async behavior without worker pooling.</p> <code>cache_store</code> <code>Any | None</code> <p>Optional key-value store for caching embeddings. Must implement get(), aget(), put(), and aput() methods. When provided, embeddings are cached using a key combining text and model configuration.</p> Notes <p>This is an abstract base class and cannot be instantiated directly. Subclasses must implement _get_query_embedding, _aget_query_embedding, and _get_text_embedding methods.</p> See Also <p>serapeum.providers.ollama.embeddings.OllamaEmbedding: Concrete implementation     for Ollama embedding models. CallMixin: Mixin providing call and acall methods. SerializableModel: Base Pydantic model with serialization support.</p> Source code in <code>libs/core/src/serapeum/core/base/embeddings/base.py</code> <pre><code>class BaseEmbedding(SerializableModel, CallMixin, ABC):\n    \"\"\"Abstract base class for all embedding model implementations.\n\n    This class provides the core interface and shared functionality for converting\n    text into dense vector embeddings. It supports both query and document embedding,\n    with optional caching, batching, and async operations.\n\n    Subclasses must implement the abstract methods for generating embeddings from\n    text and queries. The class handles caching, batching, and progress tracking\n    automatically.\n\n    Attributes:\n        model_name: Name of the embedding model. Defaults to \"unknown\".\n        batch_size: Number of texts to process in each batch. Must be between\n            1 and 2048. Defaults to 10.\n        num_workers: Number of worker threads for async operations. If None,\n            uses default async behavior without worker pooling.\n        cache_store: Optional key-value store for caching embeddings. Must implement\n            get(), aget(), put(), and aput() methods. When provided, embeddings are\n            cached using a key combining text and model configuration.\n\n    Notes:\n        This is an abstract base class and cannot be instantiated directly.\n        Subclasses must implement _get_query_embedding, _aget_query_embedding,\n        and _get_text_embedding methods.\n\n    See Also:\n        serapeum.providers.ollama.embeddings.OllamaEmbedding: Concrete implementation\n            for Ollama embedding models.\n        CallMixin: Mixin providing __call__ and acall methods.\n        SerializableModel: Base Pydantic model with serialization support.\n    \"\"\"\n\n    model_config = ConfigDict(arbitrary_types_allowed=True)\n    model_name: str = Field(\n        default=\"unknown\", description=\"The name of the embedding model.\"\n    )\n    batch_size: int = Field(\n        default=DEFAULT_EMBED_BATCH_SIZE,\n        description=\"The batch size for embedding calls.\",\n        gt=0,\n        le=2048,\n    )\n    num_workers: int | None = Field(\n        default=None,\n        description=\"The number of workers to use for async embedding calls.\",\n    )\n\n    cache_store: Any | None = Field(\n        default=None,\n        description=(\n            \"Key-value store for caching embeddings. Must implement get(), aget(), \"\n            \"put(), and aput() methods with signature: get(key: str, collection: str) -&gt; dict | None. \"\n            \"When provided, embeddings are cached using a key that combines the text and model configuration. \"\n            \"If None, embeddings are not cached and will be recomputed on each call.\"\n        ),\n    )\n\n    def _get_cache_key(self, text: str) -&gt; str:\n        \"\"\"Generate a unique cache key combining text and model configuration.\n\n        The cache key includes both the input text and a JSON representation of\n        the model configuration, ensuring that different models or configurations\n        don't share cached embeddings. Sensitive fields like api_key are excluded.\n\n        Args:\n            text: Input text to generate a cache key for.\n\n        Returns:\n            Cache key string in format \"{text}::{model_config_json}\".\n\n        Notes:\n            The following fields are excluded from the cache key:\n                - api_key: Sensitive credential information\n                - cache_store: Avoid circular reference in serialization\n        \"\"\"\n        model_dict = self.to_dict()\n        model_dict.pop(\"api_key\", None)\n        model_dict.pop(\"cache_store\", None)  # Avoid circular reference\n\n        # Create a deterministic string representation\n        model_str = json.dumps(model_dict, sort_keys=True)\n        return f\"{text}::{model_str}\"\n\n    @abstractmethod\n    def _get_query_embedding(self, query: str) -&gt; Embedding:\n        \"\"\"Embed the input query synchronously (internal implementation).\n\n        This is an internal method that subclasses must implement to provide\n        the core query embedding functionality. The public method get_query_embedding()\n        handles caching and calls this method when needed.\n\n        Query embeddings may use special instructions or prefixes depending on the\n        model. For example, some models prepend \"Represent the question for\n        retrieving supporting documents: \" to optimize for retrieval tasks.\n\n        Args:\n            query: Query text to embed.\n\n        Returns:\n            Embedding vector as a list of floats.\n\n        See Also:\n            get_query_embedding: Public method that handles caching and delegates\n                to this method.\n            _aget_query_embedding: Async version of this method.\n        \"\"\"\n\n    @abstractmethod\n    async def _aget_query_embedding(self, query: str) -&gt; Embedding:\n        \"\"\"Embed the input query asynchronously (internal implementation).\n\n        This is an internal async method that subclasses must implement to provide\n        the core query embedding functionality. The public method aget_query_embedding()\n        handles caching and calls this method when needed.\n\n        Args:\n            query: Query text to embed.\n\n        Returns:\n            Embedding vector as a list of floats.\n\n        See Also:\n            aget_query_embedding: Public async method that handles caching and\n                delegates to this method.\n            _get_query_embedding: Sync version of this method.\n        \"\"\"\n\n    def get_query_embedding(self, query: str) -&gt; Embedding:\n        \"\"\"Generate an embedding vector for a query string.\n\n        Embeds the input query into a dense vector representation optimized for\n        retrieval tasks. When caching is enabled, checks the cache first and stores\n        new embeddings automatically.\n\n        Depending on the model, a special instruction may be prepended to the raw\n        query string to optimize for specific tasks. For example, some models use\n        \"Represent the question for retrieving supporting documents: \".\n\n        Args:\n            query: Query text to embed.\n\n        Returns:\n            Embedding vector as a list of floats.\n\n        See Also:\n            aget_query_embedding: Async version of this method.\n            get_text_embedding: For embedding document text (not queries).\n            _get_query_embedding: Internal implementation method.\n        \"\"\"\n        query_embedding = None\n        if self.cache_store:\n            cache_key = self._get_cache_key(query)\n            cached = self.cache_store.get(key=cache_key, collection=\"embeddings\")\n            if cached:\n                cached_key = next(iter(cached.keys()))\n                query_embedding = cached[cached_key]\n\n        if query_embedding is None:\n            query_embedding = self._get_query_embedding(query)\n            if self.cache_store:\n                cache_key = self._get_cache_key(query)\n                self.cache_store.put(\n                    key=cache_key,\n                    val={str(uuid.uuid4()): query_embedding},\n                    collection=\"embeddings\",\n                )\n\n        return query_embedding\n\n    async def aget_query_embedding(self, query: str) -&gt; Embedding:\n        \"\"\"Asynchronously generate an embedding vector for a query string.\n\n        Async version of get_query_embedding(). Embeds the input query into a dense\n        vector representation with cache support.\n\n        Args:\n            query: Query text to embed.\n\n        Returns:\n            Embedding vector as a list of floats.\n\n        See Also:\n            get_query_embedding: Sync version of this method.\n            aget_text_embedding: For embedding document text asynchronously.\n            _aget_query_embedding: Internal async implementation method.\n        \"\"\"\n        query_embedding = None\n        if self.cache_store:\n            cache_key = self._get_cache_key(query)\n            cached = await self.cache_store.aget(key=cache_key, collection=\"embeddings\")\n            if cached:\n                cached_key = next(iter(cached.keys()))\n                query_embedding = cached[cached_key]\n\n        if query_embedding is None:\n            query_embedding = await self._aget_query_embedding(query)\n            if self.cache_store:\n                cache_key = self._get_cache_key(query)\n                await self.cache_store.aput(\n                    key=cache_key,\n                    val={str(uuid.uuid4()): query_embedding},\n                    collection=\"embeddings\",\n                )\n\n        return query_embedding\n\n    def get_agg_embedding_from_queries(\n        self,\n        queries: list[str],\n        agg_fn: Callable[..., Embedding] | None = None,\n    ) -&gt; Embedding:\n        \"\"\"Generate a single aggregated embedding from multiple query strings.\n\n        Embeds each query individually and then combines them using an aggregation\n        function. This is useful for creating a unified representation from multiple\n        related queries or questions.\n\n        Args:\n            queries: List of query strings to embed and aggregate.\n            agg_fn: Optional aggregation function that takes a list of embeddings\n                and returns a single embedding. Defaults to mean_agg (arithmetic mean).\n\n        Returns:\n            Single aggregated embedding vector as a list of floats.\n\n        See Also:\n            aget_agg_embedding_from_queries: Async version of this method.\n            mean_agg: Default aggregation function.\n            get_query_embedding: Used internally to embed each query.\n        \"\"\"\n        query_embeddings = [self.get_query_embedding(query) for query in queries]\n        agg_fn = agg_fn or mean_agg\n        return agg_fn(query_embeddings)\n\n    async def aget_agg_embedding_from_queries(\n        self,\n        queries: list[str],\n        agg_fn: Callable[..., Embedding] | None = None,\n    ) -&gt; Embedding:\n        \"\"\"Asynchronously generate an aggregated embedding from multiple queries.\n\n        Async version of get_agg_embedding_from_queries(). Embeds each query\n        asynchronously and then combines them using an aggregation function.\n\n        Args:\n            queries: List of query strings to embed and aggregate.\n            agg_fn: Optional aggregation function that takes a list of embeddings\n                and returns a single embedding. Defaults to mean_agg.\n\n        Returns:\n            Single aggregated embedding vector as a list of floats.\n\n        See Also:\n            get_agg_embedding_from_queries: Sync version of this method.\n            aget_query_embedding: Used internally to embed each query.\n            mean_agg: Default aggregation function.\n        \"\"\"\n        query_embeddings = [await self.aget_query_embedding(query) for query in queries]\n        agg_fn = agg_fn or mean_agg\n        return agg_fn(query_embeddings)\n\n    @abstractmethod\n    def _get_text_embedding(self, text: str) -&gt; Embedding:\n        \"\"\"Embed document text synchronously (internal implementation).\n\n        This is an internal method that subclasses must implement to provide\n        the core text embedding functionality. The public method get_text_embedding()\n        handles caching and calls this method when needed.\n\n        Text embeddings may use different instructions or prefixes than query\n        embeddings. For example, some models prepend \"Represent the document for\n        retrieval: \" to optimize for document representation.\n\n        Args:\n            text: Document text to embed.\n\n        Returns:\n            Embedding vector as a list of floats.\n\n        See Also:\n            get_text_embedding: Public method that handles caching and delegates\n                to this method.\n            _aget_text_embedding: Async version of this method.\n            _get_query_embedding: For embedding queries (not documents).\n        \"\"\"\n\n    async def _aget_text_embedding(self, text: str) -&gt; Embedding:\n        \"\"\"Embed document text asynchronously (internal implementation).\n\n        This is an internal async method that subclasses can override to provide\n        true async text embedding. The default implementation falls back to the\n        sync method _get_text_embedding().\n\n        Args:\n            text: Document text to embed.\n\n        Returns:\n            Embedding vector as a list of floats.\n\n        Notes:\n            Subclasses should override this method if they have a native async\n            implementation. Otherwise, the default fallback to the sync method\n            is used.\n\n        See Also:\n            aget_text_embedding: Public async method that handles caching.\n            _get_text_embedding: Sync version used as fallback.\n        \"\"\"\n        # Default implementation just falls back on _get_text_embedding\n        return self._get_text_embedding(text)\n\n    def _get_text_embeddings(self, texts: list[str]) -&gt; list[Embedding]:\n        \"\"\"Embed multiple texts synchronously in batch (internal implementation).\n\n        This internal method provides batch embedding functionality. Subclasses\n        can override this method to provide more efficient batch processing if\n        supported by the underlying model API.\n\n        Args:\n            texts: List of document texts to embed.\n\n        Returns:\n            List of embedding vectors, one for each input text, in the same order.\n\n        Notes:\n            The default implementation simply loops over _get_text_embedding().\n            Subclasses should override this if they can process batches more\n            efficiently.\n\n        See Also:\n            get_text_embedding_batch: Public method for batch embedding with\n                batching and progress tracking.\n            _aget_text_embeddings: Async version of this method.\n        \"\"\"\n        # Default implementation just loops over _get_text_embedding\n        return [self._get_text_embedding(text) for text in texts]\n\n    async def _aget_text_embeddings(self, texts: list[str]) -&gt; list[Embedding]:\n        \"\"\"Embed multiple texts asynchronously in batch (internal implementation).\n\n        This internal async method provides batch embedding functionality using\n        asyncio.gather for concurrent processing. Subclasses can override this\n        for more efficient batch processing.\n\n        Args:\n            texts: List of document texts to embed.\n\n        Returns:\n            List of embedding vectors, one for each input text, in the same order.\n\n        Notes:\n            The default implementation uses asyncio.gather to process all texts\n            concurrently via _aget_text_embedding(). Subclasses should override\n            this if they can process batches more efficiently.\n\n        See Also:\n            aget_text_embedding_batch: Public async method for batch embedding.\n            _get_text_embeddings: Sync version of this method.\n        \"\"\"\n        return await asyncio.gather(\n            *[self._aget_text_embedding(text) for text in texts]\n        )\n\n    def _get_text_embeddings_cached(self, texts: list[str]) -&gt; list[Embedding]:\n        \"\"\"Retrieve text embeddings from cache or generate if not cached.\n\n        This internal method checks the cache for each text and only generates\n        embeddings for texts not found in the cache. Newly generated embeddings\n        are automatically stored in the cache.\n\n        Args:\n            texts: List of document texts to embed.\n\n        Returns:\n            List of embedding vectors, one for each input text, preserving order.\n\n        Raises:\n            ValueError: If cache_store is None when this method is called.\n\n        See Also:\n            _aget_text_embeddings_cached: Async version of this method.\n            get_text_embedding_batch: Public method that uses this for caching.\n        \"\"\"\n        if self.cache_store is None:\n            raise ValueError(\"embeddings_cache must be defined\")\n\n        embeddings: list[Embedding | None] = [None for i in range(len(texts))]\n        # Tuples of (index, text) to be able to keep same order of embeddings\n        non_cached_texts: list[tuple[int, str]] = []\n        for i, txt in enumerate(texts):\n            cache_key = self._get_cache_key(txt)\n            cached_emb = self.cache_store.get(key=cache_key, collection=\"embeddings\")\n            if cached_emb is not None:\n                cached_key = next(iter(cached_emb.keys()))\n                embeddings[i] = cached_emb[cached_key]\n            else:\n                non_cached_texts.append((i, txt))\n        if len(non_cached_texts) &gt; 0:\n            text_embeddings = self._get_text_embeddings(\n                [x[1] for x in non_cached_texts]\n            )\n            for j, text_embedding in enumerate(text_embeddings):\n                orig_i = non_cached_texts[j][0]\n                embeddings[orig_i] = text_embedding\n\n                cache_key = self._get_cache_key(texts[orig_i])\n                self.cache_store.put(\n                    key=cache_key,\n                    val={str(uuid.uuid4()): text_embedding},\n                    collection=\"embeddings\",\n                )\n        return embeddings\n\n    async def _aget_text_embeddings_cached(self, texts: list[str]) -&gt; list[Embedding]:\n        \"\"\"Asynchronously retrieve text embeddings from cache or generate them.\n\n        Async version of _get_text_embeddings_cached(). Checks the cache for each\n        text and generates embeddings only for texts not found. Newly generated\n        embeddings are automatically stored in the cache.\n\n        Args:\n            texts: List of document texts to embed.\n\n        Returns:\n            List of embedding vectors, one for each input text, preserving order.\n\n        Raises:\n            ValueError: If cache_store is None when this method is called.\n\n        See Also:\n            _get_text_embeddings_cached: Sync version of this method.\n            aget_text_embedding_batch: Public async method that uses this.\n        \"\"\"\n        if self.cache_store is None:\n            raise ValueError(\"embeddings_cache must be defined\")\n\n        embeddings: list[Embedding | None] = [None for i in range(len(texts))]\n        # Tuples of (index, text) to be able to keep same order of embeddings\n        non_cached_texts: list[tuple[int, str]] = []\n        for i, txt in enumerate(texts):\n            cache_key = self._get_cache_key(txt)\n            cached_emb = await self.cache_store.aget(\n                key=cache_key, collection=\"embeddings\"\n            )\n            if cached_emb is not None:\n                cached_key = next(iter(cached_emb.keys()))\n                embeddings[i] = cached_emb[cached_key]\n            else:\n                non_cached_texts.append((i, txt))\n\n        if len(non_cached_texts) &gt; 0:\n            text_embeddings = await self._aget_text_embeddings(\n                [x[1] for x in non_cached_texts]\n            )\n            for j, text_embedding in enumerate(text_embeddings):\n                orig_i = non_cached_texts[j][0]\n                embeddings[orig_i] = text_embedding\n                cache_key = self._get_cache_key(texts[orig_i])\n                await self.cache_store.aput(\n                    key=cache_key,\n                    val={str(uuid.uuid4()): text_embedding},\n                    collection=\"embeddings\",\n                )\n        return embeddings\n\n    def get_text_embedding(self, text: str) -&gt; Embedding:\n        \"\"\"Generate an embedding vector for document text.\n\n        Embeds the input text into a dense vector representation optimized for\n        document representation tasks. When caching is enabled, checks the cache\n        first and stores new embeddings automatically.\n\n        Depending on the model, a special instruction may be prepended to the raw\n        text string to optimize for document retrieval. For example, some models\n        use \"Represent the document for retrieval: \".\n\n        Args:\n            text: Document text to embed.\n\n        Returns:\n            Embedding vector as a list of floats.\n\n        See Also:\n            aget_text_embedding: Async version of this method.\n            get_query_embedding: For embedding queries (not documents).\n            get_text_embedding_batch: For embedding multiple texts efficiently.\n            _get_text_embedding: Internal implementation method.\n        \"\"\"\n        if not self.cache_store:\n            text_embedding = self._get_text_embedding(text)\n        elif self.cache_store is not None:\n            cache_key = self._get_cache_key(text)\n            cached_emb = self.cache_store.get(key=cache_key, collection=\"embeddings\")\n            if cached_emb is not None:\n                cached_key = next(iter(cached_emb.keys()))\n                text_embedding = cached_emb[cached_key]\n            else:\n                text_embedding = self._get_text_embedding(text)\n                cache_key = self._get_cache_key(text)\n                self.cache_store.put(\n                    key=cache_key,\n                    val={str(uuid.uuid4()): text_embedding},\n                    collection=\"embeddings\",\n                )\n\n        return text_embedding\n\n    async def aget_text_embedding(self, text: str) -&gt; Embedding:\n        \"\"\"Asynchronously generate an embedding vector for document text.\n\n        Async version of get_text_embedding(). Embeds the input text into a dense\n        vector representation with cache support.\n\n        Args:\n            text: Document text to embed.\n\n        Returns:\n            Embedding vector as a list of floats.\n\n        See Also:\n            get_text_embedding: Sync version of this method.\n            aget_query_embedding: For embedding queries asynchronously.\n            aget_text_embedding_batch: For embedding multiple texts efficiently.\n            _aget_text_embedding: Internal async implementation method.\n        \"\"\"\n        if not self.cache_store:\n            text_embedding = await self._aget_text_embedding(text)\n        elif self.cache_store is not None:\n            cache_key = self._get_cache_key(text)\n            cached_emb = await self.cache_store.aget(\n                key=cache_key, collection=\"embeddings\"\n            )\n            if cached_emb is not None:\n                cached_key = next(iter(cached_emb.keys()))\n                text_embedding = cached_emb[cached_key]\n            else:\n                text_embedding = await self._aget_text_embedding(text)\n                cache_key = self._get_cache_key(text)\n                await self.cache_store.aput(\n                    key=cache_key,\n                    val={str(uuid.uuid4()): text_embedding},\n                    collection=\"embeddings\",\n                )\n\n        return text_embedding\n\n    def get_text_embedding_batch(\n        self,\n        texts: list[str],\n        show_progress: bool = False,\n        **kwargs: Any,\n    ) -&gt; list[Embedding]:\n        \"\"\"Generate embeddings for multiple texts with automatic batching.\n\n        Processes a list of texts in batches according to self.batch_size. Supports\n        optional progress tracking and automatic caching if cache_store is configured.\n\n        Args:\n            texts: List of document texts to embed.\n            show_progress: Whether to display a progress bar. Defaults to False.\n            **kwargs: Additional keyword arguments (reserved for future use).\n\n        Returns:\n            List of embedding vectors, one for each input text, in the same order.\n\n        See Also:\n            aget_text_embedding_batch: Async version with parallel processing.\n            get_text_embedding: For embedding a single text.\n            _get_text_embeddings: Internal batch processing method.\n            _get_text_embeddings_cached: Internal cached batch processing.\n        \"\"\"\n        cur_batch: list[str] = []\n        result_embeddings: list[Embedding] = []\n\n        queue_with_progress = enumerate(\n            get_tqdm_iterable(texts, show_progress, \"Generating embeddings\")\n        )\n\n        for idx, text in queue_with_progress:\n            cur_batch.append(text)\n            if idx == len(texts) - 1 or len(cur_batch) == self.batch_size:\n                # flush\n                if not self.cache_store:\n                    embeddings = self._get_text_embeddings(cur_batch)\n                elif self.cache_store is not None:\n                    embeddings = self._get_text_embeddings_cached(cur_batch)\n\n                result_embeddings.extend(embeddings)\n\n                cur_batch = []\n\n        return result_embeddings\n\n    async def aget_text_embedding_batch(\n        self,\n        texts: list[str],\n        show_progress: bool = False,\n        **kwargs: Any,\n    ) -&gt; list[Embedding]:\n        \"\"\"Asynchronously generate embeddings for multiple texts with batching.\n\n        Async version of get_text_embedding_batch(). Processes texts in batches\n        with concurrent execution for improved performance. Supports worker pooling\n        if num_workers is set.\n\n        Args:\n            texts: List of document texts to embed.\n            show_progress: Whether to display a progress bar. Defaults to False.\n                Requires tqdm package for progress tracking.\n            **kwargs: Additional keyword arguments (reserved for future use).\n\n        Returns:\n            List of embedding vectors, one for each input text, in the same order.\n\n        Notes:\n            When num_workers &gt; 1, uses worker pooling for concurrent batch processing.\n            When show_progress=True, attempts to use tqdm.asyncio for progress tracking.\n\n        See Also:\n            get_text_embedding_batch: Sync version of this method.\n            aget_text_embedding: For embedding a single text asynchronously.\n            _aget_text_embeddings: Internal async batch processing method.\n        \"\"\"\n        num_workers = self.num_workers\n\n        cur_batch: list[str] = []\n        embeddings_coroutines: list[Coroutine] = []\n\n        # for idx, text in queue_with_progress:\n        for idx, text in enumerate(texts):\n            cur_batch.append(text)\n            if idx == len(texts) - 1 or len(cur_batch) == self.batch_size:\n                # flush\n\n                if not self.cache_store:\n                    embeddings_coroutines.append(self._aget_text_embeddings(cur_batch))\n                elif self.cache_store is not None:\n                    embeddings_coroutines.append(\n                        self._aget_text_embeddings_cached(cur_batch)\n                    )\n\n                cur_batch = []\n\n        # flatten the results of asyncio.gather, which is a list of embeddings lists\n        if len(embeddings_coroutines) &gt; 0:\n            if num_workers and num_workers &gt; 1:\n                nested_embeddings = await run_jobs(\n                    embeddings_coroutines,\n                    show_progress=show_progress,\n                    workers=self.num_workers,\n                    desc=\"Generating embeddings\",\n                )\n            elif show_progress:\n                try:\n                    nested_embeddings = await tqdm_asyncio.gather(\n                        *embeddings_coroutines,\n                        total=len(embeddings_coroutines),\n                        desc=\"Generating embeddings\",\n                    )\n                except ImportError:\n                    nested_embeddings = await asyncio.gather(*embeddings_coroutines)\n            else:\n                nested_embeddings = await asyncio.gather(*embeddings_coroutines)\n        else:\n            nested_embeddings = []\n\n        result_embeddings = [\n            embedding for embeddings in nested_embeddings for embedding in embeddings\n        ]\n        return result_embeddings\n\n    @staticmethod\n    def similarity(\n        embedding1: Embedding,\n        embedding2: Embedding,\n        mode: SimilarityMode = SimilarityMode.DEFAULT,\n    ) -&gt; float:\n        \"\"\"Calculate similarity between two embedding vectors.\n\n        Static method wrapper for the module-level similarity() function. Provides\n        a convenient way to compute similarity directly from the class.\n\n        Args:\n            embedding1: First embedding vector (list of floats).\n            embedding2: Second embedding vector (list of floats).\n            mode: Similarity computation mode. Defaults to cosine similarity.\n\n        Returns:\n            Similarity score as a float. Interpretation depends on the mode.\n\n        Examples:\n            - Computing cosine similarity\n                ```python\n                &gt;&gt;&gt; from serapeum.core.embeddings import BaseEmbedding  # type: ignore\n                &gt;&gt;&gt; emb1 = [1.0, 0.0]\n                &gt;&gt;&gt; emb2 = [1.0, 0.0]\n                &gt;&gt;&gt; float(BaseEmbedding.similarity(emb1, emb2))\n                1.0\n\n                ```\n\n            - Using different similarity modes\n                ```python\n                &gt;&gt;&gt; emb1 = [3.0, 4.0]\n                &gt;&gt;&gt; emb2 = [3.0, 4.0]\n                &gt;&gt;&gt; float(BaseEmbedding.similarity(emb1, emb2, mode=SimilarityMode.DOT_PRODUCT))\n                25.0\n\n                ```\n\n        See Also:\n            similarity: Module-level function that performs the actual calculation.\n            SimilarityMode: Enum defining available similarity modes.\n        \"\"\"\n        return similarity(embedding1=embedding1, embedding2=embedding2, mode=mode)\n\n    def __call__(self, nodes: Sequence[BaseNode], **kwargs: Any) -&gt; Sequence[BaseNode]:\n        \"\"\"Embed a sequence of nodes by calling the embedding model.\n\n        This makes the embedding model callable, allowing it to be used as a function.\n        Extracts text content from each node, generates embeddings, and assigns them\n        back to the nodes.\n\n        Args:\n            nodes: Sequence of BaseNode objects to embed.\n            **kwargs: Additional keyword arguments passed to get_text_embedding_batch.\n\n        Returns:\n            The input sequence of nodes with embeddings assigned to each node's\n            embedding attribute.\n\n        See Also:\n            acall: Async version of this method.\n            get_text_embedding_batch: Method used internally for batch embedding.\n            MetadataMode.EMBED: Mode used to extract content from nodes.\n        \"\"\"\n        embeddings = self.get_text_embedding_batch(\n            [node.get_content(metadata_mode=MetadataMode.EMBED) for node in nodes],\n            **kwargs,\n        )\n\n        for node, embedding in zip(nodes, embeddings):\n            node.embedding = embedding\n\n        return nodes\n\n    async def acall(\n        self, nodes: Sequence[BaseNode], **kwargs: Any\n    ) -&gt; Sequence[BaseNode]:\n        \"\"\"Asynchronously embed a sequence of nodes.\n\n        Async version of __call__(). Extracts text content from each node,\n        generates embeddings asynchronously, and assigns them back to the nodes.\n\n        Args:\n            nodes: Sequence of BaseNode objects to embed.\n            **kwargs: Additional keyword arguments passed to aget_text_embedding_batch.\n\n        Returns:\n            The input sequence of nodes with embeddings assigned to each node's\n            embedding attribute.\n\n        See Also:\n            __call__: Sync version of this method.\n            aget_text_embedding_batch: Method used internally for async batch embedding.\n            MetadataMode.EMBED: Mode used to extract content from nodes.\n        \"\"\"\n        embeddings = await self.aget_text_embedding_batch(\n            [node.get_content(metadata_mode=MetadataMode.EMBED) for node in nodes],\n            **kwargs,\n        )\n\n        for node, embedding in zip(nodes, embeddings):\n            node.embedding = embedding\n\n        return nodes\n</code></pre>"},{"location":"reference/core/embeddings/module/#serapeum.core.embeddings.BaseEmbedding.__call__","title":"<code>__call__(nodes, **kwargs)</code>","text":"<p>Embed a sequence of nodes by calling the embedding model.</p> <p>This makes the embedding model callable, allowing it to be used as a function. Extracts text content from each node, generates embeddings, and assigns them back to the nodes.</p> <p>Parameters:</p> Name Type Description Default <code>nodes</code> <code>Sequence[BaseNode]</code> <p>Sequence of BaseNode objects to embed.</p> required <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments passed to get_text_embedding_batch.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Sequence[BaseNode]</code> <p>The input sequence of nodes with embeddings assigned to each node's</p> <code>Sequence[BaseNode]</code> <p>embedding attribute.</p> See Also <p>acall: Async version of this method. get_text_embedding_batch: Method used internally for batch embedding. MetadataMode.EMBED: Mode used to extract content from nodes.</p> Source code in <code>libs/core/src/serapeum/core/base/embeddings/base.py</code> <pre><code>def __call__(self, nodes: Sequence[BaseNode], **kwargs: Any) -&gt; Sequence[BaseNode]:\n    \"\"\"Embed a sequence of nodes by calling the embedding model.\n\n    This makes the embedding model callable, allowing it to be used as a function.\n    Extracts text content from each node, generates embeddings, and assigns them\n    back to the nodes.\n\n    Args:\n        nodes: Sequence of BaseNode objects to embed.\n        **kwargs: Additional keyword arguments passed to get_text_embedding_batch.\n\n    Returns:\n        The input sequence of nodes with embeddings assigned to each node's\n        embedding attribute.\n\n    See Also:\n        acall: Async version of this method.\n        get_text_embedding_batch: Method used internally for batch embedding.\n        MetadataMode.EMBED: Mode used to extract content from nodes.\n    \"\"\"\n    embeddings = self.get_text_embedding_batch(\n        [node.get_content(metadata_mode=MetadataMode.EMBED) for node in nodes],\n        **kwargs,\n    )\n\n    for node, embedding in zip(nodes, embeddings):\n        node.embedding = embedding\n\n    return nodes\n</code></pre>"},{"location":"reference/core/embeddings/module/#serapeum.core.embeddings.BaseEmbedding.acall","title":"<code>acall(nodes, **kwargs)</code>  <code>async</code>","text":"<p>Asynchronously embed a sequence of nodes.</p> <p>Async version of call(). Extracts text content from each node, generates embeddings asynchronously, and assigns them back to the nodes.</p> <p>Parameters:</p> Name Type Description Default <code>nodes</code> <code>Sequence[BaseNode]</code> <p>Sequence of BaseNode objects to embed.</p> required <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments passed to aget_text_embedding_batch.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Sequence[BaseNode]</code> <p>The input sequence of nodes with embeddings assigned to each node's</p> <code>Sequence[BaseNode]</code> <p>embedding attribute.</p> See Also <p>call: Sync version of this method. aget_text_embedding_batch: Method used internally for async batch embedding. MetadataMode.EMBED: Mode used to extract content from nodes.</p> Source code in <code>libs/core/src/serapeum/core/base/embeddings/base.py</code> <pre><code>async def acall(\n    self, nodes: Sequence[BaseNode], **kwargs: Any\n) -&gt; Sequence[BaseNode]:\n    \"\"\"Asynchronously embed a sequence of nodes.\n\n    Async version of __call__(). Extracts text content from each node,\n    generates embeddings asynchronously, and assigns them back to the nodes.\n\n    Args:\n        nodes: Sequence of BaseNode objects to embed.\n        **kwargs: Additional keyword arguments passed to aget_text_embedding_batch.\n\n    Returns:\n        The input sequence of nodes with embeddings assigned to each node's\n        embedding attribute.\n\n    See Also:\n        __call__: Sync version of this method.\n        aget_text_embedding_batch: Method used internally for async batch embedding.\n        MetadataMode.EMBED: Mode used to extract content from nodes.\n    \"\"\"\n    embeddings = await self.aget_text_embedding_batch(\n        [node.get_content(metadata_mode=MetadataMode.EMBED) for node in nodes],\n        **kwargs,\n    )\n\n    for node, embedding in zip(nodes, embeddings):\n        node.embedding = embedding\n\n    return nodes\n</code></pre>"},{"location":"reference/core/embeddings/module/#serapeum.core.embeddings.BaseEmbedding.aget_agg_embedding_from_queries","title":"<code>aget_agg_embedding_from_queries(queries, agg_fn=None)</code>  <code>async</code>","text":"<p>Asynchronously generate an aggregated embedding from multiple queries.</p> <p>Async version of get_agg_embedding_from_queries(). Embeds each query asynchronously and then combines them using an aggregation function.</p> <p>Parameters:</p> Name Type Description Default <code>queries</code> <code>list[str]</code> <p>List of query strings to embed and aggregate.</p> required <code>agg_fn</code> <code>Callable[..., Embedding] | None</code> <p>Optional aggregation function that takes a list of embeddings and returns a single embedding. Defaults to mean_agg.</p> <code>None</code> <p>Returns:</p> Type Description <code>Embedding</code> <p>Single aggregated embedding vector as a list of floats.</p> See Also <p>get_agg_embedding_from_queries: Sync version of this method. aget_query_embedding: Used internally to embed each query. mean_agg: Default aggregation function.</p> Source code in <code>libs/core/src/serapeum/core/base/embeddings/base.py</code> <pre><code>async def aget_agg_embedding_from_queries(\n    self,\n    queries: list[str],\n    agg_fn: Callable[..., Embedding] | None = None,\n) -&gt; Embedding:\n    \"\"\"Asynchronously generate an aggregated embedding from multiple queries.\n\n    Async version of get_agg_embedding_from_queries(). Embeds each query\n    asynchronously and then combines them using an aggregation function.\n\n    Args:\n        queries: List of query strings to embed and aggregate.\n        agg_fn: Optional aggregation function that takes a list of embeddings\n            and returns a single embedding. Defaults to mean_agg.\n\n    Returns:\n        Single aggregated embedding vector as a list of floats.\n\n    See Also:\n        get_agg_embedding_from_queries: Sync version of this method.\n        aget_query_embedding: Used internally to embed each query.\n        mean_agg: Default aggregation function.\n    \"\"\"\n    query_embeddings = [await self.aget_query_embedding(query) for query in queries]\n    agg_fn = agg_fn or mean_agg\n    return agg_fn(query_embeddings)\n</code></pre>"},{"location":"reference/core/embeddings/module/#serapeum.core.embeddings.BaseEmbedding.aget_query_embedding","title":"<code>aget_query_embedding(query)</code>  <code>async</code>","text":"<p>Asynchronously generate an embedding vector for a query string.</p> <p>Async version of get_query_embedding(). Embeds the input query into a dense vector representation with cache support.</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>str</code> <p>Query text to embed.</p> required <p>Returns:</p> Type Description <code>Embedding</code> <p>Embedding vector as a list of floats.</p> See Also <p>get_query_embedding: Sync version of this method. aget_text_embedding: For embedding document text asynchronously. _aget_query_embedding: Internal async implementation method.</p> Source code in <code>libs/core/src/serapeum/core/base/embeddings/base.py</code> <pre><code>async def aget_query_embedding(self, query: str) -&gt; Embedding:\n    \"\"\"Asynchronously generate an embedding vector for a query string.\n\n    Async version of get_query_embedding(). Embeds the input query into a dense\n    vector representation with cache support.\n\n    Args:\n        query: Query text to embed.\n\n    Returns:\n        Embedding vector as a list of floats.\n\n    See Also:\n        get_query_embedding: Sync version of this method.\n        aget_text_embedding: For embedding document text asynchronously.\n        _aget_query_embedding: Internal async implementation method.\n    \"\"\"\n    query_embedding = None\n    if self.cache_store:\n        cache_key = self._get_cache_key(query)\n        cached = await self.cache_store.aget(key=cache_key, collection=\"embeddings\")\n        if cached:\n            cached_key = next(iter(cached.keys()))\n            query_embedding = cached[cached_key]\n\n    if query_embedding is None:\n        query_embedding = await self._aget_query_embedding(query)\n        if self.cache_store:\n            cache_key = self._get_cache_key(query)\n            await self.cache_store.aput(\n                key=cache_key,\n                val={str(uuid.uuid4()): query_embedding},\n                collection=\"embeddings\",\n            )\n\n    return query_embedding\n</code></pre>"},{"location":"reference/core/embeddings/module/#serapeum.core.embeddings.BaseEmbedding.aget_text_embedding","title":"<code>aget_text_embedding(text)</code>  <code>async</code>","text":"<p>Asynchronously generate an embedding vector for document text.</p> <p>Async version of get_text_embedding(). Embeds the input text into a dense vector representation with cache support.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>Document text to embed.</p> required <p>Returns:</p> Type Description <code>Embedding</code> <p>Embedding vector as a list of floats.</p> See Also <p>get_text_embedding: Sync version of this method. aget_query_embedding: For embedding queries asynchronously. aget_text_embedding_batch: For embedding multiple texts efficiently. _aget_text_embedding: Internal async implementation method.</p> Source code in <code>libs/core/src/serapeum/core/base/embeddings/base.py</code> <pre><code>async def aget_text_embedding(self, text: str) -&gt; Embedding:\n    \"\"\"Asynchronously generate an embedding vector for document text.\n\n    Async version of get_text_embedding(). Embeds the input text into a dense\n    vector representation with cache support.\n\n    Args:\n        text: Document text to embed.\n\n    Returns:\n        Embedding vector as a list of floats.\n\n    See Also:\n        get_text_embedding: Sync version of this method.\n        aget_query_embedding: For embedding queries asynchronously.\n        aget_text_embedding_batch: For embedding multiple texts efficiently.\n        _aget_text_embedding: Internal async implementation method.\n    \"\"\"\n    if not self.cache_store:\n        text_embedding = await self._aget_text_embedding(text)\n    elif self.cache_store is not None:\n        cache_key = self._get_cache_key(text)\n        cached_emb = await self.cache_store.aget(\n            key=cache_key, collection=\"embeddings\"\n        )\n        if cached_emb is not None:\n            cached_key = next(iter(cached_emb.keys()))\n            text_embedding = cached_emb[cached_key]\n        else:\n            text_embedding = await self._aget_text_embedding(text)\n            cache_key = self._get_cache_key(text)\n            await self.cache_store.aput(\n                key=cache_key,\n                val={str(uuid.uuid4()): text_embedding},\n                collection=\"embeddings\",\n            )\n\n    return text_embedding\n</code></pre>"},{"location":"reference/core/embeddings/module/#serapeum.core.embeddings.BaseEmbedding.aget_text_embedding_batch","title":"<code>aget_text_embedding_batch(texts, show_progress=False, **kwargs)</code>  <code>async</code>","text":"<p>Asynchronously generate embeddings for multiple texts with batching.</p> <p>Async version of get_text_embedding_batch(). Processes texts in batches with concurrent execution for improved performance. Supports worker pooling if num_workers is set.</p> <p>Parameters:</p> Name Type Description Default <code>texts</code> <code>list[str]</code> <p>List of document texts to embed.</p> required <code>show_progress</code> <code>bool</code> <p>Whether to display a progress bar. Defaults to False. Requires tqdm package for progress tracking.</p> <code>False</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments (reserved for future use).</p> <code>{}</code> <p>Returns:</p> Type Description <code>list[Embedding]</code> <p>List of embedding vectors, one for each input text, in the same order.</p> Notes <p>When num_workers &gt; 1, uses worker pooling for concurrent batch processing. When show_progress=True, attempts to use tqdm.asyncio for progress tracking.</p> See Also <p>get_text_embedding_batch: Sync version of this method. aget_text_embedding: For embedding a single text asynchronously. _aget_text_embeddings: Internal async batch processing method.</p> Source code in <code>libs/core/src/serapeum/core/base/embeddings/base.py</code> <pre><code>async def aget_text_embedding_batch(\n    self,\n    texts: list[str],\n    show_progress: bool = False,\n    **kwargs: Any,\n) -&gt; list[Embedding]:\n    \"\"\"Asynchronously generate embeddings for multiple texts with batching.\n\n    Async version of get_text_embedding_batch(). Processes texts in batches\n    with concurrent execution for improved performance. Supports worker pooling\n    if num_workers is set.\n\n    Args:\n        texts: List of document texts to embed.\n        show_progress: Whether to display a progress bar. Defaults to False.\n            Requires tqdm package for progress tracking.\n        **kwargs: Additional keyword arguments (reserved for future use).\n\n    Returns:\n        List of embedding vectors, one for each input text, in the same order.\n\n    Notes:\n        When num_workers &gt; 1, uses worker pooling for concurrent batch processing.\n        When show_progress=True, attempts to use tqdm.asyncio for progress tracking.\n\n    See Also:\n        get_text_embedding_batch: Sync version of this method.\n        aget_text_embedding: For embedding a single text asynchronously.\n        _aget_text_embeddings: Internal async batch processing method.\n    \"\"\"\n    num_workers = self.num_workers\n\n    cur_batch: list[str] = []\n    embeddings_coroutines: list[Coroutine] = []\n\n    # for idx, text in queue_with_progress:\n    for idx, text in enumerate(texts):\n        cur_batch.append(text)\n        if idx == len(texts) - 1 or len(cur_batch) == self.batch_size:\n            # flush\n\n            if not self.cache_store:\n                embeddings_coroutines.append(self._aget_text_embeddings(cur_batch))\n            elif self.cache_store is not None:\n                embeddings_coroutines.append(\n                    self._aget_text_embeddings_cached(cur_batch)\n                )\n\n            cur_batch = []\n\n    # flatten the results of asyncio.gather, which is a list of embeddings lists\n    if len(embeddings_coroutines) &gt; 0:\n        if num_workers and num_workers &gt; 1:\n            nested_embeddings = await run_jobs(\n                embeddings_coroutines,\n                show_progress=show_progress,\n                workers=self.num_workers,\n                desc=\"Generating embeddings\",\n            )\n        elif show_progress:\n            try:\n                nested_embeddings = await tqdm_asyncio.gather(\n                    *embeddings_coroutines,\n                    total=len(embeddings_coroutines),\n                    desc=\"Generating embeddings\",\n                )\n            except ImportError:\n                nested_embeddings = await asyncio.gather(*embeddings_coroutines)\n        else:\n            nested_embeddings = await asyncio.gather(*embeddings_coroutines)\n    else:\n        nested_embeddings = []\n\n    result_embeddings = [\n        embedding for embeddings in nested_embeddings for embedding in embeddings\n    ]\n    return result_embeddings\n</code></pre>"},{"location":"reference/core/embeddings/module/#serapeum.core.embeddings.BaseEmbedding.get_agg_embedding_from_queries","title":"<code>get_agg_embedding_from_queries(queries, agg_fn=None)</code>","text":"<p>Generate a single aggregated embedding from multiple query strings.</p> <p>Embeds each query individually and then combines them using an aggregation function. This is useful for creating a unified representation from multiple related queries or questions.</p> <p>Parameters:</p> Name Type Description Default <code>queries</code> <code>list[str]</code> <p>List of query strings to embed and aggregate.</p> required <code>agg_fn</code> <code>Callable[..., Embedding] | None</code> <p>Optional aggregation function that takes a list of embeddings and returns a single embedding. Defaults to mean_agg (arithmetic mean).</p> <code>None</code> <p>Returns:</p> Type Description <code>Embedding</code> <p>Single aggregated embedding vector as a list of floats.</p> See Also <p>aget_agg_embedding_from_queries: Async version of this method. mean_agg: Default aggregation function. get_query_embedding: Used internally to embed each query.</p> Source code in <code>libs/core/src/serapeum/core/base/embeddings/base.py</code> <pre><code>def get_agg_embedding_from_queries(\n    self,\n    queries: list[str],\n    agg_fn: Callable[..., Embedding] | None = None,\n) -&gt; Embedding:\n    \"\"\"Generate a single aggregated embedding from multiple query strings.\n\n    Embeds each query individually and then combines them using an aggregation\n    function. This is useful for creating a unified representation from multiple\n    related queries or questions.\n\n    Args:\n        queries: List of query strings to embed and aggregate.\n        agg_fn: Optional aggregation function that takes a list of embeddings\n            and returns a single embedding. Defaults to mean_agg (arithmetic mean).\n\n    Returns:\n        Single aggregated embedding vector as a list of floats.\n\n    See Also:\n        aget_agg_embedding_from_queries: Async version of this method.\n        mean_agg: Default aggregation function.\n        get_query_embedding: Used internally to embed each query.\n    \"\"\"\n    query_embeddings = [self.get_query_embedding(query) for query in queries]\n    agg_fn = agg_fn or mean_agg\n    return agg_fn(query_embeddings)\n</code></pre>"},{"location":"reference/core/embeddings/module/#serapeum.core.embeddings.BaseEmbedding.get_query_embedding","title":"<code>get_query_embedding(query)</code>","text":"<p>Generate an embedding vector for a query string.</p> <p>Embeds the input query into a dense vector representation optimized for retrieval tasks. When caching is enabled, checks the cache first and stores new embeddings automatically.</p> <p>Depending on the model, a special instruction may be prepended to the raw query string to optimize for specific tasks. For example, some models use \"Represent the question for retrieving supporting documents: \".</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>str</code> <p>Query text to embed.</p> required <p>Returns:</p> Type Description <code>Embedding</code> <p>Embedding vector as a list of floats.</p> See Also <p>aget_query_embedding: Async version of this method. get_text_embedding: For embedding document text (not queries). _get_query_embedding: Internal implementation method.</p> Source code in <code>libs/core/src/serapeum/core/base/embeddings/base.py</code> <pre><code>def get_query_embedding(self, query: str) -&gt; Embedding:\n    \"\"\"Generate an embedding vector for a query string.\n\n    Embeds the input query into a dense vector representation optimized for\n    retrieval tasks. When caching is enabled, checks the cache first and stores\n    new embeddings automatically.\n\n    Depending on the model, a special instruction may be prepended to the raw\n    query string to optimize for specific tasks. For example, some models use\n    \"Represent the question for retrieving supporting documents: \".\n\n    Args:\n        query: Query text to embed.\n\n    Returns:\n        Embedding vector as a list of floats.\n\n    See Also:\n        aget_query_embedding: Async version of this method.\n        get_text_embedding: For embedding document text (not queries).\n        _get_query_embedding: Internal implementation method.\n    \"\"\"\n    query_embedding = None\n    if self.cache_store:\n        cache_key = self._get_cache_key(query)\n        cached = self.cache_store.get(key=cache_key, collection=\"embeddings\")\n        if cached:\n            cached_key = next(iter(cached.keys()))\n            query_embedding = cached[cached_key]\n\n    if query_embedding is None:\n        query_embedding = self._get_query_embedding(query)\n        if self.cache_store:\n            cache_key = self._get_cache_key(query)\n            self.cache_store.put(\n                key=cache_key,\n                val={str(uuid.uuid4()): query_embedding},\n                collection=\"embeddings\",\n            )\n\n    return query_embedding\n</code></pre>"},{"location":"reference/core/embeddings/module/#serapeum.core.embeddings.BaseEmbedding.get_text_embedding","title":"<code>get_text_embedding(text)</code>","text":"<p>Generate an embedding vector for document text.</p> <p>Embeds the input text into a dense vector representation optimized for document representation tasks. When caching is enabled, checks the cache first and stores new embeddings automatically.</p> <p>Depending on the model, a special instruction may be prepended to the raw text string to optimize for document retrieval. For example, some models use \"Represent the document for retrieval: \".</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>Document text to embed.</p> required <p>Returns:</p> Type Description <code>Embedding</code> <p>Embedding vector as a list of floats.</p> See Also <p>aget_text_embedding: Async version of this method. get_query_embedding: For embedding queries (not documents). get_text_embedding_batch: For embedding multiple texts efficiently. _get_text_embedding: Internal implementation method.</p> Source code in <code>libs/core/src/serapeum/core/base/embeddings/base.py</code> <pre><code>def get_text_embedding(self, text: str) -&gt; Embedding:\n    \"\"\"Generate an embedding vector for document text.\n\n    Embeds the input text into a dense vector representation optimized for\n    document representation tasks. When caching is enabled, checks the cache\n    first and stores new embeddings automatically.\n\n    Depending on the model, a special instruction may be prepended to the raw\n    text string to optimize for document retrieval. For example, some models\n    use \"Represent the document for retrieval: \".\n\n    Args:\n        text: Document text to embed.\n\n    Returns:\n        Embedding vector as a list of floats.\n\n    See Also:\n        aget_text_embedding: Async version of this method.\n        get_query_embedding: For embedding queries (not documents).\n        get_text_embedding_batch: For embedding multiple texts efficiently.\n        _get_text_embedding: Internal implementation method.\n    \"\"\"\n    if not self.cache_store:\n        text_embedding = self._get_text_embedding(text)\n    elif self.cache_store is not None:\n        cache_key = self._get_cache_key(text)\n        cached_emb = self.cache_store.get(key=cache_key, collection=\"embeddings\")\n        if cached_emb is not None:\n            cached_key = next(iter(cached_emb.keys()))\n            text_embedding = cached_emb[cached_key]\n        else:\n            text_embedding = self._get_text_embedding(text)\n            cache_key = self._get_cache_key(text)\n            self.cache_store.put(\n                key=cache_key,\n                val={str(uuid.uuid4()): text_embedding},\n                collection=\"embeddings\",\n            )\n\n    return text_embedding\n</code></pre>"},{"location":"reference/core/embeddings/module/#serapeum.core.embeddings.BaseEmbedding.get_text_embedding_batch","title":"<code>get_text_embedding_batch(texts, show_progress=False, **kwargs)</code>","text":"<p>Generate embeddings for multiple texts with automatic batching.</p> <p>Processes a list of texts in batches according to self.batch_size. Supports optional progress tracking and automatic caching if cache_store is configured.</p> <p>Parameters:</p> Name Type Description Default <code>texts</code> <code>list[str]</code> <p>List of document texts to embed.</p> required <code>show_progress</code> <code>bool</code> <p>Whether to display a progress bar. Defaults to False.</p> <code>False</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments (reserved for future use).</p> <code>{}</code> <p>Returns:</p> Type Description <code>list[Embedding]</code> <p>List of embedding vectors, one for each input text, in the same order.</p> See Also <p>aget_text_embedding_batch: Async version with parallel processing. get_text_embedding: For embedding a single text. _get_text_embeddings: Internal batch processing method. _get_text_embeddings_cached: Internal cached batch processing.</p> Source code in <code>libs/core/src/serapeum/core/base/embeddings/base.py</code> <pre><code>def get_text_embedding_batch(\n    self,\n    texts: list[str],\n    show_progress: bool = False,\n    **kwargs: Any,\n) -&gt; list[Embedding]:\n    \"\"\"Generate embeddings for multiple texts with automatic batching.\n\n    Processes a list of texts in batches according to self.batch_size. Supports\n    optional progress tracking and automatic caching if cache_store is configured.\n\n    Args:\n        texts: List of document texts to embed.\n        show_progress: Whether to display a progress bar. Defaults to False.\n        **kwargs: Additional keyword arguments (reserved for future use).\n\n    Returns:\n        List of embedding vectors, one for each input text, in the same order.\n\n    See Also:\n        aget_text_embedding_batch: Async version with parallel processing.\n        get_text_embedding: For embedding a single text.\n        _get_text_embeddings: Internal batch processing method.\n        _get_text_embeddings_cached: Internal cached batch processing.\n    \"\"\"\n    cur_batch: list[str] = []\n    result_embeddings: list[Embedding] = []\n\n    queue_with_progress = enumerate(\n        get_tqdm_iterable(texts, show_progress, \"Generating embeddings\")\n    )\n\n    for idx, text in queue_with_progress:\n        cur_batch.append(text)\n        if idx == len(texts) - 1 or len(cur_batch) == self.batch_size:\n            # flush\n            if not self.cache_store:\n                embeddings = self._get_text_embeddings(cur_batch)\n            elif self.cache_store is not None:\n                embeddings = self._get_text_embeddings_cached(cur_batch)\n\n            result_embeddings.extend(embeddings)\n\n            cur_batch = []\n\n    return result_embeddings\n</code></pre>"},{"location":"reference/core/embeddings/module/#serapeum.core.embeddings.BaseEmbedding.similarity","title":"<code>similarity(embedding1, embedding2, mode=SimilarityMode.DEFAULT)</code>  <code>staticmethod</code>","text":"<p>Calculate similarity between two embedding vectors.</p> <p>Static method wrapper for the module-level similarity() function. Provides a convenient way to compute similarity directly from the class.</p> <p>Parameters:</p> Name Type Description Default <code>embedding1</code> <code>Embedding</code> <p>First embedding vector (list of floats).</p> required <code>embedding2</code> <code>Embedding</code> <p>Second embedding vector (list of floats).</p> required <code>mode</code> <code>SimilarityMode</code> <p>Similarity computation mode. Defaults to cosine similarity.</p> <code>DEFAULT</code> <p>Returns:</p> Type Description <code>float</code> <p>Similarity score as a float. Interpretation depends on the mode.</p> <p>Examples:</p> <ul> <li> <p>Computing cosine similarity     <pre><code>&gt;&gt;&gt; from serapeum.core.embeddings import BaseEmbedding  # type: ignore\n&gt;&gt;&gt; emb1 = [1.0, 0.0]\n&gt;&gt;&gt; emb2 = [1.0, 0.0]\n&gt;&gt;&gt; float(BaseEmbedding.similarity(emb1, emb2))\n1.0\n</code></pre></p> </li> <li> <p>Using different similarity modes     <pre><code>&gt;&gt;&gt; emb1 = [3.0, 4.0]\n&gt;&gt;&gt; emb2 = [3.0, 4.0]\n&gt;&gt;&gt; float(BaseEmbedding.similarity(emb1, emb2, mode=SimilarityMode.DOT_PRODUCT))\n25.0\n</code></pre></p> </li> </ul> See Also <p>similarity: Module-level function that performs the actual calculation. SimilarityMode: Enum defining available similarity modes.</p> Source code in <code>libs/core/src/serapeum/core/base/embeddings/base.py</code> <pre><code>@staticmethod\ndef similarity(\n    embedding1: Embedding,\n    embedding2: Embedding,\n    mode: SimilarityMode = SimilarityMode.DEFAULT,\n) -&gt; float:\n    \"\"\"Calculate similarity between two embedding vectors.\n\n    Static method wrapper for the module-level similarity() function. Provides\n    a convenient way to compute similarity directly from the class.\n\n    Args:\n        embedding1: First embedding vector (list of floats).\n        embedding2: Second embedding vector (list of floats).\n        mode: Similarity computation mode. Defaults to cosine similarity.\n\n    Returns:\n        Similarity score as a float. Interpretation depends on the mode.\n\n    Examples:\n        - Computing cosine similarity\n            ```python\n            &gt;&gt;&gt; from serapeum.core.embeddings import BaseEmbedding  # type: ignore\n            &gt;&gt;&gt; emb1 = [1.0, 0.0]\n            &gt;&gt;&gt; emb2 = [1.0, 0.0]\n            &gt;&gt;&gt; float(BaseEmbedding.similarity(emb1, emb2))\n            1.0\n\n            ```\n\n        - Using different similarity modes\n            ```python\n            &gt;&gt;&gt; emb1 = [3.0, 4.0]\n            &gt;&gt;&gt; emb2 = [3.0, 4.0]\n            &gt;&gt;&gt; float(BaseEmbedding.similarity(emb1, emb2, mode=SimilarityMode.DOT_PRODUCT))\n            25.0\n\n            ```\n\n    See Also:\n        similarity: Module-level function that performs the actual calculation.\n        SimilarityMode: Enum defining available similarity modes.\n    \"\"\"\n    return similarity(embedding1=embedding1, embedding2=embedding2, mode=mode)\n</code></pre>"},{"location":"reference/core/embeddings/module/#serapeum.core.embeddings.BaseNode","title":"<code>BaseNode</code>","text":"<p>               Bases: <code>SerializableModel</code>, <code>ABC</code></p> <p>Abstract base class for document nodes with metadata and relationship management.</p> <p>BaseNode provides the foundational functionality for representing chunks of documents with rich metadata, embeddings, and hierarchical relationships. It supports selective metadata inclusion for different contexts (LLM vs embeddings), automatic change detection via hashing, and efficient relationship caching.</p> <p>Key features: - Automatic UUID generation for node identification - Metadata management with selective inclusion/exclusion for LLM and embedding contexts - Relationship tracking (source, parent, children, previous, next) - Embedding storage and retrieval - Cached LinkedNodes computation with automatic invalidation - Customizable metadata formatting and serialization</p> <p>Attributes:</p> Name Type Description <code>id</code> <code>str</code> <p>Unique identifier for the node (auto-generated UUID if not provided).</p> <code>embedding</code> <code>list[float] | None</code> <p>Optional vector embedding for the node's content.</p> <code>metadata</code> <code>dict[str, Any]</code> <p>Flat dictionary of metadata fields used for context and filtering.</p> <code>excluded_embed_metadata_keys</code> <code>list[str]</code> <p>Metadata keys excluded from embedding context.</p> <code>excluded_llm_metadata_keys</code> <code>list[str]</code> <p>Metadata keys excluded from LLM context.</p> <code>links</code> <code>dict[Annotated[NodeType, EnumNameSerializer], NodeInfoType]</code> <p>Dictionary mapping NodeType to NodeInfo for relationships.</p> <code>metadata_template</code> <code>str</code> <p>Template string for formatting metadata (default: \"{key}: {value}\").</p> <code>metadata_separator</code> <code>str</code> <p>Separator between metadata fields (default: newline).</p> Note <p>This is an abstract base class. Subclasses must implement: - get_type(): Return the node's content type identifier - get_content(): Return the node's content with optional metadata - set_content(): Update the node's content - hash: Property returning the content hash for change detection</p> <p>Examples:</p> <ul> <li>Creating a concrete node subclass     <pre><code>&gt;&gt;&gt; from serapeum.core.base.embeddings.types import BaseNode, MetadataMode, NodeType, NodeInfo\n&gt;&gt;&gt; import hashlib\n&gt;&gt;&gt; from pydantic import Field\n&gt;&gt;&gt;\n&gt;&gt;&gt; class TextNode(BaseNode):\n...     text: str = Field(default=\"\", description=\"Text content of the node\")\n...\n...     @classmethod\n...     def get_type(cls) -&gt; str:\n...         return \"text\"\n...\n...     def get_content(self, metadata_mode: MetadataMode = MetadataMode.ALL) -&gt; str:\n...         metadata_str = self.get_metadata_str(mode=metadata_mode)\n...         return f\"{metadata_str}\\\\n{self.text}\" if metadata_str else self.text\n...\n...     def set_content(self, value: str) -&gt; None:\n...         self.text = value\n...\n...     @property\n...     def hash(self) -&gt; str:\n...         return hashlib.sha256(self.text.encode()).hexdigest()\n</code></pre><ul> <li>Create a node with metadata <pre><code>&gt;&gt;&gt; node = TextNode(\n...     text=\"Hello world\",\n...     metadata={\"page\": 1, \"author\": \"Alice\"}\n... )\n&gt;&gt;&gt; node.get_type()\n'text'\n&gt;&gt;&gt; node.get_content(metadata_mode=MetadataMode.NONE)\n'Hello world'\n</code></pre></li> </ul> </li> <li>Using metadata exclusion for different contexts     <pre><code>&gt;&gt;&gt; node = TextNode(\n...     text=\"Sensitive content\",\n...     metadata={\"public\": \"yes\", \"internal_id\": \"secret123\"},\n...     excluded_llm_metadata_keys=[\"internal_id\"]\n... )\n</code></pre><ul> <li>For LLM context (excludes internal_id) <pre><code>&gt;&gt;&gt; content_for_llm = node.get_content(metadata_mode=MetadataMode.LLM)\n&gt;&gt;&gt; \"internal_id\" in content_for_llm\nFalse\n&gt;&gt;&gt; \"public\" in content_for_llm\nTrue\n</code></pre></li> </ul> </li> <li>Setting up node relationships     <pre><code>&gt;&gt;&gt; parent = NodeInfo(id=\"parent-doc\", type=\"document\")\n&gt;&gt;&gt; child = NodeInfo(id=\"child-chunk\", type=\"text\")\n&gt;&gt;&gt;\n&gt;&gt;&gt; node = TextNode(\n...     text=\"Child content\",\n...     links={NodeType.PARENT: parent, NodeType.SOURCE: parent}\n... )\n&gt;&gt;&gt; node.linked_nodes.parent.id\n'parent-doc'\n&gt;&gt;&gt; node.source_id\n'parent-doc'\n</code></pre></li> <li>Working with embeddings     <pre><code>&gt;&gt;&gt; node = TextNode(text=\"Sample text\")\n&gt;&gt;&gt; node.embedding = [0.1, 0.2, 0.3, 0.4, 0.5]\n&gt;&gt;&gt; embedding_vec = node.get_embedding()\n&gt;&gt;&gt; len(embedding_vec)\n5\n&gt;&gt;&gt; embedding_vec[0]\n0.1\n</code></pre></li> </ul> See Also <p>NodeInfo: Lightweight reference to a node. LinkedNodes: Container for node relationships. MetadataMode: Controls metadata inclusion in different contexts. SerializableModel: Base class providing serialization capabilities.</p> Source code in <code>libs/core/src/serapeum/core/base/embeddings/types.py</code> <pre><code>class BaseNode(SerializableModel, ABC):\n    r\"\"\"Abstract base class for document nodes with metadata and relationship management.\n\n    BaseNode provides the foundational functionality for representing chunks of\n    documents with rich metadata, embeddings, and hierarchical relationships. It\n    supports selective metadata inclusion for different contexts (LLM vs embeddings),\n    automatic change detection via hashing, and efficient relationship caching.\n\n    Key features:\n    - Automatic UUID generation for node identification\n    - Metadata management with selective inclusion/exclusion for LLM and embedding contexts\n    - Relationship tracking (source, parent, children, previous, next)\n    - Embedding storage and retrieval\n    - Cached LinkedNodes computation with automatic invalidation\n    - Customizable metadata formatting and serialization\n\n    Attributes:\n        id: Unique identifier for the node (auto-generated UUID if not provided).\n        embedding: Optional vector embedding for the node's content.\n        metadata: Flat dictionary of metadata fields used for context and filtering.\n        excluded_embed_metadata_keys: Metadata keys excluded from embedding context.\n        excluded_llm_metadata_keys: Metadata keys excluded from LLM context.\n        links: Dictionary mapping NodeType to NodeInfo for relationships.\n        metadata_template: Template string for formatting metadata (default: \"{key}: {value}\").\n        metadata_separator: Separator between metadata fields (default: newline).\n\n    Note:\n        This is an abstract base class. Subclasses must implement:\n        - get_type(): Return the node's content type identifier\n        - get_content(): Return the node's content with optional metadata\n        - set_content(): Update the node's content\n        - hash: Property returning the content hash for change detection\n\n    Examples:\n        - Creating a concrete node subclass\n            ```python\n            &gt;&gt;&gt; from serapeum.core.base.embeddings.types import BaseNode, MetadataMode, NodeType, NodeInfo\n            &gt;&gt;&gt; import hashlib\n            &gt;&gt;&gt; from pydantic import Field\n            &gt;&gt;&gt;\n            &gt;&gt;&gt; class TextNode(BaseNode):\n            ...     text: str = Field(default=\"\", description=\"Text content of the node\")\n            ...\n            ...     @classmethod\n            ...     def get_type(cls) -&gt; str:\n            ...         return \"text\"\n            ...\n            ...     def get_content(self, metadata_mode: MetadataMode = MetadataMode.ALL) -&gt; str:\n            ...         metadata_str = self.get_metadata_str(mode=metadata_mode)\n            ...         return f\"{metadata_str}\\\\n{self.text}\" if metadata_str else self.text\n            ...\n            ...     def set_content(self, value: str) -&gt; None:\n            ...         self.text = value\n            ...\n            ...     @property\n            ...     def hash(self) -&gt; str:\n            ...         return hashlib.sha256(self.text.encode()).hexdigest()\n\n            ```\n            - Create a node with metadata\n            ```python\n            &gt;&gt;&gt; node = TextNode(\n            ...     text=\"Hello world\",\n            ...     metadata={\"page\": 1, \"author\": \"Alice\"}\n            ... )\n            &gt;&gt;&gt; node.get_type()\n            'text'\n            &gt;&gt;&gt; node.get_content(metadata_mode=MetadataMode.NONE)\n            'Hello world'\n\n            ```\n        - Using metadata exclusion for different contexts\n            ```python\n            &gt;&gt;&gt; node = TextNode(\n            ...     text=\"Sensitive content\",\n            ...     metadata={\"public\": \"yes\", \"internal_id\": \"secret123\"},\n            ...     excluded_llm_metadata_keys=[\"internal_id\"]\n            ... )\n\n            ```\n            - For LLM context (excludes internal_id)\n            ```python\n            &gt;&gt;&gt; content_for_llm = node.get_content(metadata_mode=MetadataMode.LLM)\n            &gt;&gt;&gt; \"internal_id\" in content_for_llm\n            False\n            &gt;&gt;&gt; \"public\" in content_for_llm\n            True\n\n            ```\n        - Setting up node relationships\n            ```python\n            &gt;&gt;&gt; parent = NodeInfo(id=\"parent-doc\", type=\"document\")\n            &gt;&gt;&gt; child = NodeInfo(id=\"child-chunk\", type=\"text\")\n            &gt;&gt;&gt;\n            &gt;&gt;&gt; node = TextNode(\n            ...     text=\"Child content\",\n            ...     links={NodeType.PARENT: parent, NodeType.SOURCE: parent}\n            ... )\n            &gt;&gt;&gt; node.linked_nodes.parent.id\n            'parent-doc'\n            &gt;&gt;&gt; node.source_id\n            'parent-doc'\n\n            ```\n        - Working with embeddings\n            ```python\n            &gt;&gt;&gt; node = TextNode(text=\"Sample text\")\n            &gt;&gt;&gt; node.embedding = [0.1, 0.2, 0.3, 0.4, 0.5]\n            &gt;&gt;&gt; embedding_vec = node.get_embedding()\n            &gt;&gt;&gt; len(embedding_vec)\n            5\n            &gt;&gt;&gt; embedding_vec[0]\n            0.1\n\n            ```\n\n    See Also:\n        NodeInfo: Lightweight reference to a node.\n        LinkedNodes: Container for node relationships.\n        MetadataMode: Controls metadata inclusion in different contexts.\n        SerializableModel: Base class providing serialization capabilities.\n    \"\"\"\n\n    # hash is computed on a local field, during the validation process\n    model_config = ConfigDict(populate_by_name=True, validate_assignment=True)\n\n    id: str = Field(\n        default_factory=lambda: str(uuid.uuid4()), description=\"Unique ID of the node.\"\n    )\n    embedding: list[float] | None = Field(\n        default=None, description=\"Embedding of the node.\"\n    )\n\n    metadata: dict[str, Any] = Field(\n        default_factory=dict,\n        description=\"A flat dictionary of metadata fields\",\n    )\n    excluded_embed_metadata_keys: list[str] = Field(\n        default_factory=list,\n        description=\"Metadata keys that are excluded from text for the embed model.\",\n    )\n    excluded_llm_metadata_keys: list[str] = Field(\n        default_factory=list,\n        description=\"Metadata keys that are excluded from text for the LLM.\",\n    )\n    links: dict[\n        Annotated[NodeType, EnumNameSerializer],\n        NodeInfoType,\n    ] = Field(\n        default_factory=dict,\n        description=\"A mapping of links to other nodes.\",\n    )\n    metadata_template: str = Field(\n        default=DEFAULT_METADATA_TMPL,\n        description=(\n            \"Template for how metadata is formatted, with {key} and \"\n            \"{value} placeholders.\"\n        ),\n    )\n    metadata_separator: str = Field(\n        default=\"\\n\",\n        description=\"Separator between metadata fields when converting to string.\",\n    )\n\n    linked_nodes_cache: LinkedNodes | None = Field(\n        default=None,\n        exclude=True,\n        repr=False,\n        description=\"Cached LinkedNodes object, invalidated when links change.\",\n    )\n\n    # Track the links dict id to detect changes\n    links_dict_id: int | None = Field(\n        default=None,\n        exclude=True,\n        repr=False,\n        description=\"ID of the links dict to detect when it's reassigned.\",\n    )\n\n    @model_validator(mode=\"after\")\n    def _invalidate_linked_nodes_cache_on_links_change(self) -&gt; \"BaseNode\":\n        \"\"\"Invalidate the linked_nodes cache when links dict is reassigned.\n\n        This validator tracks the id of the links dict. When it changes\n        (i.e., links is reassigned), the cache is cleared.\n\n        Uses Pydantic v2's @model_validator with object.__setattr__ to avoid recursion.\n        \"\"\"\n        current_links_id = id(self.links)\n\n        # Check if links dict was reassigned (different id)\n        if self.links_dict_id is None or self.links_dict_id != current_links_id:\n            # Links changed, clear cache and update tracked id\n            object.__setattr__(self, \"linked_nodes_cache\", None)\n            object.__setattr__(self, \"links_dict_id\", current_links_id)\n\n        return self\n\n    @classmethod\n    @abstractmethod\n    def get_type(cls) -&gt; str:\n        \"\"\"Get Object type.\"\"\"\n\n    @abstractmethod\n    def get_content(self, metadata_mode: MetadataMode = MetadataMode.ALL) -&gt; str:\n        \"\"\"Get object content.\"\"\"\n\n    def get_metadata_str(self, mode: MetadataMode = MetadataMode.ALL) -&gt; str:\n        \"\"\"Metadata info string.\"\"\"\n        if mode == MetadataMode.NONE:\n            return \"\"\n\n        excluded = set()\n        if mode == MetadataMode.LLM:\n            excluded = set(self.excluded_llm_metadata_keys)\n        elif mode == MetadataMode.EMBED:\n            excluded = set(self.excluded_embed_metadata_keys)\n\n        filtered = (\n            self.metadata.items()\n            if not excluded\n            else (\n                (key, value)\n                for key, value in self.metadata.items()\n                if key not in excluded\n            )\n        )\n        return self.metadata_separator.join(\n            self.metadata_template.format(key=key, value=str(value))\n            for key, value in filtered\n        )\n\n    @abstractmethod\n    def set_content(self, value: Any) -&gt; None:\n        \"\"\"Set the content of the node.\"\"\"\n\n    @property\n    @abstractmethod\n    def hash(self) -&gt; str:\n        \"\"\"Get hash of node.\"\"\"\n\n    @property\n    def source_id(self) -&gt; str | None:\n        return self.linked_nodes.source_id\n\n    def _clear_linked_nodes_cache(self) -&gt; None:\n        \"\"\"Manually clear the linked_nodes cache.\n\n        Call this method if you mutate the links dict in-place.\n        This is necessary because Pydantic's field validators only trigger\n        on field assignment, not on in-place mutations.\n\n        Examples:\n            &gt;&gt;&gt; from serapeum.core.base.embeddings.types import BaseNode, NodeInfo, NodeType, MetadataMode\n            &gt;&gt;&gt; import hashlib\n            &gt;&gt;&gt; from pydantic import Field\n            &gt;&gt;&gt; class TextNode(BaseNode):\n            ...     text: str = Field(default=\"\")\n            ...     @classmethod\n            ...     def get_type(cls) -&gt; str:\n            ...         return \"text\"\n            ...     def get_content(self, metadata_mode=MetadataMode.ALL) -&gt; str:\n            ...         return self.text\n            ...     def set_content(self, value: str) -&gt; None:\n            ...         self.text = value\n            ...     @property\n            ...     def hash(self) -&gt; str:\n            ...         return hashlib.sha256(self.text.encode()).hexdigest()\n            &gt;&gt;&gt; node = TextNode(text=\"Sample\", links={})\n            &gt;&gt;&gt; new_source = NodeInfo(id=\"updated-source\", type=\"document\")\n            &gt;&gt;&gt; node.links[NodeType.SOURCE] = new_source\n            &gt;&gt;&gt; node._clear_linked_nodes_cache()\n            &gt;&gt;&gt; node.linked_nodes.source.id\n            'updated-source'\n        \"\"\"\n        self.linked_nodes_cache = None\n\n    @property\n    def linked_nodes(self) -&gt; LinkedNodes:\n        \"\"\"Get linked nodes from the links dictionary.\n\n        This property validates and converts the links dictionary into a\n        LinkedNodes object. The result is cached and automatically invalidated\n        when the links field is reassigned through Pydantic's field validation.\n\n        Returns:\n            LinkedNodes: A validated and cached LinkedNodes object.\n\n        Note:\n            - Cache is automatically cleared when `links` is reassigned\n            - For in-place mutations (e.g., node.links[key] = value), you must\n              either reassign the entire dict OR call _clear_linked_nodes_cache()\n            - Uses Pydantic's @field_validator to manage cache invalidation\n\n        Examples:\n            &gt;&gt;&gt; from serapeum.core.base.embeddings.types import BaseNode, NodeInfo, NodeType, MetadataMode\n            &gt;&gt;&gt; import hashlib\n            &gt;&gt;&gt; from pydantic import Field\n            &gt;&gt;&gt; class TextNode(BaseNode):\n            ...     text: str = Field(default=\"\")\n            ...     @classmethod\n            ...     def get_type(cls) -&gt; str:\n            ...         return \"text\"\n            ...     def get_content(self, metadata_mode=MetadataMode.ALL) -&gt; str:\n            ...         return self.text\n            ...     def set_content(self, value: str) -&gt; None:\n            ...         self.text = value\n            ...     @property\n            ...     def hash(self) -&gt; str:\n            ...         return hashlib.sha256(self.text.encode()).hexdigest()\n            &gt;&gt;&gt; node = TextNode(text=\"Sample\")\n            &gt;&gt;&gt; source_ref = NodeInfo(id=\"doc-123\", type=\"document\")\n            &gt;&gt;&gt; node.links = {NodeType.SOURCE: source_ref}\n            &gt;&gt;&gt; node.linked_nodes.source.id\n            'doc-123'\n\n            &gt;&gt;&gt; node = TextNode(text=\"Sample\", links={})\n            &gt;&gt;&gt; prev_ref = NodeInfo(id=\"prev-chunk\", type=\"text\")\n            &gt;&gt;&gt; node.links[NodeType.PREVIOUS] = prev_ref\n            &gt;&gt;&gt; node._clear_linked_nodes_cache()\n            &gt;&gt;&gt; node.linked_nodes.previous.id\n            'prev-chunk'\n\n            &gt;&gt;&gt; node = TextNode(text=\"Sample\")\n            &gt;&gt;&gt; parent = NodeInfo(id=\"parent-1\", type=\"document\")\n            &gt;&gt;&gt; child1 = NodeInfo(id=\"child-1\", type=\"text\")\n            &gt;&gt;&gt; child2 = NodeInfo(id=\"child-2\", type=\"text\")\n            &gt;&gt;&gt; node.links = {NodeType.PARENT: parent, NodeType.CHILD: [child1, child2]}\n            &gt;&gt;&gt; node.linked_nodes.parent.id\n            'parent-1'\n            &gt;&gt;&gt; len(node.linked_nodes.children)\n            2\n        \"\"\"\n        if self.linked_nodes_cache is None:\n            # Compute and cache the LinkedNodes\n            self.linked_nodes_cache = LinkedNodes.create(self.links)\n        return self.linked_nodes_cache\n\n    def __str__(self) -&gt; str:\n        \"\"\"str\"\"\"\n        source_text_truncated = truncate_text(\n            self.get_content().strip(), TRUNCATE_LENGTH\n        )\n        source_text_wrapped = textwrap.fill(\n            f\"Text: {source_text_truncated}\\n\", width=WRAP_WIDTH\n        )\n        return f\"Node ID: {self.id}\\n{source_text_wrapped}\"\n\n    def get_embedding(self) -&gt; list[float]:\n        \"\"\"Get embedding.\n\n        Raises:\n            ValueErrors if embedding is None.\n        \"\"\"\n        if self.embedding is None:\n            raise ValueError(\"embedding not set.\")\n        return self.embedding\n\n    def get_node_info(self) -&gt; NodeInfo:\n        \"\"\"Get node info.\"\"\"\n        return NodeInfo(\n            id=self.id,\n            type=self.get_type(),\n            metadata=self.metadata,\n            hash=self.hash,\n        )\n</code></pre>"},{"location":"reference/core/embeddings/module/#serapeum.core.embeddings.BaseNode.hash","title":"<code>hash</code>  <code>abstractmethod</code> <code>property</code>","text":"<p>Get hash of node.</p>"},{"location":"reference/core/embeddings/module/#serapeum.core.embeddings.BaseNode.linked_nodes","title":"<code>linked_nodes</code>  <code>property</code>","text":"<p>Get linked nodes from the links dictionary.</p> <p>This property validates and converts the links dictionary into a LinkedNodes object. The result is cached and automatically invalidated when the links field is reassigned through Pydantic's field validation.</p> <p>Returns:</p> Name Type Description <code>LinkedNodes</code> <code>LinkedNodes</code> <p>A validated and cached LinkedNodes object.</p> Note <ul> <li>Cache is automatically cleared when <code>links</code> is reassigned</li> <li>For in-place mutations (e.g., node.links[key] = value), you must   either reassign the entire dict OR call _clear_linked_nodes_cache()</li> <li>Uses Pydantic's @field_validator to manage cache invalidation</li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from serapeum.core.base.embeddings.types import BaseNode, NodeInfo, NodeType, MetadataMode\n&gt;&gt;&gt; import hashlib\n&gt;&gt;&gt; from pydantic import Field\n&gt;&gt;&gt; class TextNode(BaseNode):\n...     text: str = Field(default=\"\")\n...     @classmethod\n...     def get_type(cls) -&gt; str:\n...         return \"text\"\n...     def get_content(self, metadata_mode=MetadataMode.ALL) -&gt; str:\n...         return self.text\n...     def set_content(self, value: str) -&gt; None:\n...         self.text = value\n...     @property\n...     def hash(self) -&gt; str:\n...         return hashlib.sha256(self.text.encode()).hexdigest()\n&gt;&gt;&gt; node = TextNode(text=\"Sample\")\n&gt;&gt;&gt; source_ref = NodeInfo(id=\"doc-123\", type=\"document\")\n&gt;&gt;&gt; node.links = {NodeType.SOURCE: source_ref}\n&gt;&gt;&gt; node.linked_nodes.source.id\n'doc-123'\n</code></pre> <pre><code>&gt;&gt;&gt; node = TextNode(text=\"Sample\", links={})\n&gt;&gt;&gt; prev_ref = NodeInfo(id=\"prev-chunk\", type=\"text\")\n&gt;&gt;&gt; node.links[NodeType.PREVIOUS] = prev_ref\n&gt;&gt;&gt; node._clear_linked_nodes_cache()\n&gt;&gt;&gt; node.linked_nodes.previous.id\n'prev-chunk'\n</code></pre> <pre><code>&gt;&gt;&gt; node = TextNode(text=\"Sample\")\n&gt;&gt;&gt; parent = NodeInfo(id=\"parent-1\", type=\"document\")\n&gt;&gt;&gt; child1 = NodeInfo(id=\"child-1\", type=\"text\")\n&gt;&gt;&gt; child2 = NodeInfo(id=\"child-2\", type=\"text\")\n&gt;&gt;&gt; node.links = {NodeType.PARENT: parent, NodeType.CHILD: [child1, child2]}\n&gt;&gt;&gt; node.linked_nodes.parent.id\n'parent-1'\n&gt;&gt;&gt; len(node.linked_nodes.children)\n2\n</code></pre>"},{"location":"reference/core/embeddings/module/#serapeum.core.embeddings.BaseNode.__str__","title":"<code>__str__()</code>","text":"<p>str</p> Source code in <code>libs/core/src/serapeum/core/base/embeddings/types.py</code> <pre><code>def __str__(self) -&gt; str:\n    \"\"\"str\"\"\"\n    source_text_truncated = truncate_text(\n        self.get_content().strip(), TRUNCATE_LENGTH\n    )\n    source_text_wrapped = textwrap.fill(\n        f\"Text: {source_text_truncated}\\n\", width=WRAP_WIDTH\n    )\n    return f\"Node ID: {self.id}\\n{source_text_wrapped}\"\n</code></pre>"},{"location":"reference/core/embeddings/module/#serapeum.core.embeddings.BaseNode.get_content","title":"<code>get_content(metadata_mode=MetadataMode.ALL)</code>  <code>abstractmethod</code>","text":"<p>Get object content.</p> Source code in <code>libs/core/src/serapeum/core/base/embeddings/types.py</code> <pre><code>@abstractmethod\ndef get_content(self, metadata_mode: MetadataMode = MetadataMode.ALL) -&gt; str:\n    \"\"\"Get object content.\"\"\"\n</code></pre>"},{"location":"reference/core/embeddings/module/#serapeum.core.embeddings.BaseNode.get_embedding","title":"<code>get_embedding()</code>","text":"<p>Get embedding.</p> Source code in <code>libs/core/src/serapeum/core/base/embeddings/types.py</code> <pre><code>def get_embedding(self) -&gt; list[float]:\n    \"\"\"Get embedding.\n\n    Raises:\n        ValueErrors if embedding is None.\n    \"\"\"\n    if self.embedding is None:\n        raise ValueError(\"embedding not set.\")\n    return self.embedding\n</code></pre>"},{"location":"reference/core/embeddings/module/#serapeum.core.embeddings.BaseNode.get_metadata_str","title":"<code>get_metadata_str(mode=MetadataMode.ALL)</code>","text":"<p>Metadata info string.</p> Source code in <code>libs/core/src/serapeum/core/base/embeddings/types.py</code> <pre><code>def get_metadata_str(self, mode: MetadataMode = MetadataMode.ALL) -&gt; str:\n    \"\"\"Metadata info string.\"\"\"\n    if mode == MetadataMode.NONE:\n        return \"\"\n\n    excluded = set()\n    if mode == MetadataMode.LLM:\n        excluded = set(self.excluded_llm_metadata_keys)\n    elif mode == MetadataMode.EMBED:\n        excluded = set(self.excluded_embed_metadata_keys)\n\n    filtered = (\n        self.metadata.items()\n        if not excluded\n        else (\n            (key, value)\n            for key, value in self.metadata.items()\n            if key not in excluded\n        )\n    )\n    return self.metadata_separator.join(\n        self.metadata_template.format(key=key, value=str(value))\n        for key, value in filtered\n    )\n</code></pre>"},{"location":"reference/core/embeddings/module/#serapeum.core.embeddings.BaseNode.get_node_info","title":"<code>get_node_info()</code>","text":"<p>Get node info.</p> Source code in <code>libs/core/src/serapeum/core/base/embeddings/types.py</code> <pre><code>def get_node_info(self) -&gt; NodeInfo:\n    \"\"\"Get node info.\"\"\"\n    return NodeInfo(\n        id=self.id,\n        type=self.get_type(),\n        metadata=self.metadata,\n        hash=self.hash,\n    )\n</code></pre>"},{"location":"reference/core/embeddings/module/#serapeum.core.embeddings.BaseNode.get_type","title":"<code>get_type()</code>  <code>abstractmethod</code> <code>classmethod</code>","text":"<p>Get Object type.</p> Source code in <code>libs/core/src/serapeum/core/base/embeddings/types.py</code> <pre><code>@classmethod\n@abstractmethod\ndef get_type(cls) -&gt; str:\n    \"\"\"Get Object type.\"\"\"\n</code></pre>"},{"location":"reference/core/embeddings/module/#serapeum.core.embeddings.BaseNode.set_content","title":"<code>set_content(value)</code>  <code>abstractmethod</code>","text":"<p>Set the content of the node.</p> Source code in <code>libs/core/src/serapeum/core/base/embeddings/types.py</code> <pre><code>@abstractmethod\ndef set_content(self, value: Any) -&gt; None:\n    \"\"\"Set the content of the node.\"\"\"\n</code></pre>"},{"location":"reference/core/embeddings/module/#serapeum.core.embeddings.CallMixin","title":"<code>CallMixin</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Base class for node transformation components.</p> <p>CallMixin defines the interface for components that transform sequences of nodes, such as embedders, parsers, or metadata enrichers. It provides both synchronous and asynchronous calling interfaces.</p> <p>The mixin uses callable syntax (<code>obj(nodes)</code>) for synchronous transforms and <code>obj.acall(nodes)</code> for asynchronous transforms, enabling composable pipelines.</p> <p>Attributes:</p> Name Type Description <code>model_config</code> <p>Pydantic configuration allowing arbitrary types in subclasses.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from serapeum.core.base.embeddings.types import CallMixin, BaseNode, MetadataMode\n&gt;&gt;&gt; from typing import Sequence, Any\n&gt;&gt;&gt; import hashlib\n&gt;&gt;&gt; from pydantic import Field\n&gt;&gt;&gt; class TextNode(BaseNode):\n...     text: str = Field(default=\"\")\n...     @classmethod\n...     def get_type(cls) -&gt; str:\n...         return \"text\"\n...     def get_content(self, metadata_mode=MetadataMode.ALL) -&gt; str:\n...         return self.text\n...     def set_content(self, value: str) -&gt; None:\n...         self.text = value\n...     @property\n...     def hash(self) -&gt; str:\n...         return hashlib.sha256(self.text.encode()).hexdigest()\n&gt;&gt;&gt; class UppercaseTransform(CallMixin):\n...     def __call__(self, nodes: Sequence[BaseNode], **kwargs: Any) -&gt; Sequence[BaseNode]:\n...         result = []\n...         for node in nodes:\n...             node.set_content(node.get_content().upper())\n...             result.append(node)\n...         return result\n&gt;&gt;&gt; transformer = UppercaseTransform()\n&gt;&gt;&gt; nodes = [TextNode(text=\"hello\"), TextNode(text=\"world\")]\n&gt;&gt;&gt; transformed = transformer(nodes)\n&gt;&gt;&gt; transformed[0].get_content()\n'HELLO'\n&gt;&gt;&gt; transformed[1].get_content()\n'WORLD'\n</code></pre> See Also <p>BaseEmbedding: Uses CallMixin to enable embedding nodes. BaseNode: The node type that this mixin transforms.</p> Source code in <code>libs/core/src/serapeum/core/base/embeddings/types.py</code> <pre><code>class CallMixin(ABC):\n    \"\"\"Base class for node transformation components.\n\n    CallMixin defines the interface for components that transform sequences of nodes,\n    such as embedders, parsers, or metadata enrichers. It provides both synchronous\n    and asynchronous calling interfaces.\n\n    The mixin uses callable syntax (`obj(nodes)`) for synchronous transforms and\n    `obj.acall(nodes)` for asynchronous transforms, enabling composable pipelines.\n\n    Attributes:\n        model_config: Pydantic configuration allowing arbitrary types in subclasses.\n\n    Examples:\n        &gt;&gt;&gt; from serapeum.core.base.embeddings.types import CallMixin, BaseNode, MetadataMode\n        &gt;&gt;&gt; from typing import Sequence, Any\n        &gt;&gt;&gt; import hashlib\n        &gt;&gt;&gt; from pydantic import Field\n        &gt;&gt;&gt; class TextNode(BaseNode):\n        ...     text: str = Field(default=\"\")\n        ...     @classmethod\n        ...     def get_type(cls) -&gt; str:\n        ...         return \"text\"\n        ...     def get_content(self, metadata_mode=MetadataMode.ALL) -&gt; str:\n        ...         return self.text\n        ...     def set_content(self, value: str) -&gt; None:\n        ...         self.text = value\n        ...     @property\n        ...     def hash(self) -&gt; str:\n        ...         return hashlib.sha256(self.text.encode()).hexdigest()\n        &gt;&gt;&gt; class UppercaseTransform(CallMixin):\n        ...     def __call__(self, nodes: Sequence[BaseNode], **kwargs: Any) -&gt; Sequence[BaseNode]:\n        ...         result = []\n        ...         for node in nodes:\n        ...             node.set_content(node.get_content().upper())\n        ...             result.append(node)\n        ...         return result\n        &gt;&gt;&gt; transformer = UppercaseTransform()\n        &gt;&gt;&gt; nodes = [TextNode(text=\"hello\"), TextNode(text=\"world\")]\n        &gt;&gt;&gt; transformed = transformer(nodes)\n        &gt;&gt;&gt; transformed[0].get_content()\n        'HELLO'\n        &gt;&gt;&gt; transformed[1].get_content()\n        'WORLD'\n\n    See Also:\n        BaseEmbedding: Uses CallMixin to enable embedding nodes.\n        BaseNode: The node type that this mixin transforms.\n    \"\"\"\n\n    model_config = ConfigDict(arbitrary_types_allowed=True)\n\n    @abstractmethod\n    def __call__(self, nodes: Sequence[BaseNode], **kwargs: Any) -&gt; Sequence[BaseNode]:\n        \"\"\"Transform a sequence of nodes synchronously.\n\n        Subclasses must implement this method to define their transformation logic.\n        This method is called when the object is invoked directly: `obj(nodes)`.\n\n        Args:\n            nodes: Sequence of BaseNode instances to transform.\n            **kwargs: Additional keyword arguments specific to the transformation.\n\n        Returns:\n            Transformed sequence of BaseNode instances.\n\n        Examples:\n            &gt;&gt;&gt; from serapeum.core.base.embeddings.types import CallMixin, BaseNode, MetadataMode\n            &gt;&gt;&gt; import hashlib\n            &gt;&gt;&gt; from pydantic import Field\n            &gt;&gt;&gt; class TextNode(BaseNode):\n            ...     text: str = Field(default=\"\")\n            ...     @classmethod\n            ...     def get_type(cls) -&gt; str:\n            ...         return \"text\"\n            ...     def get_content(self, metadata_mode=MetadataMode.ALL) -&gt; str:\n            ...         return self.text\n            ...     def set_content(self, value: str) -&gt; None:\n            ...         self.text = value\n            ...     @property\n            ...     def hash(self) -&gt; str:\n            ...         return hashlib.sha256(self.text.encode()).hexdigest()\n            &gt;&gt;&gt; class MetadataAdder(CallMixin):\n            ...     def __call__(self, nodes, **kwargs):\n            ...         result = []\n            ...         for i, node in enumerate(nodes):\n            ...             node.metadata[\"index\"] = i\n            ...             result.append(node)\n            ...         return result\n            &gt;&gt;&gt; adder = MetadataAdder()\n            &gt;&gt;&gt; nodes = [TextNode(text=\"first\"), TextNode(text=\"second\")]\n            &gt;&gt;&gt; processed = adder(nodes)\n            &gt;&gt;&gt; processed[0].metadata[\"index\"]\n            0\n            &gt;&gt;&gt; processed[1].metadata[\"index\"]\n            1\n\n        Note:\n            Implementations should preserve node identity where possible and\n            avoid mutating input nodes unless explicitly documented.\n        \"\"\"\n\n    async def acall(\n        self, nodes: Sequence[BaseNode], **kwargs: Any\n    ) -&gt; Sequence[BaseNode]:\n        \"\"\"Transform a sequence of nodes asynchronously.\n\n        Default implementation delegates to synchronous `__call__`. Subclasses\n        can override this for true async implementations (e.g., async API calls).\n\n        Args:\n            nodes: Sequence of BaseNode instances to transform.\n            **kwargs: Additional keyword arguments specific to the transformation.\n\n        Returns:\n            Transformed sequence of BaseNode instances.\n\n        Examples:\n            &gt;&gt;&gt; import asyncio\n            &gt;&gt;&gt; from serapeum.core.base.embeddings.types import CallMixin, BaseNode, MetadataMode\n            &gt;&gt;&gt; import hashlib\n            &gt;&gt;&gt; from pydantic import Field\n            &gt;&gt;&gt; class TextNode(BaseNode):\n            ...     text: str = Field(default=\"\")\n            ...     @classmethod\n            ...     def get_type(cls) -&gt; str:\n            ...         return \"text\"\n            ...     def get_content(self, metadata_mode=MetadataMode.ALL) -&gt; str:\n            ...         return self.text\n            ...     def set_content(self, value: str) -&gt; None:\n            ...         self.text = value\n            ...     @property\n            ...     def hash(self) -&gt; str:\n            ...         return hashlib.sha256(self.text.encode()).hexdigest()\n            &gt;&gt;&gt; class AsyncTransform(CallMixin):\n            ...     def __call__(self, nodes, **kwargs):\n            ...         return nodes\n            ...     async def acall(self, nodes, **kwargs):\n            ...         await asyncio.sleep(0)\n            ...         for node in nodes:\n            ...             node.metadata[\"async_processed\"] = True\n            ...         return nodes\n            &gt;&gt;&gt; transform = AsyncTransform()\n            &gt;&gt;&gt; nodes = [TextNode(text=\"test\")]\n            &gt;&gt;&gt; result = asyncio.run(transform.acall(nodes))\n            &gt;&gt;&gt; result[0].metadata[\"async_processed\"]\n            True\n\n            &gt;&gt;&gt; class SyncOnlyTransform(CallMixin):\n            ...     def __call__(self, nodes, **kwargs):\n            ...         for node in nodes:\n            ...             node.metadata[\"processed\"] = True\n            ...         return nodes\n            &gt;&gt;&gt; sync_transform = SyncOnlyTransform()\n            &gt;&gt;&gt; nodes = [TextNode(text=\"test\")]\n            &gt;&gt;&gt; result = asyncio.run(sync_transform.acall(nodes))\n            &gt;&gt;&gt; result[0].metadata[\"processed\"]\n            True\n\n        Note:\n            If no true async implementation is needed, the default delegation\n            to `__call__` is sufficient. Override only if the transformation\n            benefits from async/await (e.g., I/O operations).\n        \"\"\"\n        return self.__call__(nodes, **kwargs)\n</code></pre>"},{"location":"reference/core/embeddings/module/#serapeum.core.embeddings.CallMixin.__call__","title":"<code>__call__(nodes, **kwargs)</code>  <code>abstractmethod</code>","text":"<p>Transform a sequence of nodes synchronously.</p> <p>Subclasses must implement this method to define their transformation logic. This method is called when the object is invoked directly: <code>obj(nodes)</code>.</p> <p>Parameters:</p> Name Type Description Default <code>nodes</code> <code>Sequence[BaseNode]</code> <p>Sequence of BaseNode instances to transform.</p> required <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments specific to the transformation.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Sequence[BaseNode]</code> <p>Transformed sequence of BaseNode instances.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from serapeum.core.base.embeddings.types import CallMixin, BaseNode, MetadataMode\n&gt;&gt;&gt; import hashlib\n&gt;&gt;&gt; from pydantic import Field\n&gt;&gt;&gt; class TextNode(BaseNode):\n...     text: str = Field(default=\"\")\n...     @classmethod\n...     def get_type(cls) -&gt; str:\n...         return \"text\"\n...     def get_content(self, metadata_mode=MetadataMode.ALL) -&gt; str:\n...         return self.text\n...     def set_content(self, value: str) -&gt; None:\n...         self.text = value\n...     @property\n...     def hash(self) -&gt; str:\n...         return hashlib.sha256(self.text.encode()).hexdigest()\n&gt;&gt;&gt; class MetadataAdder(CallMixin):\n...     def __call__(self, nodes, **kwargs):\n...         result = []\n...         for i, node in enumerate(nodes):\n...             node.metadata[\"index\"] = i\n...             result.append(node)\n...         return result\n&gt;&gt;&gt; adder = MetadataAdder()\n&gt;&gt;&gt; nodes = [TextNode(text=\"first\"), TextNode(text=\"second\")]\n&gt;&gt;&gt; processed = adder(nodes)\n&gt;&gt;&gt; processed[0].metadata[\"index\"]\n0\n&gt;&gt;&gt; processed[1].metadata[\"index\"]\n1\n</code></pre> Note <p>Implementations should preserve node identity where possible and avoid mutating input nodes unless explicitly documented.</p> Source code in <code>libs/core/src/serapeum/core/base/embeddings/types.py</code> <pre><code>@abstractmethod\ndef __call__(self, nodes: Sequence[BaseNode], **kwargs: Any) -&gt; Sequence[BaseNode]:\n    \"\"\"Transform a sequence of nodes synchronously.\n\n    Subclasses must implement this method to define their transformation logic.\n    This method is called when the object is invoked directly: `obj(nodes)`.\n\n    Args:\n        nodes: Sequence of BaseNode instances to transform.\n        **kwargs: Additional keyword arguments specific to the transformation.\n\n    Returns:\n        Transformed sequence of BaseNode instances.\n\n    Examples:\n        &gt;&gt;&gt; from serapeum.core.base.embeddings.types import CallMixin, BaseNode, MetadataMode\n        &gt;&gt;&gt; import hashlib\n        &gt;&gt;&gt; from pydantic import Field\n        &gt;&gt;&gt; class TextNode(BaseNode):\n        ...     text: str = Field(default=\"\")\n        ...     @classmethod\n        ...     def get_type(cls) -&gt; str:\n        ...         return \"text\"\n        ...     def get_content(self, metadata_mode=MetadataMode.ALL) -&gt; str:\n        ...         return self.text\n        ...     def set_content(self, value: str) -&gt; None:\n        ...         self.text = value\n        ...     @property\n        ...     def hash(self) -&gt; str:\n        ...         return hashlib.sha256(self.text.encode()).hexdigest()\n        &gt;&gt;&gt; class MetadataAdder(CallMixin):\n        ...     def __call__(self, nodes, **kwargs):\n        ...         result = []\n        ...         for i, node in enumerate(nodes):\n        ...             node.metadata[\"index\"] = i\n        ...             result.append(node)\n        ...         return result\n        &gt;&gt;&gt; adder = MetadataAdder()\n        &gt;&gt;&gt; nodes = [TextNode(text=\"first\"), TextNode(text=\"second\")]\n        &gt;&gt;&gt; processed = adder(nodes)\n        &gt;&gt;&gt; processed[0].metadata[\"index\"]\n        0\n        &gt;&gt;&gt; processed[1].metadata[\"index\"]\n        1\n\n    Note:\n        Implementations should preserve node identity where possible and\n        avoid mutating input nodes unless explicitly documented.\n    \"\"\"\n</code></pre>"},{"location":"reference/core/embeddings/module/#serapeum.core.embeddings.CallMixin.acall","title":"<code>acall(nodes, **kwargs)</code>  <code>async</code>","text":"<p>Transform a sequence of nodes asynchronously.</p> <p>Default implementation delegates to synchronous <code>__call__</code>. Subclasses can override this for true async implementations (e.g., async API calls).</p> <p>Parameters:</p> Name Type Description Default <code>nodes</code> <code>Sequence[BaseNode]</code> <p>Sequence of BaseNode instances to transform.</p> required <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments specific to the transformation.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Sequence[BaseNode]</code> <p>Transformed sequence of BaseNode instances.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import asyncio\n&gt;&gt;&gt; from serapeum.core.base.embeddings.types import CallMixin, BaseNode, MetadataMode\n&gt;&gt;&gt; import hashlib\n&gt;&gt;&gt; from pydantic import Field\n&gt;&gt;&gt; class TextNode(BaseNode):\n...     text: str = Field(default=\"\")\n...     @classmethod\n...     def get_type(cls) -&gt; str:\n...         return \"text\"\n...     def get_content(self, metadata_mode=MetadataMode.ALL) -&gt; str:\n...         return self.text\n...     def set_content(self, value: str) -&gt; None:\n...         self.text = value\n...     @property\n...     def hash(self) -&gt; str:\n...         return hashlib.sha256(self.text.encode()).hexdigest()\n&gt;&gt;&gt; class AsyncTransform(CallMixin):\n...     def __call__(self, nodes, **kwargs):\n...         return nodes\n...     async def acall(self, nodes, **kwargs):\n...         await asyncio.sleep(0)\n...         for node in nodes:\n...             node.metadata[\"async_processed\"] = True\n...         return nodes\n&gt;&gt;&gt; transform = AsyncTransform()\n&gt;&gt;&gt; nodes = [TextNode(text=\"test\")]\n&gt;&gt;&gt; result = asyncio.run(transform.acall(nodes))\n&gt;&gt;&gt; result[0].metadata[\"async_processed\"]\nTrue\n</code></pre> <pre><code>&gt;&gt;&gt; class SyncOnlyTransform(CallMixin):\n...     def __call__(self, nodes, **kwargs):\n...         for node in nodes:\n...             node.metadata[\"processed\"] = True\n...         return nodes\n&gt;&gt;&gt; sync_transform = SyncOnlyTransform()\n&gt;&gt;&gt; nodes = [TextNode(text=\"test\")]\n&gt;&gt;&gt; result = asyncio.run(sync_transform.acall(nodes))\n&gt;&gt;&gt; result[0].metadata[\"processed\"]\nTrue\n</code></pre> Note <p>If no true async implementation is needed, the default delegation to <code>__call__</code> is sufficient. Override only if the transformation benefits from async/await (e.g., I/O operations).</p> Source code in <code>libs/core/src/serapeum/core/base/embeddings/types.py</code> <pre><code>async def acall(\n    self, nodes: Sequence[BaseNode], **kwargs: Any\n) -&gt; Sequence[BaseNode]:\n    \"\"\"Transform a sequence of nodes asynchronously.\n\n    Default implementation delegates to synchronous `__call__`. Subclasses\n    can override this for true async implementations (e.g., async API calls).\n\n    Args:\n        nodes: Sequence of BaseNode instances to transform.\n        **kwargs: Additional keyword arguments specific to the transformation.\n\n    Returns:\n        Transformed sequence of BaseNode instances.\n\n    Examples:\n        &gt;&gt;&gt; import asyncio\n        &gt;&gt;&gt; from serapeum.core.base.embeddings.types import CallMixin, BaseNode, MetadataMode\n        &gt;&gt;&gt; import hashlib\n        &gt;&gt;&gt; from pydantic import Field\n        &gt;&gt;&gt; class TextNode(BaseNode):\n        ...     text: str = Field(default=\"\")\n        ...     @classmethod\n        ...     def get_type(cls) -&gt; str:\n        ...         return \"text\"\n        ...     def get_content(self, metadata_mode=MetadataMode.ALL) -&gt; str:\n        ...         return self.text\n        ...     def set_content(self, value: str) -&gt; None:\n        ...         self.text = value\n        ...     @property\n        ...     def hash(self) -&gt; str:\n        ...         return hashlib.sha256(self.text.encode()).hexdigest()\n        &gt;&gt;&gt; class AsyncTransform(CallMixin):\n        ...     def __call__(self, nodes, **kwargs):\n        ...         return nodes\n        ...     async def acall(self, nodes, **kwargs):\n        ...         await asyncio.sleep(0)\n        ...         for node in nodes:\n        ...             node.metadata[\"async_processed\"] = True\n        ...         return nodes\n        &gt;&gt;&gt; transform = AsyncTransform()\n        &gt;&gt;&gt; nodes = [TextNode(text=\"test\")]\n        &gt;&gt;&gt; result = asyncio.run(transform.acall(nodes))\n        &gt;&gt;&gt; result[0].metadata[\"async_processed\"]\n        True\n\n        &gt;&gt;&gt; class SyncOnlyTransform(CallMixin):\n        ...     def __call__(self, nodes, **kwargs):\n        ...         for node in nodes:\n        ...             node.metadata[\"processed\"] = True\n        ...         return nodes\n        &gt;&gt;&gt; sync_transform = SyncOnlyTransform()\n        &gt;&gt;&gt; nodes = [TextNode(text=\"test\")]\n        &gt;&gt;&gt; result = asyncio.run(sync_transform.acall(nodes))\n        &gt;&gt;&gt; result[0].metadata[\"processed\"]\n        True\n\n    Note:\n        If no true async implementation is needed, the default delegation\n        to `__call__` is sufficient. Override only if the transformation\n        benefits from async/await (e.g., I/O operations).\n    \"\"\"\n    return self.__call__(nodes, **kwargs)\n</code></pre>"},{"location":"reference/core/embeddings/module/#serapeum.core.embeddings.LinkedNodes","title":"<code>LinkedNodes</code>","text":"<p>               Bases: <code>SerializableModel</code></p> <p>Immutable container for node relationships in a document hierarchy.</p> <p>LinkedNodes manages references between nodes in a document structure, supporting linear sequences (previous/next), hierarchical relationships (parent/children), and source document tracking. The model is frozen to prevent accidental mutation of relationship structures.</p> <p>Attributes:</p> Name Type Description <code>source</code> <code>NodeInfo | None</code> <p>Reference to the original source document node.</p> <code>previous</code> <code>NodeInfo | None</code> <p>Reference to the previous node in a sequence.</p> <code>next</code> <code>NodeInfo | None</code> <p>Reference to the next node in a sequence.</p> <code>parent</code> <code>NodeInfo | None</code> <p>Reference to the parent node in a hierarchy.</p> <code>children</code> <code>list[NodeInfo] | None</code> <p>List of child node references in a hierarchy.</p> <p>Examples:</p> <ul> <li>Creating a linear sequence of nodes     <pre><code>&gt;&gt;&gt; from serapeum.core.base.embeddings.types import LinkedNodes, NodeInfo, NodeType\n&gt;&gt;&gt; prev_node = NodeInfo(id=\"chunk-1\")\n&gt;&gt;&gt; next_node = NodeInfo(id=\"chunk-3\")\n&gt;&gt;&gt; links = LinkedNodes(previous=prev_node, next=next_node)\n&gt;&gt;&gt; links.previous.id\n'chunk-1'\n</code></pre></li> <li>Building hierarchical relationships     <pre><code>&gt;&gt;&gt; parent = NodeInfo(id=\"section-1\")\n&gt;&gt;&gt; child1 = NodeInfo(id=\"para-1\")\n&gt;&gt;&gt; child2 = NodeInfo(id=\"para-2\")\n&gt;&gt;&gt; links = LinkedNodes(parent=parent, children=[child1, child2])\n&gt;&gt;&gt; len(links.children)\n2\n</code></pre></li> <li>Using factory method with NodeType enum     <pre><code>&gt;&gt;&gt; from serapeum.core.base.embeddings.types import NodeType\n&gt;&gt;&gt; source = NodeInfo(id=\"doc-main\")\n&gt;&gt;&gt; links_dict = {NodeType.SOURCE: source}\n&gt;&gt;&gt; links = LinkedNodes.create(links_dict)\n&gt;&gt;&gt; links.source.id\n'doc-main'\n</code></pre></li> <li>Accessing source ID property     <pre><code>&gt;&gt;&gt; source = NodeInfo(id=\"original-doc\")\n&gt;&gt;&gt; links = LinkedNodes(source=source)\n&gt;&gt;&gt; links.source_id\n'original-doc'\n</code></pre></li> </ul> See Also <p>NodeType: Enum defining relationship types. NodeInfo: References stored in relationship fields. BaseNode.linked_nodes: Property that creates LinkedNodes from links dict.</p> Source code in <code>libs/core/src/serapeum/core/base/embeddings/types.py</code> <pre><code>class LinkedNodes(SerializableModel):\n    \"\"\"Immutable container for node relationships in a document hierarchy.\n\n    LinkedNodes manages references between nodes in a document structure, supporting\n    linear sequences (previous/next), hierarchical relationships (parent/children),\n    and source document tracking. The model is frozen to prevent accidental mutation\n    of relationship structures.\n\n    Attributes:\n        source: Reference to the original source document node.\n        previous: Reference to the previous node in a sequence.\n        next: Reference to the next node in a sequence.\n        parent: Reference to the parent node in a hierarchy.\n        children: List of child node references in a hierarchy.\n\n    Examples:\n        - Creating a linear sequence of nodes\n            ```python\n            &gt;&gt;&gt; from serapeum.core.base.embeddings.types import LinkedNodes, NodeInfo, NodeType\n            &gt;&gt;&gt; prev_node = NodeInfo(id=\"chunk-1\")\n            &gt;&gt;&gt; next_node = NodeInfo(id=\"chunk-3\")\n            &gt;&gt;&gt; links = LinkedNodes(previous=prev_node, next=next_node)\n            &gt;&gt;&gt; links.previous.id\n            'chunk-1'\n\n            ```\n        - Building hierarchical relationships\n            ```python\n            &gt;&gt;&gt; parent = NodeInfo(id=\"section-1\")\n            &gt;&gt;&gt; child1 = NodeInfo(id=\"para-1\")\n            &gt;&gt;&gt; child2 = NodeInfo(id=\"para-2\")\n            &gt;&gt;&gt; links = LinkedNodes(parent=parent, children=[child1, child2])\n            &gt;&gt;&gt; len(links.children)\n            2\n\n            ```\n        - Using factory method with NodeType enum\n            ```python\n            &gt;&gt;&gt; from serapeum.core.base.embeddings.types import NodeType\n            &gt;&gt;&gt; source = NodeInfo(id=\"doc-main\")\n            &gt;&gt;&gt; links_dict = {NodeType.SOURCE: source}\n            &gt;&gt;&gt; links = LinkedNodes.create(links_dict)\n            &gt;&gt;&gt; links.source.id\n            'doc-main'\n\n            ```\n        - Accessing source ID property\n            ```python\n            &gt;&gt;&gt; source = NodeInfo(id=\"original-doc\")\n            &gt;&gt;&gt; links = LinkedNodes(source=source)\n            &gt;&gt;&gt; links.source_id\n            'original-doc'\n\n            ```\n\n    See Also:\n        NodeType: Enum defining relationship types.\n        NodeInfo: References stored in relationship fields.\n        BaseNode.linked_nodes: Property that creates LinkedNodes from links dict.\n    \"\"\"\n\n    model_config = ConfigDict(frozen=True)\n\n    source: NodeInfo | None = None\n    previous: NodeInfo | None = None\n    next: NodeInfo | None = None\n    parent: NodeInfo | None = None\n    children: list[NodeInfo] | None = None\n\n    @field_validator(\"source\", \"previous\", \"next\", \"parent\")\n    @classmethod\n    def validate_single_node(cls, v: Any) -&gt; NodeInfo | None:\n        \"\"\"Validate that single-node fields contain NodeInfo objects.\n\n        Ensures that source, previous, next, and parent fields contain exactly\n        one NodeInfo instance (not a list). Called automatically by Pydantic\n        during model instantiation and validation.\n\n        Args:\n            v: Value to validate, expected to be NodeInfo or None.\n\n        Returns:\n            The validated NodeInfo instance or None.\n\n        Raises:\n            ValueError: If v is not None and not a NodeInfo instance.\n\n        Examples:\n            - Valid single node assignment\n                ```python\n                &gt;&gt;&gt; from serapeum.core.base.embeddings.types import LinkedNodes, NodeInfo\n                &gt;&gt;&gt; node = NodeInfo(id=\"valid\")\n                &gt;&gt;&gt; links = LinkedNodes(source=node)\n                &gt;&gt;&gt; links.source.id\n                'valid'\n\n                ```\n            - Invalid list assignment to single-node field\n                ```python\n                &gt;&gt;&gt; LinkedNodes(source=[NodeInfo(id=\"bad\")])  # doctest: +SKIP\n                Traceback (most recent call last):\n                    ...\n                ValueError: Must be a NodeInfo object, not a list\n\n                ```\n\n        Note:\n            This validator applies to: source, previous, next, parent fields.\n            The children field has a separate validator for list validation.\n        \"\"\"\n        if v is not None and not isinstance(v, NodeInfo):\n            raise ValueError(\"Must be a NodeInfo object, not a list\")\n        return v\n\n    @field_validator(\"children\")\n    @classmethod\n    def validate_children_list(cls, v: Any) -&gt; list[NodeInfo] | None:\n        \"\"\"Validate that children field contains a list of NodeInfo objects.\n\n        Ensures the children field is a list (not a single NodeInfo instance).\n        Called automatically by Pydantic during model instantiation and validation.\n\n        Args:\n            v: Value to validate, expected to be list[NodeInfo] or None.\n\n        Returns:\n            The validated list of NodeInfo instances or None.\n\n        Raises:\n            ValueError: If v is not None and not a list.\n\n        Examples:\n            - Valid children list\n                ```python\n                &gt;&gt;&gt; from serapeum.core.base.embeddings.types import LinkedNodes, NodeInfo\n                &gt;&gt;&gt; child1 = NodeInfo(id=\"child-1\")\n                &gt;&gt;&gt; child2 = NodeInfo(id=\"child-2\")\n                &gt;&gt;&gt; links = LinkedNodes(children=[child1, child2])\n                &gt;&gt;&gt; len(links.children)\n                2\n\n                ```\n            - Invalid single NodeInfo for children\n                ```python\n                &gt;&gt;&gt; LinkedNodes(children=NodeInfo(id=\"bad\"))  # doctest: +SKIP\n                Traceback (most recent call last):\n                    ...\n                ValueError: Children must be a list of NodeInfo objects\n\n                ```\n            - Empty children list is valid\n                ```python\n                &gt;&gt;&gt; links = LinkedNodes(children=[])\n                &gt;&gt;&gt; links.children\n                []\n\n                ```\n\n        Note:\n            This validator is specific to the children field, which represents\n            one-to-many relationships.\n        \"\"\"\n        if v is not None and not isinstance(v, list):\n            raise ValueError(\"Children must be a list of NodeInfo objects\")\n        return v\n\n    @classmethod\n    def create(cls, linked_nodes_info: dict[NodeType, NodeInfoType]) -&gt; \"LinkedNodes\":\n        \"\"\"Create LinkedNodes from a dict mapping NodeType to NodeInfo/list.\n\n        Factory method that converts a dictionary with NodeType keys into a\n        validated LinkedNodes instance. Pydantic validators automatically check\n        that single-node fields contain NodeInfo and children contains a list.\n\n        Args:\n            linked_nodes_info: Dictionary mapping NodeType enum values to either\n                NodeInfo (for single relationships) or list[NodeInfo] (for\n                children). Missing keys are treated as None.\n\n        Returns:\n            A new LinkedNodes instance with validated relationships.\n\n        Raises:\n            ValueError: If a single-node field (SOURCE, PREVIOUS, NEXT, PARENT)\n                receives a list, or if children receives a non-list value.\n\n        Examples:\n            - Creating from a dict with mixed relationships\n                ```python\n                &gt;&gt;&gt; from serapeum.core.base.embeddings.types import LinkedNodes, NodeInfo, NodeType\n                &gt;&gt;&gt; source = NodeInfo(id=\"doc-1\")\n                &gt;&gt;&gt; parent = NodeInfo(id=\"section-1\")\n                &gt;&gt;&gt; children = [NodeInfo(id=\"para-1\"), NodeInfo(id=\"para-2\")]\n                &gt;&gt;&gt; links_dict = {\n                ...     NodeType.SOURCE: source,\n                ...     NodeType.PARENT: parent,\n                ...     NodeType.CHILD: children\n                ... }\n                &gt;&gt;&gt; links = LinkedNodes.create(links_dict)\n                &gt;&gt;&gt; links.source.id\n                'doc-1'\n\n                ```\n            - Creating with only some relationships\n                ```python\n                &gt;&gt;&gt; prev = NodeInfo(id=\"chunk-1\")\n                &gt;&gt;&gt; next_node = NodeInfo(id=\"chunk-3\")\n                &gt;&gt;&gt; links = LinkedNodes.create({\n                ...     NodeType.PREVIOUS: prev,\n                ...     NodeType.NEXT: next_node\n                ... })\n                &gt;&gt;&gt; links.previous.id\n                'chunk-1'\n\n                ```\n            - Empty dict creates all-None instance\n                ```python\n                &gt;&gt;&gt; links = LinkedNodes.create({})\n                &gt;&gt;&gt; links.source is None\n                True\n\n                ```\n\n        See Also:\n            LinkedNodes.as_dict: Inverse operation converting LinkedNodes to dict.\n            NodeType: Enum defining valid relationship types.\n        \"\"\"\n        return cls(\n            source=linked_nodes_info.get(NodeType.SOURCE),\n            previous=linked_nodes_info.get(NodeType.PREVIOUS),\n            next=linked_nodes_info.get(NodeType.NEXT),\n            parent=linked_nodes_info.get(NodeType.PARENT),\n            children=linked_nodes_info.get(NodeType.CHILD),\n        )\n\n    def as_dict(self) -&gt; dict[NodeType, NodeInfoType | None]:\n        \"\"\"Convert LinkedNodes to a dictionary mapping NodeType to NodeInfo.\n\n        Creates a dictionary representation with NodeType enum keys and NodeInfo\n        values. None values are excluded from the result to create a compact\n        representation containing only active relationships.\n\n        Returns:\n            Dictionary with NodeType keys and NodeInfo/list[NodeInfo] values.\n            Only non-None relationships are included.\n\n        Examples:\n            - Converting to dict with multiple relationships\n                ```python\n                &gt;&gt;&gt; from serapeum.core.base.embeddings.types import LinkedNodes, NodeInfo, NodeType\n                &gt;&gt;&gt; source = NodeInfo(id=\"doc-1\")\n                &gt;&gt;&gt; parent = NodeInfo(id=\"section-1\")\n                &gt;&gt;&gt; links = LinkedNodes(source=source, parent=parent)\n                &gt;&gt;&gt; result = links.as_dict()\n                &gt;&gt;&gt; result[NodeType.SOURCE].id\n                'doc-1'\n\n                ```\n            - None values are excluded\n                ```python\n                &gt;&gt;&gt; links = LinkedNodes(source=NodeInfo(id=\"doc-1\"))\n                &gt;&gt;&gt; result = links.as_dict()\n                &gt;&gt;&gt; NodeType.PREVIOUS in result\n                False\n\n                ```\n            - Round-trip with create method\n                ```python\n                &gt;&gt;&gt; original = LinkedNodes(\n                ...     source=NodeInfo(id=\"src\"),\n                ...     children=[NodeInfo(id=\"child-1\")]\n                ... )\n                &gt;&gt;&gt; as_dict = original.as_dict()\n                &gt;&gt;&gt; restored = LinkedNodes.create(as_dict)\n                &gt;&gt;&gt; restored.source.id\n                'src'\n\n                ```\n\n        See Also:\n            LinkedNodes.create: Factory method for creating from dict.\n            BaseNode.links: Uses this format for storing relationships.\n        \"\"\"\n        linked_nodes = {\n            NodeType.SOURCE: self.source,\n            NodeType.PREVIOUS: self.previous,\n            NodeType.NEXT: self.next,\n            NodeType.PARENT: self.parent,\n            NodeType.CHILD: self.children,\n        }\n\n        linked_nodes = {\n            key: value for key, value in linked_nodes.items() if value is not None\n        }\n        return linked_nodes\n\n    @property\n    def source_id(self) -&gt; str | None:\n        \"\"\"Get the ID of the source node if it exists.\n\n        Convenience property for accessing the source node's ID without\n        checking if source is None first.\n\n        Returns:\n            The source node's ID string, or None if no source is set.\n\n        Examples:\n            - Accessing source ID when source exists\n                ```python\n                &gt;&gt;&gt; from serapeum.core.base.embeddings.types import LinkedNodes, NodeInfo\n                &gt;&gt;&gt; source = NodeInfo(id=\"document-123\")\n                &gt;&gt;&gt; links = LinkedNodes(source=source)\n                &gt;&gt;&gt; links.source_id\n                'document-123'\n\n                ```\n            - Accessing when source is None\n                ```python\n                &gt;&gt;&gt; links = LinkedNodes()\n                &gt;&gt;&gt; links.source_id is None\n                True\n\n                ```\n\n        See Also:\n            BaseNode.source_id: Uses this property for node source tracking.\n        \"\"\"\n        source_id = None\n        if self.source is not None:\n            source_id = self.source.id\n        return source_id\n</code></pre>"},{"location":"reference/core/embeddings/module/#serapeum.core.embeddings.LinkedNodes.source_id","title":"<code>source_id</code>  <code>property</code>","text":"<p>Get the ID of the source node if it exists.</p> <p>Convenience property for accessing the source node's ID without checking if source is None first.</p> <p>Returns:</p> Type Description <code>str | None</code> <p>The source node's ID string, or None if no source is set.</p> <p>Examples:</p> <ul> <li>Accessing source ID when source exists     <pre><code>&gt;&gt;&gt; from serapeum.core.base.embeddings.types import LinkedNodes, NodeInfo\n&gt;&gt;&gt; source = NodeInfo(id=\"document-123\")\n&gt;&gt;&gt; links = LinkedNodes(source=source)\n&gt;&gt;&gt; links.source_id\n'document-123'\n</code></pre></li> <li>Accessing when source is None     <pre><code>&gt;&gt;&gt; links = LinkedNodes()\n&gt;&gt;&gt; links.source_id is None\nTrue\n</code></pre></li> </ul> See Also <p>BaseNode.source_id: Uses this property for node source tracking.</p>"},{"location":"reference/core/embeddings/module/#serapeum.core.embeddings.LinkedNodes.as_dict","title":"<code>as_dict()</code>","text":"<p>Convert LinkedNodes to a dictionary mapping NodeType to NodeInfo.</p> <p>Creates a dictionary representation with NodeType enum keys and NodeInfo values. None values are excluded from the result to create a compact representation containing only active relationships.</p> <p>Returns:</p> Type Description <code>dict[NodeType, NodeInfoType | None]</code> <p>Dictionary with NodeType keys and NodeInfo/list[NodeInfo] values.</p> <code>dict[NodeType, NodeInfoType | None]</code> <p>Only non-None relationships are included.</p> <p>Examples:</p> <ul> <li>Converting to dict with multiple relationships     <pre><code>&gt;&gt;&gt; from serapeum.core.base.embeddings.types import LinkedNodes, NodeInfo, NodeType\n&gt;&gt;&gt; source = NodeInfo(id=\"doc-1\")\n&gt;&gt;&gt; parent = NodeInfo(id=\"section-1\")\n&gt;&gt;&gt; links = LinkedNodes(source=source, parent=parent)\n&gt;&gt;&gt; result = links.as_dict()\n&gt;&gt;&gt; result[NodeType.SOURCE].id\n'doc-1'\n</code></pre></li> <li>None values are excluded     <pre><code>&gt;&gt;&gt; links = LinkedNodes(source=NodeInfo(id=\"doc-1\"))\n&gt;&gt;&gt; result = links.as_dict()\n&gt;&gt;&gt; NodeType.PREVIOUS in result\nFalse\n</code></pre></li> <li>Round-trip with create method     <pre><code>&gt;&gt;&gt; original = LinkedNodes(\n...     source=NodeInfo(id=\"src\"),\n...     children=[NodeInfo(id=\"child-1\")]\n... )\n&gt;&gt;&gt; as_dict = original.as_dict()\n&gt;&gt;&gt; restored = LinkedNodes.create(as_dict)\n&gt;&gt;&gt; restored.source.id\n'src'\n</code></pre></li> </ul> See Also <p>LinkedNodes.create: Factory method for creating from dict. BaseNode.links: Uses this format for storing relationships.</p> Source code in <code>libs/core/src/serapeum/core/base/embeddings/types.py</code> <pre><code>def as_dict(self) -&gt; dict[NodeType, NodeInfoType | None]:\n    \"\"\"Convert LinkedNodes to a dictionary mapping NodeType to NodeInfo.\n\n    Creates a dictionary representation with NodeType enum keys and NodeInfo\n    values. None values are excluded from the result to create a compact\n    representation containing only active relationships.\n\n    Returns:\n        Dictionary with NodeType keys and NodeInfo/list[NodeInfo] values.\n        Only non-None relationships are included.\n\n    Examples:\n        - Converting to dict with multiple relationships\n            ```python\n            &gt;&gt;&gt; from serapeum.core.base.embeddings.types import LinkedNodes, NodeInfo, NodeType\n            &gt;&gt;&gt; source = NodeInfo(id=\"doc-1\")\n            &gt;&gt;&gt; parent = NodeInfo(id=\"section-1\")\n            &gt;&gt;&gt; links = LinkedNodes(source=source, parent=parent)\n            &gt;&gt;&gt; result = links.as_dict()\n            &gt;&gt;&gt; result[NodeType.SOURCE].id\n            'doc-1'\n\n            ```\n        - None values are excluded\n            ```python\n            &gt;&gt;&gt; links = LinkedNodes(source=NodeInfo(id=\"doc-1\"))\n            &gt;&gt;&gt; result = links.as_dict()\n            &gt;&gt;&gt; NodeType.PREVIOUS in result\n            False\n\n            ```\n        - Round-trip with create method\n            ```python\n            &gt;&gt;&gt; original = LinkedNodes(\n            ...     source=NodeInfo(id=\"src\"),\n            ...     children=[NodeInfo(id=\"child-1\")]\n            ... )\n            &gt;&gt;&gt; as_dict = original.as_dict()\n            &gt;&gt;&gt; restored = LinkedNodes.create(as_dict)\n            &gt;&gt;&gt; restored.source.id\n            'src'\n\n            ```\n\n    See Also:\n        LinkedNodes.create: Factory method for creating from dict.\n        BaseNode.links: Uses this format for storing relationships.\n    \"\"\"\n    linked_nodes = {\n        NodeType.SOURCE: self.source,\n        NodeType.PREVIOUS: self.previous,\n        NodeType.NEXT: self.next,\n        NodeType.PARENT: self.parent,\n        NodeType.CHILD: self.children,\n    }\n\n    linked_nodes = {\n        key: value for key, value in linked_nodes.items() if value is not None\n    }\n    return linked_nodes\n</code></pre>"},{"location":"reference/core/embeddings/module/#serapeum.core.embeddings.LinkedNodes.create","title":"<code>create(linked_nodes_info)</code>  <code>classmethod</code>","text":"<p>Create LinkedNodes from a dict mapping NodeType to NodeInfo/list.</p> <p>Factory method that converts a dictionary with NodeType keys into a validated LinkedNodes instance. Pydantic validators automatically check that single-node fields contain NodeInfo and children contains a list.</p> <p>Parameters:</p> Name Type Description Default <code>linked_nodes_info</code> <code>dict[NodeType, NodeInfoType]</code> <p>Dictionary mapping NodeType enum values to either NodeInfo (for single relationships) or list[NodeInfo] (for children). Missing keys are treated as None.</p> required <p>Returns:</p> Type Description <code>'LinkedNodes'</code> <p>A new LinkedNodes instance with validated relationships.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If a single-node field (SOURCE, PREVIOUS, NEXT, PARENT) receives a list, or if children receives a non-list value.</p> <p>Examples:</p> <ul> <li>Creating from a dict with mixed relationships     <pre><code>&gt;&gt;&gt; from serapeum.core.base.embeddings.types import LinkedNodes, NodeInfo, NodeType\n&gt;&gt;&gt; source = NodeInfo(id=\"doc-1\")\n&gt;&gt;&gt; parent = NodeInfo(id=\"section-1\")\n&gt;&gt;&gt; children = [NodeInfo(id=\"para-1\"), NodeInfo(id=\"para-2\")]\n&gt;&gt;&gt; links_dict = {\n...     NodeType.SOURCE: source,\n...     NodeType.PARENT: parent,\n...     NodeType.CHILD: children\n... }\n&gt;&gt;&gt; links = LinkedNodes.create(links_dict)\n&gt;&gt;&gt; links.source.id\n'doc-1'\n</code></pre></li> <li>Creating with only some relationships     <pre><code>&gt;&gt;&gt; prev = NodeInfo(id=\"chunk-1\")\n&gt;&gt;&gt; next_node = NodeInfo(id=\"chunk-3\")\n&gt;&gt;&gt; links = LinkedNodes.create({\n...     NodeType.PREVIOUS: prev,\n...     NodeType.NEXT: next_node\n... })\n&gt;&gt;&gt; links.previous.id\n'chunk-1'\n</code></pre></li> <li>Empty dict creates all-None instance     <pre><code>&gt;&gt;&gt; links = LinkedNodes.create({})\n&gt;&gt;&gt; links.source is None\nTrue\n</code></pre></li> </ul> See Also <p>LinkedNodes.as_dict: Inverse operation converting LinkedNodes to dict. NodeType: Enum defining valid relationship types.</p> Source code in <code>libs/core/src/serapeum/core/base/embeddings/types.py</code> <pre><code>@classmethod\ndef create(cls, linked_nodes_info: dict[NodeType, NodeInfoType]) -&gt; \"LinkedNodes\":\n    \"\"\"Create LinkedNodes from a dict mapping NodeType to NodeInfo/list.\n\n    Factory method that converts a dictionary with NodeType keys into a\n    validated LinkedNodes instance. Pydantic validators automatically check\n    that single-node fields contain NodeInfo and children contains a list.\n\n    Args:\n        linked_nodes_info: Dictionary mapping NodeType enum values to either\n            NodeInfo (for single relationships) or list[NodeInfo] (for\n            children). Missing keys are treated as None.\n\n    Returns:\n        A new LinkedNodes instance with validated relationships.\n\n    Raises:\n        ValueError: If a single-node field (SOURCE, PREVIOUS, NEXT, PARENT)\n            receives a list, or if children receives a non-list value.\n\n    Examples:\n        - Creating from a dict with mixed relationships\n            ```python\n            &gt;&gt;&gt; from serapeum.core.base.embeddings.types import LinkedNodes, NodeInfo, NodeType\n            &gt;&gt;&gt; source = NodeInfo(id=\"doc-1\")\n            &gt;&gt;&gt; parent = NodeInfo(id=\"section-1\")\n            &gt;&gt;&gt; children = [NodeInfo(id=\"para-1\"), NodeInfo(id=\"para-2\")]\n            &gt;&gt;&gt; links_dict = {\n            ...     NodeType.SOURCE: source,\n            ...     NodeType.PARENT: parent,\n            ...     NodeType.CHILD: children\n            ... }\n            &gt;&gt;&gt; links = LinkedNodes.create(links_dict)\n            &gt;&gt;&gt; links.source.id\n            'doc-1'\n\n            ```\n        - Creating with only some relationships\n            ```python\n            &gt;&gt;&gt; prev = NodeInfo(id=\"chunk-1\")\n            &gt;&gt;&gt; next_node = NodeInfo(id=\"chunk-3\")\n            &gt;&gt;&gt; links = LinkedNodes.create({\n            ...     NodeType.PREVIOUS: prev,\n            ...     NodeType.NEXT: next_node\n            ... })\n            &gt;&gt;&gt; links.previous.id\n            'chunk-1'\n\n            ```\n        - Empty dict creates all-None instance\n            ```python\n            &gt;&gt;&gt; links = LinkedNodes.create({})\n            &gt;&gt;&gt; links.source is None\n            True\n\n            ```\n\n    See Also:\n        LinkedNodes.as_dict: Inverse operation converting LinkedNodes to dict.\n        NodeType: Enum defining valid relationship types.\n    \"\"\"\n    return cls(\n        source=linked_nodes_info.get(NodeType.SOURCE),\n        previous=linked_nodes_info.get(NodeType.PREVIOUS),\n        next=linked_nodes_info.get(NodeType.NEXT),\n        parent=linked_nodes_info.get(NodeType.PARENT),\n        children=linked_nodes_info.get(NodeType.CHILD),\n    )\n</code></pre>"},{"location":"reference/core/embeddings/module/#serapeum.core.embeddings.LinkedNodes.validate_children_list","title":"<code>validate_children_list(v)</code>  <code>classmethod</code>","text":"<p>Validate that children field contains a list of NodeInfo objects.</p> <p>Ensures the children field is a list (not a single NodeInfo instance). Called automatically by Pydantic during model instantiation and validation.</p> <p>Parameters:</p> Name Type Description Default <code>v</code> <code>Any</code> <p>Value to validate, expected to be list[NodeInfo] or None.</p> required <p>Returns:</p> Type Description <code>list[NodeInfo] | None</code> <p>The validated list of NodeInfo instances or None.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If v is not None and not a list.</p> <p>Examples:</p> <ul> <li>Valid children list     <pre><code>&gt;&gt;&gt; from serapeum.core.base.embeddings.types import LinkedNodes, NodeInfo\n&gt;&gt;&gt; child1 = NodeInfo(id=\"child-1\")\n&gt;&gt;&gt; child2 = NodeInfo(id=\"child-2\")\n&gt;&gt;&gt; links = LinkedNodes(children=[child1, child2])\n&gt;&gt;&gt; len(links.children)\n2\n</code></pre></li> <li>Invalid single NodeInfo for children     <pre><code>&gt;&gt;&gt; LinkedNodes(children=NodeInfo(id=\"bad\"))  # doctest: +SKIP\nTraceback (most recent call last):\n    ...\nValueError: Children must be a list of NodeInfo objects\n</code></pre></li> <li>Empty children list is valid     <pre><code>&gt;&gt;&gt; links = LinkedNodes(children=[])\n&gt;&gt;&gt; links.children\n[]\n</code></pre></li> </ul> Note <p>This validator is specific to the children field, which represents one-to-many relationships.</p> Source code in <code>libs/core/src/serapeum/core/base/embeddings/types.py</code> <pre><code>@field_validator(\"children\")\n@classmethod\ndef validate_children_list(cls, v: Any) -&gt; list[NodeInfo] | None:\n    \"\"\"Validate that children field contains a list of NodeInfo objects.\n\n    Ensures the children field is a list (not a single NodeInfo instance).\n    Called automatically by Pydantic during model instantiation and validation.\n\n    Args:\n        v: Value to validate, expected to be list[NodeInfo] or None.\n\n    Returns:\n        The validated list of NodeInfo instances or None.\n\n    Raises:\n        ValueError: If v is not None and not a list.\n\n    Examples:\n        - Valid children list\n            ```python\n            &gt;&gt;&gt; from serapeum.core.base.embeddings.types import LinkedNodes, NodeInfo\n            &gt;&gt;&gt; child1 = NodeInfo(id=\"child-1\")\n            &gt;&gt;&gt; child2 = NodeInfo(id=\"child-2\")\n            &gt;&gt;&gt; links = LinkedNodes(children=[child1, child2])\n            &gt;&gt;&gt; len(links.children)\n            2\n\n            ```\n        - Invalid single NodeInfo for children\n            ```python\n            &gt;&gt;&gt; LinkedNodes(children=NodeInfo(id=\"bad\"))  # doctest: +SKIP\n            Traceback (most recent call last):\n                ...\n            ValueError: Children must be a list of NodeInfo objects\n\n            ```\n        - Empty children list is valid\n            ```python\n            &gt;&gt;&gt; links = LinkedNodes(children=[])\n            &gt;&gt;&gt; links.children\n            []\n\n            ```\n\n    Note:\n        This validator is specific to the children field, which represents\n        one-to-many relationships.\n    \"\"\"\n    if v is not None and not isinstance(v, list):\n        raise ValueError(\"Children must be a list of NodeInfo objects\")\n    return v\n</code></pre>"},{"location":"reference/core/embeddings/module/#serapeum.core.embeddings.LinkedNodes.validate_single_node","title":"<code>validate_single_node(v)</code>  <code>classmethod</code>","text":"<p>Validate that single-node fields contain NodeInfo objects.</p> <p>Ensures that source, previous, next, and parent fields contain exactly one NodeInfo instance (not a list). Called automatically by Pydantic during model instantiation and validation.</p> <p>Parameters:</p> Name Type Description Default <code>v</code> <code>Any</code> <p>Value to validate, expected to be NodeInfo or None.</p> required <p>Returns:</p> Type Description <code>NodeInfo | None</code> <p>The validated NodeInfo instance or None.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If v is not None and not a NodeInfo instance.</p> <p>Examples:</p> <ul> <li>Valid single node assignment     <pre><code>&gt;&gt;&gt; from serapeum.core.base.embeddings.types import LinkedNodes, NodeInfo\n&gt;&gt;&gt; node = NodeInfo(id=\"valid\")\n&gt;&gt;&gt; links = LinkedNodes(source=node)\n&gt;&gt;&gt; links.source.id\n'valid'\n</code></pre></li> <li>Invalid list assignment to single-node field     <pre><code>&gt;&gt;&gt; LinkedNodes(source=[NodeInfo(id=\"bad\")])  # doctest: +SKIP\nTraceback (most recent call last):\n    ...\nValueError: Must be a NodeInfo object, not a list\n</code></pre></li> </ul> Note <p>This validator applies to: source, previous, next, parent fields. The children field has a separate validator for list validation.</p> Source code in <code>libs/core/src/serapeum/core/base/embeddings/types.py</code> <pre><code>@field_validator(\"source\", \"previous\", \"next\", \"parent\")\n@classmethod\ndef validate_single_node(cls, v: Any) -&gt; NodeInfo | None:\n    \"\"\"Validate that single-node fields contain NodeInfo objects.\n\n    Ensures that source, previous, next, and parent fields contain exactly\n    one NodeInfo instance (not a list). Called automatically by Pydantic\n    during model instantiation and validation.\n\n    Args:\n        v: Value to validate, expected to be NodeInfo or None.\n\n    Returns:\n        The validated NodeInfo instance or None.\n\n    Raises:\n        ValueError: If v is not None and not a NodeInfo instance.\n\n    Examples:\n        - Valid single node assignment\n            ```python\n            &gt;&gt;&gt; from serapeum.core.base.embeddings.types import LinkedNodes, NodeInfo\n            &gt;&gt;&gt; node = NodeInfo(id=\"valid\")\n            &gt;&gt;&gt; links = LinkedNodes(source=node)\n            &gt;&gt;&gt; links.source.id\n            'valid'\n\n            ```\n        - Invalid list assignment to single-node field\n            ```python\n            &gt;&gt;&gt; LinkedNodes(source=[NodeInfo(id=\"bad\")])  # doctest: +SKIP\n            Traceback (most recent call last):\n                ...\n            ValueError: Must be a NodeInfo object, not a list\n\n            ```\n\n    Note:\n        This validator applies to: source, previous, next, parent fields.\n        The children field has a separate validator for list validation.\n    \"\"\"\n    if v is not None and not isinstance(v, NodeInfo):\n        raise ValueError(\"Must be a NodeInfo object, not a list\")\n    return v\n</code></pre>"},{"location":"reference/core/embeddings/module/#serapeum.core.embeddings.MetadataMode","title":"<code>MetadataMode</code>","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>Enumeration for controlling which metadata is included in different contexts.</p> <p>Different use cases require different metadata visibility. For example, you might exclude certain metadata from embeddings (to avoid semantic pollution) while including it for LLM context (to provide additional information).</p> <p>Attributes:</p> Name Type Description <code>ALL</code> <p>Include all metadata fields.</p> <code>EMBED</code> <p>Include only metadata for embedding generation (excludes fields in excluded_embed_metadata_keys).</p> <code>LLM</code> <p>Include only metadata for LLM context (excludes fields in excluded_llm_metadata_keys).</p> <code>NONE</code> <p>Exclude all metadata.</p> <p>Examples:</p> <ul> <li>Filtering metadata for embeddings     <pre><code>&gt;&gt;&gt; from serapeum.core.base.embeddings.types import MetadataMode\n&gt;&gt;&gt; mode = MetadataMode.EMBED\n&gt;&gt;&gt; mode.value\n'embed'\n</code></pre></li> <li>Using with node content retrieval (conceptual)     <pre><code>&gt;&gt;&gt; MetadataMode.LLM == \"llm\"\nTrue\n&gt;&gt;&gt; MetadataMode.NONE == \"none\"\nTrue\n</code></pre></li> <li>Checking mode type     <pre><code>&gt;&gt;&gt; isinstance(MetadataMode.ALL, str)\nTrue\n</code></pre></li> </ul> See Also <p>BaseNode.get_content: Uses this mode to control metadata inclusion. BaseNode.get_metadata_str: Filters metadata based on this mode. BaseNode.excluded_embed_metadata_keys: Metadata excluded for EMBED mode. BaseNode.excluded_llm_metadata_keys: Metadata excluded for LLM mode.</p> Source code in <code>libs/core/src/serapeum/core/base/embeddings/types.py</code> <pre><code>class MetadataMode(str, Enum):\n    \"\"\"Enumeration for controlling which metadata is included in different contexts.\n\n    Different use cases require different metadata visibility. For example, you\n    might exclude certain metadata from embeddings (to avoid semantic pollution)\n    while including it for LLM context (to provide additional information).\n\n    Attributes:\n        ALL: Include all metadata fields.\n        EMBED: Include only metadata for embedding generation (excludes fields\n            in excluded_embed_metadata_keys).\n        LLM: Include only metadata for LLM context (excludes fields in\n            excluded_llm_metadata_keys).\n        NONE: Exclude all metadata.\n\n    Examples:\n        - Filtering metadata for embeddings\n            ```python\n            &gt;&gt;&gt; from serapeum.core.base.embeddings.types import MetadataMode\n            &gt;&gt;&gt; mode = MetadataMode.EMBED\n            &gt;&gt;&gt; mode.value\n            'embed'\n\n            ```\n        - Using with node content retrieval (conceptual)\n            ```python\n            &gt;&gt;&gt; MetadataMode.LLM == \"llm\"\n            True\n            &gt;&gt;&gt; MetadataMode.NONE == \"none\"\n            True\n\n            ```\n        - Checking mode type\n            ```python\n            &gt;&gt;&gt; isinstance(MetadataMode.ALL, str)\n            True\n\n            ```\n\n    See Also:\n        BaseNode.get_content: Uses this mode to control metadata inclusion.\n        BaseNode.get_metadata_str: Filters metadata based on this mode.\n        BaseNode.excluded_embed_metadata_keys: Metadata excluded for EMBED mode.\n        BaseNode.excluded_llm_metadata_keys: Metadata excluded for LLM mode.\n    \"\"\"\n\n    ALL = \"all\"\n    EMBED = \"embed\"\n    LLM = \"llm\"\n    NONE = \"none\"\n</code></pre>"},{"location":"reference/core/embeddings/module/#serapeum.core.embeddings.MockEmbedding","title":"<code>MockEmbedding</code>","text":"<p>               Bases: <code>BaseEmbedding</code></p> <p>Mock embedding model for testing purposes.</p> <p>Returns constant embedding vectors (all 0.5 values) for any input, allowing tests to run without requiring a real embedding model. This is useful for unit testing, integration testing, and development without the overhead of loading actual models or making API calls.</p> <p>All embeddings returned are deterministic vectors of the specified dimension, filled with 0.5 values. This makes tests reproducible and fast.</p> <p>Attributes:</p> Name Type Description <code>embed_dim</code> <code>int</code> <p>Embedding dimension (must be positive).</p> <code>model_name</code> <code>str</code> <p>Model name identifier (defaults to \"mock-embedding\").</p> <p>Examples:</p> <ul> <li> <p>Creating a mock embedding model     <pre><code>&gt;&gt;&gt; from serapeum.core.embeddings import MockEmbedding\n&gt;&gt;&gt; emb = MockEmbedding(embed_dim=3)\n&gt;&gt;&gt; emb.model_name\n'mock-embedding'\n&gt;&gt;&gt; emb.embed_dim\n3\n</code></pre></p> </li> <li> <p>Getting embeddings returns constant vectors     <pre><code>&gt;&gt;&gt; emb = MockEmbedding(embed_dim=4)\n&gt;&gt;&gt; result = emb.get_text_embedding(\"any text\")\n&gt;&gt;&gt; result\n[0.5, 0.5, 0.5, 0.5]\n</code></pre></p> </li> <li> <p>All inputs produce identical embeddings     <pre><code>&gt;&gt;&gt; emb = MockEmbedding(embed_dim=2)\n&gt;&gt;&gt; emb.get_text_embedding(\"hello\") == emb.get_text_embedding(\"world\")\nTrue\n</code></pre></p> </li> <li> <p>Validation of embed_dim     <pre><code>&gt;&gt;&gt; MockEmbedding(embed_dim=0)  # doctest: +ELLIPSIS\nTraceback (most recent call last):\n    ...\npydantic_core._pydantic_core.ValidationError: 1 validation error...\n</code></pre></p> </li> </ul> See Also <p>BaseEmbedding: Abstract base class that MockEmbedding implements.</p> Source code in <code>libs/core/src/serapeum/core/embeddings/types.py</code> <pre><code>class MockEmbedding(BaseEmbedding):\n    \"\"\"Mock embedding model for testing purposes.\n\n    Returns constant embedding vectors (all 0.5 values) for any input,\n    allowing tests to run without requiring a real embedding model. This is\n    useful for unit testing, integration testing, and development without\n    the overhead of loading actual models or making API calls.\n\n    All embeddings returned are deterministic vectors of the specified dimension,\n    filled with 0.5 values. This makes tests reproducible and fast.\n\n    Attributes:\n        embed_dim: Embedding dimension (must be positive).\n        model_name: Model name identifier (defaults to \"mock-embedding\").\n\n    Examples:\n        - Creating a mock embedding model\n            ```python\n            &gt;&gt;&gt; from serapeum.core.embeddings import MockEmbedding\n            &gt;&gt;&gt; emb = MockEmbedding(embed_dim=3)\n            &gt;&gt;&gt; emb.model_name\n            'mock-embedding'\n            &gt;&gt;&gt; emb.embed_dim\n            3\n\n            ```\n\n        - Getting embeddings returns constant vectors\n            ```python\n            &gt;&gt;&gt; emb = MockEmbedding(embed_dim=4)\n            &gt;&gt;&gt; result = emb.get_text_embedding(\"any text\")\n            &gt;&gt;&gt; result\n            [0.5, 0.5, 0.5, 0.5]\n\n            ```\n\n        - All inputs produce identical embeddings\n            ```python\n            &gt;&gt;&gt; emb = MockEmbedding(embed_dim=2)\n            &gt;&gt;&gt; emb.get_text_embedding(\"hello\") == emb.get_text_embedding(\"world\")\n            True\n\n            ```\n\n        - Validation of embed_dim\n            ```python\n            &gt;&gt;&gt; MockEmbedding(embed_dim=0)  # doctest: +ELLIPSIS\n            Traceback (most recent call last):\n                ...\n            pydantic_core._pydantic_core.ValidationError: 1 validation error...\n\n            ```\n\n    See Also:\n        BaseEmbedding: Abstract base class that MockEmbedding implements.\n    \"\"\"\n\n    embed_dim: int = Field(\n        ..., gt=0, description=\"Embedding dimension (must be positive)\"\n    )\n    model_name: str = Field(\n        default=\"mock-embedding\", description=\"Model name identifier\"\n    )\n\n    @field_validator(\"embed_dim\")\n    @classmethod\n    def validate_embed_dim(cls, v: int) -&gt; int:\n        \"\"\"Validate that embed_dim is positive.\n\n        Args:\n            v: The embed_dim value to validate.\n\n        Returns:\n            The validated embed_dim.\n\n        Raises:\n            ValueError: If embed_dim is not positive.\n        \"\"\"\n        if v &lt;= 0:\n            raise ValueError(f\"embed_dim must be positive, got {v}\")\n        return v\n\n    @classmethod\n    def class_name(cls) -&gt; str:\n        \"\"\"Return the class name identifier.\n\n        Returns:\n            String \"MockEmbedding\" identifying this class.\n\n        Examples:\n            - Getting the class name\n                ```python\n                &gt;&gt;&gt; from serapeum.core.embeddings import MockEmbedding\n                &gt;&gt;&gt; MockEmbedding.class_name()\n                'MockEmbedding'\n\n                ```\n        \"\"\"\n        return \"MockEmbedding\"\n\n    def _get_mocked_vector(self) -&gt; Embedding:\n        \"\"\"Generate a mock embedding vector of constant values.\n\n        Creates a vector of length embed_dim where all values are 0.5. This is\n        the core method that all other embedding methods delegate to.\n\n        Returns:\n            List of floats with length equal to embed_dim, all values 0.5.\n\n        Examples:\n            - Generating a mock vector\n                ```python\n                &gt;&gt;&gt; from serapeum.core.embeddings import MockEmbedding\n                &gt;&gt;&gt; emb = MockEmbedding(embed_dim=5)\n                &gt;&gt;&gt; emb._get_mocked_vector()\n                [0.5, 0.5, 0.5, 0.5, 0.5]\n\n                ```\n\n            - Vector length matches embed_dim\n                ```python\n                &gt;&gt;&gt; from serapeum.core.embeddings import MockEmbedding\n                &gt;&gt;&gt; emb = MockEmbedding(embed_dim=3)\n                &gt;&gt;&gt; len(emb._get_mocked_vector())\n                3\n\n                ```\n        \"\"\"\n        return [0.5] * self.embed_dim\n\n    def _get_query_embedding(self, query: str) -&gt; Embedding:\n        \"\"\"Get query embedding (returns constant mock vector).\n\n        This method ignores the input query and always returns the same mock\n        vector. Implements the abstract method from BaseEmbedding.\n\n        Args:\n            query: Query text (unused in mock implementation).\n\n        Returns:\n            Mock embedding vector with all values set to 0.5.\n\n        Examples:\n            - Query embedding returns mock vector\n                ```python\n                &gt;&gt;&gt; from serapeum.core.embeddings import MockEmbedding\n                &gt;&gt;&gt; emb = MockEmbedding(embed_dim=3)\n                &gt;&gt;&gt; emb._get_query_embedding(\"test query\")\n                [0.5, 0.5, 0.5]\n\n                ```\n\n            - Different queries return identical vectors\n                ```python\n                &gt;&gt;&gt; emb = MockEmbedding(embed_dim=2)\n                &gt;&gt;&gt; emb._get_query_embedding(\"query1\") == emb._get_query_embedding(\"query2\")\n                True\n\n                ```\n        \"\"\"\n        return self._get_mocked_vector()\n\n    def _get_text_embedding(self, text: str) -&gt; Embedding:\n        \"\"\"Get text embedding (returns constant mock vector).\n\n        This method ignores the input text and always returns the same mock\n        vector. Implements the abstract method from BaseEmbedding.\n\n        Args:\n            text: Input text (unused in mock implementation).\n\n        Returns:\n            Mock embedding vector with all values set to 0.5.\n\n        Examples:\n            - Text embedding returns mock vector\n                ```python\n                &gt;&gt;&gt; from serapeum.core.embeddings import MockEmbedding\n                &gt;&gt;&gt; emb = MockEmbedding(embed_dim=4)\n                &gt;&gt;&gt; emb._get_text_embedding(\"sample text\")\n                [0.5, 0.5, 0.5, 0.5]\n\n                ```\n\n            - Different texts return identical vectors\n                ```python\n                &gt;&gt;&gt; emb = MockEmbedding(embed_dim=2)\n                &gt;&gt;&gt; emb._get_text_embedding(\"text1\") == emb._get_text_embedding(\"text2\")\n                True\n\n                ```\n        \"\"\"\n        return self._get_mocked_vector()\n\n    async def _aget_query_embedding(self, query: str) -&gt; Embedding:\n        \"\"\"Asynchronously get query embedding (returns constant mock vector).\n\n        Async version of _get_query_embedding. This method ignores the input\n        query and always returns the same mock vector. Implements the abstract\n        method from BaseEmbedding.\n\n        Args:\n            query: Query text (unused in mock implementation).\n\n        Returns:\n            Mock embedding vector with all values set to 0.5.\n\n        Examples:\n            - Async query embedding\n                ```python\n                &gt;&gt;&gt; import asyncio\n                &gt;&gt;&gt; from serapeum.core.embeddings import MockEmbedding\n                &gt;&gt;&gt; emb = MockEmbedding(embed_dim=3)\n                &gt;&gt;&gt; asyncio.run(emb._aget_query_embedding(\"async query\"))\n                [0.5, 0.5, 0.5]\n\n                ```\n        \"\"\"\n        return self._get_mocked_vector()\n\n    async def _aget_text_embedding(self, text: str) -&gt; Embedding:\n        \"\"\"Asynchronously get text embedding (returns constant mock vector).\n\n        Async version of _get_text_embedding. This method ignores the input\n        text and always returns the same mock vector. Implements the abstract\n        method from BaseEmbedding.\n\n        Args:\n            text: Input text (unused in mock implementation).\n\n        Returns:\n            Mock embedding vector with all values set to 0.5.\n\n        Examples:\n            - Async text embedding\n                ```python\n                &gt;&gt;&gt; import asyncio\n                &gt;&gt;&gt; from serapeum.core.embeddings import MockEmbedding\n                &gt;&gt;&gt; emb = MockEmbedding(embed_dim=3)\n                &gt;&gt;&gt; asyncio.run(emb._aget_text_embedding(\"async text\"))\n                [0.5, 0.5, 0.5]\n\n                ```\n        \"\"\"\n        return self._get_mocked_vector()\n</code></pre>"},{"location":"reference/core/embeddings/module/#serapeum.core.embeddings.MockEmbedding.class_name","title":"<code>class_name()</code>  <code>classmethod</code>","text":"<p>Return the class name identifier.</p> <p>Returns:</p> Type Description <code>str</code> <p>String \"MockEmbedding\" identifying this class.</p> <p>Examples:</p> <ul> <li>Getting the class name     <pre><code>&gt;&gt;&gt; from serapeum.core.embeddings import MockEmbedding\n&gt;&gt;&gt; MockEmbedding.class_name()\n'MockEmbedding'\n</code></pre></li> </ul> Source code in <code>libs/core/src/serapeum/core/embeddings/types.py</code> <pre><code>@classmethod\ndef class_name(cls) -&gt; str:\n    \"\"\"Return the class name identifier.\n\n    Returns:\n        String \"MockEmbedding\" identifying this class.\n\n    Examples:\n        - Getting the class name\n            ```python\n            &gt;&gt;&gt; from serapeum.core.embeddings import MockEmbedding\n            &gt;&gt;&gt; MockEmbedding.class_name()\n            'MockEmbedding'\n\n            ```\n    \"\"\"\n    return \"MockEmbedding\"\n</code></pre>"},{"location":"reference/core/embeddings/module/#serapeum.core.embeddings.MockEmbedding.validate_embed_dim","title":"<code>validate_embed_dim(v)</code>  <code>classmethod</code>","text":"<p>Validate that embed_dim is positive.</p> <p>Parameters:</p> Name Type Description Default <code>v</code> <code>int</code> <p>The embed_dim value to validate.</p> required <p>Returns:</p> Type Description <code>int</code> <p>The validated embed_dim.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If embed_dim is not positive.</p> Source code in <code>libs/core/src/serapeum/core/embeddings/types.py</code> <pre><code>@field_validator(\"embed_dim\")\n@classmethod\ndef validate_embed_dim(cls, v: int) -&gt; int:\n    \"\"\"Validate that embed_dim is positive.\n\n    Args:\n        v: The embed_dim value to validate.\n\n    Returns:\n        The validated embed_dim.\n\n    Raises:\n        ValueError: If embed_dim is not positive.\n    \"\"\"\n    if v &lt;= 0:\n        raise ValueError(f\"embed_dim must be positive, got {v}\")\n    return v\n</code></pre>"},{"location":"reference/core/embeddings/module/#serapeum.core.embeddings.NodeContentType","title":"<code>NodeContentType</code>","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>Enumeration of content types that can be stored in a node.</p> <p>This enum classifies the type of content a node contains, which helps downstream components (LLMs, embeddings, parsers) handle the content appropriately. String-based enum values enable direct serialization.</p> <p>Attributes:</p> Name Type Description <code>TEXT</code> <p>Plain text content, the most common node type.</p> <code>IMAGE</code> <p>Image data or references to images.</p> <code>INDEX</code> <p>Index structures or metadata about other nodes.</p> <code>DOCUMENT</code> <p>Complete document content before chunking.</p> <code>MULTIMODAL</code> <p>Content combining multiple modalities (text + images).</p> <p>Examples:</p> <ul> <li>Checking content type     <pre><code>&gt;&gt;&gt; from serapeum.core.base.embeddings.types import NodeContentType\n&gt;&gt;&gt; content_type = NodeContentType.TEXT\n&gt;&gt;&gt; content_type.value\n'text'\n</code></pre></li> <li>Using in node metadata     <pre><code>&gt;&gt;&gt; from serapeum.core.base.embeddings.types import NodeInfo\n&gt;&gt;&gt; node = NodeInfo(id=\"node-1\", type=NodeContentType.IMAGE)\n&gt;&gt;&gt; node.type\n&lt;NodeContentType.IMAGE: 'image'&gt;\n</code></pre></li> <li>String comparison     <pre><code>&gt;&gt;&gt; NodeContentType.TEXT == \"text\"\nTrue\n</code></pre></li> </ul> See Also <p>NodeInfo: Uses this enum to specify node content type. BaseNode.get_type: Abstract method returning content type string.</p> Source code in <code>libs/core/src/serapeum/core/base/embeddings/types.py</code> <pre><code>class NodeContentType(str, Enum):\n    \"\"\"Enumeration of content types that can be stored in a node.\n\n    This enum classifies the type of content a node contains, which helps\n    downstream components (LLMs, embeddings, parsers) handle the content\n    appropriately. String-based enum values enable direct serialization.\n\n    Attributes:\n        TEXT: Plain text content, the most common node type.\n        IMAGE: Image data or references to images.\n        INDEX: Index structures or metadata about other nodes.\n        DOCUMENT: Complete document content before chunking.\n        MULTIMODAL: Content combining multiple modalities (text + images).\n\n    Examples:\n        - Checking content type\n            ```python\n            &gt;&gt;&gt; from serapeum.core.base.embeddings.types import NodeContentType\n            &gt;&gt;&gt; content_type = NodeContentType.TEXT\n            &gt;&gt;&gt; content_type.value\n            'text'\n\n            ```\n        - Using in node metadata\n            ```python\n            &gt;&gt;&gt; from serapeum.core.base.embeddings.types import NodeInfo\n            &gt;&gt;&gt; node = NodeInfo(id=\"node-1\", type=NodeContentType.IMAGE)\n            &gt;&gt;&gt; node.type\n            &lt;NodeContentType.IMAGE: 'image'&gt;\n\n            ```\n        - String comparison\n            ```python\n            &gt;&gt;&gt; NodeContentType.TEXT == \"text\"\n            True\n\n            ```\n\n    See Also:\n        NodeInfo: Uses this enum to specify node content type.\n        BaseNode.get_type: Abstract method returning content type string.\n    \"\"\"\n\n    TEXT = \"text\"\n    IMAGE = \"image\"\n    INDEX = \"index\"\n    DOCUMENT = \"document\"\n    MULTIMODAL = \"multimodal\"\n</code></pre>"},{"location":"reference/core/embeddings/module/#serapeum.core.embeddings.NodeInfo","title":"<code>NodeInfo</code>","text":"<p>               Bases: <code>SerializableModel</code></p> <p>Lightweight reference to a node with essential identification metadata.</p> <p>NodeInfo provides a compact representation of a node without its full content, useful for creating references and relationships between nodes. It includes the node's ID, content type, metadata, and optional hash for change detection.</p> <p>Attributes:</p> Name Type Description <code>id</code> <code>str</code> <p>Unique identifier for the node.</p> <code>type</code> <code>Annotated[NodeContentType, EnumNameSerializer] | str | None</code> <p>Content type classification (NodeContentType enum or string).</p> <code>metadata</code> <code>dict[str, Any]</code> <p>Arbitrary metadata dictionary for the node.</p> <code>hash</code> <code>str | None</code> <p>Optional hash value for detecting content changes.</p> <p>Examples:</p> <ul> <li>Creating a basic node reference     <pre><code>&gt;&gt;&gt; from serapeum.core.base.embeddings.types import NodeInfo, NodeContentType\n&gt;&gt;&gt; ref = NodeInfo(\n...     id=\"doc-456\",\n...     type=NodeContentType.TEXT,\n...     metadata={\"page\": 1}\n... )\n&gt;&gt;&gt; ref.id\n'doc-456'\n</code></pre></li> <li>Serialization and deserialization     <pre><code>&gt;&gt;&gt; ref = NodeInfo(id=\"node-1\", type=NodeContentType.DOCUMENT)\n&gt;&gt;&gt; json_str = ref.to_json()\n&gt;&gt;&gt; restored = NodeInfo.from_json(json_str)\n&gt;&gt;&gt; restored.id\n'node-1'\n</code></pre></li> <li>Using with hash for change detection     <pre><code>&gt;&gt;&gt; import hashlib\n&gt;&gt;&gt; content = \"Sample text\"\n&gt;&gt;&gt; content_hash = hashlib.sha256(content.encode()).hexdigest()\n&gt;&gt;&gt; ref = NodeInfo(id=\"node-2\", hash=content_hash)\n&gt;&gt;&gt; ref.hash[:8]  # First 8 chars of hash  # doctest: +SKIP\n'e3b0c442'\n</code></pre></li> </ul> See Also <p>BaseNode: Full node implementation that generates NodeInfo. LinkedNodes: Container for node relationships using NodeInfo. SerializableModel: Base class providing serialization methods.</p> Source code in <code>libs/core/src/serapeum/core/base/embeddings/types.py</code> <pre><code>class NodeInfo(SerializableModel):\n    \"\"\"Lightweight reference to a node with essential identification metadata.\n\n    NodeInfo provides a compact representation of a node without its full content,\n    useful for creating references and relationships between nodes. It includes\n    the node's ID, content type, metadata, and optional hash for change detection.\n\n    Attributes:\n        id: Unique identifier for the node.\n        type: Content type classification (NodeContentType enum or string).\n        metadata: Arbitrary metadata dictionary for the node.\n        hash: Optional hash value for detecting content changes.\n\n    Examples:\n        - Creating a basic node reference\n            ```python\n            &gt;&gt;&gt; from serapeum.core.base.embeddings.types import NodeInfo, NodeContentType\n            &gt;&gt;&gt; ref = NodeInfo(\n            ...     id=\"doc-456\",\n            ...     type=NodeContentType.TEXT,\n            ...     metadata={\"page\": 1}\n            ... )\n            &gt;&gt;&gt; ref.id\n            'doc-456'\n\n            ```\n        - Serialization and deserialization\n            ```python\n            &gt;&gt;&gt; ref = NodeInfo(id=\"node-1\", type=NodeContentType.DOCUMENT)\n            &gt;&gt;&gt; json_str = ref.to_json()\n            &gt;&gt;&gt; restored = NodeInfo.from_json(json_str)\n            &gt;&gt;&gt; restored.id\n            'node-1'\n\n            ```\n        - Using with hash for change detection\n            ```python\n            &gt;&gt;&gt; import hashlib\n            &gt;&gt;&gt; content = \"Sample text\"\n            &gt;&gt;&gt; content_hash = hashlib.sha256(content.encode()).hexdigest()\n            &gt;&gt;&gt; ref = NodeInfo(id=\"node-2\", hash=content_hash)\n            &gt;&gt;&gt; ref.hash[:8]  # First 8 chars of hash  # doctest: +SKIP\n            'e3b0c442'\n\n            ```\n\n    See Also:\n        BaseNode: Full node implementation that generates NodeInfo.\n        LinkedNodes: Container for node relationships using NodeInfo.\n        SerializableModel: Base class providing serialization methods.\n    \"\"\"\n\n    id: str\n    type: Annotated[NodeContentType, EnumNameSerializer] | str | None = None\n    metadata: dict[str, Any] = Field(default_factory=dict)\n    hash: str | None = None\n\n    @classmethod\n    def class_name(cls) -&gt; str:\n        \"\"\"Return the class name identifier for serialization.\n\n        Returns:\n            Always returns \"NodeInfo\" as the stable class identifier.\n\n        Examples:\n            - Getting class name\n                ```python\n                &gt;&gt;&gt; from serapeum.core.base.embeddings.types import NodeInfo\n                &gt;&gt;&gt; NodeInfo.class_name()\n                'NodeInfo'\n\n                ```\n        \"\"\"\n        return \"NodeInfo\"\n</code></pre>"},{"location":"reference/core/embeddings/module/#serapeum.core.embeddings.NodeInfo.class_name","title":"<code>class_name()</code>  <code>classmethod</code>","text":"<p>Return the class name identifier for serialization.</p> <p>Returns:</p> Type Description <code>str</code> <p>Always returns \"NodeInfo\" as the stable class identifier.</p> <p>Examples:</p> <ul> <li>Getting class name     <pre><code>&gt;&gt;&gt; from serapeum.core.base.embeddings.types import NodeInfo\n&gt;&gt;&gt; NodeInfo.class_name()\n'NodeInfo'\n</code></pre></li> </ul> Source code in <code>libs/core/src/serapeum/core/base/embeddings/types.py</code> <pre><code>@classmethod\ndef class_name(cls) -&gt; str:\n    \"\"\"Return the class name identifier for serialization.\n\n    Returns:\n        Always returns \"NodeInfo\" as the stable class identifier.\n\n    Examples:\n        - Getting class name\n            ```python\n            &gt;&gt;&gt; from serapeum.core.base.embeddings.types import NodeInfo\n            &gt;&gt;&gt; NodeInfo.class_name()\n            'NodeInfo'\n\n            ```\n    \"\"\"\n    return \"NodeInfo\"\n</code></pre>"},{"location":"reference/core/embeddings/module/#serapeum.core.embeddings.NodeType","title":"<code>NodeType</code>","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>Node links used in <code>BaseNode</code> class.</p> <p>Attributes:</p> Name Type Description <code>SOURCE</code> <p>The node is the source document.</p> <code>PREVIOUS</code> <p>The node is the previous node in the document.</p> <code>NEXT</code> <p>The node is the next node in the document.</p> <code>PARENT</code> <p>The node is the parent node in the document.</p> <code>CHILD</code> <p>The node is a child node in the document.</p> Source code in <code>libs/core/src/serapeum/core/base/embeddings/types.py</code> <pre><code>class NodeType(str, Enum):\n    \"\"\"\n    Node links used in `BaseNode` class.\n\n    Attributes:\n        SOURCE: The node is the source document.\n        PREVIOUS: The node is the previous node in the document.\n        NEXT: The node is the next node in the document.\n        PARENT: The node is the parent node in the document.\n        CHILD: The node is a child node in the document.\n\n    \"\"\"\n\n    SOURCE = \"source\"\n    PREVIOUS = \"previous\"\n    NEXT = \"next\"\n    PARENT = \"parent\"\n    CHILD = \"child\"\n</code></pre>"},{"location":"reference/core/llms/llm-classes-comparison/","title":"LLM Classes Comparison","text":"<p>This document provides a comprehensive comparison of the four main LLM classes in Serapeum's core library, explaining their purposes, relationships, and when to use each one.</p>"},{"location":"reference/core/llms/llm-classes-comparison/#overview","title":"Overview","text":"<p>Serapeum provides four distinct LLM classes organized across two architectural layers:</p> <pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  Orchestration Layer (High-level workflows)                 \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502  ToolOrchestratingLLM       \u2502  TextCompletionLLM             \u2502\n\u2502  (uses function calling)    \u2502  (uses text parsing)          \u2502\n\u2502  - Converts models to tools \u2502  - Binds prompt+parser+LLM    \u2502\n\u2502  - Executes tool calls      \u2502  - Parses raw text output     \u2502\n\u2502  - Returns Pydantic models  \u2502  - Returns Pydantic models    \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                            \u25b2\n                            \u2502 uses\n                            \u2502\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  LLM Layer (Core abstractions)                              \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502  FunctionCallingLLM         \u2502  StructuredOutputLLM                 \u2502\n\u2502  (base for providers)       \u2502  (wrapper for structured IO)  \u2502\n\u2502  - Tool calling interface   \u2502  - Forces Pydantic outputs    \u2502\n\u2502  - Provider implementations \u2502  - Wraps any LLM              \u2502\n\u2502  - Abstract methods         \u2502  - Format conversion          \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"reference/core/llms/llm-classes-comparison/#detailed-comparison","title":"Detailed Comparison","text":""},{"location":"reference/core/llms/llm-classes-comparison/#1-functioncallingllm","title":"1. FunctionCallingLLM","text":"<p>Location: <code>libs/core/src/serapeum/core/llms/function_calling.py:21</code> Layer: LLM Layer (core abstraction) Type: Base class for provider implementations</p>"},{"location":"reference/core/llms/llm-classes-comparison/#purpose","title":"Purpose","text":"<p>Provides the foundation for LLM providers that support function/tool calling. This is an abstract base class that concrete provider implementations (like Ollama, OpenAI) should inherit from.</p>"},{"location":"reference/core/llms/llm-classes-comparison/#key-features","title":"Key Features","text":"<ul> <li>Extends the base <code>LLM</code> class with tool-calling capabilities</li> <li>Provides convenience methods for tool workflows:</li> <li><code>chat_with_tools()</code> - Chat with function calling (sync)</li> <li><code>achat_with_tools()</code> - Chat with function calling (async)</li> <li><code>stream_chat_with_tools()</code> - Streaming chat with tools (sync)</li> <li><code>astream_chat_with_tools()</code> - Streaming chat with tools (async)</li> <li><code>predict_and_call()</code> - Predict and execute tool (sync)</li> <li><code>apredict_and_call()</code> - Predict and execute tool (async)</li> <li><code>get_tool_calls_from_response()</code> - Extract tool calls from response</li> <li>Abstract method <code>_prepare_chat_with_tools()</code> that providers must implement</li> </ul>"},{"location":"reference/core/llms/llm-classes-comparison/#when-to-use","title":"When to Use","text":"<ul> <li>You're implementing a new provider (e.g., OpenAI, Anthropic, Cohere)</li> <li>You need the base functionality for tool/function calling</li> <li>You're building low-level LLM integrations</li> </ul>"},{"location":"reference/core/llms/llm-classes-comparison/#example","title":"Example","text":"<pre><code>from serapeum.core.llms import FunctionCallingLLM\n\nclass MyProviderLLM(FunctionCallingLLM):\n    \"\"\"Custom provider implementation.\"\"\"\n\n    def _prepare_chat_with_tools(self, tools, **kwargs):\n        # Convert tools to provider-specific format\n        tool_schemas = [tool.to_json_schema() for tool in tools]\n        return {\n            \"messages\": kwargs.get(\"chat_history\", []),\n            \"tools\": tool_schemas,\n        }\n\n    def get_tool_calls_from_response(self, response, **kwargs):\n        # Extract tool calls from provider response\n        return response.tool_calls\n</code></pre>"},{"location":"reference/core/llms/llm-classes-comparison/#2-structuredoutputllm","title":"2. StructuredOutputLLM","text":"<p>Location: <code>libs/core/src/serapeum/core/llms/structured_output_llm.py:25</code> Layer: LLM Layer (wrapper) Type: Wrapper class for structured outputs</p>"},{"location":"reference/core/llms/llm-classes-comparison/#purpose_1","title":"Purpose","text":"<p>Wraps an existing LLM to force all outputs into a specific Pydantic model format. Acts as an adapter that converts any LLM into a structured output generator.</p>"},{"location":"reference/core/llms/llm-classes-comparison/#key-features_1","title":"Key Features","text":"<ul> <li>Takes two inputs:</li> <li><code>llm</code>: Any LLM instance (base LLM, function-calling LLM, etc.)</li> <li><code>output_cls</code>: A Pydantic model class defining the output structure</li> <li>Delegates to the underlying LLM's <code>structured_predict()</code> method</li> <li>Converts all responses to JSON representations of the output model</li> <li>Maintains the same interface as the base LLM (chat, stream_chat, etc.)</li> <li>Supports streaming structured outputs</li> </ul>"},{"location":"reference/core/llms/llm-classes-comparison/#when-to-use_1","title":"When to Use","text":"<ul> <li>You want to guarantee a specific output format from any LLM</li> <li>You're wrapping an existing LLM to enforce schema compliance</li> <li>You need structured outputs without manually handling parsing</li> </ul>"},{"location":"reference/core/llms/llm-classes-comparison/#example_1","title":"Example","text":"<pre><code>from pydantic import BaseModel\nfrom serapeum.ollama import Ollama\nfrom serapeum.core.llms import StructuredOutputLLM\nfrom serapeum.core.base.llms.types import Message\n\nclass PersonInfo(BaseModel):\n    name: str\n    age: int\n    occupation: str\n\n# Wrap an LLM to always return PersonInfo\nbase_llm = Ollama(model=\"llama3.1\", request_timeout=90)\nstructured_llm = StructuredOutputLLM(\n    llm=base_llm,\n    output_cls=PersonInfo\n)\n\n# All responses will be PersonInfo instances\nresponse = structured_llm.chat([\n    Message(role=\"user\", content=\"Tell me about Alice, a 30-year-old engineer\")\n])\nprint(response.raw)\nPersonInfo(name='Alice', age=30, occupation='Engineer')\n</code></pre>"},{"location":"reference/core/llms/llm-classes-comparison/#3-toolorchestratingllm","title":"3. ToolOrchestratingLLM","text":"<p>Location: <code>libs/core/src/serapeum/core/llms/orchestrators/tool_based.py:33</code> Layer: Orchestration Layer (high-level) Type: Orchestrator for function-calling workflows</p>"},{"location":"reference/core/llms/llm-classes-comparison/#purpose_2","title":"Purpose","text":"<p>High-level orchestrator that converts Pydantic models or Python functions into tools, executes them via function-calling, and returns structured outputs. This is the recommended way to get structured outputs from function-calling models.</p>"},{"location":"reference/core/llms/llm-classes-comparison/#key-features_2","title":"Key Features","text":"<ul> <li>Automatic tool creation: Converts Pydantic models OR Python functions to <code>CallableTool</code> instances</li> <li>Full orchestration: Handles prompt formatting, LLM invocation, tool execution, and output parsing</li> <li>Flexible inputs:</li> <li><code>output_cls</code>: Either a Pydantic model or a callable function</li> <li><code>prompt</code>: Template string or <code>BasePromptTemplate</code></li> <li><code>llm</code>: A <code>FunctionCallingLLM</code> instance</li> <li>Advanced capabilities:</li> <li>Streaming support via <code>stream_call()</code> and <code>astream_call()</code></li> <li>Parallel tool calls with <code>allow_parallel_tool_calls=True</code></li> <li>Custom tool selection with <code>tool_choice</code> parameter</li> <li>Sync and async: Both <code>__call__()</code> and <code>acall()</code> methods</li> </ul>"},{"location":"reference/core/llms/llm-classes-comparison/#when-to-use_2","title":"When to Use","text":"<ul> <li>You want structured outputs from a function-calling model (recommended approach)</li> <li>You're building applications that need reliable Pydantic outputs</li> <li>You want automatic tool creation from your data models</li> <li>You need streaming structured outputs</li> <li>You're using modern LLMs with function-calling support (GPT-4, Claude, Llama 3.1+)</li> </ul>"},{"location":"reference/core/llms/llm-classes-comparison/#example-with-pydantic-model","title":"Example with Pydantic Model","text":"<pre><code>from pydantic import BaseModel\nfrom serapeum.ollama import Ollama\nfrom serapeum.core.llms import ToolOrchestratingLLM\n\nclass WeatherInfo(BaseModel):\n    \"\"\"Weather information for a location.\"\"\"\n    location: str\n    temperature: float\n    conditions: str\n\n# Create orchestrator\nweather_extractor = ToolOrchestratingLLM(\n    output_cls=WeatherInfo,\n    prompt=\"Extract weather information from: {text}\",\n    llm=Ollama(model=\"llama3.1\"),\n)\n\n# Get structured output\nresult = weather_extractor(\n    text=\"It's 72 degrees and sunny in San Francisco\"\n)\nprint(result)\nWeatherInfo(location='San Francisco', temperature=72.0, conditions='sunny')\n</code></pre>"},{"location":"reference/core/llms/llm-classes-comparison/#example-with-function","title":"Example with Function","text":"<pre><code>from serapeum.ollama import Ollama\nfrom serapeum.core.llms import ToolOrchestratingLLM\n\ndef calculate_sum(a: int, b: int) -&gt; dict:\n    \"\"\"Calculate the sum of two numbers.\"\"\"\n    return {\"result\": a + b}\n\n# Create orchestrator with function\ncalculator = ToolOrchestratingLLM(\n    output_cls=calculate_sum,\n    prompt=\"Calculate the sum of {x} and {y}\",\n    llm=Ollama(model=\"llama3.1\"),\n)\n\nresult = calculator(x=5, y=3)\nprint(result)\n{'result': 8}\n</code></pre>"},{"location":"reference/core/llms/llm-classes-comparison/#example-with-streaming","title":"Example with Streaming","text":"<pre><code>from pydantic import BaseModel\nfrom serapeum.ollama import Ollama\nfrom serapeum.core.llms import ToolOrchestratingLLM\n\nclass Story(BaseModel):\n    title: str\n    content: str\n    genre: str\n\nstory_generator = ToolOrchestratingLLM(\n    output_cls=Story,\n    prompt=\"Generate a short {genre} story\",\n    llm=Ollama(model=\"llama3.1\", request_timeout=90),\n)\n\n# Stream partial results\nfor partial_story in story_generator.stream_call(genre=\"sci-fi\"):\n    print(partial_story)  # Progressively complete Story objects\n</code></pre>"},{"location":"reference/core/llms/llm-classes-comparison/#4-textcompletionllm","title":"4. TextCompletionLLM","text":"<p>Location: <code>libs/core/src/serapeum/core/llms/orchestrators/text_completion_llm.py:14</code> Layer: Orchestration Layer (simpler alternative) Type: Text-based structured output generator</p>"},{"location":"reference/core/llms/llm-classes-comparison/#purpose_3","title":"Purpose","text":"<p>Provides structured outputs by parsing raw text completions (without using function calling). This is useful for models that don't support function calling or when you prefer text-based parsing.</p>"},{"location":"reference/core/llms/llm-classes-comparison/#key-features_3","title":"Key Features","text":"<ul> <li>Simple pipeline: Binds prompt + output parser + LLM together</li> <li>Text-based parsing: Uses <code>PydanticParser</code> to parse raw LLM output into Pydantic models</li> <li>No function calling required: Works with any LLM (chat or completion models)</li> <li>Explicit parsing: Uses output parsers to handle the conversion</li> <li>Lightweight: Less overhead than function-calling approaches</li> </ul>"},{"location":"reference/core/llms/llm-classes-comparison/#when-to-use_3","title":"When to Use","text":"<ul> <li>Your LLM doesn't support function calling (older models, smaller models)</li> <li>You prefer text-based parsing over function calling</li> <li>You want explicit control over the parsing logic</li> <li>You're working with completion-style models (non-chat)</li> <li>You need a simpler, more transparent approach</li> </ul>"},{"location":"reference/core/llms/llm-classes-comparison/#example_2","title":"Example","text":"<pre><code>from pydantic import BaseModel\nfrom serapeum.ollama import Ollama\nfrom serapeum.core.output_parsers import PydanticParser\nfrom serapeum.core.llms import TextCompletionLLM\n\nclass Task(BaseModel):\n    title: str\n    priority: int\n    completed: bool\n\n# Create text completion LLM\ntask_extractor = TextCompletionLLM(\n    output_parser=PydanticParser(output_cls=Task),\n    prompt=\"Extract task information from: {text}. Return as JSON.\",\n    llm=Ollama(model=\"llama3.1\", request_timeout=90),\n)\n\nresult = task_extractor(\n    text=\"Finish the report - high priority, not done yet\"\n)\nresult\nTask(title='Finish the report', priority=1, completed=False)\n</code></pre>"},{"location":"reference/core/llms/llm-classes-comparison/#example-with-just-output_cls","title":"Example with Just output_cls","text":"<pre><code>from pydantic import BaseModel\nfrom serapeum.ollama import Ollama\nfrom serapeum.core.llms import TextCompletionLLM\n\nclass Product(BaseModel):\n    name: str\n    price: float\n\n# Parser is auto-created from output_cls\nproduct_extractor = TextCompletionLLM(\n    output_cls=Product,  # Parser created automatically\n    prompt=\"Extract product: {description}\",\n    llm=Ollama(model=\"llama3.1\", request_timeout=90),\n)\n\nresult = product_extractor(description=\"iPhone 15 Pro - $999\")\nresult\nProduct(name='iPhone 15 Pro', price=999.0)\n</code></pre>"},{"location":"reference/core/llms/llm-classes-comparison/#comparison-matrix","title":"Comparison Matrix","text":"Feature FunctionCallingLLM StructuredOutputLLM ToolOrchestratingLLM TextCompletionLLM Layer LLM LLM Orchestration Orchestration Type Base class Wrapper Orchestrator Pipeline Requires Function Calling N/A No Yes No Primary Use Case Building providers Enforcing output format Structured outputs (recommended) Text-based structured outputs Input N/A LLM + output_cls output_cls + prompt + LLM prompt + parser + LLM Output ChatResponse ChatResponse (with Pydantic in raw) Pydantic model(s) Pydantic model Streaming Support Yes Yes Yes No Parallel Tool Calls N/A No Yes No Complexity High (abstract) Low Medium Low Flexibility High Low High Medium"},{"location":"reference/core/llms/llm-classes-comparison/#decision-tree-which-class-should-i-use","title":"Decision Tree: Which Class Should I Use?","text":"<pre><code>Are you implementing a new LLM provider?\n\u251c\u2500 YES \u2192 Use FunctionCallingLLM (inherit from it)\n\u2514\u2500 NO \u2192 Continue...\n\nDo you need structured Pydantic outputs?\n\u251c\u2500 NO \u2192 Use base LLM classes\n\u2514\u2500 YES \u2192 Continue...\n\nDoes your LLM support function calling?\n\u251c\u2500 NO \u2192 Use TextCompletionLLM\n\u2514\u2500 YES \u2192 Continue...\n\nDo you just want to wrap an existing LLM to enforce a format?\n\u251c\u2500 YES \u2192 Use StructuredOutputLLM\n\u2514\u2500 NO \u2192 Use ToolOrchestratingLLM (recommended for most use cases)\n</code></pre>"},{"location":"reference/core/llms/llm-classes-comparison/#best-practices","title":"Best Practices","text":""},{"location":"reference/core/llms/llm-classes-comparison/#for-application-developers","title":"For Application Developers","text":"<ol> <li>Default to <code>ToolOrchestratingLLM</code> for structured outputs with modern LLMs</li> <li>Most flexible and powerful</li> <li>Handles tool creation automatically</li> <li> <p>Supports streaming and parallel calls</p> </li> <li> <p>Use <code>TextCompletionLLM</code> when:</p> </li> <li>Your model doesn't support function calling</li> <li>You prefer explicit text parsing</li> <li> <p>You need simpler, more predictable behavior</p> </li> <li> <p>Use <code>StructuredOutputLLM</code> when:</p> </li> <li>You have an existing LLM instance you want to wrap</li> <li>You just need to enforce an output format</li> <li>You don't need tool orchestration features</li> </ol>"},{"location":"reference/core/llms/llm-classes-comparison/#for-framework-developers","title":"For Framework Developers","text":"<ol> <li>Inherit from <code>FunctionCallingLLM</code> when building provider integrations</li> <li>Implement <code>_prepare_chat_with_tools()</code> for your provider's format</li> <li>Implement <code>get_tool_calls_from_response()</code> to extract tool calls</li> <li> <p>Follow the async/streaming patterns from existing providers (e.g., Ollama)</p> </li> <li> <p>Compose higher-level abstractions using the orchestration layer</p> </li> <li>Build on <code>ToolOrchestratingLLM</code> for complex workflows</li> <li>Create domain-specific wrappers around <code>TextCompletionLLM</code></li> </ol>"},{"location":"reference/core/llms/llm-classes-comparison/#code-references","title":"Code References","text":"<ul> <li>FunctionCallingLLM: <code>libs/core/src/serapeum/core/llms/function_calling.py</code></li> <li>StructuredOutputLLM: <code>libs/core/src/serapeum/core/llms/structured_output_llm.py</code></li> <li>ToolOrchestratingLLM: <code>libs/core/src/serapeum/core/llms/orchestrators/tool_based.py</code></li> <li>TextCompletionLLM: <code>libs/core/src/serapeum/core/llms/orchestrators/text_completion_llm.py</code></li> </ul>"},{"location":"reference/core/llms/llm-classes-comparison/#related-documentation","title":"Related Documentation","text":"<ul> <li>Callable Tools Guide</li> <li>Provider Integration Guide</li> <li>Architecture Overview</li> </ul>"},{"location":"reference/core/llms/orchestrators/text_completion_llm/examples/","title":"TextCompletionLLM Usage Examples","text":"<p>This guide provides comprehensive examples covering all possible ways to use <code>TextCompletionLLM</code>.</p>"},{"location":"reference/core/llms/orchestrators/text_completion_llm/examples/#table-of-contents","title":"Table of Contents","text":"<ol> <li>Basic Usage</li> <li>Initialization Patterns</li> <li>Prompt Formats</li> <li>Execution Modes</li> <li>Advanced Usage</li> <li>Error Handling</li> </ol>"},{"location":"reference/core/llms/orchestrators/text_completion_llm/examples/#basic-usage","title":"Basic Usage","text":""},{"location":"reference/core/llms/orchestrators/text_completion_llm/examples/#simple-string-prompt-with-variables","title":"Simple String Prompt with Variables","text":"<p>The most straightforward way to use <code>TextCompletionLLM</code>:</p> <pre><code>from pydantic import BaseModel\nfrom serapeum.core.output_parsers import PydanticParser\nfrom serapeum.core.llms import TextCompletionLLM\nfrom serapeum.ollama import Ollama\n\n\n# Define your output schema\nclass Greeting(BaseModel):\n    message: str\n    language: str\n\n# Initialize the LLM\nllm = Ollama(model=\"llama3.1\", request_timeout=180)\n\n# Create output parser\noutput_parser = PydanticParser(output_cls=Greeting)\n\n# Create TextCompletionLLM with string prompt\ntext_llm = TextCompletionLLM(\n    output_parser=output_parser,\n    prompt=\"Generate a greeting in {language} for {name}. Return as JSON.\",\n    llm=llm,\n)\n\n# Execute with variables\nresult = text_llm(language=\"dutch\", name=\"Ahmed\")\nprint(result.message)  # \"Hallo Ahmed\"\nprint(result.language)  # \"Dutch\"\n</code></pre>"},{"location":"reference/core/llms/orchestrators/text_completion_llm/examples/#initialization-patterns","title":"Initialization Patterns","text":""},{"location":"reference/core/llms/orchestrators/text_completion_llm/examples/#1-with-explicit-output-parser","title":"1. With Explicit Output Parser","text":"<p>Provide a fully configured <code>PydanticParser</code>:</p> <pre><code>from pydantic import BaseModel, Field\nfrom serapeum.core.output_parsers import PydanticParser\nfrom serapeum.core.llms import TextCompletionLLM\nfrom serapeum.ollama import Ollama\n\n\nclass Product(BaseModel):\n    name: str = Field(description=\"Product name\")\n    price: float = Field(description=\"Product price in USD\")\n    in_stock: bool = Field(description=\"Availability status\")\n\nllm = Ollama(model=\"llama3.1\", request_timeout=180)\noutput_parser = PydanticParser(output_cls=Product)\n\ntext_llm = TextCompletionLLM(\n    output_parser=output_parser,\n    prompt=\"Extract product information from: {text}\",\n    llm=llm,\n)\n\nresult = text_llm(text=\"iPhone 15 costs $999 and is available\")\n# Returns: Product(name=\"iPhone 15\", price=999.0, in_stock=True)\n</code></pre>"},{"location":"reference/core/llms/orchestrators/text_completion_llm/examples/#2-with-output-class-only-auto-creates-parser","title":"2. With Output Class Only (Auto-creates Parser)","text":"<p>Let <code>TextCompletionLLM</code> create the parser for you:</p> <pre><code>from pydantic import BaseModel\nfrom serapeum.core.llms import TextCompletionLLM\nfrom serapeum.ollama import Ollama\n\nclass Person(BaseModel):\n    name: str\n    age: int\n\nllm = Ollama(model=\"llama3.1\", request_timeout=180)\n\n# Parser is created automatically from output_cls\ntext_llm = TextCompletionLLM(\n    output_parser=None,  # Will be auto-created\n    prompt=\"Extract person info from: {bio}\",\n    output_cls=Person,  # Parser created from this\n    llm=llm,\n)\n\nresult = text_llm(bio=\"John Smith is 30 years old\")\n# Returns: Person(name=\"John Smith\", age=30)\n</code></pre>"},{"location":"reference/core/llms/orchestrators/text_completion_llm/examples/#3-using-global-llm-from-configs","title":"3. Using Global LLM from Configs","text":"<p>Set a default LLM for the entire application:</p> <pre><code>from pydantic import BaseModel\nfrom serapeum.core.configs.configs import Configs\nfrom serapeum.core.llms import TextCompletionLLM\nfrom serapeum.ollama import Ollama\n\n# Set global LLM\nConfigs.llm = Ollama(model=\"llama3.1\", request_timeout=180)\n\nclass Task(BaseModel):\n    id: str\n    priority: int\n\n# No need to pass llm parameter\ntext_llm = TextCompletionLLM(\n    output_cls=Task,\n    prompt=\"Create a task from: {description}\",\n    # llm=None uses Configs.llm by default\n)\n\nresult = text_llm(description=\"Fix critical bug in authentication\")\n# Returns: Task(id=\"Fix critical bug in authentication\", priority=1)\n</code></pre>"},{"location":"reference/core/llms/orchestrators/text_completion_llm/examples/#prompt-formats","title":"Prompt Formats","text":""},{"location":"reference/core/llms/orchestrators/text_completion_llm/examples/#1-string-prompt-auto-converted-to-prompttemplate","title":"1. String Prompt (Auto-converted to PromptTemplate)","text":"<p>Simple string prompts are automatically wrapped in <code>PromptTemplate</code>:</p> <pre><code>from pydantic import BaseModel\nfrom serapeum.core.llms import TextCompletionLLM\nfrom serapeum.ollama import Ollama\n\nclass Summary(BaseModel):\n    summary: str\n    word_count: int\n\nllm = Ollama(model=\"llama3.1\", request_timeout=180)\n\ntext_llm = TextCompletionLLM(\n    output_cls=Summary,\n    prompt=\"Generate an essay with a maximum number of words {max_words} about the topic: {text}\",  # String prompt\n    llm=llm,\n)\n\nresult = text_llm(\n    text=\"AI\",\n    max_words=50\n)\n</code></pre>"},{"location":"reference/core/llms/orchestrators/text_completion_llm/examples/#2-prompttemplate-object","title":"2. PromptTemplate Object","text":"<p>Use <code>PromptTemplate</code> for more control:</p> <pre><code>from pydantic import BaseModel\nfrom serapeum.core.prompts.base import PromptTemplate\nfrom serapeum.core.llms import TextCompletionLLM\nfrom serapeum.ollama import Ollama\n\nclass Sentiment(BaseModel):\n    sentiment: str\n    confidence: float\n\nllm = Ollama(model=\"llama3.1\", request_timeout=180)\n\n# Create explicit PromptTemplate\nprompt_template = PromptTemplate(\n    \"Analyze sentiment of: {review}\"\n)\n\ntext_llm = TextCompletionLLM(\n    output_cls=Sentiment,\n    prompt=prompt_template,\n    llm=llm,\n)\n\nresult = text_llm(review=\"This product is amazing!\")\n# Returns: Sentiment(sentiment=\"positive\", confidence=1.0)\n</code></pre>"},{"location":"reference/core/llms/orchestrators/text_completion_llm/examples/#3-chatprompttemplate-with-messages","title":"3. ChatPromptTemplate with Messages","text":"<p>Use structured message templates for chat models:</p> <pre><code>from pydantic import BaseModel\nfrom serapeum.core.llms import Message, MessageRole\nfrom serapeum.core.prompts import ChatPromptTemplate\nfrom serapeum.core.llms import TextCompletionLLM\nfrom serapeum.ollama import Ollama\n\nclass Translation(BaseModel):\n    translated_text: str\n    source_language: str\n    target_language: str\n\nllm = Ollama(model=\"llama3.1\", request_timeout=180)\n\n# Create message templates\nmessages = [\n    Message(\n        role=MessageRole.SYSTEM,\n        content=\"You are a professional translator.\"\n    ),\n    Message(\n        role=MessageRole.USER,\n        content=\"Translate to {target_lang}: {text}\"\n    ),\n]\n\nprompt = ChatPromptTemplate(message_templates=messages)\n\ntext_llm = TextCompletionLLM(\n    output_cls=Translation,\n    prompt=prompt,\n    llm=llm,\n)\n\nresult = text_llm(target_lang=\"French\", text=\"Hello, world!\")\n# Returns: Translation(translated_text=\"Bonjour, monde!\", ...)\n</code></pre>"},{"location":"reference/core/llms/orchestrators/text_completion_llm/examples/#execution-modes","title":"Execution Modes","text":""},{"location":"reference/core/llms/orchestrators/text_completion_llm/examples/#1-synchronous-execution","title":"1. Synchronous Execution","text":"<p>Standard blocking execution:</p> <pre><code>from pydantic import BaseModel\nfrom serapeum.core.llms import TextCompletionLLM\nfrom serapeum.ollama import Ollama\n\nclass Answer(BaseModel):\n    answer: str\n    reasoning: str\n\nllm = Ollama(model=\"llama3.1\", request_timeout=180)\n\ntext_llm = TextCompletionLLM(\n    output_cls=Answer,\n    prompt=\"Answer this question: {question}\",\n    llm=llm,\n)\n\n# Synchronous call using __call__\nresult = text_llm(question=\"What is the capital of France?\")\nprint(result.answer)  # \"Paris\"\n</code></pre>"},{"location":"reference/core/llms/orchestrators/text_completion_llm/examples/#2-asynchronous-execution","title":"2. Asynchronous Execution","text":"<p>Non-blocking async execution:</p> <pre><code>import asyncio\nfrom pydantic import BaseModel\nfrom serapeum.core.llms import TextCompletionLLM\nfrom serapeum.ollama import Ollama\n\nclass Analysis(BaseModel):\n    result: str\n    confidence: float\n\nllm = Ollama(model=\"llama3.1\", request_timeout=180)\n\ntext_llm = TextCompletionLLM(\n    output_cls=Analysis,\n    prompt=\"Analyze: {data}\",\n    llm=llm,\n)\n\nasync def analyze_data(data: str) -&gt; Analysis:\n    # Asynchronous call using acall\n    result = await text_llm.acall(data=data)\n    return result\n\n# Run async function\nresult = asyncio.run(analyze_data(\"Sample data\"))\n</code></pre>"},{"location":"reference/core/llms/orchestrators/text_completion_llm/examples/#3-batch-processing","title":"3. Batch Processing","text":"<p>Process multiple inputs efficiently:</p> <pre><code>import asyncio\nfrom pydantic import BaseModel\nfrom serapeum.core.llms import TextCompletionLLM\nfrom serapeum.ollama import Ollama\n\nclass Category(BaseModel):\n    category: str\n    subcategory: str\n\nllm = Ollama(model=\"llama3.1\", request_timeout=180)\n\ntext_llm = TextCompletionLLM(\n    output_cls=Category,\n    prompt=\"Categorize this item: {item}\",\n    llm=llm,\n)\n\nasync def categorize_batch(items: list[str]) -&gt; list[Category]:\n    tasks = [text_llm.acall(item=item) for item in items]\n    results = await asyncio.gather(*tasks)\n    return results\n\nitems = [\"Laptop\", \"Apple\", \"T-shirt\", \"Novel\"]\ncategories = asyncio.run(categorize_batch(items))\nfor item, cat in zip(items, categories):\n    print(f\"{item}: {cat.category}/{cat.subcategory}\")\n</code></pre>"},{"location":"reference/core/llms/orchestrators/text_completion_llm/examples/#4-passing-llm-specific-parameters","title":"4. Passing LLM-specific Parameters","text":"<p>Forward parameters directly to the LLM:</p> <pre><code>from pydantic import BaseModel\nfrom serapeum.core.llms import TextCompletionLLM\nfrom serapeum.ollama import Ollama\n\nclass Story(BaseModel):\n    title: str\n    content: str\n\nllm = Ollama(model=\"llama3.1\", request_timeout=180)\n\ntext_llm = TextCompletionLLM(\n    output_cls=Story,\n    prompt=\"Write a {genre} story about {topic}\",\n    llm=llm,\n)\n\n# Pass LLM-specific kwargs\nresult = text_llm(\n    llm_kwargs={\n        \"temperature\": 0.8,     # Higher temperature for creativity\n        \"top_p\": 0.9,\n        \"max_tokens\": 500,\n    },\n    genre=\"sci-fi\",\n    topic=\"time travel\"\n)\n</code></pre>"},{"location":"reference/core/llms/orchestrators/text_completion_llm/examples/#advanced-usage","title":"Advanced Usage","text":""},{"location":"reference/core/llms/orchestrators/text_completion_llm/examples/#1-dynamic-prompt-updates","title":"1. Dynamic Prompt Updates","text":"<p>Change the prompt at runtime:</p> <pre><code>from pydantic import BaseModel\nfrom serapeum.core.prompts.base import PromptTemplate\nfrom serapeum.core.llms import TextCompletionLLM\nfrom serapeum.ollama import Ollama\n\nclass Response(BaseModel):\n    response: str\n\nllm = Ollama(model=\"llama3.1\", request_timeout=180)\n\ntext_llm = TextCompletionLLM(\n    output_cls=Response,\n    prompt=\"Default prompt: {input}\",\n    llm=llm,\n)\n\n# Use with initial prompt\nresult1 = text_llm(input=\"test\")\n\n# Update prompt dynamically\ntext_llm.prompt = PromptTemplate(\"Updated prompt: {input}\")\n\n# Use with new prompt\nresult2 = text_llm(input=\"test\")\n</code></pre>"},{"location":"reference/core/llms/orchestrators/text_completion_llm/examples/#2-reusable-instance-pattern","title":"2. Reusable Instance Pattern","text":"<p>Create once, use many times:</p> <pre><code>from pydantic import BaseModel\nfrom serapeum.core.llms import TextCompletionLLM\nfrom serapeum.ollama import Ollama\n\nclass Entity(BaseModel):\n    name: str\n    type: str\n\nllm = Ollama(model=\"llama3.1\", request_timeout=180)\n\n# Create reusable instance\nentity_extractor = TextCompletionLLM(\n    output_cls=Entity,\n    prompt=\"Extract the main entity from: {text}\",\n    llm=llm,\n)\n\n# Reuse multiple times\ntexts = [\n    \"Apple Inc. announced new products\",\n    \"Paris is the capital of France\",\n    \"Python is a programming language\",\n]\n\nfor text in texts:\n    entity = entity_extractor(text=text)\n    print(f\"{entity.name} ({entity.type})\")\n</code></pre>"},{"location":"reference/core/llms/orchestrators/text_completion_llm/examples/#3-complex-nested-models","title":"3. Complex Nested Models","text":"<p>Use deeply nested Pydantic models:</p> <pre><code>from pydantic import BaseModel, Field\nfrom serapeum.core.llms import TextCompletionLLM\nfrom serapeum.ollama import Ollama\n\nclass Address(BaseModel):\n    street: str\n    city: str\n    country: str\n\nclass Contact(BaseModel):\n    email: str\n    phone: str\n\nclass Person(BaseModel):\n    name: str\n    age: int\n    address: Address\n    contact: Contact\n\nllm = Ollama(model=\"llama3.1\", request_timeout=180)\n\ntext_llm = TextCompletionLLM(\n    output_cls=Person,\n    prompt=\"Extract person information from: {bio}\",\n    llm=llm,\n)\n\nbio = \"\"\"\nJohn Doe, 30 years old\nLives at 123 Main St, New York, USA\nEmail: john@example.com\nPhone: +1-555-0123\n\"\"\"\n\nresult = text_llm(bio=bio)\nprint(result.name)              # \"John Doe\"\nprint(result.address.city)      # \"New York\"\nprint(result.contact.email)     # \"john@example.com\"\n</code></pre>"},{"location":"reference/core/llms/orchestrators/text_completion_llm/examples/#4-optional-and-union-types","title":"4. Optional and Union Types","text":"<p>Handle optional fields and union types:</p> <pre><code>from typing import Optional, Union\nfrom pydantic import BaseModel\nfrom serapeum.core.llms import TextCompletionLLM\nfrom serapeum.ollama import Ollama\n\nclass Event(BaseModel):\n    title: str\n    date: str\n    location: Optional[str] = None\n    attendees: Optional[int] = None\n    type: Union[str, None] = \"general\"\n\nllm = Ollama(model=\"llama3.1\", request_timeout=180)\n\ntext_llm = TextCompletionLLM(\n    output_cls=Event,\n    prompt=\"Extract event details from: {text}\",\n    llm=llm,\n)\n\n# Works with partial information\nresult = text_llm(text=\"Python Conference on March 15\")\nprint(result.title)      # \"Python Conference\"\nprint(result.date)       # \"March 15\"\nprint(result.location)   # None (optional field)\n</code></pre>"},{"location":"reference/core/llms/orchestrators/text_completion_llm/examples/#5-list-and-array-fields","title":"5. List and Array Fields","text":"<p>Extract lists of items:</p> <pre><code>from pydantic import BaseModel\nfrom serapeum.core.llms import TextCompletionLLM\nfrom serapeum.ollama import Ollama\n\nclass Recipe(BaseModel):\n    name: str\n    ingredients: list[str]\n    steps: list[str]\n    prep_time: int  # in minutes\n\nllm = Ollama(model=\"llama3.1\", request_timeout=180)\n\ntext_llm = TextCompletionLLM(\n    output_cls=Recipe,\n    prompt=\"Extract recipe from: {text}\",\n    llm=llm,\n)\n\nrecipe_text = \"\"\"\nChocolate Chip Cookies\nIngredients: flour, butter, sugar, eggs, chocolate chips\nSteps: Mix dry ingredients, cream butter and sugar, add eggs, fold in chips, bake\nPrep time: 30 minutes\n\"\"\"\n\nresult = text_llm(text=recipe_text)\nprint(result.name)\nprint(f\"Ingredients: {', '.join(result.ingredients)}\")\nprint(f\"Steps: {len(result.steps)}\")\n</code></pre>"},{"location":"reference/core/llms/orchestrators/text_completion_llm/examples/#error-handling","title":"Error Handling","text":""},{"location":"reference/core/llms/orchestrators/text_completion_llm/examples/#1-handling-validation-errors","title":"1. Handling Validation Errors","text":"<p>Catch and handle Pydantic validation errors:</p> <pre><code>from pydantic import BaseModel, ValidationError\nfrom serapeum.core.llms import TextCompletionLLM\nfrom serapeum.ollama import Ollama\n\nclass StrictModel(BaseModel):\n    count: int  # Must be integer\n    ratio: float  # Must be float\n\nllm = Ollama(model=\"llama3.1\", request_timeout=180)\n\ntext_llm = TextCompletionLLM(\n    output_cls=StrictModel,\n    prompt=\"Extract numbers from: {text}\",\n    llm=llm,\n)\n\ntry:\n    result = text_llm(text=\"Invalid data\")\nexcept ValidationError as e:\n    print(f\"Validation failed: {e}\")\n    # Handle invalid response format\nexcept ValueError as e:\n    print(f\"Type mismatch: {e}\")\n    # Handle type checking errors\n</code></pre>"},{"location":"reference/core/llms/orchestrators/text_completion_llm/examples/#2-handling-missing-llm","title":"2. Handling Missing LLM","text":"<p>Gracefully handle missing LLM configuration:</p> <pre><code>from pydantic import BaseModel\nfrom serapeum.core.configs.configs import Configs\nfrom serapeum.core.llms import TextCompletionLLM\n\nclass Data(BaseModel):\n    value: str\n\n# Clear global LLM\nConfigs.llm = None\n\ntry:\n    text_llm = TextCompletionLLM(\n        output_cls=Data,\n        prompt=\"Process: {input}\",\n        llm=None,  # No LLM provided\n    )\nexcept AssertionError as e:\n    print(\"LLM must be provided or set in Configs\")\n    # Provide fallback or configuration instructions\n</code></pre>"},{"location":"reference/core/llms/orchestrators/text_completion_llm/examples/#3-handling-type-mismatches","title":"3. Handling Type Mismatches","text":"<p>Handle cases where parser returns wrong type:</p> <pre><code>from pydantic import BaseModel\nfrom serapeum.core.output_parsers import BaseParser\nfrom serapeum.core.llms import TextCompletionLLM\nfrom serapeum.ollama import Ollama\n\n\nclass ExpectedModel(BaseModel):\n    value: str\n\nclass WrongModel(BaseModel):\n    other: str\n\n# Custom parser that returns wrong type\nclass FaultyParser(BaseParser):\n\n    def parse(self, output: str):\n        return WrongModel(other=output)  # Wrong type!\n\nllm = Ollama(model=\"llama3.1\", request_timeout=180)\n\ntext_llm = TextCompletionLLM(\n    output_parser=FaultyParser(),\n    output_cls=ExpectedModel,\n    prompt=\"Process: {input}\",\n    llm=llm,\n)\n\ntry:\n    result = text_llm(input=\"test\")\nexcept ValueError as e:\n    print(f\"Type check failed: {e}\")\n    # Parser returned wrong type\n</code></pre>"},{"location":"reference/core/llms/orchestrators/text_completion_llm/examples/#4-retry-logic","title":"4. Retry Logic","text":"<p>Implement retry logic for robustness:</p> <pre><code>import asyncio\nfrom pydantic import BaseModel\nfrom serapeum.core.llms import TextCompletionLLM\nfrom serapeum.ollama import Ollama\n\nclass Result(BaseModel):\n    data: str\n\nllm = Ollama(model=\"llama3.1\", request_timeout=180)\n\ntext_llm = TextCompletionLLM(\n    output_cls=Result,\n    prompt=\"Process: {input}\",\n    llm=llm,\n)\n\nasync def call_with_retry(\n    text_llm: TextCompletionLLM,\n    max_retries: int = 3,\n    **kwargs\n) -&gt; Result:\n    \"\"\"Call TextCompletionLLM with retry logic.\"\"\"\n    for attempt in range(max_retries):\n        try:\n            return await text_llm.acall(**kwargs)\n        except Exception as e:\n            if attempt == max_retries - 1:\n                raise\n            print(f\"Attempt {attempt + 1} failed: {e}. Retrying...\")\n            await asyncio.sleep(2 ** attempt)  # Exponential backoff\n\n# Use with retry\nresult = asyncio.run(call_with_retry(text_llm, input=\"test\"))\n</code></pre>"},{"location":"reference/core/llms/orchestrators/text_completion_llm/examples/#best-practices","title":"Best Practices","text":""},{"location":"reference/core/llms/orchestrators/text_completion_llm/examples/#1-model-validation","title":"1. Model Validation","text":"<p>Always define clear Pydantic models with validation:</p> <pre><code>from pydantic import BaseModel, Field, field_validator\nfrom serapeum.core.llms import TextCompletionLLM\nfrom serapeum.ollama import Ollama\n\nclass ValidatedData(BaseModel):\n    email: str = Field(pattern=r'^[\\w\\.-]+@[\\w\\.-]+\\.\\w+$')\n    age: int = Field(ge=0, le=150)\n    score: float = Field(ge=0.0, le=1.0)\n\n    @field_validator('email')\n    def validate_email(cls, v):\n        if '@' not in v:\n            raise ValueError('Invalid email format')\n        return v.lower()\n\nllm = Ollama(model=\"llama3.1\", request_timeout=180)\n\ntext_llm = TextCompletionLLM(\n    output_cls=ValidatedData,\n    prompt=\"Extract data from: {text}\",\n    llm=llm,\n)\n</code></pre>"},{"location":"reference/core/llms/orchestrators/text_completion_llm/examples/#2-clear-prompt-instructions","title":"2. Clear Prompt Instructions","text":"<p>Provide clear instructions for JSON output:</p> <pre><code>from pydantic import BaseModel\nfrom serapeum.core.llms import TextCompletionLLM\nfrom serapeum.ollama import Ollama\n\nclass Output(BaseModel):\n    result: str\n\nllm = Ollama(model=\"llama3.1\", request_timeout=180)\n\n# Good: Clear instructions\ntext_llm = TextCompletionLLM(\n    output_cls=Output,\n    prompt=\"\"\"\n    Analyze the following text and return ONLY valid JSON.\n    Do not include any explanation or markdown formatting.\n\n    Text: {text}\n\n    Return format: {{\"result\": \"your analysis here\"}}\n    \"\"\",\n    llm=llm,\n)\n</code></pre>"},{"location":"reference/core/llms/orchestrators/text_completion_llm/examples/#3-instance-reuse","title":"3. Instance Reuse","text":"<p>Create instances once and reuse them:</p> <pre><code>from pydantic import BaseModel\nfrom serapeum.core.llms import TextCompletionLLM\nfrom serapeum.ollama import Ollama\n\nclass Classification(BaseModel):\n    category: str\n    confidence: float\n\n# Create once\nllm = Ollama(model=\"llama3.1\", request_timeout=180)\nclassifier = TextCompletionLLM(\n    output_cls=Classification,\n    prompt=\"Classify: {text}\",\n    llm=llm,\n)\ntexts = [\"Apple\", \"Banana\", \"Pear\"]\n\n# Reuse many times - this is efficient!\nfor text in texts:\n    result = classifier(text=text)\n</code></pre>"},{"location":"reference/core/llms/orchestrators/text_completion_llm/examples/#see-also","title":"See Also","text":"<ul> <li>Execution Flow and Method Calls - Detailed sequence diagram</li> <li>Architecture and Class Relationships - Class structure</li> <li>Data Transformations and Validation - Data flow details</li> <li>Component Boundaries and Interactions - System components</li> <li>Lifecycle States and Transitions - State management</li> </ul>"},{"location":"reference/core/llms/orchestrators/text_completion_llm/general/","title":"TextCompletionLLM Workflow","text":"<p>This directory contains comprehensive explaining the complete workflow of the <code>TextCompletionLLM</code> class, from initialization to execution and output parsing.</p>"},{"location":"reference/core/llms/orchestrators/text_completion_llm/general/#overview","title":"Overview","text":"<p>The <code>TextCompletionLLM</code> is a structured text completion runner that orchestrates: 1. Prompt formatting with template variables 2. LLM execution (chat or completion mode) 3. Output parsing into validated Pydantic models</p>"},{"location":"reference/core/llms/orchestrators/text_completion_llm/general/#example-usage","title":"Example Usage","text":"<pre><code>from pydantic import BaseModel\nfrom serapeum.core.output_parsers import PydanticParser\nfrom serapeum.core.llms import TextCompletionLLM\nfrom serapeum.ollama import Ollama\n\n\n# Define the output schema\nclass ModelTest(BaseModel):\n    hello: str\n\n# Initialize components\nLLM = Ollama(model=\"llama3.1\", request_timeout=180)\noutput_parser = PydanticParser(output_cls=ModelTest)\n\n# Create TextCompletionLLM instance\ntext_llm = TextCompletionLLM(\n    output_parser=output_parser,\n    prompt=\"This is a test prompt with a {test_input}.\",\n    llm=LLM,\n)\n\n# Execute and get structured output\nobj_output = text_llm(test_input=\"hello\")\n# Returns: ModelTest(hello=\"...\")\n</code></pre>"},{"location":"reference/core/llms/orchestrators/text_completion_llm/general/#understanding-the-workflow","title":"Understanding the Workflow","text":""},{"location":"reference/core/llms/orchestrators/text_completion_llm/general/#1-execution-flow-and-method-calls","title":"1. Execution Flow and Method Calls","text":"<p>Shows the chronological flow of method calls and interactions.</p> <p>Best for: - Understanding the order of operations - Seeing how objects communicate - Debugging execution flow</p> <p>Key Sections: - Initialization phase (validation and component setup) - Execution phase (prompt formatting and LLM invocation) - Parsing phase (JSON extraction and Pydantic validation)</p>"},{"location":"reference/core/llms/orchestrators/text_completion_llm/general/#2-architecture-and-class-relationships","title":"2. Architecture and Class Relationships","text":"<p>Illustrates the static structure and relationships between classes.</p> <p>Best for: - Understanding the architecture - Seeing inheritance and composition - Identifying class responsibilities</p> <p>Key Classes: - <code>TextCompletionLLM</code>: Main orchestrator - <code>PydanticParser</code>: Output validation - <code>BasePromptTemplate</code> hierarchy: Prompt formatting - <code>LLM</code> hierarchy: Model execution</p>"},{"location":"reference/core/llms/orchestrators/text_completion_llm/general/#3-data-transformations-and-validation","title":"3. Data Transformations and Validation","text":"<p>Tracks how data transforms through the system.</p> <p>Best for: - Understanding data transformations - Identifying validation points - Seeing error handling paths</p> <p>Key Flows: - Initialization validation pipeline - Chat model execution path - Completion model execution path - JSON parsing and validation</p>"},{"location":"reference/core/llms/orchestrators/text_completion_llm/general/#4-component-boundaries-and-interactions","title":"4. Component Boundaries and Interactions","text":"<p>Shows component boundaries and interaction patterns.</p> <p>Best for: - Understanding system architecture - Seeing component responsibilities - Identifying interaction patterns</p> <p>Key Components: - User space (application code) - TextCompletionLLM layer (orchestration) - Prompt layer (template formatting) - LLM layer (model execution) - Parser layer (output validation) - External service (Ollama server)</p>"},{"location":"reference/core/llms/orchestrators/text_completion_llm/general/#5-lifecycle-states-and-transitions","title":"5. Lifecycle States and Transitions","text":"<p>Depicts the lifecycle states and transitions.</p> <p>Best for: - Understanding instance lifecycle - Seeing state transitions - Identifying error states</p> <p>Key States: - Initialization states (validation) - Execution states (chat vs completion) - Parsing states (JSON extraction and validation) - Error states (initialization, execution, parsing)</p>"},{"location":"reference/core/llms/orchestrators/text_completion_llm/general/#workflow-summary","title":"Workflow Summary","text":""},{"location":"reference/core/llms/orchestrators/text_completion_llm/general/#initialization-workflow","title":"Initialization Workflow","text":"<pre><code>1. Create PydanticParser with output schema (ModelTest)\n2. Initialize TextCompletionLLM with:\n   - output_parser: PydanticParser\n   - prompt: String or BasePromptTemplate\n   - llm: Ollama instance\n3. Validation occurs:\n   - Parser type and output_cls extraction\n   - LLM instance availability check\n   - Prompt conversion to template if needed\n4. Components stored in instance\n5. Instance ready for execution\n</code></pre>"},{"location":"reference/core/llms/orchestrators/text_completion_llm/general/#execution-workflow","title":"Execution Workflow","text":"<pre><code>1. User calls text_llm(test_input=\"hello\")\n2. Check LLM.metadata.is_chat_model:\n\n   If Chat Model (True):\n   a. Format prompt to messages with variables\n   b. Extend messages with system prompts\n   c. Call Ollama.chat() \u2192 HTTP POST /api/chat\n   d. Extract message.content from ChatResponse\n\n   If Completion Model (False):\n   a. Format prompt to string with variables\n   b. Extend prompt with system prompts\n   c. Call Ollama.complete() \u2192 HTTP POST /api/generate\n   d. Extract text from CompletionResponse\n\n3. Parse output:\n   a. Extract JSON string from raw text\n   b. Validate JSON against Pydantic schema\n   c. Create ModelTest instance\n   d. Type check output\n\n4. Return validated ModelTest instance\n</code></pre>"},{"location":"reference/core/llms/orchestrators/text_completion_llm/general/#key-design-patterns","title":"Key Design Patterns","text":""},{"location":"reference/core/llms/orchestrators/text_completion_llm/general/#1-validation-at-construction","title":"1. Validation at Construction","text":"<p>All components are validated during <code>__init__</code>, ensuring errors are caught early: - Parser type checking - LLM availability verification - Prompt format conversion</p>"},{"location":"reference/core/llms/orchestrators/text_completion_llm/general/#2-adapter-pattern","title":"2. Adapter Pattern","text":"<p><code>TextCompletionLLM</code> adapts different prompt types and LLM modes: - String prompts \u2192 PromptTemplate - Chat models \u2192 format_messages - Completion models \u2192 format string</p>"},{"location":"reference/core/llms/orchestrators/text_completion_llm/general/#3-strategy-pattern","title":"3. Strategy Pattern","text":"<p>Output parsing strategy is injected via <code>PydanticParser</code>: - Customizable JSON extraction - Flexible schema validation - Extensible error handling</p>"},{"location":"reference/core/llms/orchestrators/text_completion_llm/general/#4-template-method-pattern","title":"4. Template Method Pattern","text":"<p><code>__call__</code> defines the algorithm skeleton: 1. Check model type 2. Format prompt (strategy varies) 3. Execute LLM (path varies) 4. Parse output (consistent)</p>"},{"location":"reference/core/llms/orchestrators/text_completion_llm/general/#performance-considerations","title":"Performance Considerations","text":"<ol> <li>Reusable Instances: <code>TextCompletionLLM</code> instances are reusable after initialization</li> <li>Async Support: <code>acall()</code> method provides async execution</li> <li>Streaming Support: LLM layer supports streaming responses</li> <li>Stateless Execution: Each call creates independent transient state</li> </ol>"},{"location":"reference/core/llms/orchestrators/text_completion_llm/text_completion_llm_class/","title":"Architecture and Class Relationships","text":"<p>This diagram shows the class relationships and structure for <code>TextCompletionLLM</code>.</p>  Hold \"Ctrl\" to enable pan &amp; zoom  <pre><code>classDiagram\n    class BasePydanticLLM~BaseModel~ {\n        &lt;&lt;abstract&gt;&gt;\n        +output_cls: Type[BaseModel]\n        +__call__(**kwargs) BaseModel\n        +acall(**kwargs) BaseModel\n    }\n\n    class TextCompletionLLM~BaseModel~ {\n        +__init__(output_parser, prompt, output_cls, llm, verbose)\n        +prompt: BasePromptTemplate\n        +output_cls: Type[BaseModel]\n        +__call__(llm_kwargs, **kwargs) BaseModel\n        +acall(llm_kwargs, **kwargs) BaseModel\n        -_output_parser: BaseParser\n        -_output_cls: Type[BaseModel]\n        -_llm: LLM\n        -_prompt: BasePromptTemplate\n        -_verbose: bool\n        +_validate_prompt(prompt) BasePromptTemplate\n        +_validate_llm(llm) LLM\n        +_validate_output_parser_cls(parser, cls) Tuple\n    }\n\n    class BaseParser {\n        &lt;&lt;abstract&gt;&gt;\n        +parse(output: str) Any\n        +format(query: str) str\n        +format_messages(messages) List[Message]\n    }\n\n    class PydanticParser~Model~ {\n        -_output_cls: Type[Model]\n        -_excluded_schema_keys_from_format: List\n        -_pydantic_format_tmpl: str\n        +__init__(output_cls, excluded_schema_keys, pydantic_format_tmpl)\n        +output_cls: Type[Model]\n        +format_string: str\n        +get_format_string(escape_json) str\n        +parse(text: str) Any\n        +format(query: str) str\n    }\n\n    class BasePromptTemplate {\n        &lt;&lt;abstract&gt;&gt;\n        +metadata: Dict[str, Any]\n        +template_vars: List[str]\n        +kwargs: Dict[str, str]\n        +output_parser: Optional[BaseParser]\n        +template_var_mappings: Optional[Dict]\n        +function_mappings: Optional[Dict]\n        +partial_format(**kwargs) BasePromptTemplate\n        +format(llm, **kwargs) str\n        +format_messages(llm, **kwargs) List[Message]\n        +get_template(llm) str\n    }\n\n    class PromptTemplate {\n        +template: str\n        +__init__(template, prompt_type, output_parser, metadata, ...)\n        +partial_format(**kwargs) PromptTemplate\n        +format(llm, completion_to_prompt, **kwargs) str\n        +format_messages(llm, **kwargs) List[Message]\n        +get_template(llm) str\n    }\n\n    class ChatPromptTemplate {\n        +message_templates: List[Message]\n        +__init__(message_templates, prompt_type, output_parser, ...)\n        +from_messages(message_templates, **kwargs) ChatPromptTemplate\n        +partial_format(**kwargs) ChatPromptTemplate\n        +format(llm, messages_to_prompt, **kwargs) str\n        +format_messages(llm, **kwargs) List[Message]\n        +get_template(llm) str\n    }\n\n    class BaseLLM {\n        &lt;&lt;abstract&gt;&gt;\n        +metadata: Metadata\n        +chat(messages, **kwargs) ChatResponse\n        +stream_chat(messages, **kwargs) ChatResponseGen\n        +achat(messages, **kwargs) ChatResponse\n        +astream_chat(messages, **kwargs) ChatResponseAsyncGen\n        +complete(prompt, **kwargs) CompletionResponse\n        +stream_complete(prompt, **kwargs) CompletionResponseGen\n        +acomplete(prompt, **kwargs) CompletionResponse\n        +astream_complete(prompt, **kwargs) CompletionResponseAsyncGen\n    }\n\n    class LLM {\n        +system_prompt: Optional[str]\n        +messages_to_prompt: MessagesToPromptCallable\n        +completion_to_prompt: CompletionToPromptCallable\n        +output_parser: Optional[BaseParser]\n        +pydantic_program_mode: StructuredLLMMode\n        +_get_prompt(prompt, **kwargs) str\n        +_get_messages(prompt, **kwargs) List[Message]\n        +_parse_output(output) str\n        +_extend_prompt(formatted_prompt) str\n        +_extend_messages(messages) List[Message]\n        +predict(prompt, **kwargs) str\n        +stream(prompt, **kwargs) TokenGen\n        +apredict(prompt, **kwargs) str\n        +astream(prompt, **kwargs) TokenAsyncGen\n        +structured_predict(output_cls, prompt, **kwargs) Model\n    }\n\n    class Ollama {\n        +model: str\n        +base_url: str\n        +request_timeout: int\n        +temperature: float\n        +metadata: Metadata\n        +__init__(model, base_url, request_timeout, ...)\n        +chat(messages, **kwargs) ChatResponse\n        +achat(messages, **kwargs) ChatResponse\n        +complete(prompt, **kwargs) CompletionResponse\n        +acomplete(prompt, **kwargs) CompletionResponse\n        -_chat_request(messages, stream, **kwargs) dict\n        -_complete_request(prompt, stream, **kwargs) dict\n    }\n\n    class BaseModel {\n        &lt;&lt;pydantic&gt;&gt;\n        +model_validate_json(json_data) BaseModel\n        +model_json_schema() dict\n    }\n\n    class ModelTest {\n        +hello: str\n    }\n\n    BasePydanticLLM &lt;|-- TextCompletionLLM\n    BaseLLM &lt;|-- LLM\n    LLM &lt;|-- Ollama\n    BaseParser &lt;|-- PydanticParser\n    BasePromptTemplate &lt;|-- PromptTemplate\n    BasePromptTemplate &lt;|-- ChatPromptTemplate\n    BaseModel &lt;|-- ModelTest\n\n    TextCompletionLLM o-- PydanticParser : uses\n    TextCompletionLLM o-- BasePromptTemplate : uses\n    TextCompletionLLM o-- LLM : uses\n    TextCompletionLLM ..&gt; ModelTest : produces\n    PydanticParser o-- ModelTest : validates against\n    PromptTemplate --|&gt; BasePromptTemplate\n    ChatPromptTemplate --|&gt; BasePromptTemplate\n\n    note for TextCompletionLLM \"Main orchestrator that:\\n1. Validates parser, prompt, LLM\\n2. Formats prompts with variables\\n3. Calls LLM (chat or complete)\\n4. Parses output to Pydantic model\"\n    note for PydanticParser \"Extracts JSON from text\\nand validates against schema\"\n    note for Ollama \"Concrete LLM implementation\\nfor Ollama server\"</code></pre>"},{"location":"reference/core/llms/orchestrators/text_completion_llm/text_completion_llm_class/#class-responsibilities","title":"Class Responsibilities","text":""},{"location":"reference/core/llms/orchestrators/text_completion_llm/text_completion_llm_class/#textcompletionllm","title":"TextCompletionLLM","text":"<ul> <li>Orchestrates the complete workflow from prompt to structured output</li> <li>Validates all components during initialization</li> <li>Routes to chat or completion based on LLM metadata</li> <li>Ensures type safety of output</li> </ul>"},{"location":"reference/core/llms/orchestrators/text_completion_llm/text_completion_llm_class/#pydanticparser","title":"PydanticParser","text":"<ul> <li>Extracts JSON from raw LLM output</li> <li>Validates JSON against Pydantic schema</li> <li>Formats prompts with schema hints</li> </ul>"},{"location":"reference/core/llms/orchestrators/text_completion_llm/text_completion_llm_class/#ollama-llm","title":"Ollama (LLM)","text":"<ul> <li>Executes requests to Ollama server</li> <li>Supports both chat and completion modes</li> <li>Handles streaming and async operations</li> </ul>"},{"location":"reference/core/llms/orchestrators/text_completion_llm/text_completion_llm_class/#prompttemplatechatprompttemplate","title":"PromptTemplate/ChatPromptTemplate","text":"<ul> <li>Formats prompts with variables</li> <li>Supports both string and message-based templates</li> <li>Manages template variables and mappings</li> </ul>"},{"location":"reference/core/llms/orchestrators/text_completion_llm/text_completion_llm_components/","title":"Component Boundaries and Interactions","text":"<p>This diagram shows how components interact during the complete lifecycle.</p>  Hold \"Ctrl\" to enable pan &amp; zoom  <pre><code>graph TB\n    subgraph User Space\n        UC[User Code]\n        MT[ModelTest Schema]\n    end\n\n    subgraph TextCompletionLLM Components\n        TCL[TextCompletionLLM]\n\n        subgraph Validators\n            VP[validate_prompt]\n            VL[validate_llm]\n            VO[validate_output_parser_cls]\n        end\n\n        subgraph Stored State\n            SP[\"_prompt: BasePromptTemplate\"]\n            SL[\"_llm: LLM\"]\n            SO[\"_output_parser: PydanticParser\"]\n            SC[\"_output_cls: Type ModelTest\"]\n        end\n    end\n\n    subgraph Output Parser Layer\n        POP[PydanticParser]\n        EJ[extract_json_str]\n        VJ[model_validate_json]\n    end\n\n    subgraph Prompt Layer\n        PT[PromptTemplate]\n        CPT[ChatPromptTemplate]\n\n        subgraph Prompt Operations\n            PF[format]\n            FM[format_messages]\n            AV[Apply Variables]\n        end\n    end\n\n    subgraph LLM Layer\n        OL[Ollama]\n\n        subgraph LLM Operations\n            GM[_get_messages]\n            GP[_get_prompt]\n            EM[_extend_messages]\n            EP[_extend_prompt]\n        end\n\n        subgraph API Methods\n            CH[chat]\n            CO[complete]\n        end\n\n        MD[metadata.is_chat_model]\n    end\n\n    subgraph External Service\n        OS[Ollama Server]\n\n        subgraph Endpoints\n            EC[\"/api/chat\"]\n            EG[\"/api/generate\"]\n        end\n    end\n\n    subgraph Response Models\n        CHR[ChatResponse]\n        COR[CompletionResponse]\n    end\n\n    %% Initialization Flow\n    UC --&gt;|1. Create parser| POP\n    POP --&gt;|output_cls| MT\n\n    UC --&gt;|2. Initialize| TCL\n    TCL --&gt;|Validate| VO\n    VO --&gt;|Extract| MT\n\n    TCL --&gt;|Validate| VL\n    VL --&gt;|Check/Fallback| OL\n\n    TCL --&gt;|Validate/Convert| VP\n    VP --&gt;|Create if string| PT\n\n    VO --&gt;|Store| SO\n    VO --&gt;|Store| SC\n    VL --&gt;|Store| SL\n    VP --&gt;|Store| SP\n\n    %% Execution Flow - Chat Path\n    UC --&gt;|3. Call with kwargs| TCL\n    TCL --&gt;|Check| MD\n\n    MD --&gt;|True| FM\n    FM --&gt;|Use kwargs| AV\n    AV --&gt;|Messages| GM\n    GM --&gt;|Add system| EM\n    EM --&gt;|Send| CH\n    CH --&gt;|HTTP POST| EC\n    EC --&gt;|Response| CHR\n    CHR --&gt;|message.content| EJ\n\n    %% Execution Flow - Completion Path\n    MD --&gt;|False| PF\n    PF --&gt;|Use kwargs| AV\n    AV --&gt;|String| GP\n    GP --&gt;|Add system| EP\n    EP --&gt;|Send| CO\n    CO --&gt;|HTTP POST| EG\n    EG --&gt;|Response| COR\n    COR --&gt;|text| EJ\n\n    %% Parsing Flow\n    EJ --&gt;|Extract| VJ\n    VJ --&gt;|Validate| MT\n    MT --&gt;|Instance| TCL\n    TCL --&gt;|Type check| UC\n\n    %% Styling\n    classDef userClass fill:#e1f5ff,stroke:#01579b\n    classDef validatorClass fill:#fff9c4,stroke:#f57f17\n    classDef stateClass fill:#f3e5f5,stroke:#4a148c\n    classDef parserClass fill:#e8f5e9,stroke:#1b5e20\n    classDef promptClass fill:#fce4ec,stroke:#880e4f\n    classDef llmClass fill:#e0f2f1,stroke:#004d40\n    classDef responseClass fill:#fff3e0,stroke:#e65100\n    classDef externalClass fill:#efebe9,stroke:#3e2723\n\n    class UC,MT userClass\n    class VP,VL,VO validatorClass\n    class SP,SL,SO,SC stateClass\n    class POP,EJ,VJ parserClass\n    class PT,CPT,PF,FM,AV promptClass\n    class OL,GM,GP,EM,EP,CH,CO,MD llmClass\n    class CHR,COR responseClass\n    class OS,EC,EG externalClass</code></pre>"},{"location":"reference/core/llms/orchestrators/text_completion_llm/text_completion_llm_components/#component-responsibilities-matrix","title":"Component Responsibilities Matrix","text":"Component Initialization Execution Parsing TextCompletionLLM Validates &amp; stores all components Routes to chat/complete Type checks output PydanticParser Stores output_cls schema - Extracts JSON &amp; validates PromptTemplate Created/validated Formats with variables - Ollama (LLM) Validated/stored Executes requests - ModelTest Defines schema - Validates parsed data"},{"location":"reference/core/llms/orchestrators/text_completion_llm/text_completion_llm_components/#interaction-patterns","title":"Interaction Patterns","text":""},{"location":"reference/core/llms/orchestrators/text_completion_llm/text_completion_llm_components/#1-initialization-pattern-constructor","title":"1. Initialization Pattern (Constructor)","text":"<pre><code>User \u2192 TextCompletionLLM.__init__\n  \u251c\u2500\u2192 validate_output_parser_cls\n  \u2502   \u2514\u2500\u2192 Extract/Create parser &amp; output_cls\n  \u251c\u2500\u2192 validate_llm\n  \u2502   \u2514\u2500\u2192 Use provided or fallback to Configs.llm\n  \u2514\u2500\u2192 validate_prompt\n      \u2514\u2500\u2192 Convert string to PromptTemplate if needed\n</code></pre>"},{"location":"reference/core/llms/orchestrators/text_completion_llm/text_completion_llm_components/#2-chat-model-execution-pattern","title":"2. Chat Model Execution Pattern","text":"<pre><code>User \u2192 TextCompletionLLM.__call__\n  \u251c\u2500\u2192 Check metadata.is_chat_model == True\n  \u251c\u2500\u2192 PromptTemplate.format_messages(kwargs)\n  \u251c\u2500\u2192 LLM._extend_messages\n  \u251c\u2500\u2192 Ollama.chat \u2192 HTTP \u2192 ChatResponse\n  \u251c\u2500\u2192 Extract message.content\n  \u2514\u2500\u2192 PydanticParser.parse \u2192 ModelTest\n</code></pre>"},{"location":"reference/core/llms/orchestrators/text_completion_llm/text_completion_llm_components/#3-completion-model-execution-pattern","title":"3. Completion Model Execution Pattern","text":"<pre><code>User \u2192 TextCompletionLLM.__call__\n  \u251c\u2500\u2192 Check metadata.is_chat_model == False\n  \u251c\u2500\u2192 PromptTemplate.format(kwargs)\n  \u251c\u2500\u2192 LLM._extend_prompt\n  \u251c\u2500\u2192 Ollama.complete \u2192 HTTP \u2192 CompletionResponse\n  \u251c\u2500\u2192 Extract response.text\n  \u2514\u2500\u2192 PydanticParser.parse \u2192 ModelTest\n</code></pre>"},{"location":"reference/core/llms/orchestrators/text_completion_llm/text_completion_llm_components/#state-management","title":"State Management","text":""},{"location":"reference/core/llms/orchestrators/text_completion_llm/text_completion_llm_components/#immutable-state-post-initialization","title":"Immutable State (Post-Initialization)","text":"<ul> <li><code>_output_parser</code>: PydanticParser instance</li> <li><code>_output_cls</code>: Type[ModelTest]</li> <li><code>_llm</code>: Ollama instance</li> </ul>"},{"location":"reference/core/llms/orchestrators/text_completion_llm/text_completion_llm_components/#mutable-state","title":"Mutable State","text":"<ul> <li><code>_prompt</code>: Can be updated via setter</li> <li><code>_verbose</code>: Controls logging output</li> </ul>"},{"location":"reference/core/llms/orchestrators/text_completion_llm/text_completion_llm_components/#transient-state-per-call","title":"Transient State (Per Call)","text":"<ul> <li><code>llm_kwargs</code>: Forwarded to LLM methods</li> <li><code>**kwargs</code>: Template variables for prompt formatting</li> <li><code>raw_output</code>: Intermediate text from LLM</li> <li><code>parsed_output</code>: Final validated Pydantic model</li> </ul>"},{"location":"reference/core/llms/orchestrators/text_completion_llm/text_completion_llm_dataflow/","title":"Data Transformations and Validation","text":"<p>This diagram shows how data flows through the <code>TextCompletionLLM</code> system.</p>  Hold \"Ctrl\" to enable pan &amp; zoom  <pre><code>flowchart TD\n    Start([User Code]) --&gt; Init{Initialize TextCompletionLLM}\n\n    Init --&gt; ValidateParser[Validate Output Parser]\n    ValidateParser --&gt; CheckParser{Parser Type?}\n    CheckParser --&gt;|PydanticParser| ExtractClass[Extract output_cls]\n    CheckParser --&gt;|None + output_cls| CreateParser[Create PydanticParser]\n    CheckParser --&gt;|Other| Error1[Raise ValueError]\n\n    ExtractClass --&gt; ValidateLLM[Validate LLM]\n    CreateParser --&gt; ValidateLLM\n\n    ValidateLLM --&gt; CheckLLM{LLM Provided?}\n    CheckLLM --&gt;|Yes| ValidatePrompt[Validate Prompt]\n    CheckLLM --&gt;|No| CheckConfigs{Configs.llm Set?}\n    CheckConfigs --&gt;|Yes| ValidatePrompt\n    CheckConfigs --&gt;|No| Error2[Raise AssertionError]\n\n    ValidatePrompt --&gt; CheckPrompt{Prompt Type?}\n    CheckPrompt --&gt;|BasePromptTemplate| StoreComponents[Store All Components]\n    CheckPrompt --&gt;|String| ConvertPrompt[Convert to PromptTemplate]\n    CheckPrompt --&gt;|Other| Error3[Raise ValueError]\n\n    ConvertPrompt --&gt; StoreComponents\n    StoreComponents --&gt; Ready([TextCompletionLLM Ready])\n\n    Ready --&gt; Call{User Calls text_llm}\n    Call --&gt;|With kwargs| MergeKwargs[Merge kwargs with defaults]\n\n    MergeKwargs --&gt; CheckModel{LLM.metadata.is_chat_model?}\n\n    CheckModel --&gt;|True - Chat Model| FormatMessages[Format Messages]\n    CheckModel --&gt;|False - Completion Model| FormatPrompt[Format Prompt String]\n\n    FormatMessages --&gt; ApplyVars1[Apply template variables]\n    ApplyVars1 --&gt; AddSchema1[Add JSON schema to messages]\n    AddSchema1 --&gt; ExtendMessages[Extend with system prompts]\n    ExtendMessages --&gt; ChatCall[Call LLM.chat]\n    ChatCall --&gt; ChatRequest[HTTP POST to /api/chat]\n    ChatRequest --&gt; ChatResponse[ChatResponse with message]\n    ChatResponse --&gt; ExtractContent1[Extract message.content]\n    ExtractContent1 --&gt; ParseOutput\n\n    FormatPrompt --&gt; ApplyVars2[Apply template variables]\n    ApplyVars2 --&gt; AddSchema2[Add JSON schema to prompt]\n    AddSchema2 --&gt; ExtendPrompt[Extend with system prompts]\n    ExtendPrompt --&gt; CompleteCall[Call LLM.complete]\n    CompleteCall --&gt; CompleteRequest[HTTP POST to /api/generate]\n    CompleteRequest --&gt; CompleteResponse[CompletionResponse with text]\n    CompleteResponse --&gt; ExtractContent2[Extract response.text]\n    ExtractContent2 --&gt; ParseOutput[Parse Output]\n\n    ParseOutput --&gt; ExtractJSON[Extract JSON string]\n    ExtractJSON --&gt; ValidateJSON{Valid JSON?}\n    ValidateJSON --&gt;|No| Error4[Raise ValueError]\n    ValidateJSON --&gt;|Yes| ValidateSchema[Validate against Pydantic schema]\n\n    ValidateSchema --&gt; CheckSchema{Schema Valid?}\n    CheckSchema --&gt;|No| Error5[Raise ValidationError]\n    CheckSchema --&gt;|Yes| CreateModel[Create Pydantic instance]\n\n    CreateModel --&gt; TypeCheck{isinstance check}\n    TypeCheck --&gt;|Pass| Return([Return ModelTest instance])\n    TypeCheck --&gt;|Fail| Error6[Raise ValueError]\n\n    style Start fill:#e1f5ff\n    style Ready fill:#e1f5ff\n    style Return fill:#c8e6c9\n    style Error1 fill:#ffcdd2\n    style Error2 fill:#ffcdd2\n    style Error3 fill:#ffcdd2\n    style Error4 fill:#ffcdd2\n    style Error5 fill:#ffcdd2\n    style Error6 fill:#ffcdd2\n    style ChatRequest fill:#fff9c4\n    style CompleteRequest fill:#fff9c4</code></pre>"},{"location":"reference/core/llms/orchestrators/text_completion_llm/text_completion_llm_dataflow/#data-transformations","title":"Data Transformations","text":""},{"location":"reference/core/llms/orchestrators/text_completion_llm/text_completion_llm_dataflow/#initialization-phase","title":"Initialization Phase","text":"<pre><code>Input:\n  - output_parser: PydanticParser(output_cls=ModelTest)\n  - prompt: \"This is a test prompt with a {test_input}.\"\n  - llm: Ollama(model=\"llama3.1\")\n\nOutput:\n  - TextCompletionLLM instance with validated components\n</code></pre>"},{"location":"reference/core/llms/orchestrators/text_completion_llm/text_completion_llm_dataflow/#execution-phase-chat-model","title":"Execution Phase (Chat Model)","text":"<pre><code>Input:\n  text_llm(test_input=\"hello\")\n\nTransformations:\n  1. kwargs: {test_input: \"hello\"}\n  2. Formatted messages: [Message(role=USER, content=\"This is a test...\")]\n  3. Extended messages: [Message(role=SYSTEM, ...), Message(role=USER, ...)]\n  4. LLM request: {\"model\": \"llama3.1\", \"messages\": [...]}\n  5. Raw response: '{\"hello\": \"world\"}'\n  6. Extracted JSON: {\"hello\": \"world\"}\n  7. Validated model: ModelTest(hello=\"world\")\n\nOutput:\n  ModelTest(hello=\"world\")\n</code></pre>"},{"location":"reference/core/llms/orchestrators/text_completion_llm/text_completion_llm_dataflow/#execution-phase-completion-model","title":"Execution Phase (Completion Model)","text":"<pre><code>Input:\n  text_llm(test_input=\"hello\")\n\nTransformations:\n  1. kwargs: {test_input: \"hello\"}\n  2. Formatted prompt: \"This is a test prompt with a hello.\"\n  3. Extended prompt: \"System: ...\\n\\nThis is a test prompt with a hello.\"\n  4. LLM request: {\"model\": \"llama3.1\", \"prompt\": \"...\"}\n  5. Raw response: '{\"hello\": \"world\"}'\n  6. Extracted JSON: {\"hello\": \"world\"}\n  7. Validated model: ModelTest(hello=\"world\")\n\nOutput:\n  ModelTest(hello=\"world\")\n</code></pre>"},{"location":"reference/core/llms/orchestrators/text_completion_llm/text_completion_llm_dataflow/#error-handling-points","title":"Error Handling Points","text":"<ol> <li>Parser validation: Ensures PydanticParser is used</li> <li>LLM validation: Ensures LLM instance is available</li> <li>Prompt validation: Ensures prompt is convertible to template</li> <li>JSON extraction: Handles malformed JSON responses</li> <li>Schema validation: Catches Pydantic validation errors</li> <li>Type checking: Ensures output matches expected type</li> </ol>"},{"location":"reference/core/llms/orchestrators/text_completion_llm/text_completion_llm_sequence/","title":"Execution Flow and Method Calls","text":"<p>This diagram shows the complete workflow from initialization to execution of <code>TextCompletionLLM</code>.</p>  Hold \"Ctrl\" to enable pan &amp; zoom  <pre><code>sequenceDiagram\n    participant User\n    participant PydanticParser\n    participant TextCompletionLLM\n    participant PromptTemplate\n    participant Ollama\n    participant LLMChat/Complete\n\n    Note over User: Initialization Phase\n    User-&gt;&gt;PydanticParser: Create parser with ModelTest\n    activate PydanticParser\n    PydanticParser--&gt;&gt;User: parser instance\n    deactivate PydanticParser\n\n    User-&gt;&gt;TextCompletionLLM: __init__(output_parser, prompt, llm)\n    activate TextCompletionLLM\n\n    TextCompletionLLM-&gt;&gt;TextCompletionLLM: validate_output_parser_cls(parser, None)\n    Note over TextCompletionLLM: Validates parser is PydanticParser&lt;br/&gt;Extracts output_cls (ModelTest)\n\n    TextCompletionLLM-&gt;&gt;TextCompletionLLM: validate_llm(llm)\n    Note over TextCompletionLLM: Ensures LLM instance is provided&lt;br/&gt;or falls back to Configs.llm\n\n    TextCompletionLLM-&gt;&gt;TextCompletionLLM: validate_prompt(prompt)\n    Note over TextCompletionLLM: Converts string to PromptTemplate&lt;br/&gt;if needed\n\n    TextCompletionLLM-&gt;&gt;PromptTemplate: Create/validate prompt template\n    activate PromptTemplate\n    PromptTemplate--&gt;&gt;TextCompletionLLM: template instance\n    deactivate PromptTemplate\n\n    TextCompletionLLM--&gt;&gt;User: text_llm instance\n    deactivate TextCompletionLLM\n\n    Note over User: Execution Phase\n    User-&gt;&gt;TextCompletionLLM: __call__(test_input=\"hello\")\n    activate TextCompletionLLM\n\n    TextCompletionLLM-&gt;&gt;Ollama: Check metadata.is_chat_model\n    activate Ollama\n    Ollama--&gt;&gt;TextCompletionLLM: True/False\n    deactivate Ollama\n\n    alt is_chat_model == True\n        TextCompletionLLM-&gt;&gt;PromptTemplate: format_messages(llm, test_input=\"hello\")\n        activate PromptTemplate\n        PromptTemplate--&gt;&gt;TextCompletionLLM: List[Message]\n        deactivate PromptTemplate\n\n        TextCompletionLLM-&gt;&gt;Ollama: _extend_messages(messages)\n        activate Ollama\n        Note over Ollama: Add system prompts if configured\n        Ollama--&gt;&gt;TextCompletionLLM: extended messages\n        deactivate Ollama\n\n        TextCompletionLLM-&gt;&gt;Ollama: chat(messages, **llm_kwargs)\n        activate Ollama\n        Ollama-&gt;&gt;LLMChat/Complete: HTTP request to Ollama server\n        activate LLMChat/Complete\n        LLMChat/Complete--&gt;&gt;Ollama: JSON response\n        deactivate LLMChat/Complete\n        Ollama--&gt;&gt;TextCompletionLLM: ChatResponse\n        deactivate Ollama\n\n        TextCompletionLLM-&gt;&gt;TextCompletionLLM: Extract message.content\n        Note over TextCompletionLLM: raw_output = chat_response.message.content\n    else is_chat_model == False\n        TextCompletionLLM-&gt;&gt;PromptTemplate: format(llm, test_input=\"hello\")\n        activate PromptTemplate\n        PromptTemplate--&gt;&gt;TextCompletionLLM: formatted prompt string\n        deactivate PromptTemplate\n\n        TextCompletionLLM-&gt;&gt;Ollama: complete(formatted_prompt, **llm_kwargs)\n        activate Ollama\n        Ollama-&gt;&gt;LLMChat/Complete: HTTP request to Ollama server\n        activate LLMChat/Complete\n        LLMChat/Complete--&gt;&gt;Ollama: JSON response\n        deactivate LLMChat/Complete\n        Ollama--&gt;&gt;TextCompletionLLM: CompletionResponse\n        deactivate Ollama\n\n        TextCompletionLLM-&gt;&gt;TextCompletionLLM: Extract text\n        Note over TextCompletionLLM: raw_output = response.text\n    end\n\n    TextCompletionLLM-&gt;&gt;PydanticParser: parse(raw_output)\n    activate PydanticParser\n    PydanticParser-&gt;&gt;PydanticParser: extract_json_str(raw_output)\n    Note over PydanticParser: Extracts JSON from text\n\n    PydanticParser-&gt;&gt;PydanticParser: model_validate_json(json_str)\n    Note over PydanticParser: Validates against ModelTest schema\n\n    PydanticParser--&gt;&gt;TextCompletionLLM: ModelTest instance\n    deactivate PydanticParser\n\n    TextCompletionLLM-&gt;&gt;TextCompletionLLM: Validate isinstance(output, ModelTest)\n    Note over TextCompletionLLM: Raises ValueError if type mismatch\n\n    TextCompletionLLM--&gt;&gt;User: ModelTest instance\n    deactivate TextCompletionLLM</code></pre>"},{"location":"reference/core/llms/orchestrators/text_completion_llm/text_completion_llm_sequence/#key-points","title":"Key Points","text":"<ol> <li>Initialization validates all components before storing them</li> <li>Prompt formatting adapts based on whether the LLM is a chat model or completion model</li> <li>Output parsing extracts JSON and validates against the Pydantic schema</li> <li>Type checking ensures the parsed output matches the expected output class</li> </ol>"},{"location":"reference/core/llms/orchestrators/text_completion_llm/text_completion_llm_state/","title":"Lifecycle States and Transitions","text":"<p>This diagram shows the lifecycle states of a <code>TextCompletionLLM</code> instance.</p>  Hold \"Ctrl\" to enable pan &amp; zoom  <pre><code>stateDiagram-v2\n    [*] --&gt; Uninitialized: User creates instance\n\n    Uninitialized --&gt; ValidatingParser: __init__ called\n    ValidatingParser --&gt; ValidatingLLM: Parser validated\n    ValidatingParser --&gt; InitError: Invalid parser type\n    ValidatingLLM --&gt; ValidatingPrompt: LLM validated\n    ValidatingLLM --&gt; InitError: No LLM available\n    ValidatingPrompt --&gt; Ready: Prompt validated\n    ValidatingPrompt --&gt; InitError: Invalid prompt type\n\n    Ready --&gt; ExecutingChat: __call__ invoked\\n(is_chat_model=True)\n    Ready --&gt; ExecutingComplete: __call__ invoked\\n(is_chat_model=False)\n    Ready --&gt; ExecutingAsync: acall invoked\n\n    ExecutingChat --&gt; FormattingMessages: Format prompt to messages\n    FormattingMessages --&gt; ExtendingMessages: Apply template vars\n    ExtendingMessages --&gt; CallingChatAPI: Add system prompts\n    CallingChatAPI --&gt; WaitingChatResponse: HTTP request sent\n    WaitingChatResponse --&gt; ParsingOutput: ChatResponse received\n    WaitingChatResponse --&gt; ExecutionError: Network/API error\n\n    ExecutingComplete --&gt; FormattingPrompt: Format prompt string\n    FormattingPrompt --&gt; ExtendingPrompt: Apply template vars\n    ExtendingPrompt --&gt; CallingCompleteAPI: Add system prompts\n    CallingCompleteAPI --&gt; WaitingCompleteResponse: HTTP request sent\n    WaitingCompleteResponse --&gt; ParsingOutput: CompletionResponse received\n    WaitingCompleteResponse --&gt; ExecutionError: Network/API error\n\n    ExecutingAsync --&gt; FormattingMessages: Similar to sync\n    ExecutingAsync --&gt; FormattingPrompt: but async\n\n    ParsingOutput --&gt; ExtractingJSON: Parse raw text\n    ExtractingJSON --&gt; ValidatingSchema: JSON extracted\n    ExtractingJSON --&gt; ParsingError: Malformed JSON\n    ValidatingSchema --&gt; TypeChecking: Pydantic validation passed\n    ValidatingSchema --&gt; ParsingError: Schema validation failed\n    TypeChecking --&gt; Completed: Type check passed\n    TypeChecking --&gt; ParsingError: Type mismatch\n\n    Completed --&gt; Ready: Return to caller\n    InitError --&gt; [*]: Exception raised\n    ExecutionError --&gt; [*]: Exception raised\n    ParsingError --&gt; [*]: Exception raised\n\n    note right of Ready\n        Instance is reusable\n        Can be called multiple times\n    end note\n\n    note right of ExecutingChat\n        Branch depends on\n        LLM.metadata.is_chat_model\n    end note\n\n    note right of ParsingOutput\n        Output parser extracts\n        structured data from\n        raw LLM response\n    end note\n\n    note right of Completed\n        Returns validated\n        Pydantic model instance\n        matching output_cls\n    end note</code></pre>"},{"location":"reference/core/llms/orchestrators/text_completion_llm/text_completion_llm_state/#state-descriptions","title":"State Descriptions","text":""},{"location":"reference/core/llms/orchestrators/text_completion_llm/text_completion_llm_state/#initialization-states","title":"Initialization States","text":"State Description Exit Conditions Uninitialized Object construction in progress <code>__init__</code> called ValidatingParser Checking output_parser and output_cls compatibility Valid: \u2192 ValidatingLLMInvalid: \u2192 InitError ValidatingLLM Ensuring LLM instance is available Valid: \u2192 ValidatingPromptMissing: \u2192 InitError ValidatingPrompt Converting/validating prompt template Valid: \u2192 ReadyInvalid: \u2192 InitError Ready Instance fully initialized and ready for calls User calls instance"},{"location":"reference/core/llms/orchestrators/text_completion_llm/text_completion_llm_state/#execution-states-chat-path","title":"Execution States (Chat Path)","text":"State Description Exit Conditions ExecutingChat Entry point for chat model execution \u2192 FormattingMessages FormattingMessages Converting prompt to message list \u2192 ExtendingMessages ExtendingMessages Applying template variables to messages \u2192 CallingChatAPI CallingChatAPI Invoking Ollama.chat method \u2192 WaitingChatResponse WaitingChatResponse HTTP request in flight Success: \u2192 ParsingOutputError: \u2192 ExecutionError"},{"location":"reference/core/llms/orchestrators/text_completion_llm/text_completion_llm_state/#execution-states-completion-path","title":"Execution States (Completion Path)","text":"State Description Exit Conditions ExecutingComplete Entry point for completion model execution \u2192 FormattingPrompt FormattingPrompt Converting prompt to string \u2192 ExtendingPrompt ExtendingPrompt Applying template variables to prompt \u2192 CallingCompleteAPI CallingCompleteAPI Invoking Ollama.complete method \u2192 WaitingCompleteResponse WaitingCompleteResponse HTTP request in flight Success: \u2192 ParsingOutputError: \u2192 ExecutionError"},{"location":"reference/core/llms/orchestrators/text_completion_llm/text_completion_llm_state/#parsing-states","title":"Parsing States","text":"State Description Exit Conditions ParsingOutput Starting output parsing process \u2192 ExtractingJSON ExtractingJSON Extracting JSON from raw text Valid JSON: \u2192 ValidatingSchemaInvalid: \u2192 ParsingError ValidatingSchema Validating against Pydantic schema Valid: \u2192 TypeCheckingInvalid: \u2192 ParsingError TypeChecking Verifying output matches output_cls Match: \u2192 CompletedMismatch: \u2192 ParsingError Completed Successful execution with valid output \u2192 Ready (return result)"},{"location":"reference/core/llms/orchestrators/text_completion_llm/text_completion_llm_state/#error-states","title":"Error States","text":"State Description Exceptions Raised InitError Initialization validation failed ValueError, AssertionError ExecutionError LLM execution failed ConnectionError, TimeoutError, HTTPError ParsingError Output parsing/validation failed ValueError, ValidationError"},{"location":"reference/core/llms/orchestrators/text_completion_llm/text_completion_llm_state/#state-transitions-example","title":"State Transitions Example","text":""},{"location":"reference/core/llms/orchestrators/text_completion_llm/text_completion_llm_state/#successful-execution-flow","title":"Successful Execution Flow","text":"<pre><code>[*]\n  \u2192 Uninitialized\n  \u2192 ValidatingParser\n  \u2192 ValidatingLLM\n  \u2192 ValidatingPrompt\n  \u2192 Ready\n  \u2192 ExecutingChat (or ExecutingComplete)\n  \u2192 FormattingMessages (or FormattingPrompt)\n  \u2192 ExtendingMessages (or ExtendingPrompt)\n  \u2192 CallingChatAPI (or CallingCompleteAPI)\n  \u2192 WaitingChatResponse (or WaitingCompleteResponse)\n  \u2192 ParsingOutput\n  \u2192 ExtractingJSON\n  \u2192 ValidatingSchema\n  \u2192 TypeChecking\n  \u2192 Completed\n  \u2192 Ready (reusable for next call)\n</code></pre>"},{"location":"reference/core/llms/orchestrators/text_completion_llm/text_completion_llm_state/#error-during-initialization","title":"Error During Initialization","text":"<pre><code>[*]\n  \u2192 Uninitialized\n  \u2192 ValidatingParser\n  \u2192 ValidatingLLM\n  \u2192 InitError (no LLM provided and Configs.llm is None)\n  \u2192 [*] (AssertionError raised)\n</code></pre>"},{"location":"reference/core/llms/orchestrators/text_completion_llm/text_completion_llm_state/#error-during-execution","title":"Error During Execution","text":"<pre><code>Ready\n  \u2192 ExecutingComplete\n  \u2192 FormattingPrompt\n  \u2192 ExtendingPrompt\n  \u2192 CallingCompleteAPI\n  \u2192 WaitingCompleteResponse\n  \u2192 ExecutionError (Ollama server not reachable)\n  \u2192 [*] (ConnectionError raised)\n</code></pre>"},{"location":"reference/core/llms/orchestrators/text_completion_llm/text_completion_llm_state/#error-during-parsing","title":"Error During Parsing","text":"<pre><code>WaitingChatResponse\n  \u2192 ParsingOutput (raw_output: \"This is not JSON\")\n  \u2192 ExtractingJSON\n  \u2192 ParsingError (cannot find JSON in text)\n  \u2192 [*] (ValueError raised)\n</code></pre>"},{"location":"reference/core/llms/orchestrators/text_completion_llm/text_completion_llm_state/#concurrency-notes","title":"Concurrency Notes","text":"<ul> <li>Thread-safe: The instance is reusable after reaching <code>Ready</code> state</li> <li>Stateless execution: Each <code>__call__</code> creates new transient state</li> <li>Immutable config: Parser, LLM, and output_cls don't change after initialization</li> <li>Async support: <code>acall</code> follows similar state transitions but with async operations</li> </ul>"},{"location":"reference/core/llms/orchestrators/tool_orchestrating_llm/examples/","title":"ToolOrchestratingLLM Usage Examples","text":"<p>This guide provides comprehensive examples covering all possible ways to use <code>ToolOrchestratingLLM</code>.</p>"},{"location":"reference/core/llms/orchestrators/tool_orchestrating_llm/examples/#table-of-contents","title":"Table of Contents","text":"<ol> <li>Basic Usage</li> <li>Initialization Patterns</li> <li>Prompt Formats</li> <li>Execution Modes</li> <li>Parallel Tool Calls</li> <li>Advanced Usage</li> <li>Error Handling</li> </ol>"},{"location":"reference/core/llms/orchestrators/tool_orchestrating_llm/examples/#basic-usage","title":"Basic Usage","text":""},{"location":"reference/core/llms/orchestrators/tool_orchestrating_llm/examples/#simple-string-prompt-with-variables","title":"Simple String Prompt with Variables","text":"<p>The most straightforward way to use <code>ToolOrchestratingLLM</code>:</p> <pre><code>from typing import List\nfrom pydantic import BaseModel\nfrom serapeum.core.llms import ToolOrchestratingLLM\nfrom serapeum.ollama import Ollama\n\n# Define your output schema\nclass Song(BaseModel):\n    title: str\n    duration: int  # in seconds\n\nclass Album(BaseModel):\n    title: str\n    artist: str\n    songs: List[Song]\n\n# Initialize the LLM with function calling support\nllm = Ollama(model=\"llama3.1\", request_timeout=80)\n\n# Create ToolOrchestratingLLM with string prompt\ntools_llm = ToolOrchestratingLLM(\n    output_cls=Album,\n    prompt=\"Create an album about {topic} with {num_songs} songs.\",\n    llm=llm,\n)\n\n# Execute with variables - LLM uses function calling to return structured data\nresult = tools_llm(topic=\"space exploration\", num_songs=3)\nprint(result.title)  # \"Journey Through the Cosmos\"\nprint(result.artist)  # \"The Astronomers\"\nprint(len(result.songs))  # 3\n</code></pre>"},{"location":"reference/core/llms/orchestrators/tool_orchestrating_llm/examples/#initialization-patterns","title":"Initialization Patterns","text":""},{"location":"reference/core/llms/orchestrators/tool_orchestrating_llm/examples/#1-with-explicit-llm","title":"1. With Explicit LLM","text":"<p>Provide a fully configured function-calling LLM:</p> <pre><code>from typing import List\nfrom pydantic import BaseModel, Field\nfrom serapeum.core.llms import ToolOrchestratingLLM\nfrom serapeum.ollama import Ollama\n\nclass Task(BaseModel):\n    id: str = Field(description=\"Task identifier\")\n    description: str = Field(description=\"Task description\")\n    priority: int = Field(description=\"Priority level 1-5\", ge=1, le=5)\n    subtasks: List[str] = Field(description=\"List of subtasks\")\n\n# Initialize Ollama with function calling support\nllm = Ollama(\n    model=\"llama3.1\",\n    request_timeout=80,\n    temperature=0.7,\n)\n\ntools_llm = ToolOrchestratingLLM(\n    output_cls=Task,\n    prompt=\"Break down this project into tasks: {project}\",\n    llm=llm,\n)\n\nresult = tools_llm(project=\"Build a web application\")\n# Returns: Task with properly structured data via function calling\n</code></pre>"},{"location":"reference/core/llms/orchestrators/tool_orchestrating_llm/examples/#2-using-global-llm-from-configs","title":"2. Using Global LLM from Configs","text":"<p>Set a default function-calling LLM for the entire application:</p> <pre><code>from pydantic import BaseModel\nfrom serapeum.core.configs.configs import Configs\nfrom serapeum.core.llms import ToolOrchestratingLLM\nfrom serapeum.ollama import Ollama\n\n# Set global LLM\nConfigs.llm = Ollama(model=\"llama3.1\", request_timeout=80)\n\nclass Entity(BaseModel):\n    name: str\n    type: str\n    properties: dict\n\n# No need to pass llm parameter\ntools_llm = ToolOrchestratingLLM(\n    output_cls=Entity,\n    prompt=\"Extract entity from: {text}\",\n    # llm=None uses Configs.llm by default\n)\n\nresult = tools_llm(text=\"Apple Inc. is a technology company\")\n# Returns: Entity(name=\"Apple Inc.\", type=\"company\", ...)\n</code></pre>"},{"location":"reference/core/llms/orchestrators/tool_orchestrating_llm/examples/#3-with-tool-choice-strategy","title":"3. With Tool Choice Strategy","text":"<p>Control which tool the LLM should use:</p> <pre><code>from pydantic import BaseModel\nfrom serapeum.core.llms import ToolOrchestratingLLM\nfrom serapeum.ollama import Ollama\n\nclass Response(BaseModel):\n    answer: str\n    confidence: float\n\nllm = Ollama(model=\"llama3.1\", request_timeout=80)\n\n# Force the LLM to use the tool\ntools_llm = ToolOrchestratingLLM(\n    output_cls=Response,\n    prompt=\"Answer: {question}\",\n    llm=llm,\n    tool_choice=\"auto\",  # or \"required\" to force tool use\n)\n\nresult = tools_llm(question=\"What is Python?\")\n</code></pre>"},{"location":"reference/core/llms/orchestrators/tool_orchestrating_llm/examples/#prompt-formats","title":"Prompt Formats","text":""},{"location":"reference/core/llms/orchestrators/tool_orchestrating_llm/examples/#1-string-prompt-auto-converted-to-prompttemplate","title":"1. String Prompt (Auto-converted to PromptTemplate)","text":"<p>Simple string prompts are automatically wrapped in <code>PromptTemplate</code>:</p> <pre><code>from typing import List\nfrom pydantic import BaseModel\nfrom serapeum.core.llms import ToolOrchestratingLLM\nfrom serapeum.ollama import Ollama\n\nclass Recipe(BaseModel):\n    name: str\n    ingredients: List[str]\n    steps: List[str]\n    prep_time: int\n\nllm = Ollama(model=\"llama3.1\", request_timeout=80)\n\ntools_llm = ToolOrchestratingLLM(\n    output_cls=Recipe,\n    prompt=\"Create a {cuisine} recipe for {dish}\",  # String prompt\n    llm=llm,\n)\n\nresult = tools_llm(cuisine=\"Italian\", dish=\"pasta\")\n</code></pre>"},{"location":"reference/core/llms/orchestrators/tool_orchestrating_llm/examples/#2-prompttemplate-object","title":"2. PromptTemplate Object","text":"<p>Use <code>PromptTemplate</code> for more control:</p> <pre><code>from pydantic import BaseModel\nfrom serapeum.core.prompts.base import PromptTemplate\nfrom serapeum.core.llms import ToolOrchestratingLLM\nfrom serapeum.ollama import Ollama\n\nclass Analysis(BaseModel):\n    sentiment: str\n    topics: list[str]\n    summary: str\n\nllm = Ollama(model=\"llama3.1\", request_timeout=80)\n\n# Create explicit PromptTemplate\nprompt_template = PromptTemplate(\n    \"Analyze this text: {text}\\n\\nProvide sentiment, topics, and summary.\"\n)\n\ntools_llm = ToolOrchestratingLLM(\n    output_cls=Analysis,\n    prompt=prompt_template,\n    llm=llm,\n)\n\nresult = tools_llm(text=\"AI is transforming industries worldwide\")\n</code></pre>"},{"location":"reference/core/llms/orchestrators/tool_orchestrating_llm/examples/#3-chatprompttemplate-with-messages","title":"3. ChatPromptTemplate with Messages","text":"<p>Use structured message templates for complex prompts:</p> <pre><code>from pydantic import BaseModel\nfrom serapeum.core.base.llms.models import Message, MessageRole\nfrom serapeum.core.prompts import ChatPromptTemplate\nfrom serapeum.core.llms import ToolOrchestratingLLM\nfrom serapeum.ollama import Ollama\n\nclass CodeReview(BaseModel):\n    issues: list[str]\n    suggestions: list[str]\n    rating: int  # 1-10\n\nllm = Ollama(model=\"llama3.1\", request_timeout=80)\n\n# Create message templates\nmessages = [\n    Message(\n        role=MessageRole.SYSTEM,\n        content=\"You are an expert code reviewer.\"\n    ),\n    Message(\n        role=MessageRole.USER,\n        content=\"Review this {language} code:\\n\\n{code}\"\n    ),\n]\n\nprompt = ChatPromptTemplate(message_templates=messages)\n\ntools_llm = ToolOrchestratingLLM(\n    output_cls=CodeReview,\n    prompt=prompt,\n    llm=llm,\n)\n\nresult = tools_llm(language=\"Python\", code=\"def foo(): pass\")\n</code></pre>"},{"location":"reference/core/llms/orchestrators/tool_orchestrating_llm/examples/#execution-modes","title":"Execution Modes","text":""},{"location":"reference/core/llms/orchestrators/tool_orchestrating_llm/examples/#1-synchronous-execution","title":"1. Synchronous Execution","text":"<p>Standard blocking execution:</p> <pre><code>from pydantic import BaseModel\nfrom serapeum.core.llms import ToolOrchestratingLLM\nfrom serapeum.ollama import Ollama\n\nclass Summary(BaseModel):\n    main_points: list[str]\n    conclusion: str\n\nllm = Ollama(model=\"llama3.1\", request_timeout=80)\n\ntools_llm = ToolOrchestratingLLM(\n    output_cls=Summary,\n    prompt=\"Summarize: {text}\",\n    llm=llm,\n)\n\n# Synchronous call using __call__\nresult = tools_llm(text=\"Long article text here...\")\nprint(result.main_points)\nprint(result.conclusion)\n</code></pre>"},{"location":"reference/core/llms/orchestrators/tool_orchestrating_llm/examples/#2-asynchronous-execution","title":"2. Asynchronous Execution","text":"<p>Non-blocking async execution:</p> <pre><code>import asyncio\nfrom pydantic import BaseModel\nfrom serapeum.core.llms import ToolOrchestratingLLM\nfrom serapeum.ollama import Ollama\n\nclass Classification(BaseModel):\n    category: str\n    subcategory: str\n    confidence: float\n\nllm = Ollama(model=\"llama3.1\", request_timeout=80)\n\ntools_llm = ToolOrchestratingLLM(\n    output_cls=Classification,\n    prompt=\"Classify: {item}\",\n    llm=llm,\n)\n\nasync def classify_item(item: str) -&gt; Classification:\n    # Asynchronous call using acall\n    result = await tools_llm.acall(item=item)\n    return result\n\n# Run async function\nresult = asyncio.run(classify_item(\"Laptop computer\"))\nprint(f\"{result.category} &gt; {result.subcategory}\")\n</code></pre>"},{"location":"reference/core/llms/orchestrators/tool_orchestrating_llm/examples/#3-batch-processing-with-async","title":"3. Batch Processing with Async","text":"<p>Process multiple inputs concurrently:</p> <pre><code>import asyncio\nfrom typing import List\nfrom pydantic import BaseModel\nfrom serapeum.core.llms import ToolOrchestratingLLM\nfrom serapeum.ollama import Ollama\n\nclass EntityExtraction(BaseModel):\n    entities: List[str]\n    entity_types: List[str]\n\nllm = Ollama(model=\"llama3.1\", request_timeout=80)\n\ntools_llm = ToolOrchestratingLLM(\n    output_cls=EntityExtraction,\n    prompt=\"Extract entities from: {text}\",\n    llm=llm,\n)\n\nasync def extract_batch(texts: List[str]) -&gt; List[EntityExtraction]:\n    tasks = [tools_llm.acall(text=text) for text in texts]\n    results = await asyncio.gather(*tasks)\n    return results\n\ntexts = [\n    \"Apple Inc. is in California\",\n    \"Microsoft was founded by Bill Gates\",\n    \"Paris is the capital of France\"\n]\nresults = asyncio.run(extract_batch(texts))\nfor text, result in zip(texts, results):\n    print(f\"{text}: {result.entities}\")\n</code></pre>"},{"location":"reference/core/llms/orchestrators/tool_orchestrating_llm/examples/#4-streaming-execution","title":"4. Streaming Execution","text":"<p>Stream progressive updates:</p> <pre><code>from pydantic import BaseModel\nfrom serapeum.core.llms import ToolOrchestratingLLM\nfrom serapeum.ollama import Ollama\n\nclass Article(BaseModel):\n    title: str\n    sections: list[str]\n    word_count: int\n\nllm = Ollama(model=\"llama3.1\", request_timeout=80)\n\ntools_llm = ToolOrchestratingLLM(\n    output_cls=Article,\n    prompt=\"Write an article about {topic}\",\n    llm=llm,\n)\n\n# Stream results as they arrive\nfor partial_article in tools_llm.stream_call(topic=\"AI\"):\n    print(f\"Current title: {partial_article.title}\")\n    print(f\"Sections so far: {len(partial_article.sections)}\")\n    # Display progressive updates in UI\n</code></pre>"},{"location":"reference/core/llms/orchestrators/tool_orchestrating_llm/examples/#5-async-streaming","title":"5. Async Streaming","text":"<p>Async version of streaming:</p> <pre><code>import asyncio\nfrom pydantic import BaseModel\nfrom serapeum.core.llms import ToolOrchestratingLLM\nfrom serapeum.ollama import Ollama\n\nclass Report(BaseModel):\n    title: str\n    findings: list[str]\n    recommendations: list[str]\n\nllm = Ollama(model=\"llama3.1\", request_timeout=80)\n\ntools_llm = ToolOrchestratingLLM(\n    output_cls=Report,\n    prompt=\"Generate report on {subject}\",\n    llm=llm,\n)\n\nasync def stream_report(subject: str):\n    stream = await tools_llm.astream_call(subject=subject)\n    async for partial_report in stream:\n        print(f\"Findings: {len(partial_report.findings)}\")\n        print(f\"Recommendations: {len(partial_report.recommendations)}\")\n\nasyncio.run(stream_report(\"Market analysis\"))\n</code></pre>"},{"location":"reference/core/llms/orchestrators/tool_orchestrating_llm/examples/#6-passing-llm-specific-parameters","title":"6. Passing LLM-specific Parameters","text":"<p>Forward parameters directly to the LLM:</p> <pre><code>from pydantic import BaseModel\nfrom serapeum.core.llms import ToolOrchestratingLLM\nfrom serapeum.ollama import Ollama\n\nclass Story(BaseModel):\n    title: str\n    plot: str\n    characters: list[str]\n\nllm = Ollama(model=\"llama3.1\", request_timeout=80)\n\ntools_llm = ToolOrchestratingLLM(\n    output_cls=Story,\n    prompt=\"Write a {genre} story\",\n    llm=llm,\n)\n\n# Pass LLM-specific kwargs\nresult = tools_llm(\n    llm_kwargs={\n        \"temperature\": 0.9,  # Higher for creativity\n        \"top_p\": 0.95,\n        \"max_tokens\": 1000,\n    },\n    genre=\"science fiction\"\n)\n</code></pre>"},{"location":"reference/core/llms/orchestrators/tool_orchestrating_llm/examples/#parallel-tool-calls","title":"Parallel Tool Calls","text":""},{"location":"reference/core/llms/orchestrators/tool_orchestrating_llm/examples/#single-output-default","title":"Single Output (Default)","text":"<p>By default, only one tool call is expected:</p> <pre><code>from pydantic import BaseModel\nfrom serapeum.core.llms import ToolOrchestratingLLM\nfrom serapeum.ollama import Ollama\n\nclass Product(BaseModel):\n    name: str\n    price: float\n    description: str\n\nllm = Ollama(model=\"llama3.1\", request_timeout=80)\n\ntools_llm = ToolOrchestratingLLM(\n    output_cls=Product,\n    prompt=\"Extract product info: {text}\",\n    llm=llm,\n    allow_parallel_tool_calls=False,  # Default\n)\n\n# Returns single Product instance\nresult = tools_llm(text=\"iPhone 15 costs $999\")\nprint(type(result))  # &lt;class 'Product'&gt;\n</code></pre>"},{"location":"reference/core/llms/orchestrators/tool_orchestrating_llm/examples/#multiple-outputs-parallel","title":"Multiple Outputs (Parallel)","text":"<p>Enable parallel tool calls to receive multiple objects:</p> <pre><code>from typing import List\nfrom pydantic import BaseModel\nfrom serapeum.core.llms import ToolOrchestratingLLM\nfrom serapeum.ollama import Ollama\n\nclass Item(BaseModel):\n    name: str\n    category: str\n\nllm = Ollama(model=\"llama3.1\", request_timeout=80)\n\ntools_llm = ToolOrchestratingLLM(\n    output_cls=Item,\n    prompt=\"Extract all items from this list: {text}\",\n    llm=llm,\n    allow_parallel_tool_calls=True,  # Enable parallel calls\n)\n\n# Returns List[Item]\nresults = tools_llm(text=\"apples, laptops, books, phones\")\nprint(type(results))  # &lt;class 'list'&gt;\nprint(len(results))   # 4\nfor item in results:\n    print(f\"{item.name}: {item.category}\")\n</code></pre>"},{"location":"reference/core/llms/orchestrators/tool_orchestrating_llm/examples/#parallel-with-streaming","title":"Parallel with Streaming","text":"<p>Stream multiple objects as they're generated:</p> <pre><code>from pydantic import BaseModel\nfrom serapeum.core.llms import ToolOrchestratingLLM\nfrom serapeum.ollama import Ollama\n\nclass Question(BaseModel):\n    question: str\n    difficulty: str\n\nllm = Ollama(model=\"llama3.1\", request_timeout=80)\n\ntools_llm = ToolOrchestratingLLM(\n    output_cls=Question,\n    prompt=\"Generate 5 questions about {topic}\",\n    llm=llm,\n    allow_parallel_tool_calls=True,\n)\n\n# Stream list of questions as they arrive\nfor questions_so_far in tools_llm.stream_call(topic=\"Python\"):\n    if isinstance(questions_so_far, list):\n        print(f\"Questions generated: {len(questions_so_far)}\")\n        # Show latest question\n        if questions_so_far:\n            latest = questions_so_far[-1]\n            print(f\"  Latest: {latest.question}\")\n</code></pre>"},{"location":"reference/core/llms/orchestrators/tool_orchestrating_llm/examples/#advanced-usage","title":"Advanced Usage","text":""},{"location":"reference/core/llms/orchestrators/tool_orchestrating_llm/examples/#1-dynamic-prompt-updates","title":"1. Dynamic Prompt Updates","text":"<p>Change the prompt at runtime:</p> <pre><code>from pydantic import BaseModel\nfrom serapeum.core.prompts.base import PromptTemplate\nfrom serapeum.core.llms import ToolOrchestratingLLM\nfrom serapeum.ollama import Ollama\n\nclass Response(BaseModel):\n    answer: str\n    reasoning: str\n\nllm = Ollama(model=\"llama3.1\", request_timeout=80)\n\ntools_llm = ToolOrchestratingLLM(\n    output_cls=Response,\n    prompt=\"Answer briefly: {question}\",\n    llm=llm,\n)\n\n# Use with initial prompt\nresult1 = tools_llm(question=\"What is AI?\")\n\n# Update prompt dynamically\ntools_llm.prompt = PromptTemplate(\"Answer in detail: {question}\")\n\n# Use with new prompt\nresult2 = tools_llm(question=\"What is AI?\")\n</code></pre>"},{"location":"reference/core/llms/orchestrators/tool_orchestrating_llm/examples/#2-reusable-instance-pattern","title":"2. Reusable Instance Pattern","text":"<p>Create once, use many times:</p> <pre><code>from pydantic import BaseModel\nfrom serapeum.core.llms import ToolOrchestratingLLM\nfrom serapeum.ollama import Ollama\n\nclass Sentiment(BaseModel):\n    sentiment: str  # positive, negative, neutral\n    confidence: float\n    keywords: list[str]\n\nllm = Ollama(model=\"llama3.1\", request_timeout=80)\n\n# Create reusable instance\nsentiment_analyzer = ToolOrchestratingLLM(\n    output_cls=Sentiment,\n    prompt=\"Analyze sentiment of: {text}\",\n    llm=llm,\n)\n\n# Reuse multiple times\nreviews = [\n    \"This product is amazing!\",\n    \"Terrible experience, very disappointed\",\n    \"It's okay, nothing special\",\n]\n\nfor review in reviews:\n    analysis = sentiment_analyzer(text=review)\n    print(f\"{review[:20]}... \u2192 {analysis.sentiment} ({analysis.confidence})\")\n</code></pre>"},{"location":"reference/core/llms/orchestrators/tool_orchestrating_llm/examples/#3-complex-nested-models","title":"3. Complex Nested Models","text":"<p>Use deeply nested Pydantic models:</p> <pre><code>from typing import List\nfrom pydantic import BaseModel, Field\nfrom serapeum.core.llms import ToolOrchestratingLLM\nfrom serapeum.ollama import Ollama\n\nclass Author(BaseModel):\n    name: str\n    email: str\n\nclass Comment(BaseModel):\n    author: Author\n    text: str\n    upvotes: int\n\nclass Article(BaseModel):\n    title: str\n    content: str\n    author: Author\n    tags: List[str]\n    comments: List[Comment]\n\nllm = Ollama(model=\"llama3.1\", request_timeout=80)\n\ntools_llm = ToolOrchestratingLLM(\n    output_cls=Article,\n    prompt=\"Create a blog article about {topic} with comments\",\n    llm=llm,\n)\n\nresult = tools_llm(topic=\"Machine Learning\")\nprint(result.title)\nprint(result.author.name)\nprint(f\"Tags: {', '.join(result.tags)}\")\nprint(f\"Comments: {len(result.comments)}\")\nfor comment in result.comments:\n    print(f\"  - {comment.author.name}: {comment.text[:50]}...\")\n</code></pre>"},{"location":"reference/core/llms/orchestrators/tool_orchestrating_llm/examples/#4-using-with-verbose-mode","title":"4. Using with Verbose Mode","text":"<p>Enable detailed logging:</p> <pre><code>from pydantic import BaseModel\nfrom serapeum.core.llms import ToolOrchestratingLLM\nfrom serapeum.ollama import Ollama\n\nclass Data(BaseModel):\n    result: str\n\nllm = Ollama(model=\"llama3.1\", request_timeout=80)\n\ntools_llm = ToolOrchestratingLLM(\n    output_cls=Data,\n    prompt=\"Process: {input}\",\n    llm=llm,\n    verbose=True,  # Enable verbose logging\n)\n\n# Will log detailed information about tool calls\nresult = tools_llm(input=\"test data\")\n</code></pre>"},{"location":"reference/core/llms/orchestrators/tool_orchestrating_llm/examples/#5-custom-tool-choice","title":"5. Custom Tool Choice","text":"<p>Control tool selection:</p> <pre><code>from pydantic import BaseModel\nfrom serapeum.core.llms import ToolOrchestratingLLM\nfrom serapeum.ollama import Ollama\n\nclass Output(BaseModel):\n    data: str\n\nllm = Ollama(model=\"llama3.1\", request_timeout=80)\n\n# Force the LLM to always use the tool\ntools_llm = ToolOrchestratingLLM(\n    output_cls=Output,\n    prompt=\"Generate output for: {input}\",\n    llm=llm,\n    tool_choice=\"required\",  # Force tool use\n)\n\nresult = tools_llm(input=\"test\")\n</code></pre>"},{"location":"reference/core/llms/orchestrators/tool_orchestrating_llm/examples/#using-regular-functions-with-toolorchestratingllm","title":"Using Regular Functions with ToolOrchestratingLLM","text":"<p><code>ToolOrchestratingLLM</code> now supports both Pydantic models and regular Python functions as <code>output_cls</code>. When you pass a function, the system automatically detects it and creates the appropriate tool.</p>"},{"location":"reference/core/llms/orchestrators/tool_orchestrating_llm/examples/#1-using-regular-functions","title":"1. Using Regular Functions","text":"<p>Pass regular Python functions directly as <code>output_cls</code>:</p> <pre><code>from serapeum.core.llms import ToolOrchestratingLLM\nfrom serapeum.ollama import Ollama\n\ndef calculate_statistics(numbers: list[float], operation: str) -&gt; dict[str, float]:\n    \"\"\"Calculate statistics on a list of numbers.\n\n    Args:\n        numbers: List of numbers to analyze\n        operation: Type of operation (mean, sum, max, min)\n\n    Returns:\n        Dictionary with the result\n    \"\"\"\n    if operation == \"mean\":\n        result = sum(numbers) / len(numbers)\n    elif operation == \"sum\":\n        result = sum(numbers)\n    elif operation == \"max\":\n        result = max(numbers)\n    elif operation == \"min\":\n        result = min(numbers)\n    else:\n        raise ValueError(f\"Unknown operation: {operation}\")\n\n    return {\n        \"operation\": operation,\n        \"result\": result,\n        \"count\": len(numbers)\n    }\n\n# Use function directly with ToolOrchestratingLLM\nllm = Ollama(model=\"llama3.1\", request_timeout=80)\n\ntools_llm = ToolOrchestratingLLM(\n    output_cls=calculate_statistics,  # Pass function directly!\n    prompt=\"Calculate the mean of these numbers: {text}\",\n    llm=llm,\n)\n\n# Call with input\nresult = tools_llm(text=\"10, 20, 30, 40, 50\")\n\nprint(f\"Operation: {result['operation']}\")\nprint(f\"Result: {result['result']}\")\nprint(f\"Count: {result['count']}\")\n</code></pre>"},{"location":"reference/core/llms/orchestrators/tool_orchestrating_llm/examples/#2-using-functions-with-regular-classes","title":"2. Using Functions with Regular Classes","text":"<p>Wrap regular Python classes in functions and use with ToolOrchestratingLLM:</p> <pre><code>from serapeum.core.llms import ToolOrchestratingLLM\nfrom serapeum.ollama import Ollama\n\nclass EmailValidator:\n    \"\"\"Regular Python class for email validation.\"\"\"\n\n    def __init__(self, email: str, check_mx: bool = False):\n        self.email = email\n        self.check_mx = check_mx\n        self.is_valid = self._validate()\n\n    def _validate(self) -&gt; bool:\n        \"\"\"Simple email validation.\"\"\"\n        return \"@\" in self.email and \".\" in self.email.split(\"@\")[1]\n\n    def to_dict(self) -&gt; dict:\n        return {\n            \"email\": self.email,\n            \"is_valid\": self.is_valid,\n            \"check_mx\": self.check_mx\n        }\n\ndef validate_email(email: str, check_mx: bool = False) -&gt; dict:\n    \"\"\"Validate an email address.\n\n    Args:\n        email: Email address to validate\n        check_mx: Whether to check MX records (not implemented)\n\n    Returns:\n        Validation result dictionary\n    \"\"\"\n    validator = EmailValidator(email, check_mx)\n    return validator.to_dict()\n\n# Use function with ToolOrchestratingLLM\nllm = Ollama(model=\"llama3.1\", request_timeout=80)\n\ntools_llm = ToolOrchestratingLLM(\n    output_cls=validate_email,  # Pass function that uses the class\n    prompt=\"Validate this email: {email_text}\",\n    llm=llm,\n)\n\nresult = tools_llm(email_text=\"user@example.com\")\n\nprint(f\"Email: {result['email']}\")\nprint(f\"Valid: {result['is_valid']}\")\n</code></pre>"},{"location":"reference/core/llms/orchestrators/tool_orchestrating_llm/examples/#3-factory-functions-with-dataclasses","title":"3. Factory Functions with Dataclasses","text":"<p>Use factory functions that return dataclass instances:</p> <pre><code>from dataclasses import dataclass\nfrom serapeum.core.llms import ToolOrchestratingLLM\nfrom serapeum.llms.ollama import Ollama\n\n@dataclass\nclass Product:\n    \"\"\"Regular dataclass (not Pydantic).\"\"\"\n    name: str\n    price: float\n    category: str\n    in_stock: bool\n\n    def to_dict(self) -&gt; dict:\n        return {\n            \"name\": self.name,\n            \"price\": self.price,\n            \"category\": self.category,\n            \"in_stock\": self.in_stock\n        }\n\ndef create_product(name: str, price: float, category: str, in_stock: bool = True) -&gt; dict:\n    \"\"\"Create a product from parameters.\n\n    Args:\n        name: Product name\n        price: Product price in USD\n        category: Product category\n        in_stock: Whether product is in stock\n\n    Returns:\n        Product data as dictionary\n    \"\"\"\n    product = Product(name, price, category, in_stock)\n    return product.to_dict()\n\n# Use factory function with ToolOrchestratingLLM\nllm = Ollama(model=\"llama3.1\", request_timeout=80)\n\ntools_llm = ToolOrchestratingLLM(\n    output_cls=create_product,  # Pass factory function\n    prompt=\"Create a product entry for: {product_info}\",\n    llm=llm,\n)\n\nresult = tools_llm(product_info=\"Laptop, $999, Electronics, available\")\n\nprint(f\"Product: {result['name']}\")\nprint(f\"Price: ${result['price']}\")\nprint(f\"Category: {result['category']}\")\n</code></pre>"},{"location":"reference/core/llms/orchestrators/tool_orchestrating_llm/examples/#4-async-functions","title":"4. Async Functions","text":"<p>Use async functions directly with ToolOrchestratingLLM:</p> <pre><code>import asyncio\nfrom serapeum.core.llms import ToolOrchestratingLLM\nfrom serapeum.llms.ollama import Ollama\n\nasync def fetch_user_data(user_id: int, include_posts: bool = False) -&gt; dict:\n    \"\"\"Asynchronously fetch user data.\n\n    Args:\n        user_id: User ID to fetch\n        include_posts: Whether to include user posts\n\n    Returns:\n        User data dictionary\n    \"\"\"\n    # Simulate async API call\n    await asyncio.sleep(0.1)\n\n    user_data = {\n        \"user_id\": user_id,\n        \"username\": f\"user_{user_id}\",\n        \"email\": f\"user{user_id}@example.com\"\n    }\n\n    if include_posts:\n        user_data[\"posts\"] = [\n            {\"id\": 1, \"title\": \"First post\"},\n            {\"id\": 2, \"title\": \"Second post\"}\n        ]\n\n    return user_data\n\n# Use async function with ToolOrchestratingLLM\nasync def main():\n    llm = Ollama(model=\"llama3.1\", request_timeout=80)\n\n    tools_llm = ToolOrchestratingLLM(\n        output_cls=fetch_user_data,  # Pass async function\n        prompt=\"Fetch data for user ID {user_id_text} with their posts\",\n        llm=llm,\n    )\n\n    # Use acall for async execution\n    result = await tools_llm.acall(user_id_text=\"42\")\n\n    print(f\"User: {result['username']}\")\n    print(f\"Email: {result['email']}\")\n    if \"posts\" in result:\n        print(f\"Posts: {len(result['posts'])}\")\n\n# Run async example\nasyncio.run(main())\n</code></pre>"},{"location":"reference/core/llms/orchestrators/tool_orchestrating_llm/examples/#5-lambda-functions","title":"5. Lambda Functions","text":"<p>Use lambda functions for simple transformations:</p> <pre><code>from serapeum.core.llms import ToolOrchestratingLLM\nfrom serapeum.llms.ollama import Ollama\n\n# Simple lambda function for temperature conversion\nconvert_temp = lambda celsius: {\n    \"celsius\": celsius,\n    \"fahrenheit\": (celsius * 9/5) + 32,\n    \"kelvin\": celsius + 273.15\n}\n\nllm = Ollama(model=\"llama3.1\", request_timeout=80)\n\ntools_llm = ToolOrchestratingLLM(\n    output_cls=convert_temp,  # Pass lambda function\n    prompt=\"Convert {temperature} degrees Celsius\",\n    llm=llm,\n)\n\nresult = tools_llm(temperature=\"25\")\n\nprint(f\"Celsius: {result['celsius']}\")\nprint(f\"Fahrenheit: {result['fahrenheit']}\")\nprint(f\"Kelvin: {result['kelvin']}\")\n</code></pre> <p>Note: Lambda functions work but have limitations: - No docstring for the LLM to understand the function - Parameters aren't well-documented - Better to use regular functions with proper documentation for complex cases</p>"},{"location":"reference/core/llms/orchestrators/tool_orchestrating_llm/examples/#6-functions-with-complex-return-types","title":"6. Functions with Complex Return Types","text":"<p>Use functions that return complex data structures:</p> <pre><code>from serapeum.core.llms import ToolOrchestratingLLM\nfrom serapeum.llms.ollama import Ollama\n\ndef analyze_text(text: str, language: str = \"en\") -&gt; dict:\n    \"\"\"Analyze text and return basic metrics.\n\n    Args:\n        text: Text to analyze\n        language: Language code\n\n    Returns:\n        Analysis metrics including word count, character count, and average word length\n    \"\"\"\n    words = text.split()\n    return {\n        \"word_count\": len(words),\n        \"char_count\": len(text),\n        \"language\": language,\n        \"avg_word_length\": sum(len(w) for w in words) / len(words) if words else 0\n    }\n\nllm = Ollama(model=\"llama3.1\", request_timeout=80)\n\ntools_llm = ToolOrchestratingLLM(\n    output_cls=analyze_text,  # Pass function with complex return\n    prompt=\"Analyze this text: {text_input}\",\n    llm=llm,\n)\n\nresult = tools_llm(text_input=\"Hello world, this is a test message.\")\n\nprint(f\"Word count: {result['word_count']}\")\nprint(f\"Character count: {result['char_count']}\")\nprint(f\"Average word length: {result['avg_word_length']:.2f}\")\n</code></pre>"},{"location":"reference/core/llms/orchestrators/tool_orchestrating_llm/examples/#important-notes","title":"Important Notes","text":"<p>Advantages of using functions as output_cls: 1. Works with existing Python functions - no need to convert to Pydantic 2. Full access to all <code>ToolOrchestratingLLM</code> features (streaming, async, etc.) 3. Automatic tool creation and orchestration 4. Simple and direct - just pass your function</p> <p>When to use functions vs Pydantic models: - Use functions when:   - You have existing functions you want to reuse   - You need simple dict/list returns   - You're prototyping quickly   - Working with legacy code</p> <ul> <li>Use Pydantic models when:</li> <li>You need strict validation of outputs</li> <li>You want better type safety and IDE support</li> <li>Building production systems with clear schemas</li> <li>Need automatic documentation from models</li> </ul> <p>Both approaches work equally well with <code>ToolOrchestratingLLM</code> - choose based on your needs!</p>"},{"location":"reference/core/llms/orchestrators/tool_orchestrating_llm/examples/#error-handling","title":"Error Handling","text":""},{"location":"reference/core/llms/orchestrators/tool_orchestrating_llm/examples/#1-handling-llm-validation-errors","title":"1. Handling LLM Validation Errors","text":"<p>Catch initialization errors:</p> <pre><code>from pydantic import BaseModel\nfrom serapeum.core.llms import ToolOrchestratingLLM\nfrom serapeum.llms.ollama import Ollama\n\nclass Data(BaseModel):\n    value: str\n\n# Create LLM that doesn't support function calling (hypothetically)\n# llm = SomeLLM(...)  # without function calling support\n\ntry:\n    tools_llm = ToolOrchestratingLLM(\n        output_cls=Data,\n        prompt=\"Process: {input}\",\n        llm=None,  # No LLM provided\n    )\nexcept AssertionError as e:\n    print(\"LLM must be provided or set in Configs\")\nexcept ValueError as e:\n    print(f\"LLM doesn't support function calling: {e}\")\n</code></pre>"},{"location":"reference/core/llms/orchestrators/tool_orchestrating_llm/examples/#2-handling-tool-execution-errors","title":"2. Handling Tool Execution Errors","text":"<p>Handle runtime errors:</p> <pre><code>from pydantic import BaseModel, ValidationError\nfrom serapeum.core.llms import ToolOrchestratingLLM\nfrom serapeum.llms.ollama import Ollama\n\nclass StrictData(BaseModel):\n    number: int  # Must be integer\n    ratio: float  # Must be float\n\nllm = Ollama(model=\"llama3.1\", request_timeout=80)\n\ntools_llm = ToolOrchestratingLLM(\n    output_cls=StrictData,\n    prompt=\"Extract numbers from: {text}\",\n    llm=llm,\n)\n\ntry:\n    result = tools_llm(text=\"Some text with invalid data\")\nexcept ValidationError as e:\n    print(f\"Validation failed: {e}\")\n    # LLM generated invalid tool arguments\nexcept ValueError as e:\n    print(f\"Error: {e}\")\n    # Other errors (network, timeout, etc.)\n</code></pre>"},{"location":"reference/core/llms/orchestrators/tool_orchestrating_llm/examples/#3-retry-logic","title":"3. Retry Logic","text":"<p>Implement retry logic for robustness:</p> <pre><code>import asyncio\nfrom pydantic import BaseModel\nfrom serapeum.core.llms import ToolOrchestratingLLM\nfrom serapeum.llms.ollama import Ollama\n\nclass Result(BaseModel):\n    data: str\n\nllm = Ollama(model=\"llama3.1\", request_timeout=80)\n\ntools_llm = ToolOrchestratingLLM(\n    output_cls=Result,\n    prompt=\"Process: {input}\",\n    llm=llm,\n)\n\nasync def call_with_retry(\n    tools_llm: ToolOrchestratingLLM,\n    max_retries: int = 3,\n    **kwargs\n) -&gt; Result:\n    \"\"\"Call ToolOrchestratingLLM with retry logic.\"\"\"\n    for attempt in range(max_retries):\n        try:\n            return await tools_llm.acall(**kwargs)\n        except Exception as e:\n            if attempt == max_retries - 1:\n                raise\n            print(f\"Attempt {attempt + 1} failed: {e}. Retrying...\")\n            await asyncio.sleep(2 ** attempt)  # Exponential backoff\n\n# Use with retry\nresult = asyncio.run(call_with_retry(tools_llm, input=\"test\"))\n</code></pre>"},{"location":"reference/core/llms/orchestrators/tool_orchestrating_llm/examples/#4-handling-missing-tool-calls","title":"4. Handling Missing Tool Calls","text":"<p>Handle cases where LLM doesn't generate tool calls:</p> <pre><code>from pydantic import BaseModel\nfrom serapeum.core.llms import ToolOrchestratingLLM\nfrom serapeum.llms.ollama import Ollama\n\nclass Output(BaseModel):\n    result: str\n\nllm = Ollama(model=\"llama3.1\", request_timeout=80)\n\ntools_llm = ToolOrchestratingLLM(\n    output_cls=Output,\n    prompt=\"Generate output\",\n    llm=llm,\n    tool_choice=\"required\",  # Force tool use to prevent this\n)\n\ntry:\n    result = tools_llm()\nexcept ValueError as e:\n    print(f\"No tool calls generated: {e}\")\n    # Try with different prompt or parameters\n</code></pre>"},{"location":"reference/core/llms/orchestrators/tool_orchestrating_llm/examples/#best-practices","title":"Best Practices","text":""},{"location":"reference/core/llms/orchestrators/tool_orchestrating_llm/examples/#1-clear-model-definitions","title":"1. Clear Model Definitions","text":"<p>Always define clear Pydantic models with descriptions:</p> <pre><code>from pydantic import BaseModel, Field\nfrom serapeum.core.llms import ToolOrchestratingLLM\nfrom serapeum.llms.ollama import Ollama\n\nclass WellDefinedModel(BaseModel):\n    \"\"\"A well-documented model for structured output.\"\"\"\n\n    name: str = Field(description=\"The name of the entity\")\n    category: str = Field(description=\"Category classification\")\n    confidence: float = Field(\n        description=\"Confidence score\",\n        ge=0.0,\n        le=1.0\n    )\n    tags: list[str] = Field(\n        description=\"Relevant tags\",\n        default_factory=list\n    )\n\nllm = Ollama(model=\"llama3.1\", request_timeout=80)\n\ntools_llm = ToolOrchestratingLLM(\n    output_cls=WellDefinedModel,\n    prompt=\"Extract information from: {text}\",\n    llm=llm,\n)\n</code></pre>"},{"location":"reference/core/llms/orchestrators/tool_orchestrating_llm/examples/#2-use-function-calling-compatible-models","title":"2. Use Function Calling Compatible Models","text":"<p>Ensure your LLM supports function calling:</p> <pre><code>from serapeum.llms.ollama import Ollama\n\n# Good: Models that support function calling\ngood_models = [\n    \"llama3.1\",\n    \"llama3.2\",\n    \"mistral\",\n    # Check Ollama docs for function calling support\n]\n\nllm = Ollama(model=\"llama3.1\", request_timeout=80)\n# llm.metadata.is_function_calling_model should be True\n</code></pre>"},{"location":"reference/core/llms/orchestrators/tool_orchestrating_llm/examples/#3-instance-reuse","title":"3. Instance Reuse","text":"<p>Create instances once and reuse them:</p> <pre><code>from pydantic import BaseModel\nfrom serapeum.core.llms import ToolOrchestratingLLM\nfrom serapeum.llms.ollama import Ollama\n\nclass Classification(BaseModel):\n    category: str\n    confidence: float\n\n# Create once\nllm = Ollama(model=\"llama3.1\", request_timeout=80)\nclassifier = ToolOrchestratingLLM(\n    output_cls=Classification,\n    prompt=\"Classify: {text}\",\n    llm=llm,\n)\n\n# Reuse many times - this is efficient!\ntexts = [\"text1\", \"text2\", \"text3\"]\nfor text in texts:\n    result = classifier(text=text)\n</code></pre>"},{"location":"reference/core/llms/orchestrators/tool_orchestrating_llm/examples/#4-use-parallel-calls-for-lists","title":"4. Use Parallel Calls for Lists","text":"<p>When extracting multiple items, use parallel tool calls:</p> <pre><code>from pydantic import BaseModel\nfrom serapeum.core.llms import ToolOrchestratingLLM\nfrom serapeum.llms.ollama import Ollama\n\nclass Item(BaseModel):\n    name: str\n    type: str\n\nllm = Ollama(model=\"llama3.1\", request_timeout=80)\n\n# Good: Enable parallel for extracting multiple items\ntools_llm = ToolOrchestratingLLM(\n    output_cls=Item,\n    prompt=\"Extract ALL items from: {text}\",\n    llm=llm,\n    allow_parallel_tool_calls=True,  # Enable for lists\n)\n\n# Returns List[Item] with all extracted items\nresults = tools_llm(text=\"apples, oranges, bananas\")\n</code></pre>"},{"location":"reference/core/llms/orchestrators/tool_orchestrating_llm/examples/#see-also","title":"See Also","text":"<ul> <li>General Overview - Complete workflow explanation</li> <li>Execution Flow and Method Calls - Detailed sequence diagram</li> <li>Architecture and Class Relationships - Class structure</li> <li>Data Transformations and Validation - Data flow details</li> <li>Component Boundaries and Interactions - System components</li> <li>Lifecycle States and Transitions - State management</li> </ul>"},{"location":"reference/core/llms/orchestrators/tool_orchestrating_llm/general/","title":"ToolOrchestratingLLM Workflow","text":"<p>This directory contains comprehensive documentation explaining the complete workflow of the <code>ToolOrchestratingLLM</code> class, from initialization to execution and structured output extraction.</p>"},{"location":"reference/core/llms/orchestrators/tool_orchestrating_llm/general/#overview","title":"Overview","text":"<p>The <code>ToolOrchestratingLLM</code> is a function-calling orchestrator that: 1. Converts Pydantic models into callable tools with JSON schemas 2. Formats prompts with template variables 3. Executes function calling via LLM with tool schemas 4. Parses tool outputs into validated Pydantic model instances</p>"},{"location":"reference/core/llms/orchestrators/tool_orchestrating_llm/general/#example-usage","title":"Example Usage","text":"<pre><code>from pydantic import BaseModel\nfrom serapeum.core.llms import ToolOrchestratingLLM\nfrom serapeum.ollama import Ollama\n\n# Define the output schema\nclass MockAlbum(BaseModel):\n    title: str\n    artist: str\n    songs: List[MockSong]\n\nclass MockSong(BaseModel):\n    title: str\n\n# Initialize LLM with function calling support\nllm = Ollama(model='llama3.1', request_timeout=80)\n\n# Create ToolOrchestratingLLM instance\ntools_llm = ToolOrchestratingLLM(\n    output_cls=MockAlbum,\n    prompt='This is a test album with {topic}',\n    llm=llm,\n)\n\n# Execute and get structured output via function calling\nobj_output = tools_llm(topic=\"songs\")\n# Returns: MockAlbum(title=\"hello\", artist=\"world\", songs=[...])\n</code></pre>"},{"location":"reference/core/llms/orchestrators/tool_orchestrating_llm/general/#understanding-the-workflow","title":"Understanding the Workflow","text":""},{"location":"reference/core/llms/orchestrators/tool_orchestrating_llm/general/#1-execution-flow-and-method-calls","title":"1. Execution Flow and Method Calls","text":"<p>Shows the chronological flow of method calls and interactions.</p> <p>Best for: - Understanding the order of operations - Seeing how function calling works - Debugging execution flow</p> <p>Key Sections: - Initialization phase (validation and component setup) - Tool creation (CallableTool.from_model) - Execution phase (predict_and_call with tools) - Tool execution and output parsing</p>"},{"location":"reference/core/llms/orchestrators/tool_orchestrating_llm/general/#2-architecture-and-class-relationships","title":"2. Architecture and Class Relationships","text":"<p>Illustrates the static structure and relationships between classes.</p> <p>Best for: - Understanding the architecture - Seeing inheritance and composition - Identifying class responsibilities</p> <p>Key Classes: - <code>ToolOrchestratingLLM</code>: Main orchestrator - <code>CallableTool</code>: Pydantic-to-tool converter - <code>FunctionCallingLLM</code>: Abstract function-calling interface - <code>Ollama</code>: Concrete LLM implementation - <code>AgentChatResponse</code>: Response container with tool outputs</p>"},{"location":"reference/core/llms/orchestrators/tool_orchestrating_llm/general/#3-data-transformations-and-validation","title":"3. Data Transformations and Validation","text":"<p>Tracks how data transforms through the system.</p> <p>Best for: - Understanding data transformations - Identifying validation points - Seeing error handling paths</p> <p>Key Flows: - Initialization validation pipeline - Tool schema generation - Function calling request preparation - Tool execution and validation - Single vs. parallel output extraction</p>"},{"location":"reference/core/llms/orchestrators/tool_orchestrating_llm/general/#4-component-boundaries-and-interactions","title":"4. Component Boundaries and Interactions","text":"<p>Shows component boundaries and interaction patterns.</p> <p>Best for: - Understanding system architecture - Seeing component responsibilities - Identifying interaction patterns</p> <p>Key Components: - User space (application code) - ToolOrchestratingLLM layer (orchestration) - Tool layer (CallableTool) - Prompt layer (template formatting) - LLM layer (function calling execution) - Response layer (AgentChatResponse, ToolOutput)</p>"},{"location":"reference/core/llms/orchestrators/tool_orchestrating_llm/general/#5-lifecycle-states-and-transitions","title":"5. Lifecycle States and Transitions","text":"<p>Depicts the lifecycle states and transitions.</p> <p>Best for: - Understanding instance lifecycle - Seeing state transitions - Identifying error states</p> <p>Key States: - Initialization states (validation) - Execution states (sync/async/streaming) - Tool execution states (single/parallel) - Parsing states (output extraction)</p>"},{"location":"reference/core/llms/orchestrators/tool_orchestrating_llm/general/#workflow-summary","title":"Workflow Summary","text":""},{"location":"reference/core/llms/orchestrators/tool_orchestrating_llm/general/#initialization-workflow","title":"Initialization Workflow","text":"<pre><code>1. Initialize ToolOrchestratingLLM with:\n   - output_cls: Pydantic model (MockAlbum)\n   - prompt: String or BasePromptTemplate\n   - llm: Function-calling LLM (Ollama)\n   - tool_choice: Optional tool selection strategy\n   - allow_parallel_tool_calls: Single vs. multiple outputs\n\n2. Validation occurs:\n   - LLM must support function calling (is_function_calling_model=True)\n   - Prompt conversion to template if needed\n\n3. Components stored in instance\n\n4. Instance ready for execution\n</code></pre>"},{"location":"reference/core/llms/orchestrators/tool_orchestrating_llm/general/#execution-workflow","title":"Execution Workflow","text":"<pre><code>1. User calls tools_llm(topic=\"songs\")\n\n2. Convert Pydantic model to tool:\n   a. CallableTool.from_model(MockAlbum)\n   b. Extract JSON schema from model\n   c. Create callable tool with validation\n\n3. Format prompt:\n   a. Apply template variables: topic=\"songs\"\n   b. Create messages: [Message(role=USER, content=\"...\")]\n   c. Extend with system prompts\n\n4. Execute function calling:\n   a. Prepare request with tool schemas\n   b. Call predict_and_call([tool], messages, ...)\n   c. HTTP POST to Ollama server with tools parameter\n   d. LLM generates tool_calls in response\n\n5. Execute tools:\n   a. Parse tool_calls from response\n   b. Extract tool arguments\n   c. Validate arguments against Pydantic schema\n   d. Create MockAlbum instance(s)\n   e. Wrap in ToolOutput with raw_output\n\n6. Create AgentChatResponse:\n   a. Container with response text\n   b. sources: List[ToolOutput]\n\n7. Parse outputs:\n   a. Extract raw_output from ToolOutput(s)\n   b. Return MockAlbum or List[MockAlbum]\n\n8. Return validated Pydantic instance(s)\n</code></pre>"},{"location":"reference/core/llms/orchestrators/tool_orchestrating_llm/general/#key-design-patterns","title":"Key Design Patterns","text":""},{"location":"reference/core/llms/orchestrators/tool_orchestrating_llm/general/#1-function-calling-pattern","title":"1. Function Calling Pattern","text":"<p>Uses LLM's native function calling capabilities: - Pydantic models \u2192 Tool schemas - LLM generates tool_calls - Tools execute with validation - Structured outputs guaranteed</p>"},{"location":"reference/core/llms/orchestrators/tool_orchestrating_llm/general/#2-tool-abstraction","title":"2. Tool Abstraction","text":"<p>Automatic conversion from Pydantic models to tools: - JSON schema extraction - Argument validation - Output wrapping</p>"},{"location":"reference/core/llms/orchestrators/tool_orchestrating_llm/general/#3-flexible-output-modes","title":"3. Flexible Output Modes","text":"<p>Single or parallel tool calls: - <code>allow_parallel_tool_calls=False</code> \u2192 Single model - <code>allow_parallel_tool_calls=True</code> \u2192 List of models</p>"},{"location":"reference/core/llms/orchestrators/tool_orchestrating_llm/general/#4-multi-modal-execution","title":"4. Multi-Modal Execution","text":"<p>Supports multiple execution modes: - Sync: <code>__call__</code> - Async: <code>acall</code> - Streaming: <code>stream_call</code> - Async streaming: <code>astream_call</code></p>"},{"location":"reference/core/llms/orchestrators/tool_orchestrating_llm/general/#comparison-with-textcompletionllm","title":"Comparison with TextCompletionLLM","text":"Feature ToolOrchestratingLLM TextCompletionLLM Method Function calling Text completion with parsing LLM Requirement Must support function calling Any chat/completion model Schema Handling Native tool schemas JSON in prompt Validation Before execution (by LLM) After generation (by parser) Reliability Higher (structured by design) Depends on LLM output quality Parallel Outputs Native support Not supported Streaming Partial tool_calls Not applicable Use Case When function calling available Fallback for non-function-calling models"},{"location":"reference/core/llms/orchestrators/tool_orchestrating_llm/general/#performance-considerations","title":"Performance Considerations","text":"<ol> <li>Reusable Instances: <code>ToolOrchestratingLLM</code> instances are reusable after initialization</li> <li>Async Support: <code>acall()</code> method provides async execution</li> <li>Streaming Support: <code>stream_call()</code> yields progressive updates</li> <li>Parallel Tool Calls: Enable with <code>allow_parallel_tool_calls=True</code></li> <li>Stateless Execution: Each call creates independent transient state</li> <li>Tool Schema Caching: Tool schemas are generated once per call</li> </ol>"},{"location":"reference/core/llms/orchestrators/tool_orchestrating_llm/general/#error-handling","title":"Error Handling","text":""},{"location":"reference/core/llms/orchestrators/tool_orchestrating_llm/general/#initialization-errors","title":"Initialization Errors","text":"<ul> <li><code>AssertionError</code>: No LLM provided</li> <li><code>ValueError</code>: LLM doesn't support function calling</li> </ul>"},{"location":"reference/core/llms/orchestrators/tool_orchestrating_llm/general/#execution-errors","title":"Execution Errors","text":"<ul> <li>Network/timeout errors from LLM</li> <li>Missing tool_calls in response</li> <li>Invalid tool arguments</li> </ul>"},{"location":"reference/core/llms/orchestrators/tool_orchestrating_llm/general/#validation-errors","title":"Validation Errors","text":"<ul> <li>Pydantic validation fails on tool arguments</li> <li>Type mismatch in output</li> </ul>"},{"location":"reference/core/llms/orchestrators/tool_orchestrating_llm/general/#when-to-use-toolorchestratingllm","title":"When to Use ToolOrchestratingLLM","text":""},{"location":"reference/core/llms/orchestrators/tool_orchestrating_llm/general/#use-when","title":"Use When:","text":"<p>\u2705 Your LLM supports function calling (OpenAI, Ollama, etc.) \u2705 You need guaranteed structured outputs \u2705 You want parallel tool calls \u2705 You need streaming with structured updates \u2705 Reliability is critical</p>"},{"location":"reference/core/llms/orchestrators/tool_orchestrating_llm/general/#use-textcompletionllm-when","title":"Use TextCompletionLLM When:","text":"<p>\u274c LLM doesn't support function calling \u274c You need simple text-to-JSON parsing \u274c Function calling overhead is unnecessary</p>"},{"location":"reference/core/llms/orchestrators/tool_orchestrating_llm/general/#next-steps","title":"Next Steps","text":"<ul> <li>View Examples - Comprehensive usage examples</li> <li>Sequence Diagram - Detailed flow</li> <li>Class Diagram - Architecture</li> <li>Data Flow - Transformations</li> <li>Components - Interactions</li> <li>State Machine - Lifecycle</li> </ul>"},{"location":"reference/core/llms/orchestrators/tool_orchestrating_llm/tool_orchestrating_llm_class/","title":"Architecture and Class Relationships","text":"<p>This diagram shows the class relationships and structure for <code>ToolOrchestratingLLM</code>.</p>  Hold \"Ctrl\" to enable pan &amp; zoom  <pre><code>classDiagram\n    class BasePydanticLLM~BaseModel~ {\n        &lt;&lt;abstract&gt;&gt;\n        +output_cls: Type[BaseModel]\n        +__call__(**kwargs) BaseModel\n        +acall(**kwargs) BaseModel\n    }\n\n    class ToolOrchestratingLLM~Model~ {\n        +__init__(output_cls, prompt, llm, tool_choice, allow_parallel, verbose)\n        +output_cls: Type[Model]\n        +prompt: BasePromptTemplate\n        +__call__(llm_kwargs, **kwargs) Union[Model, List[Model]]\n        +acall(llm_kwargs, **kwargs) Union[Model, List[Model]]\n        +stream_call(llm_kwargs, **kwargs) Generator[Model, None, None]\n        +astream_call(llm_kwargs, **kwargs) AsyncGenerator[Model, None]\n        -_output_cls: Type[Model]\n        -_llm: FunctionCallingLLM\n        -_prompt: BasePromptTemplate\n        -_verbose: bool\n        -_allow_parallel_tool_calls: bool\n        -_tool_choice: Optional[Union[str, Dict]]\n        +_validate_prompt(prompt) BasePromptTemplate\n        +_validate_llm(llm) LLM\n    }\n\n    class BasePydanticLLM~BaseModel~ {\n        &lt;&lt;abstract&gt;&gt;\n        +output_cls: Type[BaseModel]\n    }\n\n    class BaseTool {\n        &lt;&lt;abstract&gt;&gt;\n        +metadata: ToolMetadata\n        +call(**kwargs) ToolOutput\n        +acall(**kwargs) ToolOutput\n    }\n\n    class CallableTool {\n        +__init__(fn, metadata)\n        +from_function(fn) CallableTool\n        +from_model(model_cls) CallableTool\n        +call(**kwargs) ToolOutput\n        +acall(**kwargs) ToolOutput\n        -_fn: Callable\n        -_async_fn: Optional[Callable]\n        -_metadata: ToolMetadata\n    }\n\n    class BasePromptTemplate {\n        &lt;&lt;abstract&gt;&gt;\n        +metadata: Dict[str, Any]\n        +template_vars: List[str]\n        +kwargs: Dict[str, str]\n        +output_parser: Optional[BaseParser]\n        +template_var_mappings: Optional[Dict]\n        +function_mappings: Optional[Dict]\n        +partial_format(**kwargs) BasePromptTemplate\n        +format(llm, **kwargs) str\n        +format_messages(llm, **kwargs) List[Message]\n        +get_template(llm) str\n    }\n\n    class PromptTemplate {\n        +template: str\n        +__init__(template, prompt_type, output_parser, metadata, ...)\n        +partial_format(**kwargs) PromptTemplate\n        +format(llm, completion_to_prompt, **kwargs) str\n        +format_messages(llm, **kwargs) List[Message]\n        +get_template(llm) str\n    }\n\n    class ChatPromptTemplate {\n        +message_templates: List[Message]\n        +__init__(message_templates, prompt_type, output_parser, ...)\n        +from_messages(message_templates, **kwargs) ChatPromptTemplate\n        +partial_format(**kwargs) ChatPromptTemplate\n        +format(llm, messages_to_prompt, **kwargs) str\n        +format_messages(llm, **kwargs) List[Message]\n        +get_template(llm) str\n    }\n\n    class BaseLLM {\n        &lt;&lt;abstract&gt;&gt;\n        +metadata: Metadata\n        +chat(messages, **kwargs) ChatResponse\n        +stream_chat(messages, **kwargs) ChatResponseGen\n        +achat(messages, **kwargs) ChatResponse\n        +astream_chat(messages, **kwargs) ChatResponseAsyncGen\n        +complete(prompt, **kwargs) CompletionResponse\n        +stream_complete(prompt, **kwargs) CompletionResponseGen\n        +acomplete(prompt, **kwargs) CompletionResponse\n        +astream_complete(prompt, **kwargs) CompletionResponseAsyncGen\n    }\n\n    class LLM {\n        +system_prompt: Optional[str]\n        +messages_to_prompt: MessagesToPromptCallable\n        +completion_to_prompt: CompletionToPromptCallable\n        +output_parser: Optional[BaseParser]\n        +pydantic_program_mode: StructuredLLMMode\n        +_get_prompt(prompt, **kwargs) str\n        +_get_messages(prompt, **kwargs) List[Message]\n        +_parse_output(output) str\n        +_extend_prompt(formatted_prompt) str\n        +_extend_messages(messages) List[Message]\n        +predict(prompt, **kwargs) str\n        +stream(prompt, **kwargs) TokenGen\n        +apredict(prompt, **kwargs) str\n        +astream(prompt, **kwargs) TokenAsyncGen\n        +structured_predict(output_cls, prompt, **kwargs) Model\n    }\n\n    class FunctionCallingLLM {\n        &lt;&lt;abstract&gt;&gt;\n        +predict_and_call(tools, chat_history, ...) AgentChatResponse\n        +apredict_and_call(tools, chat_history, ...) AgentChatResponse\n        +stream_chat_with_tools(tools, chat_history, ...) Generator\n        +astream_chat_with_tools(tools, chat_history, ...) AsyncGenerator\n        +chat_with_tools(tools, chat_history, ...) AgentChatResponse\n        +achat_with_tools(tools, chat_history, ...) AgentChatResponse\n        -_prepare_chat_with_tools(tools, chat_history, ...) Tuple\n        -_validate_chat_with_tools_response(...) AgentChatResponse\n    }\n\n    class Ollama {\n        +model: str\n        +base_url: str\n        +request_timeout: int\n        +temperature: float\n        +metadata: Metadata\n        +__init__(model, base_url, request_timeout, ...)\n        +chat(messages, **kwargs) ChatResponse\n        +achat(messages, **kwargs) ChatResponse\n        +complete(prompt, **kwargs) CompletionResponse\n        +acomplete(prompt, **kwargs) CompletionResponse\n        +predict_and_call(tools, ...) AgentChatResponse\n        +apredict_and_call(tools, ...) AgentChatResponse\n        +stream_chat_with_tools(tools, ...) Generator\n        +astream_chat_with_tools(tools, ...) AsyncGenerator\n        -_chat_request(messages, stream, **kwargs) dict\n        -_complete_request(prompt, stream, **kwargs) dict\n        -_prepare_tools_schema(tools) List[Dict]\n    }\n\n    class AgentChatResponse {\n        +response: str\n        +sources: List[ToolOutput]\n        +is_dummy_stream: bool\n        +metadata: Optional[Dict]\n        +__str__() str\n        +response_gen: Generator[str, None, None]\n        +async_response_gen: AsyncGenerator[str, None]\n        +parse_tool_outputs(allow_parallel) Union[Any, List[Any]]\n    }\n\n    class ToolOutput {\n        +content: str\n        +tool_name: str\n        +raw_input: Dict[str, Any]\n        +raw_output: Any\n        +is_error: bool\n    }\n\n    class StreamingObjectProcessor {\n        +__init__(output_cls, flexible_mode, allow_parallel, llm)\n        +process(partial_resp, cur_objects) Union[Model, List[Model]]\n        -_output_cls: Type[Model]\n        -_flexible_mode: bool\n        -_allow_parallel_tool_calls: bool\n        -_llm: FunctionCallingLLM\n    }\n\n    class BaseModel {\n        &lt;&lt;pydantic&gt;&gt;\n        +model_validate(data) BaseModel\n        +model_validate_json(json_data) BaseModel\n        +model_json_schema() dict\n    }\n\n    class MockAlbum {\n        +title: str\n        +artist: str\n        +songs: List[MockSong]\n    }\n\n    class MockSong {\n        +title: str\n    }\n\n    BasePydanticLLM &lt;|-- ToolOrchestratingLLM\n    BaseLLM &lt;|-- LLM\n    LLM &lt;|-- FunctionCallingLLM\n    FunctionCallingLLM &lt;|-- Ollama\n    BaseTool &lt;|-- CallableTool\n    BasePromptTemplate &lt;|-- PromptTemplate\n    BasePromptTemplate &lt;|-- ChatPromptTemplate\n    BaseModel &lt;|-- MockAlbum\n    BaseModel &lt;|-- MockSong\n\n    ToolOrchestratingLLM o-- FunctionCallingLLM : uses\n    ToolOrchestratingLLM o-- BasePromptTemplate : uses\n    ToolOrchestratingLLM ..&gt; CallableTool : creates from model\n    ToolOrchestratingLLM ..&gt; AgentChatResponse : receives\n    ToolOrchestratingLLM ..&gt; MockAlbum : produces\n    ToolOrchestratingLLM ..&gt; StreamingObjectProcessor : uses for streaming\n    CallableTool ..&gt; MockAlbum : validates against schema\n    CallableTool ..&gt; ToolOutput : produces\n    AgentChatResponse o-- ToolOutput : contains list\n    ToolOutput o-- MockAlbum : contains as raw_output\n    MockAlbum o-- MockSong : contains list\n\n    note for ToolOrchestratingLLM \"Main orchestrator that:\\n1. Converts Pydantic model to tool\\n2. Formats prompts with variables\\n3. Calls LLM with function calling\\n4. Parses tool outputs to models\\n5. Supports sync/async/streaming\"\n    note for CallableTool \"Converts Pydantic models\\nto callable tools with\\nJSON schema validation\"\n    note for AgentChatResponse \"Container for LLM response\\nand tool execution results\"\n    note for Ollama \"Concrete implementation\\nfor Ollama with\\nfunction calling support\"</code></pre>"},{"location":"reference/core/llms/orchestrators/tool_orchestrating_llm/tool_orchestrating_llm_class/#class-responsibilities","title":"Class Responsibilities","text":""},{"location":"reference/core/llms/orchestrators/tool_orchestrating_llm/tool_orchestrating_llm_class/#toolorchestratingllm","title":"ToolOrchestratingLLM","text":"<ul> <li>Orchestrates the complete function-calling workflow</li> <li>Validates LLM supports function calling during initialization</li> <li>Converts Pydantic models to callable tools</li> <li>Routes execution through predict_and_call</li> <li>Supports single or parallel tool calls</li> <li>Handles sync, async, and streaming modes</li> </ul>"},{"location":"reference/core/llms/orchestrators/tool_orchestrating_llm/tool_orchestrating_llm_class/#callabletool","title":"CallableTool","text":"<ul> <li>Converts Pydantic models to function schemas</li> <li>Validates tool arguments against JSON schema</li> <li>Executes tool functions (sync/async)</li> <li>Wraps results in ToolOutput</li> </ul>"},{"location":"reference/core/llms/orchestrators/tool_orchestrating_llm/tool_orchestrating_llm_class/#functioncallingllm-abstract","title":"FunctionCallingLLM (Abstract)","text":"<ul> <li>Defines interface for function-calling LLMs</li> <li>Provides predict_and_call abstraction</li> <li>Handles tool schema preparation</li> <li>Manages tool execution orchestration</li> </ul>"},{"location":"reference/core/llms/orchestrators/tool_orchestrating_llm/tool_orchestrating_llm_class/#ollama","title":"Ollama","text":"<ul> <li>Implements FunctionCallingLLM for Ollama server</li> <li>Formats requests with tool schemas</li> <li>Parses tool_calls from responses</li> <li>Executes tools and aggregates results</li> </ul>"},{"location":"reference/core/llms/orchestrators/tool_orchestrating_llm/tool_orchestrating_llm_class/#agentchatresponse","title":"AgentChatResponse","text":"<ul> <li>Contains LLM response text and tool outputs</li> <li>Parses tool outputs to extract structured models</li> <li>Supports single or list of outputs</li> <li>Provides streaming helpers</li> </ul>"},{"location":"reference/core/llms/orchestrators/tool_orchestrating_llm/tool_orchestrating_llm_class/#streamingobjectprocessor","title":"StreamingObjectProcessor","text":"<ul> <li>Processes partial streaming responses</li> <li>Maintains state across chunks</li> <li>Yields progressively updated models</li> <li>Handles flexible/strict parsing modes</li> </ul>"},{"location":"reference/core/llms/orchestrators/tool_orchestrating_llm/tool_orchestrating_llm_class/#key-design-patterns","title":"Key Design Patterns","text":"<ol> <li>Protocol-Based Interfaces: Uses abstract base classes for extensibility</li> <li>Pydantic Integration: First-class support for structured outputs</li> <li>Async-First: All operations support sync/async/streaming</li> <li>Tool Abstraction: Pydantic models become callable tools automatically</li> <li>Response Aggregation: AgentChatResponse unifies text and structured outputs</li> </ol>"},{"location":"reference/core/llms/orchestrators/tool_orchestrating_llm/tool_orchestrating_llm_components/","title":"Component Boundaries and Interactions","text":"<p>This diagram shows how components interact during the complete lifecycle.</p>  Hold \"Ctrl\" to enable pan &amp; zoom  <pre><code>graph TB\n    subgraph User Space\n        UC[User Code]\n        MA[MockAlbum Schema]\n        MS[MockSong Schema]\n    end\n\n    subgraph ToolOrchestratingLLM Components\n        TOL[ToolOrchestratingLLM]\n\n        subgraph Validators\n            VL[validate_llm]\n            VP[validate_prompt]\n        end\n\n        subgraph Stored State\n            SL[\"_llm: FunctionCallingLLM\"]\n            SP[\"_prompt: BasePromptTemplate\"]\n            SO[\"_output_cls: Type[MockAlbum]\"]\n            SV[\"_verbose: bool\"]\n            SAP[\"_allow_parallel_tool_calls: bool\"]\n            STC[\"_tool_choice: Optional\"]\n        end\n\n        subgraph Execution Methods\n            CALL[\"__call__\"]\n            ACALL[\"acall\"]\n            STREAM[\"stream_call\"]\n            ASTREAM[\"astream_call\"]\n        end\n    end\n\n    subgraph Tool Layer\n        CT[CallableTool]\n\n        subgraph Tool Operations\n            FM[from_model]\n            ES[extract_schema]\n            TC[tool.call]\n        end\n    end\n\n    subgraph Prompt Layer\n        PT[PromptTemplate]\n        CPT[ChatPromptTemplate]\n\n        subgraph Prompt Operations\n            FMS[format_messages]\n            AV[Apply Variables]\n            GT[get_template]\n        end\n    end\n\n    subgraph LLM Layer - FunctionCallingLLM\n        OL[Ollama]\n\n        subgraph Orchestration Methods\n            PAC[predict_and_call]\n            APAC[apredict_and_call]\n            SCT[stream_chat_with_tools]\n            ASCT[astream_chat_with_tools]\n        end\n\n        subgraph Message Processing\n            EM[_extend_messages]\n            PM[_prepare_chat_with_tools]\n        end\n\n        subgraph Tool Schema\n            PTS[_prepare_tools_schema]\n        end\n\n        MD[metadata]\n    end\n\n    subgraph External Service\n        OS[Ollama Server]\n\n        subgraph Endpoints\n            EC[\"/api/chat\"]\n            EG[\"/api/generate\"]\n        end\n    end\n\n    subgraph Response Models\n        ACR[AgentChatResponse]\n\n        subgraph Response Components\n            RES[response: str]\n            SRC[\"sources: List[ToolOutput]\"]\n        end\n\n        subgraph Parsing Methods\n            PTO[parse_tool_outputs]\n        end\n    end\n\n    subgraph Tool Output\n        TOUT[ToolOutput]\n\n        subgraph Output Fields\n            TCONT[content]\n            TNAME[tool_name]\n            TINPUT[raw_input]\n            TOUTPUT[raw_output: MockAlbum]\n        end\n    end\n\n    subgraph Streaming Support\n        SOP[StreamingObjectProcessor]\n\n        subgraph Processor Methods\n            PROC[process]\n            PS[Parse partial JSON]\n            US[Update state]\n        end\n    end\n\n    %% Initialization Flow\n    UC --&gt;|1. Create instance| TOL\n    TOL --&gt;|Validate| VL\n    VL --&gt;|Check metadata| MD\n    MD --&gt;|is_function_calling_model| VL\n    VL --&gt;|Store| SL\n\n    TOL --&gt;|Validate/Convert| VP\n    VP --&gt;|Create if string| PT\n    VP --&gt;|Store| SP\n\n    TOL --&gt;|Store| SO\n    TOL --&gt;|Store| SV\n    TOL --&gt;|Store| SAP\n    TOL --&gt;|Store| STC\n\n    MA --&gt;|Defines schema| SO\n\n    %% Execution Flow - Standard Call\n    UC --&gt;|2. Call with kwargs| CALL\n    CALL --&gt;|Create tool| FM\n    FM --&gt;|Extract schema from| MA\n    FM --&gt;|Returns| CT\n\n    CALL --&gt;|Format with kwargs| FMS\n    FMS --&gt;|Uses template| PT\n    FMS --&gt;|Apply variables| AV\n    AV --&gt;|Returns| Messages\n\n    CALL --&gt;|Extend| EM\n    EM --&gt;|Add system prompts| Messages\n\n    CALL --&gt;|Invoke| PAC\n    PAC --&gt;|Prepare| PM\n    PM --&gt;|Add tools| PTS\n    PTS --&gt;|From| CT\n\n    PAC --&gt;|HTTP POST| EC\n    EC --&gt;|In| OS\n    OS --&gt;|Returns| Response\n\n    PAC --&gt;|Parse tool_calls| Response\n    PAC --&gt;|Execute| TC\n    TC --&gt;|Validate against| MA\n    TC --&gt;|Create instance| MA\n    TC --&gt;|Wrap in| TOUT\n\n    PAC --&gt;|Create| ACR\n    ACR --&gt;|Contains| TOUT\n    ACR --&gt;|In| SRC\n\n    CALL --&gt;|Parse| PTO\n    PTO --&gt;|Extract from| SRC\n    PTO --&gt;|Get| TOUTPUT\n    TOUTPUT --&gt;|Returns| MA\n\n    MA --&gt;|Instance to| UC\n\n    %% Async Flow\n    UC --&gt;|3. Async call| ACALL\n    ACALL --&gt;|Invoke| APAC\n    APAC --&gt;|Similar flow| PAC\n\n    %% Streaming Flow\n    UC --&gt;|4. Stream call| STREAM\n    STREAM --&gt;|Invoke| SCT\n    SCT --&gt;|Yields chunks| Response\n\n    STREAM --&gt;|Process| PROC\n    PROC --&gt;|Uses| SOP\n    SOP --&gt;|Parse| PS\n    PS --&gt;|Update| US\n    US --&gt;|Yields partial| MA\n\n    %% Async Streaming Flow\n    UC --&gt;|5. Async stream| ASTREAM\n    ASTREAM --&gt;|Invoke| ASCT\n    ASCT --&gt;|Similar flow| SCT\n\n    %% Parallel Tool Calls\n    SAP --&gt;|If True| PTO\n    PTO --&gt;|Returns List| MA\n\n    %% Styling\n    classDef userClass fill:#e1f5ff,stroke:#01579b\n    classDef validatorClass fill:#fff9c4,stroke:#f57f17\n    classDef stateClass fill:#f3e5f5,stroke:#4a148c\n    classDef toolClass fill:#e8f5e9,stroke:#1b5e20\n    classDef promptClass fill:#fce4ec,stroke:#880e4f\n    classDef llmClass fill:#e0f2f1,stroke:#004d40\n    classDef responseClass fill:#fff3e0,stroke:#e65100\n    classDef externalClass fill:#efebe9,stroke:#3e2723\n    classDef streamClass fill:#e3f2fd,stroke:#0d47a1\n\n    class UC,MA,MS userClass\n    class VL,VP validatorClass\n    class SL,SP,SO,SV,SAP,STC stateClass\n    class CT,FM,ES,TC toolClass\n    class PT,CPT,FMS,AV,GT promptClass\n    class OL,PAC,APAC,SCT,ASCT,EM,PM,PTS,MD llmClass\n    class ACR,RES,SRC,PTO responseClass\n    class TOUT,TCONT,TNAME,TINPUT,TOUTPUT responseClass\n    class OS,EC,EG externalClass\n    class SOP,PROC,PS,US streamClass</code></pre>"},{"location":"reference/core/llms/orchestrators/tool_orchestrating_llm/tool_orchestrating_llm_components/#component-responsibilities-matrix","title":"Component Responsibilities Matrix","text":"Component Initialization Execution Parsing Streaming ToolOrchestratingLLM Validates &amp; stores components Routes to predict_and_call Extracts from AgentChatResponse Processes with StreamingObjectProcessor CallableTool - Created from Pydantic model Validates tool arguments - PromptTemplate Created/validated Formats with variables - - Ollama Validated for function calling Executes predict_and_call - Streams chat_with_tools AgentChatResponse - Created by LLM Contains ToolOutputs - ToolOutput - Created by tool execution Contains raw_output (Pydantic) - MockAlbum Defines schema - Validates parsed data Progressively built StreamingObjectProcessor - - - Parses partial responses"},{"location":"reference/core/llms/orchestrators/tool_orchestrating_llm/tool_orchestrating_llm_components/#interaction-patterns","title":"Interaction Patterns","text":""},{"location":"reference/core/llms/orchestrators/tool_orchestrating_llm/tool_orchestrating_llm_components/#1-initialization-pattern-constructor","title":"1. Initialization Pattern (Constructor)","text":"<pre><code>User \u2192 ToolOrchestratingLLM.__init__\n  \u251c\u2500\u2192 validate_llm\n  \u2502   \u251c\u2500\u2192 Use provided or fallback to Configs.llm\n  \u2502   \u2514\u2500\u2192 Check metadata.is_function_calling_model == True\n  \u251c\u2500\u2192 validate_prompt\n  \u2502   \u2514\u2500\u2192 Convert string to PromptTemplate if needed\n  \u2514\u2500\u2192 Store all components:\n      \u251c\u2500\u2192 _output_cls (MockAlbum)\n      \u251c\u2500\u2192 _llm (Ollama)\n      \u251c\u2500\u2192 _prompt (PromptTemplate)\n      \u251c\u2500\u2192 _verbose\n      \u251c\u2500\u2192 _allow_parallel_tool_calls\n      \u2514\u2500\u2192 _tool_choice\n</code></pre>"},{"location":"reference/core/llms/orchestrators/tool_orchestrating_llm/tool_orchestrating_llm_components/#2-standard-execution-pattern-sync","title":"2. Standard Execution Pattern (Sync)","text":"<pre><code>User \u2192 ToolOrchestratingLLM.__call__(topic=\"songs\")\n  \u251c\u2500\u2192 CallableTool.from_model(MockAlbum)\n  \u2502   \u251c\u2500\u2192 Extract JSON schema from MockAlbum\n  \u2502   \u2514\u2500\u2192 Create callable tool with validation\n  \u251c\u2500\u2192 PromptTemplate.format_messages(topic=\"songs\")\n  \u2502   \u251c\u2500\u2192 Apply template variables\n  \u2502   \u2514\u2500\u2192 Return List[Message]\n  \u251c\u2500\u2192 Ollama._extend_messages(messages)\n  \u2502   \u2514\u2500\u2192 Add system prompts if configured\n  \u251c\u2500\u2192 Ollama.predict_and_call([tool], messages, ...)\n  \u2502   \u251c\u2500\u2192 Prepare chat request with tool schemas\n  \u2502   \u251c\u2500\u2192 HTTP POST to /api/chat\n  \u2502   \u251c\u2500\u2192 Parse tool_calls from response\n  \u2502   \u251c\u2500\u2192 Execute tool.call(args)\n  \u2502   \u2502   \u251c\u2500\u2192 Validate args against MockAlbum schema\n  \u2502   \u2502   \u251c\u2500\u2192 Create MockAlbum instance\n  \u2502   \u2502   \u2514\u2500\u2192 Wrap in ToolOutput\n  \u2502   \u2514\u2500\u2192 Create AgentChatResponse with sources\n  \u2514\u2500\u2192 AgentChatResponse.parse_tool_outputs(allow_parallel=False)\n      \u251c\u2500\u2192 Extract sources[0].raw_output\n      \u2514\u2500\u2192 Return MockAlbum instance\n</code></pre>"},{"location":"reference/core/llms/orchestrators/tool_orchestrating_llm/tool_orchestrating_llm_components/#3-parallel-execution-pattern","title":"3. Parallel Execution Pattern","text":"<pre><code>User \u2192 ToolOrchestratingLLM.__call__(..., allow_parallel_tool_calls=True)\n  \u251c\u2500\u2192 [Same tool creation and message formatting]\n  \u251c\u2500\u2192 Ollama.predict_and_call([tool], ..., allow_parallel=True)\n  \u2502   \u251c\u2500\u2192 LLM generates multiple tool_calls\n  \u2502   \u251c\u2500\u2192 Execute each tool.call(args)\n  \u2502   \u2502   \u251c\u2500\u2192 ToolOutput 1: MockAlbum(title=\"hello\", ...)\n  \u2502   \u2502   \u2514\u2500\u2192 ToolOutput 2: MockAlbum(title=\"hello2\", ...)\n  \u2502   \u2514\u2500\u2192 Create AgentChatResponse with multiple sources\n  \u2514\u2500\u2192 AgentChatResponse.parse_tool_outputs(allow_parallel=True)\n      \u251c\u2500\u2192 Extract all sources[i].raw_output\n      \u2514\u2500\u2192 Return List[MockAlbum]\n</code></pre>"},{"location":"reference/core/llms/orchestrators/tool_orchestrating_llm/tool_orchestrating_llm_components/#4-async-execution-pattern","title":"4. Async Execution Pattern","text":"<pre><code>User \u2192 await ToolOrchestratingLLM.acall(...)\n  \u251c\u2500\u2192 [Same tool creation and message formatting]\n  \u251c\u2500\u2192 await Ollama.apredict_and_call([tool], messages, ...)\n  \u2502   \u251c\u2500\u2192 Async HTTP request\n  \u2502   \u251c\u2500\u2192 Async tool execution\n  \u2502   \u2514\u2500\u2192 Return AgentChatResponse\n  \u2514\u2500\u2192 AgentChatResponse.parse_tool_outputs(...)\n      \u2514\u2500\u2192 Return MockAlbum or List[MockAlbum]\n</code></pre>"},{"location":"reference/core/llms/orchestrators/tool_orchestrating_llm/tool_orchestrating_llm_components/#5-streaming-execution-pattern-sync","title":"5. Streaming Execution Pattern (Sync)","text":"<pre><code>User \u2192 for obj in ToolOrchestratingLLM.stream_call(...):\n  \u251c\u2500\u2192 [Same tool creation and message formatting]\n  \u251c\u2500\u2192 Ollama.stream_chat_with_tools([tool], messages, ...)\n  \u2502   \u2514\u2500\u2192 Yields partial ChatResponse chunks\n  \u2514\u2500\u2192 For each chunk:\n      \u251c\u2500\u2192 StreamingObjectProcessor.process(chunk, cur_objects)\n      \u2502   \u251c\u2500\u2192 Parse partial tool_calls JSON\n      \u2502   \u251c\u2500\u2192 Validate against MockAlbum schema (flexible mode)\n      \u2502   \u251c\u2500\u2192 Update cur_objects state\n      \u2502   \u2514\u2500\u2192 Return progressively updated MockAlbum\n      \u2514\u2500\u2192 Yield MockAlbum (partial or complete)\n</code></pre>"},{"location":"reference/core/llms/orchestrators/tool_orchestrating_llm/tool_orchestrating_llm_components/#6-async-streaming-pattern","title":"6. Async Streaming Pattern","text":"<pre><code>User \u2192 async for obj in await ToolOrchestratingLLM.astream_call(...):\n  \u251c\u2500\u2192 [Same tool creation and message formatting]\n  \u251c\u2500\u2192 await Ollama.astream_chat_with_tools([tool], messages, ...)\n  \u2502   \u2514\u2500\u2192 Async yields partial ChatResponse chunks\n  \u2514\u2500\u2192 For each chunk:\n      \u251c\u2500\u2192 StreamingObjectProcessor.process(chunk, cur_objects)\n      \u2514\u2500\u2192 Yield MockAlbum (partial or complete)\n</code></pre>"},{"location":"reference/core/llms/orchestrators/tool_orchestrating_llm/tool_orchestrating_llm_components/#state-management","title":"State Management","text":""},{"location":"reference/core/llms/orchestrators/tool_orchestrating_llm/tool_orchestrating_llm_components/#immutable-state-post-initialization","title":"Immutable State (Post-Initialization)","text":"<ul> <li><code>_output_cls</code>: Type[MockAlbum] - Schema for structured output</li> <li><code>_llm</code>: FunctionCallingLLM - Language model instance</li> <li><code>_verbose</code>: bool - Logging control</li> <li><code>_allow_parallel_tool_calls</code>: bool - Single vs. multiple outputs</li> <li><code>_tool_choice</code>: Optional - Tool selection strategy</li> </ul>"},{"location":"reference/core/llms/orchestrators/tool_orchestrating_llm/tool_orchestrating_llm_components/#mutable-state","title":"Mutable State","text":"<ul> <li><code>_prompt</code>: BasePromptTemplate - Can be updated via setter</li> </ul>"},{"location":"reference/core/llms/orchestrators/tool_orchestrating_llm/tool_orchestrating_llm_components/#transient-state-per-call","title":"Transient State (Per Call)","text":"<ul> <li><code>llm_kwargs</code>: Forwarded to LLM methods (temperature, max_tokens, etc.)</li> <li><code>**kwargs</code>: Template variables for prompt formatting</li> <li><code>tool</code>: CallableTool instance created from output_cls</li> <li><code>messages</code>: Formatted and extended message list</li> <li><code>agent_response</code>: AgentChatResponse from LLM</li> <li><code>parsed_output</code>: Final MockAlbum or List[MockAlbum]</li> </ul>"},{"location":"reference/core/llms/orchestrators/tool_orchestrating_llm/tool_orchestrating_llm_components/#streaming-state-per-stream","title":"Streaming State (Per Stream)","text":"<ul> <li><code>cur_objects</code>: List of partial/complete objects maintained across chunks</li> <li><code>partial_resp</code>: Current chunk being processed</li> <li><code>objects</code>: Progressively updated Pydantic instances</li> </ul>"},{"location":"reference/core/llms/orchestrators/tool_orchestrating_llm/tool_orchestrating_llm_components/#data-flow-between-components","title":"Data Flow Between Components","text":"<pre><code>User Input (kwargs)\n  \u2193\nToolOrchestratingLLM\n  \u2193\nCallableTool (from MockAlbum schema)\n  \u2193\nPromptTemplate (formatted with kwargs)\n  \u2193\nMessages (List[Message])\n  \u2193\nOllama (extended with system prompts)\n  \u2193\nHTTP Request (with tool schemas)\n  \u2193\nOllama Server\n  \u2193\nHTTP Response (with tool_calls)\n  \u2193\nOllama (parse and execute tools)\n  \u2193\nToolOutput (with raw_output=MockAlbum)\n  \u2193\nAgentChatResponse (with sources=[ToolOutput])\n  \u2193\nparse_tool_outputs (extract raw_output)\n  \u2193\nMockAlbum instance or List[MockAlbum]\n  \u2193\nUser Output\n</code></pre>"},{"location":"reference/core/llms/orchestrators/tool_orchestrating_llm/tool_orchestrating_llm_components/#error-boundaries","title":"Error Boundaries","text":"<ol> <li>Initialization: validate_llm, validate_prompt</li> <li>Tool Creation: CallableTool.from_model - schema extraction</li> <li>Prompt Formatting: format_messages - template variable errors</li> <li>LLM Execution: predict_and_call - network errors, timeout</li> <li>Tool Parsing: Parse tool_calls - missing/malformed data</li> <li>Tool Execution: Validate args - Pydantic ValidationError</li> <li>Output Extraction: parse_tool_outputs - missing raw_output</li> </ol>"},{"location":"reference/core/llms/orchestrators/tool_orchestrating_llm/tool_orchestrating_llm_dataflow/","title":"Data Transformations and Validation","text":"<p>This diagram shows how data flows through the <code>ToolOrchestratingLLM</code> system.</p>  Hold \"Ctrl\" to enable pan &amp; zoom  <pre><code>flowchart TD\n    Start([User Code]) --&gt; Init{Initialize ToolOrchestratingLLM}\n\n    Init --&gt; ValidateLLM[Validate LLM]\n    ValidateLLM --&gt; CheckLLM{LLM Provided?}\n    CheckLLM --&gt;|Yes| CheckFunctionCalling{is_function_calling_model?}\n    CheckLLM --&gt;|No| CheckConfigs{Configs.llm Set?}\n    CheckConfigs --&gt;|Yes| CheckFunctionCalling\n    CheckConfigs --&gt;|No| Error1[Raise AssertionError]\n    CheckFunctionCalling --&gt;|True| ValidatePrompt\n    CheckFunctionCalling --&gt;|False| Error2[Raise ValueError:&lt;br/&gt;Model does not support function calling]\n\n    ValidatePrompt --&gt; CheckPrompt{Prompt Type?}\n    CheckPrompt --&gt;|BasePromptTemplate| StoreComponents\n    CheckPrompt --&gt;|String| ConvertPrompt[Convert to PromptTemplate]\n    CheckPrompt --&gt;|Other| Error3[Raise ValueError]\n\n    ConvertPrompt --&gt; StoreComponents[Store All Components]\n    StoreComponents --&gt; Ready([ToolOrchestratingLLM Ready])\n\n    Ready --&gt; Call{User Calls tools_llm}\n    Call --&gt;|With kwargs| CreateTool[CallableTool.from_model]\n\n    CreateTool --&gt; ExtractSchema[Extract Pydantic schema]\n    ExtractSchema --&gt; BuildToolSchema[Build tool JSON schema]\n    BuildToolSchema --&gt; FormatMessages[Format Messages]\n\n    FormatMessages --&gt; ApplyVars[Apply template variables]\n    ApplyVars --&gt; CreateMessages[Create Message list]\n    CreateMessages --&gt; ExtendMessages[Extend with system prompts]\n\n    ExtendMessages --&gt; PredictAndCall[Call predict_and_call]\n    PredictAndCall --&gt; PrepareRequest[Prepare chat request with tools]\n\n    PrepareRequest --&gt; AddToolSchemas[Add tool schemas to request]\n    AddToolSchemas --&gt; SendRequest[HTTP POST to Ollama server]\n    SendRequest --&gt; ReceiveResponse[Receive response with tool_calls]\n\n    ReceiveResponse --&gt; ParseToolCalls{tool_calls present?}\n    ParseToolCalls --&gt;|No| Error4[No tool calls generated]\n    ParseToolCalls --&gt;|Yes| ExtractArgs[Extract tool arguments]\n\n    ExtractArgs --&gt; CheckParallel{allow_parallel_tool_calls?}\n    CheckParallel --&gt;|False| ExecuteSingle[Execute single tool call]\n    CheckParallel --&gt;|True| ExecuteMultiple[Execute multiple tool calls]\n\n    ExecuteSingle --&gt; ValidateArgs1[Validate args against schema]\n    ValidateArgs1 --&gt; CreateModel1[Create Pydantic instance]\n    CreateModel1 --&gt; WrapOutput1[Wrap in ToolOutput]\n    WrapOutput1 --&gt; CreateResponse1[Create AgentChatResponse]\n    CreateResponse1 --&gt; ParseSingle\n\n    ExecuteMultiple --&gt; ValidateArgsN[Validate each args against schema]\n    ValidateArgsN --&gt; CreateModelN[Create multiple Pydantic instances]\n    CreateModelN --&gt; WrapOutputN[Wrap each in ToolOutput]\n    WrapOutputN --&gt; CreateResponseN[Create AgentChatResponse]\n    CreateResponseN --&gt; ParseMultiple\n\n    ParseSingle[parse_tool_outputs&lt;br/&gt;allow_parallel=False]\n    ParseSingle --&gt; ExtractFirst[Extract first source.raw_output]\n    ExtractFirst --&gt; ReturnSingle([Return MockAlbum instance])\n\n    ParseMultiple[parse_tool_outputs&lt;br/&gt;allow_parallel=True]\n    ParseMultiple --&gt; ExtractAll[Extract all sources.raw_output]\n    ExtractAll --&gt; ReturnList([Return List of MockAlbum])\n\n    style Start fill:#e1f5ff\n    style Ready fill:#e1f5ff\n    style ReturnSingle fill:#c8e6c9\n    style ReturnList fill:#c8e6c9\n    style Error1 fill:#ffcdd2\n    style Error2 fill:#ffcdd2\n    style Error3 fill:#ffcdd2\n    style Error4 fill:#ffcdd2\n    style SendRequest fill:#fff9c4\n    style ReceiveResponse fill:#fff9c4</code></pre>"},{"location":"reference/core/llms/orchestrators/tool_orchestrating_llm/tool_orchestrating_llm_dataflow/#data-transformations","title":"Data Transformations","text":""},{"location":"reference/core/llms/orchestrators/tool_orchestrating_llm/tool_orchestrating_llm_dataflow/#initialization-phase","title":"Initialization Phase","text":"<pre><code>Input:\n  - output_cls: MockAlbum (Pydantic model class)\n  - prompt: \"This is a test album with {topic}\"\n  - llm: Ollama(model=\"llama3.1\")\n\nValidations:\n  1. Check llm.metadata.is_function_calling_model == True\n  2. Convert prompt string to PromptTemplate if needed\n  3. Store all validated components\n\nOutput:\n  - ToolOrchestratingLLM instance ready for execution\n</code></pre>"},{"location":"reference/core/llms/orchestrators/tool_orchestrating_llm/tool_orchestrating_llm_dataflow/#execution-phase-single-output","title":"Execution Phase - Single Output","text":"<pre><code>Input:\n  tools_llm(topic=\"songs\")\n\nTransformations:\n  1. kwargs: {topic: \"songs\"}\n\n  2. CallableTool.from_model(MockAlbum)\n     \u2192 tool with JSON schema:\n     {\n       \"type\": \"function\",\n       \"function\": {\n         \"name\": \"MockAlbum\",\n         \"parameters\": {\n           \"type\": \"object\",\n           \"properties\": {\n             \"title\": {\"type\": \"string\"},\n             \"artist\": {\"type\": \"string\"},\n             \"songs\": {\"type\": \"array\", \"items\": {...}}\n           }\n         }\n       }\n     }\n\n  3. format_messages(topic=\"songs\")\n     \u2192 [Message(role=USER, content=\"This is a test album with songs\")]\n\n  4. _extend_messages(messages)\n     \u2192 [Message(role=SYSTEM, ...), Message(role=USER, ...)]\n\n  5. predict_and_call([tool], messages, ...)\n     \u2192 HTTP request:\n     {\n       \"model\": \"llama3.1\",\n       \"messages\": [...],\n       \"tools\": [{\"type\": \"function\", \"function\": {...}}]\n     }\n\n  6. LLM response:\n     {\n       \"message\": {\n         \"role\": \"assistant\",\n         \"tool_calls\": [{\n           \"id\": \"call_123\",\n           \"type\": \"function\",\n           \"function\": {\n             \"name\": \"MockAlbum\",\n             \"arguments\": '{\"title\":\"hello\",\"artist\":\"world\",\"songs\":[...]}'\n           }\n         }]\n       }\n     }\n\n  7. Parse arguments \u2192 Pydantic validation\n     \u2192 MockAlbum(title=\"hello\", artist=\"world\", songs=[...])\n\n  8. Wrap in ToolOutput\n     \u2192 ToolOutput(\n         content=\"...\",\n         tool_name=\"MockAlbum\",\n         raw_output=MockAlbum(...)\n       )\n\n  9. Create AgentChatResponse\n     \u2192 AgentChatResponse(response=\"...\", sources=[ToolOutput(...)])\n\n  10. parse_tool_outputs(allow_parallel=False)\n      \u2192 Extract sources[0].raw_output\n      \u2192 MockAlbum(title=\"hello\", artist=\"world\", songs=[...])\n\nOutput:\n  MockAlbum(title=\"hello\", artist=\"world\", songs=[...])\n</code></pre>"},{"location":"reference/core/llms/orchestrators/tool_orchestrating_llm/tool_orchestrating_llm_dataflow/#execution-phase-parallel-outputs","title":"Execution Phase - Parallel Outputs","text":"<pre><code>Input:\n  tools_llm(topic=\"songs\")  # with allow_parallel_tool_calls=True\n\nTransformations:\n  1-6. [Same as single output]\n\n  7. LLM response with multiple tool_calls:\n     {\n       \"message\": {\n         \"tool_calls\": [\n           {\"function\": {\"arguments\": '{\"title\":\"hello\",\"artist\":\"world\",...}'}},\n           {\"function\": {\"arguments\": '{\"title\":\"hello2\",\"artist\":\"world2\",...}'}}\n         ]\n       }\n     }\n\n  8. Parse each \u2192 Multiple Pydantic instances\n     \u2192 [MockAlbum(...), MockAlbum(...)]\n\n  9. Wrap each in ToolOutput\n     \u2192 [ToolOutput(raw_output=MockAlbum(...)), ToolOutput(raw_output=MockAlbum(...))]\n\n  10. Create AgentChatResponse\n      \u2192 AgentChatResponse(sources=[ToolOutput(...), ToolOutput(...)])\n\n  11. parse_tool_outputs(allow_parallel=True)\n      \u2192 Extract all sources[i].raw_output\n      \u2192 [MockAlbum(...), MockAlbum(...)]\n\nOutput:\n  [MockAlbum(title=\"hello\", ...), MockAlbum(title=\"hello2\", ...)]\n</code></pre>"},{"location":"reference/core/llms/orchestrators/tool_orchestrating_llm/tool_orchestrating_llm_dataflow/#error-handling-points","title":"Error Handling Points","text":"<ol> <li>LLM validation:</li> <li>Ensures LLM instance is provided or available in Configs</li> <li> <p>Validates <code>is_function_calling_model == True</code></p> </li> <li> <p>Prompt validation:</p> </li> <li> <p>Ensures prompt is BasePromptTemplate or string</p> </li> <li> <p>Tool schema generation:</p> </li> <li>Extracts JSON schema from Pydantic model</li> <li> <p>Handles complex nested models</p> </li> <li> <p>Tool call parsing:</p> </li> <li>Validates LLM generated tool_calls</li> <li> <p>Handles missing or malformed tool calls</p> </li> <li> <p>Argument validation:</p> </li> <li>Validates tool arguments against Pydantic schema</li> <li> <p>Catches ValidationError and reports issues</p> </li> <li> <p>Output extraction:</p> </li> <li>Ensures raw_output exists in ToolOutput</li> <li>Handles single vs. list outputs based on allow_parallel_tool_calls</li> </ol>"},{"location":"reference/core/llms/orchestrators/tool_orchestrating_llm/tool_orchestrating_llm_dataflow/#streaming-data-flow","title":"Streaming Data Flow","text":"Hold \"Ctrl\" to enable pan &amp; zoom  <pre><code>flowchart TD\n    StreamStart([stream_call called]) --&gt; CreateTool[Create CallableTool]\n    CreateTool --&gt; FormatMsgs[Format messages]\n    FormatMsgs --&gt; StreamChat[stream_chat_with_tools]\n\n    StreamChat --&gt; ReceiveChunk{Receive chunk}\n    ReceiveChunk --&gt; ProcessChunk[StreamingObjectProcessor.process]\n\n    ProcessChunk --&gt; ParsePartial[Parse partial tool_calls]\n    ParsePartial --&gt; UpdateState[Update cur_objects state]\n    UpdateState --&gt; YieldPartial[Yield partial/complete model]\n\n    YieldPartial --&gt; MoreChunks{More chunks?}\n    MoreChunks --&gt;|Yes| ReceiveChunk\n    MoreChunks --&gt;|No| StreamEnd([End streaming])\n\n    style StreamStart fill:#e1f5ff\n    style YieldPartial fill:#c8e6c9\n    style StreamEnd fill:#c8e6c9</code></pre> <p>In streaming mode: 1. Each chunk may contain partial JSON for tool arguments 2. <code>StreamingObjectProcessor</code> maintains state across chunks 3. Yields progressively updated Pydantic instances 4. Handles flexible parsing for incomplete JSON</p>"},{"location":"reference/core/llms/orchestrators/tool_orchestrating_llm/tool_orchestrating_llm_sequence/","title":"Execution Flow and Method Calls","text":"<p>This diagram shows the complete workflow from initialization to execution of <code>ToolOrchestratingLLM</code>.</p>  Hold \"Ctrl\" to enable pan &amp; zoom  <pre><code>sequenceDiagram\n    participant User\n    participant ToolOrchestratingLLM\n    participant CallableTool\n    participant PromptTemplate\n    participant Ollama\n    participant LLMServer\n    participant AgentChatResponse\n\n    Note over User: Initialization Phase\n    User-&gt;&gt;ToolOrchestratingLLM: __init__(output_cls=MockAlbum, prompt, llm)\n    activate ToolOrchestratingLLM\n\n    ToolOrchestratingLLM-&gt;&gt;ToolOrchestratingLLM: validate_llm(llm)\n    Note over ToolOrchestratingLLM: Checks if llm is provided or uses Configs.llm&lt;br/&gt;Validates is_function_calling_model=True\n\n    ToolOrchestratingLLM-&gt;&gt;ToolOrchestratingLLM: validate_prompt(prompt)\n    Note over ToolOrchestratingLLM: Converts string to PromptTemplate&lt;br/&gt;if not already BasePromptTemplate\n\n    ToolOrchestratingLLM-&gt;&gt;PromptTemplate: Create/validate prompt template\n    activate PromptTemplate\n    PromptTemplate--&gt;&gt;ToolOrchestratingLLM: template instance\n    deactivate PromptTemplate\n\n    Note over ToolOrchestratingLLM: Store: _output_cls, _llm, _prompt,&lt;br/&gt;_verbose, _allow_parallel_tool_calls,&lt;br/&gt;_tool_choice\n\n    ToolOrchestratingLLM--&gt;&gt;User: tools_llm instance\n    deactivate ToolOrchestratingLLM\n\n    Note over User: Execution Phase\n    User-&gt;&gt;ToolOrchestratingLLM: __call__(topic=\"songs\")\n    activate ToolOrchestratingLLM\n\n    ToolOrchestratingLLM-&gt;&gt;CallableTool: from_model(MockAlbum)\n    activate CallableTool\n    Note over CallableTool: Convert Pydantic model to tool&lt;br/&gt;with JSON schema\n    CallableTool--&gt;&gt;ToolOrchestratingLLM: tool instance\n    deactivate CallableTool\n\n    ToolOrchestratingLLM-&gt;&gt;PromptTemplate: format_messages(topic=\"songs\")\n    activate PromptTemplate\n    Note over PromptTemplate: Apply template variables:&lt;br/&gt;\"This is a test album with songs\"\n    PromptTemplate--&gt;&gt;ToolOrchestratingLLM: List[Message]\n    deactivate PromptTemplate\n\n    ToolOrchestratingLLM-&gt;&gt;Ollama: _extend_messages(messages)\n    activate Ollama\n    Note over Ollama: Add system prompts if configured\n    Ollama--&gt;&gt;ToolOrchestratingLLM: extended messages\n    deactivate Ollama\n\n    ToolOrchestratingLLM-&gt;&gt;Ollama: predict_and_call(tools=[tool], messages, ...)\n    activate Ollama\n    Note over Ollama: Prepare function calling request&lt;br/&gt;with tool schemas\n\n    Ollama-&gt;&gt;LLMServer: HTTP POST /api/chat\n    activate LLMServer\n    Note over LLMServer: Process chat with tools&lt;br/&gt;Generate tool calls\n    LLMServer--&gt;&gt;Ollama: Response with tool_calls\n    deactivate LLMServer\n\n    Ollama-&gt;&gt;Ollama: Parse tool_calls from response\n    Note over Ollama: Extract tool arguments\n\n    Ollama-&gt;&gt;CallableTool: Execute tool with parsed arguments\n    activate CallableTool\n    CallableTool-&gt;&gt;CallableTool: Validate args against schema\n    CallableTool-&gt;&gt;CallableTool: Create MockAlbum instance\n    CallableTool--&gt;&gt;Ollama: ToolOutput with raw_output=MockAlbum\n    deactivate CallableTool\n\n    Ollama-&gt;&gt;AgentChatResponse: Create response with sources\n    activate AgentChatResponse\n    AgentChatResponse--&gt;&gt;Ollama: response instance\n    deactivate AgentChatResponse\n\n    Ollama--&gt;&gt;ToolOrchestratingLLM: AgentChatResponse\n    deactivate Ollama\n\n    ToolOrchestratingLLM-&gt;&gt;AgentChatResponse: parse_tool_outputs(allow_parallel=False)\n    activate AgentChatResponse\n    Note over AgentChatResponse: Extract raw_output from&lt;br/&gt;first ToolOutput in sources\n    AgentChatResponse--&gt;&gt;ToolOrchestratingLLM: MockAlbum instance\n    deactivate AgentChatResponse\n\n    ToolOrchestratingLLM--&gt;&gt;User: MockAlbum(title=\"hello\", artist=\"world\", ...)\n    deactivate ToolOrchestratingLLM</code></pre>"},{"location":"reference/core/llms/orchestrators/tool_orchestrating_llm/tool_orchestrating_llm_sequence/#key-points","title":"Key Points","text":"<ol> <li>Initialization validates all components before storing them - LLM must support function calling</li> <li>Tool creation converts Pydantic model to CallableTool with JSON schema</li> <li>Prompt formatting applies template variables to create messages</li> <li>predict_and_call orchestrates the function calling flow with the LLM</li> <li>Tool execution happens automatically after LLM generates tool calls</li> <li>Response parsing extracts structured Pydantic instances from ToolOutput</li> </ol>"},{"location":"reference/core/llms/orchestrators/tool_orchestrating_llm/tool_orchestrating_llm_sequence/#parallel-tool-calls","title":"Parallel Tool Calls","text":"<p>When <code>allow_parallel_tool_calls=True</code>:</p>  Hold \"Ctrl\" to enable pan &amp; zoom  <pre><code>sequenceDiagram\n    participant User\n    participant ToolOrchestratingLLM\n    participant Ollama\n    participant AgentChatResponse\n\n    User-&gt;&gt;ToolOrchestratingLLM: __call__(allow_parallel_tool_calls=True)\n    activate ToolOrchestratingLLM\n\n    ToolOrchestratingLLM-&gt;&gt;Ollama: predict_and_call(..., allow_parallel=True)\n    activate Ollama\n    Note over Ollama: LLM generates multiple tool calls\n\n    Ollama-&gt;&gt;Ollama: Execute tool 1 \u2192 ToolOutput 1\n    Ollama-&gt;&gt;Ollama: Execute tool 2 \u2192 ToolOutput 2\n\n    Ollama--&gt;&gt;ToolOrchestratingLLM: AgentChatResponse with multiple sources\n    deactivate Ollama\n\n    ToolOrchestratingLLM-&gt;&gt;AgentChatResponse: parse_tool_outputs(allow_parallel=True)\n    activate AgentChatResponse\n    Note over AgentChatResponse: Extract all raw_outputs&lt;br/&gt;from sources list\n    AgentChatResponse--&gt;&gt;ToolOrchestratingLLM: List[MockAlbum]\n    deactivate AgentChatResponse\n\n    ToolOrchestratingLLM--&gt;&gt;User: [MockAlbum(...), MockAlbum(...)]\n    deactivate ToolOrchestratingLLM</code></pre>"},{"location":"reference/core/llms/orchestrators/tool_orchestrating_llm/tool_orchestrating_llm_sequence/#async-execution-flow","title":"Async Execution Flow","text":"<p>The async flow (<code>acall</code>) follows the same pattern but uses: - <code>apredict_and_call</code> instead of <code>predict_and_call</code> - Async tool execution - All operations are awaited</p>"},{"location":"reference/core/llms/orchestrators/tool_orchestrating_llm/tool_orchestrating_llm_sequence/#streaming-execution-flow","title":"Streaming Execution Flow","text":"<p>For <code>stream_call</code>: 1. Uses <code>stream_chat_with_tools</code> instead of <code>predict_and_call</code> 2. Yields partial responses as <code>StreamingObjectProcessor</code> parses incremental tool calls 3. Maintains <code>cur_objects</code> state across chunks 4. Each yield contains progressively updated Pydantic instances</p>"},{"location":"reference/core/llms/orchestrators/tool_orchestrating_llm/tool_orchestrating_llm_state/","title":"State Transitions and Lifecycle","text":"<p>This diagram shows the state transitions and lifecycle of <code>ToolOrchestratingLLM</code>.</p>  Hold \"Ctrl\" to enable pan &amp; zoom  <pre><code>stateDiagram-v2\n    [*] --&gt; Uninitialized: User creates instance\n\n    Uninitialized --&gt; Validating: __init__ called\n\n    state Validating {\n        [*] --&gt; ValidatingLLM\n        ValidatingLLM --&gt; ValidatingPrompt: LLM valid\n        ValidatingPrompt --&gt; StoringState: Prompt valid\n        StoringState --&gt; [*]\n\n        ValidatingLLM --&gt; Error: No LLM or not function-calling\n        ValidatingPrompt --&gt; Error: Invalid prompt type\n    }\n\n    Validating --&gt; Ready: All validations pass\n    Validating --&gt; [*]: Validation error\n\n    Ready --&gt; ExecutingSync: __call__ invoked\n    Ready --&gt; ExecutingAsync: acall invoked\n    Ready --&gt; ExecutingStream: stream_call invoked\n    Ready --&gt; ExecutingAsyncStream: astream_call invoked\n    Ready --&gt; UpdatingPrompt: prompt.setter called\n\n    UpdatingPrompt --&gt; Ready: Prompt updated\n\n    state ExecutingSync {\n        [*] --&gt; CreatingTool\n        CreatingTool --&gt; FormattingPrompt\n        FormattingPrompt --&gt; ExtendingMessages\n        ExtendingMessages --&gt; CallingLLM\n        CallingLLM --&gt; ParsingToolCalls\n        ParsingToolCalls --&gt; ExecutingTools\n        ExecutingTools --&gt; CreatingResponse\n        CreatingResponse --&gt; ExtractingOutput\n        ExtractingOutput --&gt; [*]\n\n        CallingLLM --&gt; SyncError: Network/timeout error\n        ParsingToolCalls --&gt; SyncError: No tool_calls\n        ExecutingTools --&gt; SyncError: Validation error\n    }\n\n    state ExecutingAsync {\n        [*] --&gt; CreatingToolAsync\n        CreatingToolAsync --&gt; FormattingPromptAsync\n        FormattingPromptAsync --&gt; ExtendingMessagesAsync\n        ExtendingMessagesAsync --&gt; CallingLLMAsync\n        CallingLLMAsync --&gt; ParsingToolCallsAsync\n        ParsingToolCallsAsync --&gt; ExecutingToolsAsync\n        ExecutingToolsAsync --&gt; CreatingResponseAsync\n        CreatingResponseAsync --&gt; ExtractingOutputAsync\n        ExtractingOutputAsync --&gt; [*]\n\n        CallingLLMAsync --&gt; AsyncError: Network/timeout error\n        ParsingToolCallsAsync --&gt; AsyncError: No tool_calls\n        ExecutingToolsAsync --&gt; AsyncError: Validation error\n    }\n\n    state ExecutingStream {\n        [*] --&gt; CreatingToolStream\n        CreatingToolStream --&gt; FormattingPromptStream\n        FormattingPromptStream --&gt; InitiatingStream\n        InitiatingStream --&gt; StreamingChunks\n\n        state StreamingChunks {\n            [*] --&gt; ReceivingChunk\n            ReceivingChunk --&gt; ProcessingChunk\n            ProcessingChunk --&gt; ParsingPartialToolCalls\n            ParsingPartialToolCalls --&gt; UpdatingObjects\n            UpdatingObjects --&gt; YieldingPartial\n            YieldingPartial --&gt; MoreChunks: Has more chunks\n            MoreChunks --&gt; ReceivingChunk\n            YieldingPartial --&gt; [*]: Stream complete\n\n            ProcessingChunk --&gt; SkippingChunk: Parse error (logged)\n            SkippingChunk --&gt; MoreChunks\n        }\n\n        StreamingChunks --&gt; [*]\n    }\n\n    state ExecutingAsyncStream {\n        [*] --&gt; CreatingToolAsyncStream\n        CreatingToolAsyncStream --&gt; FormattingPromptAsyncStream\n        FormattingPromptAsyncStream --&gt; InitiatingAsyncStream\n        InitiatingAsyncStream --&gt; StreamingChunksAsync\n\n        state StreamingChunksAsync {\n            [*] --&gt; ReceivingChunkAsync\n            ReceivingChunkAsync --&gt; ProcessingChunkAsync\n            ProcessingChunkAsync --&gt; ParsingPartialAsync\n            ParsingPartialAsync --&gt; UpdatingObjectsAsync\n            UpdatingObjectsAsync --&gt; YieldingPartialAsync\n            YieldingPartialAsync --&gt; MoreChunksAsync: Has more chunks\n            MoreChunksAsync --&gt; ReceivingChunkAsync\n            YieldingPartialAsync --&gt; [*]: Stream complete\n\n            ProcessingChunkAsync --&gt; SkippingChunkAsync: Parse error (logged)\n            SkippingChunkAsync --&gt; MoreChunksAsync\n        }\n\n        StreamingChunksAsync --&gt; [*]\n    }\n\n    ExecutingSync --&gt; Ready: Returns output\n    ExecutingSync --&gt; [*]: Error raised\n\n    ExecutingAsync --&gt; Ready: Returns output\n    ExecutingAsync --&gt; [*]: Error raised\n\n    ExecutingStream --&gt; Ready: Stream complete\n    ExecutingStream --&gt; [*]: Error raised\n\n    ExecutingAsyncStream --&gt; Ready: Stream complete\n    ExecutingAsyncStream --&gt; [*]: Error raised\n\n    Ready --&gt; [*]: Instance destroyed</code></pre>"},{"location":"reference/core/llms/orchestrators/tool_orchestrating_llm/tool_orchestrating_llm_state/#state-descriptions","title":"State Descriptions","text":""},{"location":"reference/core/llms/orchestrators/tool_orchestrating_llm/tool_orchestrating_llm_state/#uninitialized","title":"Uninitialized","text":"<ul> <li>Entry: Instance creation started</li> <li>Actions: None</li> <li>Exit: When <code>__init__</code> is called</li> <li>Data: Constructor arguments available</li> </ul>"},{"location":"reference/core/llms/orchestrators/tool_orchestrating_llm/tool_orchestrating_llm_state/#validating","title":"Validating","text":"<ul> <li>Entry: <code>__init__</code> method executing</li> <li>Substates:</li> <li><code>ValidatingLLM</code>: Check LLM exists and supports function calling</li> <li><code>ValidatingPrompt</code>: Convert/validate prompt template</li> <li><code>StoringState</code>: Store all validated components</li> <li>Exit: All validations pass or error raised</li> <li>Data: Validated components stored in instance attributes</li> </ul>"},{"location":"reference/core/llms/orchestrators/tool_orchestrating_llm/tool_orchestrating_llm_state/#ready","title":"Ready","text":"<ul> <li>Entry: Instance fully initialized</li> <li>Actions: Waiting for execution call</li> <li>Transitions:</li> <li><code>__call__</code> \u2192 ExecutingSync</li> <li><code>acall</code> \u2192 ExecutingAsync</li> <li><code>stream_call</code> \u2192 ExecutingStream</li> <li><code>astream_call</code> \u2192 ExecutingAsyncStream</li> <li><code>prompt.setter</code> \u2192 UpdatingPrompt</li> <li>Data: All instance state available and immutable (except prompt)</li> </ul>"},{"location":"reference/core/llms/orchestrators/tool_orchestrating_llm/tool_orchestrating_llm_state/#executingsync-synchronous-execution","title":"ExecutingSync (Synchronous Execution)","text":"<ul> <li>Entry: <code>__call__</code> invoked</li> <li>Substates:</li> <li><code>CreatingTool</code>: CallableTool.from_model(output_cls)</li> <li><code>FormattingPrompt</code>: format_messages(**kwargs)</li> <li><code>ExtendingMessages</code>: _extend_messages(messages)</li> <li><code>CallingLLM</code>: predict_and_call(tools, messages, ...)</li> <li><code>ParsingToolCalls</code>: Extract tool_calls from response</li> <li><code>ExecutingTools</code>: Validate args and create Pydantic instances</li> <li><code>CreatingResponse</code>: Build AgentChatResponse</li> <li><code>ExtractingOutput</code>: parse_tool_outputs(allow_parallel)</li> <li>Exit: Returns Pydantic instance(s) or raises error</li> <li>Data: Transient execution state (tool, messages, response)</li> </ul>"},{"location":"reference/core/llms/orchestrators/tool_orchestrating_llm/tool_orchestrating_llm_state/#executingasync-asynchronous-execution","title":"ExecutingAsync (Asynchronous Execution)","text":"<ul> <li>Entry: <code>acall</code> invoked</li> <li>Substates: Same as ExecutingSync but async</li> <li>Exit: Returns Pydantic instance(s) or raises error</li> <li>Data: Same as ExecutingSync</li> </ul>"},{"location":"reference/core/llms/orchestrators/tool_orchestrating_llm/tool_orchestrating_llm_state/#executingstream-synchronous-streaming","title":"ExecutingStream (Synchronous Streaming)","text":"<ul> <li>Entry: <code>stream_call</code> invoked</li> <li>Substates:</li> <li><code>CreatingToolStream</code>: Create CallableTool</li> <li><code>FormattingPromptStream</code>: Format messages</li> <li><code>InitiatingStream</code>: Start stream_chat_with_tools</li> <li><code>StreamingChunks</code>: Process chunks loop<ul> <li><code>ReceivingChunk</code>: Get next chunk from generator</li> <li><code>ProcessingChunk</code>: Initialize StreamingObjectProcessor</li> <li><code>ParsingPartialToolCalls</code>: Parse partial JSON</li> <li><code>UpdatingObjects</code>: Update cur_objects state</li> <li><code>YieldingPartial</code>: Yield partial/complete model</li> <li><code>MoreChunks</code>: Check if more chunks available</li> <li><code>SkippingChunk</code>: Handle parse errors gracefully</li> </ul> </li> <li>Exit: Generator exhausted or error raised</li> <li>Data: Streaming state (cur_objects, partial_resp)</li> </ul>"},{"location":"reference/core/llms/orchestrators/tool_orchestrating_llm/tool_orchestrating_llm_state/#executingasyncstream-asynchronous-streaming","title":"ExecutingAsyncStream (Asynchronous Streaming)","text":"<ul> <li>Entry: <code>astream_call</code> invoked</li> <li>Substates: Same as ExecutingStream but async</li> <li>Exit: Async generator exhausted or error raised</li> <li>Data: Same as ExecutingStream</li> </ul>"},{"location":"reference/core/llms/orchestrators/tool_orchestrating_llm/tool_orchestrating_llm_state/#updatingprompt","title":"UpdatingPrompt","text":"<ul> <li>Entry: <code>prompt.setter</code> called</li> <li>Actions: Update <code>_prompt</code> attribute</li> <li>Exit: Immediately returns to Ready</li> <li>Data: New prompt template stored</li> </ul>"},{"location":"reference/core/llms/orchestrators/tool_orchestrating_llm/tool_orchestrating_llm_state/#state-data-by-phase","title":"State Data by Phase","text":""},{"location":"reference/core/llms/orchestrators/tool_orchestrating_llm/tool_orchestrating_llm_state/#initialization-phase","title":"Initialization Phase","text":"<pre><code># Uninitialized \u2192 Validating \u2192 Ready\n{\n    \"_output_cls\": Type[MockAlbum],\n    \"_llm\": Ollama,\n    \"_prompt\": PromptTemplate,\n    \"_verbose\": False,\n    \"_allow_parallel_tool_calls\": False,\n    \"_tool_choice\": None\n}\n</code></pre>"},{"location":"reference/core/llms/orchestrators/tool_orchestrating_llm/tool_orchestrating_llm_state/#execution-phase-sync","title":"Execution Phase (Sync)","text":"<pre><code># Ready \u2192 ExecutingSync \u2192 Ready\n{\n    # Instance state (immutable during execution)\n    \"_output_cls\": Type[MockAlbum],\n    \"_llm\": Ollama,\n    \"_prompt\": PromptTemplate,\n\n    # Transient state (created during execution)\n    \"kwargs\": {\"topic\": \"songs\"},\n    \"tool\": CallableTool,\n    \"messages\": List[Message],\n    \"agent_response\": AgentChatResponse,\n    \"output\": MockAlbum\n}\n</code></pre>"},{"location":"reference/core/llms/orchestrators/tool_orchestrating_llm/tool_orchestrating_llm_state/#streaming-phase","title":"Streaming Phase","text":"<pre><code># Ready \u2192 ExecutingStream \u2192 Ready\n{\n    # Instance state\n    \"_output_cls\": Type[MockAlbum],\n    \"_llm\": Ollama,\n    \"_prompt\": PromptTemplate,\n\n    # Streaming state (maintained across chunks)\n    \"tool\": CallableTool,\n    \"messages\": List[Message],\n    \"chat_response_gen\": Generator,\n    \"cur_objects\": List[MockAlbum],  # Progressive state\n    \"partial_resp\": StreamingChatResponse\n}\n</code></pre>"},{"location":"reference/core/llms/orchestrators/tool_orchestrating_llm/tool_orchestrating_llm_state/#transition-triggers","title":"Transition Triggers","text":""},{"location":"reference/core/llms/orchestrators/tool_orchestrating_llm/tool_orchestrating_llm_state/#user-triggered-transitions","title":"User-Triggered Transitions","text":"<ol> <li><code>__init__</code> \u2192 Starts validation</li> <li><code>__call__</code> \u2192 Starts sync execution</li> <li><code>acall</code> \u2192 Starts async execution</li> <li><code>stream_call</code> \u2192 Starts streaming</li> <li><code>astream_call</code> \u2192 Starts async streaming</li> <li><code>prompt.setter</code> \u2192 Updates prompt</li> </ol>"},{"location":"reference/core/llms/orchestrators/tool_orchestrating_llm/tool_orchestrating_llm_state/#system-triggered-transitions","title":"System-Triggered Transitions","text":"<ol> <li>Validation success \u2192 Ready state</li> <li>Validation failure \u2192 Error state</li> <li>Execution complete \u2192 Return to Ready</li> <li>Execution error \u2192 Error state</li> <li>Stream chunk received \u2192 Process and yield</li> <li>Stream exhausted \u2192 Return to Ready</li> </ol>"},{"location":"reference/core/llms/orchestrators/tool_orchestrating_llm/tool_orchestrating_llm_state/#error-states","title":"Error States","text":""},{"location":"reference/core/llms/orchestrators/tool_orchestrating_llm/tool_orchestrating_llm_state/#validation-errors-initialization","title":"Validation Errors (Initialization)","text":"<ul> <li><code>AssertionError</code>: No LLM provided and Configs.llm not set</li> <li><code>ValueError</code>: LLM does not support function calling</li> <li><code>ValueError</code>: Invalid prompt type</li> </ul>"},{"location":"reference/core/llms/orchestrators/tool_orchestrating_llm/tool_orchestrating_llm_state/#execution-errors-runtime","title":"Execution Errors (Runtime)","text":"<ul> <li><code>ValueError</code>: LLM errors (network, timeout, invalid args)</li> <li><code>ValidationError</code>: Tool argument validation fails</li> <li><code>AttributeError</code>: Missing raw_output in ToolOutput</li> </ul>"},{"location":"reference/core/llms/orchestrators/tool_orchestrating_llm/tool_orchestrating_llm_state/#streaming-errors-runtime","title":"Streaming Errors (Runtime)","text":"<ul> <li>Parse errors are logged as warnings and skipped</li> <li>Critical errors raised and propagated</li> </ul>"},{"location":"reference/core/llms/orchestrators/tool_orchestrating_llm/tool_orchestrating_llm_state/#state-persistence","title":"State Persistence","text":""},{"location":"reference/core/llms/orchestrators/tool_orchestrating_llm/tool_orchestrating_llm_state/#persistent-across-calls","title":"Persistent Across Calls","text":"<ul> <li><code>_output_cls</code>: Never changes</li> <li><code>_llm</code>: Never changes</li> <li><code>_prompt</code>: Can be updated via setter</li> <li><code>_verbose</code>: Never changes</li> <li><code>_allow_parallel_tool_calls</code>: Never changes</li> <li><code>_tool_choice</code>: Never changes</li> </ul>"},{"location":"reference/core/llms/orchestrators/tool_orchestrating_llm/tool_orchestrating_llm_state/#transient-per-call","title":"Transient Per Call","text":"<ul> <li>Tool instance</li> <li>Formatted messages</li> <li>LLM response</li> <li>Parsed output</li> </ul>"},{"location":"reference/core/llms/orchestrators/tool_orchestrating_llm/tool_orchestrating_llm_state/#transient-per-stream","title":"Transient Per Stream","text":"<ul> <li><code>cur_objects</code>: Maintains progressive parsing state</li> <li>Each new stream_call creates fresh state</li> </ul>"},{"location":"reference/core/llms/orchestrators/tool_orchestrating_llm/tool_orchestrating_llm_state/#parallel-execution-states","title":"Parallel Execution States","text":"<p>When <code>allow_parallel_tool_calls=True</code>:</p>  Hold \"Ctrl\" to enable pan &amp; zoom  <pre><code>stateDiagram-v2\n    [*] --&gt; ExecutingTools: Multiple tool_calls\n\n    state ExecutingTools {\n        [*] --&gt; ExecuteTool1\n        [*] --&gt; ExecuteTool2\n\n        ExecuteTool1 --&gt; ValidateTool1\n        ExecuteTool2 --&gt; ValidateTool2\n\n        ValidateTool1 --&gt; CreateModel1\n        ValidateTool2 --&gt; CreateModel2\n\n        CreateModel1 --&gt; WrapOutput1\n        CreateModel2 --&gt; WrapOutput2\n\n        WrapOutput1 --&gt; CollectOutputs\n        WrapOutput2 --&gt; CollectOutputs\n\n        CollectOutputs --&gt; [*]: All complete\n    }\n\n    ExecutingTools --&gt; [*]: List[MockAlbum]</code></pre>"},{"location":"reference/core/llms/orchestrators/tool_orchestrating_llm/tool_orchestrating_llm_state/#lifecycle-summary","title":"Lifecycle Summary","text":"<ol> <li>Birth: Instance created \u2192 Validation \u2192 Ready</li> <li>Active: Ready \u2192 Execute \u2192 Ready (repeatable)</li> <li>Mutable: Prompt can be updated at any time when Ready</li> <li>Death: Instance destroyed when no longer referenced</li> </ol> <p>Each execution is independent and returns the instance to Ready state, allowing multiple calls with different arguments.</p>"},{"location":"reference/core/tools/callable_tools/","title":"Callable Tools","text":"Hold \"Ctrl\" to enable pan &amp; zoom"},{"location":"reference/core/tools/tools/","title":"serapeum.core.tools \u2014 Developer Documentation","text":"<p>This documentation provides a complete, visual, and navigable overview of the <code>serapeum.core.tools</code> submodule: purpose, structure, classes and functions, relationships, diagrams, and usage examples.</p>"},{"location":"reference/core/tools/tools/#high-level-overview","title":"High-level overview","text":"<p>The <code>tools</code> submodule turns ordinary Python callables into LLM-callable \"tools\" and provides a unified runtime to execute them synchronously or asynchronously. It standardizes:</p> <ul> <li>Tool metadata (name, description, input schema) for function-calling providers.</li> <li>Tool outputs (text and multimodal chunks) via a common <code>ToolOutput</code> container.</li> <li>A consistent calling surface for both sync and async functions/classes.</li> <li>A robust executor with optional single-argument auto-unpacking and standardized error handling.</li> </ul> <p>Main capabilities: - Derive Pydantic input schemas from Python function signatures and docstrings. - Wrap a function into a <code>CallableTool</code> that returns <code>ToolOutput</code> and carries <code>ToolMetadata</code>. - Execute tools with <code>ToolExecutor</code> (sync/async) or adapt a synchronous tool to async flows.</p>"},{"location":"reference/core/tools/tools/#package-hierarchy","title":"Package hierarchy","text":"<p>See the package and file layout.</p>  Hold \"Ctrl\" to enable pan &amp; zoom  <pre><code>%% Mermaid Package Hierarchy for serapeum.core.tools\n%% Save as docs/tools/diagrams/package-hierarchy.mmd\n\nflowchart TD\n    A[serapeum] --&gt; B[core]\n    B --&gt; C[tools]\n    C --&gt; C1[__init__.py]\n    C --&gt; C2[callable_tool.py]\n    C --&gt; C3[models.py]\n    C --&gt; C4[utils.py]\n\n    %% Context (other top-level packages, optional)\n    A -.-&gt; LLMS[llms]\n    A -.-&gt; CORE_BASE[core/base]</code></pre>"},{"location":"reference/core/tools/tools/#module-dependency-diagram","title":"Module dependency diagram","text":"<p>This shows how the modules in <code>serapeum.core.tools</code> depend on one another and on external packages inside the project.</p> <ul> <li>Draw.io:  diagrams/module-deps.drawio</li> </ul>  Hold \"Ctrl\" to enable pan &amp; zoom  <pre><code>%% Mermaid Module Dependency Diagram for serapeum.core.tools\n%% Save as docs/tools/diagrams/module-deps.mmd\n\ngraph LR\n    subgraph \"serapeum.core.tools\"\n        tools_init[\"tools.__init__\"]\n        callable_tool[\"callable_tool.py\"]\n        models[\"models.py\"]\n        utils[\"utils.py\"]\n    end\n\n    tools_init --&gt; callable_tool\n    tools_init --&gt; models\n    tools_init --&gt; utils\n\n    callable_tool --&gt; models\n    callable_tool --&gt; utils\n    callable_tool --&gt; async_utils[\"serapeum.core.utils.async_utils\"]\n    callable_tool --&gt; core_llms_models[\"serapeum.core.base.llms.types\"]\n\n    models --&gt; core_llms_models\n    models --&gt; pydantic[\"pydantic\"]\n\n    utils --&gt; models\n    utils --&gt; pydantic\n    utils --&gt; inspect[\"inspect\"]\n    utils --&gt; datetime[\"datetime\"]</code></pre>"},{"location":"reference/core/tools/tools/#module-by-module-breakdown","title":"Module-by-module breakdown","text":"<ul> <li>init.py</li> <li> <p>Re-exports key public types for convenience: <code>CallableTool</code>, <code>ToolOutput</code>, <code>ToolCallArguments</code>.</p> </li> <li> <p>models.py</p> </li> <li>MinimalToolSchema: Default args schema when no custom schema is provided (<code>{\"input\": str}</code>).</li> <li>ToolMetadata: Name/description/schema of a tool; exports OpenAI-style function tool specs.</li> <li>ToolOutput: Standard output holder containing text/image/audio chunks, raw input/output, errors.</li> <li>BaseTool / AsyncBaseTool: Base interfaces for tools (sync/async contracting).</li> <li>BaseToolAsyncAdapter / adapt_to_async_tool: Adapts sync tools to the async interface.</li> <li> <p>ToolCallArguments: Selected tool name/id and kwargs to pass at runtime.</p> </li> <li> <p>callable_tool.py</p> </li> <li>SyncAsyncConverter: Bridges sync&lt;-&gt;async callables in both directions.</li> <li> <p>CallableTool: Wraps a Python callable (sync or async), infers metadata and schema when needed, and returns <code>ToolOutput</code>.</p> </li> <li> <p>utils.py</p> </li> <li>Docstring: Parses Google/Sphinx/Javadoc-style parameter descriptions from function docstrings.</li> <li>FunctionArgument: Converts <code>inspect.Parameter</code> to <code>(type, FieldInfo)</code> with sensible defaults.</li> <li>FunctionConverter: Builds a Pydantic model from a function signature (+Annotated, +datetime formats).</li> <li>ExecutionConfig: Flags for executor behavior.</li> <li>ToolExecutor: Safe execution harness (sync/async) with optional single-arg auto-unpack and error standardization.</li> </ul>"},{"location":"reference/core/tools/tools/#uml-class-diagram","title":"UML class diagram","text":"<p>Quick preview (key relationships only; see the full file for details):</p>  Hold \"Ctrl\" to enable pan &amp; zoom  <pre><code>classDiagram\n  BaseTool &lt;|-- AsyncBaseTool\n  AsyncBaseTool &lt;|-- BaseToolAsyncAdapter\n  AsyncBaseTool &lt;|-- CallableTool\n  CallableTool ..&gt; ToolMetadata\n  CallableTool ..&gt; ToolOutput\n  ToolExecutor --&gt; ExecutionConfig\n  ToolExecutor ..&gt; ToolOutput\n  ToolCallArguments ..&gt; ToolExecutor\n  ToolMetadata ..&gt; MinimalToolSchema</code></pre> <ul> <li>Detailed class diagram</li> </ul>  Hold \"Ctrl\" to enable pan &amp; zoom  <pre><code>%% Mermaid Class Diagram for serapeum.core.tools\n%% Save as docs/tools/diagrams/uml-classes.mmd\n\nclassDiagram\n    %% Core models\n    class MinimalToolSchema {\n        +input: str\n    }\n\n    class ToolMetadata {\n        +name: str?\n        +description: str\n        +tool_schema: BaseModel?\n        +return_direct: bool\n        +get_schema() dict\n        +tool_schema_str: str\n        +get_name() str\n        +to_openai_tool(skip_length_check=False) Dict\n    }\n\n    class ToolOutput {\n        +chunks: List~ChunkType~\n        +tool_name: str\n        +raw_input: Dict\n        +raw_output: Any\n        +is_error: bool\n        +content: str\n        +__str__() str\n    }\n\n    class ToolCallArguments {\n        +tool_id: str\n        +tool_name: str\n        +tool_kwargs: Dict~str, Any~\n    }\n\n    %% Tool interfaces and adapters\n    class BaseTool {\n        &lt;&lt;abstract&gt;&gt;\n        +metadata: ToolMetadata\n        +__call__(input_values) ToolOutput\n    }\n\n    class AsyncBaseTool {\n        &lt;&lt;abstract&gt;&gt;\n        +call(input_values) ToolOutput\n        +acall(input_values) ToolOutput\n    }\n\n    class BaseToolAsyncAdapter {\n        +base_tool: BaseTool\n        +call(input_values) ToolOutput\n        +acall(input_values) ToolOutput\n    }\n\n    %% Callable adapter\n    class CallableTool {\n        +metadata: ToolMetadata\n        +default_arguments: Dict\n        +sync_func(*args, **kwargs) Any\n        +async_func(*args, **kwargs) Awaitable\n        +call(*args, **kwargs) ToolOutput\n        +acall(*args, **kwargs) ToolOutput\n        +from_function(...)\n    }\n\n    class SyncAsyncConverter {\n        +is_async(func) bool\n        +to_async(fn) AsyncCallable\n        +async_to_sync(func_async) Callable\n        -sync_func\n        -async_func\n    }\n\n    %% Schema utilities\n    class Docstring {\n        +signature\n        +extract_param_docs() (dict, set)\n        +get_short_summary_line() str\n    }\n\n    class FunctionArgument {\n        +to_field() (Type, FieldInfo)\n    }\n\n    class FunctionConverter {\n        +to_schema() Type~BaseModel~\n    }\n\n    %% Execution utilities\n    class ExecutionConfig {\n        +verbose: bool\n        +single_arg_auto_unpack: bool\n        +raise_on_error: bool\n    }\n\n    class ToolExecutor {\n        +execute(tool, arguments) ToolOutput\n        +execute_async(tool, arguments) ToolOutput\n        +execute_with_selection(sel, tools) ToolOutput\n        +execute_async_with_selection(sel, tools) ToolOutput\n    }\n\n    %% External content chunk types\n    class TextChunk\n    class Image\n    class Audio\n\n    %% Inheritance\n    BaseTool &lt;|-- AsyncBaseTool\n    AsyncBaseTool &lt;|-- BaseToolAsyncAdapter\n    AsyncBaseTool &lt;|-- CallableTool\n\n    %% Associations/uses\n    MinimalToolSchema &lt;.. ToolMetadata : default\n    ToolMetadata ..&gt; MinimalToolSchema : fallback\n    ToolOutput ..&gt; TextChunk : contains\n    CallableTool ..&gt; ToolMetadata : uses\n    CallableTool ..&gt; ToolOutput : produces\n    CallableTool ..&gt; Docstring : parses\n    CallableTool ..&gt; FunctionConverter : builds schema\n    CallableTool ..&gt; SyncAsyncConverter : wraps\n    CallableTool ..&gt; TextChunk\n    CallableTool ..&gt; Image\n    CallableTool ..&gt; Audio\n    FunctionConverter ..&gt; FunctionArgument : converts\n    BaseToolAsyncAdapter --&gt; BaseTool : wraps\n    ToolExecutor --&gt; ExecutionConfig : configured by\n    ToolExecutor ..&gt; ToolOutput : returns\n    ToolExecutor ..&gt; AsyncBaseTool : executes\n    ToolCallArguments ..&gt; ToolExecutor : inputs to</code></pre>"},{"location":"reference/core/tools/tools/#call-graph-key-flows","title":"Call graph (key flows)","text":"<ul> <li>Draw.io:  diagrams/call-graph.drawio</li> </ul>  Hold \"Ctrl\" to enable pan &amp; zoom  <pre><code>%% Mermaid Call Graphs for serapeum.core.tools\n%% Save as docs/tools/diagrams/call-graph.mmd\n\nflowchart TD\n    %% CallableTool flow\n    subgraph CallableTool\n        CT_call[\"__call__(*args, **kwargs)\"] --&gt; CT_merge[\"merge default_arguments\"]\n        CT_merge --&gt; CT_call2[\"call(*args, **kwargs)\"]\n        CT_call2 --&gt; CT_sync[\"_sync_func(*args, **kwargs)\"]\n        CT_sync --&gt; CT_parse[\"_parse_tool_output(raw)\"]\n        CT_parse --&gt; CT_out[\"ToolOutput(... )&lt;br/&gt;(chunks, tool_name, raw_input, raw_output)\"]\n\n        CT_acall[\"acall(*args, **kwargs)\"] --&gt; CT_async[\"_async_func(*args, **kwargs)\"]\n        CT_async --&gt; CT_parse\n    end\n\n    %% ToolExecutor flow\n    subgraph ToolExecutor\n        EX_exec[\"execute(tool, arguments)\"] --&gt; EX_logStart[\"_log_execution_start (optional)\"]\n        EX_logStart --&gt; EX_invoke{\"_should_unpack_single_arg?\"}\n        EX_invoke -- yes --&gt; EX_try1[\"_try_single_arg_then_kwargs\"]\n        EX_invoke -- no  --&gt; EX_direct[\"tool(**arguments)\"]\n        EX_try1 --&gt; EX_result[\"ToolOutput\"]\n        EX_direct --&gt; EX_result\n        EX_exec --&gt;|exception| EX_err[\"_create_error_output\"]\n        EX_result --&gt; EX_logRes[\"_log_execution_result (optional)\"]\n\n        EX_async[\"execute_async(tool, arguments)\"] --&gt; EX_logStart2[\"_log_execution_start (optional)\"]\n        EX_logStart2 --&gt; EX_adapt[\"adapt_to_async_tool(tool)\"]\n        EX_adapt --&gt; EX_invokeA{\"_should_unpack_single_arg?\"}\n        EX_invokeA -- yes --&gt; EX_try1A[\"_try_single_arg_then_kwargs_async\"]\n        EX_invokeA -- no  --&gt; EX_directA[\"async_tool.acall(**arguments)\"]\n        EX_try1A --&gt; EX_resultA[\"ToolOutput\"]\n        EX_directA --&gt; EX_resultA\n        EX_async --&gt;|exception| EX_errA[\"_create_error_output\"]\n        EX_resultA --&gt; EX_logRes2[\"_log_execution_result (optional)\"]\n    end</code></pre>"},{"location":"reference/core/tools/tools/#summary-of-main-classes-and-functions","title":"Summary of main classes and functions","text":"Name Kind Purpose Key methods/properties MinimalToolSchema Pydantic model Default function args schema <code>{input: str}</code> when no custom schema is provided <code>input: str</code> ToolMetadata dataclass Describes tool (name, description, schema, return_direct) and exports to provider formats <code>get_schema()</code>, <code>tool_schema_str</code>, <code>get_name()</code>, <code>to_openai_tool()</code> ToolOutput Pydantic model Standard tool response: chunks, <code>content</code> view, raw input/output, error flag <code>content</code> property, <code>__str__</code> BaseTool abstract class Sync tool interface <code>metadata</code>, <code>__call__</code> AsyncBaseTool abstract class Async-capable tool interface <code>call()</code>, <code>acall()</code> BaseToolAsyncAdapter class Wraps <code>BaseTool</code> to provide async interface <code>metadata</code>, <code>call()</code>, <code>acall()</code> adapt_to_async_tool function Returns <code>AsyncBaseTool</code> (adapts if needed) \u2014 ToolCallArguments Pydantic model Tool selection and kwargs with forgiving validation validator: ignore non-dict <code>tool_kwargs</code> SyncAsyncConverter helper class Converts sync-&gt;async and async-&gt;sync wrappers <code>to_async()</code>, <code>async_to_sync()</code> CallableTool class Turns a Python callable into a Tool with metadata/schema and standardized output <code>from_function()</code>, <code>metadata</code>, <code>sync_func</code>, <code>async_func</code>, <code>call()</code>, <code>acall()</code> Docstring helper class Extracts param docs and brief summary from docstrings <code>extract_param_docs()</code>, <code>get_short_summary_line()</code> FunctionArgument helper class Converts <code>inspect.Parameter</code> to <code>(type, FieldInfo)</code>, supports Annotated <code>to_field()</code> FunctionConverter helper class Creates Pydantic model from function signature (+extra fields) <code>to_schema()</code> ExecutionConfig dataclass Executor behavior flags <code>verbose</code>, <code>single_arg_auto_unpack</code>, <code>raise_on_error</code> ToolExecutor class Orchestrates tool execution with error handling <code>execute()</code>, <code>execute_async()</code>, <code>execute_with_selection()</code>, <code>execute_async_with_selection()</code>"},{"location":"reference/core/tools/tools/#usage-examples","title":"Usage examples","text":"<p>Wrap a synchronous function and call it</p> <pre><code>from serapeum.core.tools import CallableTool, ToolMetadata\n\ndef greet(name: str) -&gt; str:\n    \"\"\"Greet a user by name.\"\"\"\n    return f\"Hello, {name}!\"\n\ntool = CallableTool(func=greet, metadata=ToolMetadata(name=\"greet\", description=\"Greets by name\"))\nresult = tool(\"Ada\")  # or tool.call(\"Ada\")\nprint(result.content)  # \"Hello, Ada!\"\n</code></pre> <p>Wrap an async function and await it</p> <pre><code>import asyncio\nfrom serapeum.core.tools import CallableTool, ToolMetadata\n\nasync def add(a: int, b: int) -&gt; int:\n    return a + b\n\ntool = CallableTool(func=add, metadata=ToolMetadata(name=\"add\", description=\"Add two ints\"))\nresult = asyncio.run(tool.acall(2, 3))\nprint(result.content)  # \"5\"\n</code></pre> <p>Infer schema and metadata with from_function</p> <pre><code>from serapeum.core.tools import CallableTool\n\n# from_function inspects signature and docstring to infer schema and description\n# (first docstring line + signature)\n\ndef power(base: int, exp: int = 2) -&gt; int:\n    \"\"\"Exponentiation.\"\"\"\n    return base ** exp\n\ntool = CallableTool.from_function(power)\nprint(tool.metadata.get_name())  # \"power\"\nprint(tool(\"3\").content)       # \"9\" (exp defaults to 2 unless overridden)\n</code></pre> <p>Export as an OpenAI function tool</p> <pre><code>from pydantic import BaseModel\nfrom serapeum.core.tools import ToolMetadata\n\nclass SearchArgs(BaseModel):\n    query: str\n    limit: int | None = None\n\nmeta = ToolMetadata(name=\"search\", description=\"Search items\", tool_schema=SearchArgs)\nprint(meta.to_openai_tool())\n</code></pre> <p>Execute with ToolExecutor (selection-based)</p> <pre><code>from serapeum.core.tools import ToolCallArguments, ToolExecutor, CallableTool, ToolMetadata\n\n# Prepare two tools\nsay = CallableTool.from_function(lambda text: text, name=\"say\")\ninc = CallableTool(func=lambda x: x + 1, metadata=ToolMetadata(name=\"inc\", description=\"Increment\"))\n\n# Simulate a selection coming from an LLM\nselection = ToolCallArguments(tool_id=\"1\", tool_name=\"say\", tool_kwargs={\"text\": \"hi\"})\n\nexecutor = ToolExecutor()\nout = executor.execute_with_selection(selection, [say, inc])\nprint(out.content)  # \"hi\"\n</code></pre> <p>Single-argument auto-unpack</p> <pre><code>from serapeum.core.tools import ToolExecutor, ExecutionConfig, ToolMetadata, CallableTool\n\ndef echo_list(lst: list[int]):\n    return \",\".join(map(str, lst))\n\ntool = CallableTool(func=echo_list, metadata=ToolMetadata(name=\"echo_list\", description=\"Echo list\"))\nexecutor = ToolExecutor(ExecutionConfig(single_arg_auto_unpack=True))\nprint(executor.execute(tool, {\"lst\": [1, 2, 3]}).content)  # \"1,2,3\"\n</code></pre>"},{"location":"reference/providers/add_new_providers/","title":"Adding a New LLM Provider to Serapeum","text":"<p>This guide explains how to add a new LLM provider integration to the Serapeum framework.</p>"},{"location":"reference/providers/add_new_providers/#overview","title":"Overview","text":"<p>Serapeum uses a provider-based architecture where each LLM provider is implemented as a separate package under <code>libs/providers/</code>. Each provider package is self-contained and includes all features that provider offers (LLM, embeddings, etc.).</p>"},{"location":"reference/providers/add_new_providers/#quick-start","title":"Quick Start","text":"<p>To add a new provider (e.g., OpenAI):</p> <ol> <li>Create package structure in <code>libs/providers/openai/</code></li> <li>Implement LLM class inheriting from <code>ChatToCompletionMixin</code> and <code>FunctionCallingLLM</code></li> <li>Add tests</li> <li>Configure workspace</li> <li>Document</li> </ol>"},{"location":"reference/providers/add_new_providers/#step-by-step-guide","title":"Step-by-Step Guide","text":""},{"location":"reference/providers/add_new_providers/#1-create-package-structure","title":"1. Create Package Structure","text":"<p>Create a new provider package following this structure:</p> <pre><code>libs/providers/{provider-name}/\n\u251c\u2500\u2500 src/\n\u2502   \u2514\u2500\u2500 serapeum/\n\u2502       \u2514\u2500\u2500 providers/\n\u2502           \u2514\u2500\u2500 {provider_name}/\n\u2502               \u251c\u2500\u2500 __init__.py\n\u2502               \u251c\u2500\u2500 llm.py              # Chat/completion LLM implementation\n\u2502               \u251c\u2500\u2500 embeddings.py       # Embeddings (if available)\n\u2502               \u2514\u2500\u2500 shared/             # Shared utilities\n\u2502                   \u251c\u2500\u2500 __init__.py\n\u2502                   \u251c\u2500\u2500 client.py       # HTTP client, config\n\u2502                   \u2514\u2500\u2500 errors.py       # Provider-specific errors\n\u251c\u2500\u2500 tests/\n\u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u251c\u2500\u2500 conftest.py\n\u2502   \u251c\u2500\u2500 test_llm.py\n\u2502   \u251c\u2500\u2500 test_embeddings.py\n\u2502   \u2514\u2500\u2500 fixtures/\n\u251c\u2500\u2500 pyproject.toml\n\u2514\u2500\u2500 README.md\n</code></pre>"},{"location":"reference/providers/add_new_providers/#2-create-pyprojecttoml","title":"2. Create <code>pyproject.toml</code>","text":"<p>File: <code>libs/providers/{provider}/pyproject.toml</code></p> <pre><code>[project]\nname = \"serapeum-{provider}\"\nversion = \"0.1.0\"\ndescription = \"{Provider} integration for Serapeum LLM framework\"\nreadme = \"README.md\"\nrequires-python = \"&gt;=3.11\"\ndependencies = [\n    \"serapeum-core\",\n    \"{provider-sdk}&gt;=1.0.0\",  # The official provider SDK\n]\n\n[project.optional-dependencies]\ndev = [\n    \"pytest&gt;=8.0.0\",\n    \"pytest-asyncio&gt;=0.24.0\",\n    \"pytest-cov&gt;=6.0.0\",\n]\n\n[build-system]\nrequires = [\"hatchling\"]\nbuild-backend = \"hatchling.build\"\n\n[tool.hatch.build.targets.wheel]\npackages = [\"src/serapeum\"]\n\n# Workspace source - points to local serapeum-core during development\n[tool.uv.sources]\nserapeum-core = { workspace = true }\n\n# Pytest configuration\n[tool.pytest.ini_options]\ntestpaths = [\"tests\"]\nasyncio_mode = \"strict\"\nasyncio_default_fixture_loop_scope = \"function\"\n\n# Test markers\nmarkers = [\n    \"e2e: End-to-end tests requiring external services\",\n    \"integration: Integration tests\",\n    \"unit: Unit tests\",\n]\n</code></pre>"},{"location":"reference/providers/add_new_providers/#3-update-root-workspace","title":"3. Update Root Workspace","text":"<p>File: <code>pyproject.toml</code> (root)</p> <p>Add your provider to the workspace members:</p> <pre><code>[tool.uv.workspace]\nmembers = [\n    \"libs/core\",\n    \"libs/providers/*\",  # This includes your new provider\n]\n\n[tool.uv.sources]\nserapeum-{provider} = { workspace = true }\n</code></pre>"},{"location":"reference/providers/add_new_providers/#4-implement-the-llm-class","title":"4. Implement the LLM Class","text":"<p>File: <code>libs/providers/{provider}/src/serapeum/providers/{provider}/llm.py</code></p> <pre><code>\"\"\"LLM implementation for {Provider}.\"\"\"\n\nfrom typing import Any, Sequence\n\nfrom pydantic import Field, PrivateAttr\n\nfrom serapeum.core.llms import (\n    ChatResponse,\n    ChatResponseAsyncGen,\n    ChatResponseGen,\n    ChatToCompletionMixin,\n    FunctionCallingLLM,\n    Message,\n    MessageList,\n    Metadata,\n)\nfrom serapeum.core.tools import ToolCallArguments\n\n\nclass {Provider}LLM(ChatToCompletionMixin, FunctionCallingLLM):\n    \"\"\"LLM implementation for {Provider}.\n\n    This class provides chat and completion interfaces for {Provider} models.\n    Completion methods are automatically provided by ChatToCompletionMixin.\n\n    Args:\n        model: Model identifier (e.g., \"gpt-4\", \"claude-3-opus\")\n        api_key: API key for authentication (optional if set via env)\n        temperature: Sampling temperature (0.0-1.0)\n        max_tokens: Maximum tokens in response\n        **kwargs: Additional provider-specific options\n\n    Examples:\n        Basic chat:\n        ```python\n        from serapeum.providers.{provider} import {Provider}LLM\n\n        llm = {Provider}LLM(model=\"model-name\", api_key=\"your-key\")\n        response = llm.chat([Message(role=MessageRole.USER, content=\"Hello\")])\n        print(response.message.content)\n        ```\n\n        Streaming chat:\n        ```python\n        for chunk in llm.stream_chat([Message(role=MessageRole.USER, content=\"Hi\")]):\n            print(chunk.delta, end=\"\", flush=True)\n        ```\n\n        Completion interface (via mixin):\n        ```python\n        response = llm.complete(\"What is 2+2?\")\n        print(response.text)\n        ```\n    \"\"\"\n\n    model: str = Field(description=\"Model identifier\")\n    api_key: str | None = Field(default=None, description=\"API key\")\n    temperature: float = Field(default=0.7, ge=0.0, le=1.0)\n    max_tokens: int = Field(default=1000, gt=0)\n\n    # Private attributes for client instances\n    _client: Any = PrivateAttr(default=None)\n    _async_client: Any = PrivateAttr(default=None)\n\n    def __init__(\n        self,\n        model: str,\n        api_key: str | None = None,\n        temperature: float = 0.7,\n        max_tokens: int = 1000,\n        **kwargs: Any,\n    ):\n        \"\"\"Initialize the {Provider} LLM.\"\"\"\n        super().__init__(\n            model=model,\n            api_key=api_key,\n            temperature=temperature,\n            max_tokens=max_tokens,\n            **kwargs,\n        )\n        self._client = None\n        self._async_client = None\n\n    @classmethod\n    def class_name(cls) -&gt; str:\n        \"\"\"Return the class identifier.\"\"\"\n        return \"{Provider}LLM\"\n\n    @property\n    def metadata(self) -&gt; Metadata:\n        \"\"\"Return LLM metadata.\"\"\"\n        return Metadata(\n            context_window=self._get_context_window(),\n            num_output=self.max_tokens,\n            model_name=self.model,\n            is_chat_model=True,\n            is_function_calling_model=True,\n        )\n\n    @property\n    def client(self):\n        \"\"\"Lazy-loaded synchronous client.\"\"\"\n        if self._client is None:\n            # Import and initialize the provider's SDK client\n            from {provider}_sdk import Client\n            self._client = Client(api_key=self.api_key or self._get_api_key_from_env())\n        return self._client\n\n    @property\n    def async_client(self):\n        \"\"\"Lazy-loaded async client.\"\"\"\n        if self._async_client is None:\n            from {provider}_sdk import AsyncClient\n            self._async_client = AsyncClient(api_key=self.api_key or self._get_api_key_from_env())\n        return self._async_client\n\n    def _get_api_key_from_env(self) -&gt; str:\n        \"\"\"Get API key from environment variable.\"\"\"\n        import os\n        api_key = os.getenv(\"{PROVIDER}_API_KEY\")\n        if not api_key:\n            raise ValueError(\n                f\"{PROVIDER}_API_KEY environment variable not set. \"\n                \"Please set it or pass api_key parameter.\"\n            )\n        return api_key\n\n    def _get_context_window(self) -&gt; int:\n        \"\"\"Get context window size for the model.\"\"\"\n        # Map model names to context windows\n        context_windows = {\n            \"model-small\": 4096,\n            \"model-large\": 128000,\n            # Add more models\n        }\n        return context_windows.get(self.model, 4096)\n\n    def _convert_messages(self, messages: MessageList) -&gt; list[dict]:\n        \"\"\"Convert MessageList to provider's format.\"\"\"\n        provider_messages = []\n        for msg in messages:\n            provider_messages.append({\n                \"role\": msg.role.value,\n                \"content\": msg.content or \"\",\n            })\n        return provider_messages\n\n    # ===== REQUIRED: Implement these 4 chat methods =====\n\n    def chat(self, messages: MessageList, **kwargs: Any) -&gt; ChatResponse:\n        \"\"\"Send a chat request.\n\n        Args:\n            messages: List of messages in the conversation\n            **kwargs: Provider-specific options\n\n        Returns:\n            ChatResponse with the assistant's message\n        \"\"\"\n        provider_messages = self._convert_messages(messages)\n\n        response = self.client.chat.completions.create(\n            model=self.model,\n            messages=provider_messages,\n            temperature=self.temperature,\n            max_tokens=self.max_tokens,\n            **kwargs,\n        )\n\n        return ChatResponse(\n            message=Message(\n                role=MessageRole.ASSISTANT,\n                content=response.choices[0].message.content,\n            ),\n            raw=response,\n        )\n\n    def stream_chat(self, messages: MessageList, **kwargs: Any) -&gt; ChatResponseGen:\n        \"\"\"Stream chat responses.\n\n        Args:\n            messages: List of messages in the conversation\n            **kwargs: Provider-specific options\n\n        Yields:\n            ChatResponse chunks with delta content\n        \"\"\"\n        provider_messages = self._convert_messages(messages)\n\n        def gen():\n            response_stream = self.client.chat.completions.create(\n                model=self.model,\n                messages=provider_messages,\n                temperature=self.temperature,\n                max_tokens=self.max_tokens,\n                stream=True,\n                **kwargs,\n            )\n\n            full_content = \"\"\n            for chunk in response_stream:\n                delta = chunk.choices[0].delta.content or \"\"\n                full_content += delta\n\n                yield ChatResponse(\n                    message=Message(\n                        role=MessageRole.ASSISTANT,\n                        content=full_content,\n                    ),\n                    delta=delta,\n                    raw=chunk,\n                )\n\n        return gen()\n\n    async def achat(self, messages: MessageList, **kwargs: Any) -&gt; ChatResponse:\n        \"\"\"Async chat request.\n\n        Args:\n            messages: List of messages in the conversation\n            **kwargs: Provider-specific options\n\n        Returns:\n            ChatResponse with the assistant's message\n        \"\"\"\n        provider_messages = self._convert_messages(messages)\n\n        response = await self.async_client.chat.completions.create(\n            model=self.model,\n            messages=provider_messages,\n            temperature=self.temperature,\n            max_tokens=self.max_tokens,\n            **kwargs,\n        )\n\n        return ChatResponse(\n            message=Message(\n                role=MessageRole.ASSISTANT,\n                content=response.choices[0].message.content,\n            ),\n            raw=response,\n        )\n\n    async def astream_chat(\n        self, messages: MessageList, **kwargs: Any\n    ) -&gt; ChatResponseAsyncGen:\n        \"\"\"Async streaming chat.\n\n        Args:\n            messages: List of messages in the conversation\n            **kwargs: Provider-specific options\n\n        Returns:\n            Async generator yielding ChatResponse chunks\n        \"\"\"\n        provider_messages = self._convert_messages(messages)\n\n        async def gen():\n            response_stream = await self.async_client.chat.completions.create(\n                model=self.model,\n                messages=provider_messages,\n                temperature=self.temperature,\n                max_tokens=self.max_tokens,\n                stream=True,\n                **kwargs,\n            )\n\n            full_content = \"\"\n            async for chunk in response_stream:\n                delta = chunk.choices[0].delta.content or \"\"\n                full_content += delta\n\n                yield ChatResponse(\n                    message=Message(\n                        role=MessageRole.ASSISTANT,\n                        content=full_content,\n                    ),\n                    delta=delta,\n                    raw=chunk,\n                )\n\n        return gen()\n\n    # ===== COMPLETION METHODS =====\n    # complete(), stream_complete(), acomplete(), astream_complete()\n    # are automatically provided by ChatToCompletionMixin!\n\n    # ===== FUNCTION CALLING (Optional) =====\n\n    def _prepare_chat_with_tools(\n        self,\n        tools: Sequence[\"BaseTool\"],\n        user_msg: str | Message | None = None,\n        chat_history: list[Message] | None = None,\n        verbose: bool = False,\n        allow_parallel_tool_calls: bool = False,\n        **kwargs: Any,\n    ) -&gt; dict[str, Any]:\n        \"\"\"Prepare chat request with tools.\"\"\"\n        tool_specs = [\n            tool.metadata.to_openai_tool(skip_length_check=True) for tool in tools\n        ]\n\n        if isinstance(user_msg, str):\n            user_msg = Message(role=MessageRole.USER, content=user_msg)\n\n        messages = chat_history or []\n        if user_msg:\n            messages.append(user_msg)\n\n        return {\n            \"messages\": messages,\n            \"tools\": tool_specs or None,\n        }\n\n    def get_tool_calls_from_response(\n        self,\n        response: ChatResponse,\n        error_on_no_tool_call: bool = True,\n    ) -&gt; list[ToolCallArguments]:\n        \"\"\"Extract tool calls from response.\"\"\"\n        tool_calls = response.message.additional_kwargs.get(\"tool_calls\", [])\n\n        if not tool_calls:\n            if error_on_no_tool_call:\n                raise ValueError(\"No tool calls found in response\")\n            return []\n\n        # Parse tool calls into ToolCallArguments\n        selections = []\n        for tool_call in tool_calls:\n            selections.append(\n                ToolCallArguments(\n                    tool_id=tool_call.get(\"id\", tool_call[\"function\"][\"name\"]),\n                    tool_name=tool_call[\"function\"][\"name\"],\n                    tool_kwargs=tool_call[\"function\"][\"arguments\"],\n                )\n            )\n\n        return selections\n</code></pre>"},{"location":"reference/providers/add_new_providers/#5-implement-package-__init__py","title":"5. Implement Package <code>__init__.py</code>","text":"<p>File: <code>libs/providers/{provider}/src/serapeum/providers/{provider}/__init__.py</code></p> <pre><code>\"\"\"{Provider} integration for Serapeum LLM framework.\"\"\"\n\nfrom serapeum.providers.{provider}.llm import {Provider}LLM\n\n# Add embeddings if available\n# from serapeum.providers.{provider}.embeddings import {Provider}Embeddings\n\n__all__ = [\n    \"{Provider}LLM\",\n    # \"{Provider}Embeddings\",\n]\n</code></pre>"},{"location":"reference/providers/add_new_providers/#6-create-tests","title":"6. Create Tests","text":"<p>File: <code>libs/providers/{provider}/tests/test_llm.py</code></p> <pre><code>\"\"\"Tests for {Provider}LLM.\"\"\"\n\nimport pytest\nfrom unittest.mock import Mock, patch\n\nfrom serapeum.core.base.llms.types import Message, MessageRole, MessageList\nfrom serapeum.providers.{provider} import {Provider}LLM\n\n\n@pytest.fixture\ndef mock_client():\n    \"\"\"Create a mock {Provider} client.\"\"\"\n    with patch(\"{provider}_sdk.Client\") as mock:\n        yield mock\n\n\n@pytest.fixture\ndef llm(mock_client):\n    \"\"\"Create a {Provider}LLM instance with mocked client.\"\"\"\n    return {Provider}LLM(model=\"test-model\", api_key=\"test-key\")\n\n\nclass TestChat:\n    \"\"\"Test chat functionality.\"\"\"\n\n    def test_chat_basic(self, llm, mock_client):\n        \"\"\"Test basic chat.\"\"\"\n        # Mock response\n        mock_response = Mock()\n        mock_response.choices = [Mock()]\n        mock_response.choices[0].message.content = \"Hello!\"\n\n        llm.client.chat.completions.create.return_value = mock_response\n\n        # Test\n        messages = MessageList.from_list([\n            Message(role=MessageRole.USER, content=\"Hi\")\n        ])\n        response = llm.chat(messages)\n\n        assert response.message.content == \"Hello!\"\n        assert response.message.role == MessageRole.ASSISTANT\n\n    def test_stream_chat(self, llm, mock_client):\n        \"\"\"Test streaming chat.\"\"\"\n        # Mock streaming response\n        mock_chunks = [\n            Mock(choices=[Mock(delta=Mock(content=\"Hel\"))]),\n            Mock(choices=[Mock(delta=Mock(content=\"lo!\"))]),\n        ]\n\n        llm.client.chat.completions.create.return_value = iter(mock_chunks)\n\n        # Test\n        messages = MessageList.from_list([\n            Message(role=MessageRole.USER, content=\"Hi\")\n        ])\n        chunks = list(llm.stream_chat(messages))\n\n        assert len(chunks) == 2\n        assert chunks[0].delta == \"Hel\"\n        assert chunks[1].delta == \"lo!\"\n\n    @pytest.mark.asyncio\n    async def test_achat(self, llm, mock_client):\n        \"\"\"Test async chat.\"\"\"\n        # Mock async response\n        mock_response = Mock()\n        mock_response.choices = [Mock()]\n        mock_response.choices[0].message.content = \"Async hello!\"\n\n        llm.async_client.chat.completions.create.return_value = mock_response\n\n        # Test\n        messages = MessageList.from_list([\n            Message(role=MessageRole.USER, content=\"Hi\")\n        ])\n        response = await llm.achat(messages)\n\n        assert response.message.content == \"Async hello!\"\n\n\nclass TestCompletion:\n    \"\"\"Test completion functionality (from ChatToCompletionMixin).\"\"\"\n\n    def test_complete(self, llm, mock_client):\n        \"\"\"Test that complete() works via mixin.\"\"\"\n        mock_response = Mock()\n        mock_response.choices = [Mock()]\n        mock_response.choices[0].message.content = \"Answer: 42\"\n\n        llm.client.chat.completions.create.return_value = mock_response\n\n        # Test completion interface\n        response = llm.complete(\"What is the answer?\")\n\n        assert response.text == \"Answer: 42\"\n\n\n@pytest.mark.e2e\nclass TestE2E:\n    \"\"\"End-to-end tests requiring actual API.\"\"\"\n\n    def test_real_chat(self):\n        \"\"\"Test with real API (requires API key).\"\"\"\n        import os\n\n        api_key = os.getenv(\"{PROVIDER}_API_KEY\")\n        if not api_key:\n            pytest.skip(\"{PROVIDER}_API_KEY not set\")\n\n        llm = {Provider}LLM(model=\"model-name\", api_key=api_key)\n        response = llm.chat(\n            MessageList.from_list([\n                Message(role=MessageRole.USER, content=\"Say 'test'\")\n            ])\n        )\n\n        assert response.message.content\n        assert isinstance(response.message.content, str)\n</code></pre>"},{"location":"reference/providers/add_new_providers/#7-create-readme","title":"7. Create README","text":"<p>File: <code>libs/providers/{provider}/README.md</code></p> <pre><code># Serapeum {Provider} Integration\n\n{Provider} integration for the Serapeum LLM framework.\n\n## Installation\n\n```bash\npip install serapeum-{provider}\n</code></pre>"},{"location":"reference/providers/add_new_providers/#usage","title":"Usage","text":""},{"location":"reference/providers/add_new_providers/#basic-chat","title":"Basic Chat","text":"<pre><code>from serapeum.providers.{provider} import {Provider}LLM\nfrom serapeum.core.base.llms.types import Message, MessageRole\n\nllm = {Provider}LLM(\n    model=\"model-name\",\n    api_key=\"your-api-key\",  # or set {PROVIDER}_API_KEY env var\n)\n\nresponse = llm.chat([\n    Message(role=MessageRole.USER, content=\"Hello!\")\n])\n\nprint(response.message.content)\n</code></pre>"},{"location":"reference/providers/add_new_providers/#streaming","title":"Streaming","text":"<pre><code>for chunk in llm.stream_chat([\n    Message(role=MessageRole.USER, content=\"Tell me a story\")\n]):\n    print(chunk.delta, end=\"\", flush=True)\n</code></pre>"},{"location":"reference/providers/add_new_providers/#completion-interface","title":"Completion Interface","text":"<pre><code>response = llm.complete(\"What is 2+2?\")\nprint(response.text)\n</code></pre>"},{"location":"reference/providers/add_new_providers/#async","title":"Async","text":"<pre><code>import asyncio\n\nasync def main():\n    response = await llm.achat([\n        Message(role=MessageRole.USER, content=\"Hi\")\n    ])\n    print(response.message.content)\n\nasyncio.run(main())\n</code></pre>"},{"location":"reference/providers/add_new_providers/#configuration","title":"Configuration","text":"<ul> <li><code>model</code>: Model identifier</li> <li><code>api_key</code>: API key (or set <code>{PROVIDER}_API_KEY</code> environment variable)</li> <li><code>temperature</code>: Sampling temperature (0.0-1.0, default: 0.7)</li> <li><code>max_tokens</code>: Maximum response tokens (default: 1000)</li> </ul>"},{"location":"reference/providers/add_new_providers/#features","title":"Features","text":"<ul> <li>\u2705 Chat interface</li> <li>\u2705 Streaming</li> <li>\u2705 Async support</li> <li>\u2705 Completion interface (via ChatToCompletionMixin)</li> <li>\u2705 Function calling (if supported by provider)</li> <li>\u2705 Embeddings (if available)</li> </ul>"},{"location":"reference/providers/add_new_providers/#testing","title":"Testing","text":"<pre><code># Unit tests (mocked)\nuv run pytest libs/providers/{provider}/tests/ -m \"not e2e\"\n\n# E2E tests (requires API key)\nexport {PROVIDER}_API_KEY=\"your-key\"\nuv run pytest libs/providers/{provider}/tests/ -m e2e\n</code></pre>"},{"location":"reference/providers/add_new_providers/#license","title":"License","text":"<p>Same as Serapeum core. <pre><code>### 8. Build and Install\n\n```bash\n# Sync workspace dependencies\nuv sync --dev\n\n# Build the provider package\nuv build --wheel -o dist libs/providers/{provider}\n\n# Install locally for testing\nuv pip install dist/serapeum_{provider}-0.1.0-py3-none-any.whl\n\n# Or use workspace install\nuv sync --package serapeum-{provider}\n</code></pre></p>"},{"location":"reference/providers/add_new_providers/#9-test-the-provider","title":"9. Test the Provider","text":"<pre><code># Run unit tests (mocked)\nuv run pytest libs/providers/{provider}/tests/ -v -m \"not e2e\"\n\n# Run E2E tests (requires API key)\nexport {PROVIDER}_API_KEY=\"your-api-key\"\nuv run pytest libs/providers/{provider}/tests/ -v -m e2e\n\n# Run with coverage\nuv run pytest libs/providers/{provider}/tests/ --cov=serapeum.providers.{provider}\n</code></pre>"},{"location":"reference/providers/add_new_providers/#key-points","title":"Key Points","text":""},{"location":"reference/providers/add_new_providers/#do-this","title":"\u2705 Do This","text":"<ol> <li> <p>Inherit from ChatToCompletionMixin FIRST:    <pre><code>class YourLLM(ChatToCompletionMixin, FunctionCallingLLM):\n</code></pre>    Order matters for Method Resolution Order (MRO)!</p> </li> <li> <p>Implement 4 chat methods only:</p> </li> <li><code>chat(messages, **kwargs)</code></li> <li><code>stream_chat(messages, **kwargs)</code></li> <li><code>achat(messages, **kwargs)</code></li> <li> <p><code>astream_chat(messages, **kwargs)</code></p> </li> <li> <p>Get completion methods for free from the mixin</p> </li> <li> <p>Use lazy-loaded clients for better resource management</p> </li> <li> <p>Add proper error handling for API errors</p> </li> <li> <p>Write comprehensive tests with mocks and E2E tests</p> </li> </ol>"},{"location":"reference/providers/add_new_providers/#dont-do-this","title":"\u274c Don't Do This","text":"<ol> <li>Don't implement completion methods manually - use the mixin</li> <li>Don't put FunctionCallingLLM before ChatToCompletionMixin in inheritance</li> <li>Don't forget to handle async properly (event loops, etc.)</li> <li>Don't hardcode API keys - use environment variables</li> <li>Don't skip testing - both mocked and E2E</li> </ol>"},{"location":"reference/providers/add_new_providers/#examples","title":"Examples","text":""},{"location":"reference/providers/add_new_providers/#existing-providers","title":"Existing Providers","text":"<p>See these for reference:</p> <ul> <li>Ollama: <code>libs/providers/ollama/</code> - Complete implementation with streaming, async, and tool calling</li> <li>Core abstractions: <code>libs/core/src/serapeum/core/llms/abstractions/</code> - Base classes and mixins</li> </ul>"},{"location":"reference/providers/add_new_providers/#minimal-example","title":"Minimal Example","text":"<p>For a minimal provider implementation, you only need ~100 lines of code:</p> <pre><code>from serapeum.core.llms import ChatToCompletionMixin, FunctionCallingLLM\n\nclass MinimalLLM(ChatToCompletionMixin, FunctionCallingLLM):\n    def chat(self, messages, **kwargs):\n        # Call your provider's API\n        return ChatResponse(...)\n\n    def stream_chat(self, messages, **kwargs):\n        # Stream from provider\n        yield ChatResponse(...)\n\n    async def achat(self, messages, **kwargs):\n        # Async call\n        return ChatResponse(...)\n\n    async def astream_chat(self, messages, **kwargs):\n        # Async stream\n        async def gen():\n            yield ChatResponse(...)\n        return gen()\n\n    # That's it! Completion methods come from ChatToCompletionMixin\n</code></pre>"},{"location":"reference/providers/add_new_providers/#checklist","title":"Checklist","text":"<p>Use this checklist when adding a new provider:</p> <ul> <li>[ ] Created package structure under <code>libs/providers/{provider}/</code></li> <li>[ ] Created <code>pyproject.toml</code> with dependencies</li> <li>[ ] Updated root <code>pyproject.toml</code> workspace members</li> <li>[ ] Implemented LLM class with ChatToCompletionMixin</li> <li>[ ] Implemented 4 chat methods</li> <li>[ ] Verified completion methods work (from mixin)</li> <li>[ ] Added <code>_prepare_chat_with_tools()</code> for function calling</li> <li>[ ] Created comprehensive tests (unit + E2E)</li> <li>[ ] Created README.md with examples</li> <li>[ ] Added conftest.py with fixtures</li> <li>[ ] Tested with <code>uv sync --package serapeum-{provider}</code></li> <li>[ ] Verified MRO is correct: <code>ChatToCompletionMixin</code> before <code>FunctionCallingLLM</code></li> <li>[ ] All tests passing</li> <li>[ ] Added to documentation</li> </ul>"},{"location":"reference/providers/add_new_providers/#next-steps","title":"Next Steps","text":"<p>After implementing your provider:</p> <ol> <li>Add to documentation: Update main docs with provider info</li> <li>Add examples: Create example notebooks/scripts</li> <li>Publish: Build and publish to PyPI (if public)</li> <li>Announce: Add to changelog and release notes</li> </ol>"},{"location":"reference/providers/add_new_providers/#need-help","title":"Need Help?","text":"<ul> <li>Example code: See <code>examples/chat_to_completion_mixin_example.py</code></li> <li>Architecture: See <code>docs/architecture/</code> for detailed docs</li> </ul> <p>Happy coding! \ud83d\ude80</p>"},{"location":"reference/providers/ollama/api_reference/","title":"Core Embeddings","text":""},{"location":"reference/providers/ollama/api_reference/#llm-module","title":"llm module","text":""},{"location":"reference/providers/ollama/api_reference/#serapeum.ollama.llm","title":"<code>serapeum.ollama.llm</code>","text":"<p>Ollama LLM implementation providing chat, streaming, and structured output capabilities.</p> <p>This module implements the Ollama provider for the Serapeum framework, offering a complete LLM interface with support for: - Synchronous and asynchronous chat completions - Streaming responses with delta updates - Tool/function calling when supported by the model - Structured outputs using JSON mode and Pydantic validation - Multi-modal inputs (text and images) - Automatic client management with event loop handling</p> <p>The implementation follows the FunctionCallingLLM protocol and integrates with Ollama's local or remote servers for model inference.</p>"},{"location":"reference/providers/ollama/api_reference/#serapeum.ollama.llm.Ollama","title":"<code>Ollama</code>","text":"<p>               Bases: <code>ChatToCompletionMixin</code>, <code>FunctionCallingLLM</code></p> <p>Ollama.</p> <p>This class integrates with the local/remote Ollama server to provide chat, streaming, and structured output capabilities. It supports tool/function calling and JSON mode formatting when the model allows it.</p> <p>Visit https://ollama.com/ to install Ollama and run <code>ollama serve</code> to start the server.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>str</code> <p>Identifier of the Ollama model to use (e.g., <code>\"llama3.1:latest\"</code>).</p> required <code>base_url</code> <code>str</code> <p>Base URL where the Ollama server is hosted. Defaults to <code>\"http://localhost:11434\"</code>.</p> <code>'http://localhost:11434'</code> <code>temperature</code> <code>float</code> <p>Sampling temperature in the range [0.0, 1.0]. Higher values increase randomness. Defaults to 0.75.</p> <code>0.75</code> <code>context_window</code> <code>int</code> <p>Maximum context tokens for the model. Defaults to the project <code>DEFAULT_CONTEXT_WINDOW</code>.</p> <code>DEFAULT_CONTEXT_WINDOW</code> <code>request_timeout</code> <code>float</code> <p>Timeout (seconds) for API calls. Defaults to 60.0.</p> <code>DEFAULT_REQUEST_TIMEOUT</code> <code>prompt_key</code> <code>str</code> <p>Key used for prompt formatting when applicable. Defaults to <code>\"prompt\"</code>.</p> <code>'prompt'</code> <code>json_mode</code> <code>bool</code> <p>Whether to request JSON-formatted responses when supported. Defaults to <code>False</code>.</p> <code>False</code> <code>additional_kwargs</code> <code>dict[str, Any]</code> <p>Extra provider-specific options forwarded under <code>options</code>.</p> <code>None</code> <code>client</code> <code>Client | None</code> <p>Pre-constructed synchronous Ollama client. When omitted, the client is created lazily from <code>base_url</code> and <code>request_timeout</code>.</p> <code>None</code> <code>async_client</code> <code>AsyncClient | None</code> <p>Pre-constructed asynchronous Ollama client. If omitted, a client is created per event loop.</p> <code>None</code> <code>is_function_calling_model</code> <code>bool</code> <p>Flag indicating whether the selected model supports tool/function calling. Defaults to <code>True</code>.</p> <code>True</code> <code>keep_alive</code> <code>float | str | None</code> <p>Controls how long the model stays loaded following the request (e.g., <code>\"5m\"</code>). When <code>None</code>, provider defaults apply.</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Reserved for future extensions and compatibility with the base class.</p> <code>{}</code> <p>Examples:</p> <ul> <li>Basic chat using a real Ollama server (requires a running server and a pulled model)     <pre><code>&gt;&gt;&gt; from serapeum.core.llms import Message, MessageRole\n&gt;&gt;&gt; from serapeum.ollama import Ollama      # type: ignore[attr-defined]\n&gt;&gt;&gt; # Ensure `ollama serve` is running locally and the model is pulled, e.g.:\n&gt;&gt;&gt; #   ollama pull llama3.1\n&gt;&gt;&gt; llm = Ollama(model=\"llama3.1\", request_timeout=120)  # doctest: +SKIP\n&gt;&gt;&gt; response = llm.chat([Message(role=MessageRole.USER, content=\"Say 'pong'.\")])  # doctest: +SKIP\n&gt;&gt;&gt; response # doctest: +SKIP\nChatResponse(raw={'model': 'llama3.1', 'created_at': '2026-02-15T20:16:34.1386099Z', 'done': True, 'done_reason': 'stop', 'total_duration': 1133539400, 'load_duration': 481943100, 'prompt_eval_count': 18, 'prompt_eval_duration': 349949700, 'eval_count': 7, 'eval_duration': 81735000, 'message': Message(role='assistant', content='{ \"ok\": true }', thinking=None, images=None, tool_name=None, tool_calls=None), 'logprobs': None, 'usage': {'prompt_tokens': 18, 'completion_tokens': 7, 'total_tokens': 25}}, likelihood_score=None, additional_kwargs={}, delta=None, message=Message(role=&lt;MessageRole.ASSISTANT: 'assistant'&gt;, additional_kwargs={'tool_calls': None}, chunks=[TextChunk(content='{ \"ok\": true }', path=None, url=None, type='text')]))\n&gt;&gt;&gt; print(response)  # doctest: +SKIP\nassistant: Pong!\n</code></pre></li> <li>Enabling JSON mode for structured outputs with a real server     <pre><code>&gt;&gt;&gt; from serapeum.core.llms import Message, MessageRole\n&gt;&gt;&gt; from serapeum.ollama import Ollama          # type: ignore[attr-defined]\n&gt;&gt;&gt; # When json_mode=True, this adapter sets format=\"json\" under the hood.\n&gt;&gt;&gt; llm = Ollama(model=\"llama3.1\", json_mode=True, request_timeout=120)  # doctest: +SKIP\n&gt;&gt;&gt; response = llm.chat([Message(role=MessageRole.USER, content='Return {\"ok\": true} as JSON')])  # doctest: +SKIP\n&gt;&gt;&gt; print(response)         # doctest: +SKIP\nassistant: {\"ok\":true}\n</code></pre></li> </ul> See Also <ul> <li>chat: Synchronous chat API.</li> <li>stream_chat: Streaming chat API yielding deltas.</li> <li>structured_predict: Parse pydantic models from model output.</li> </ul> Source code in <code>libs/providers/ollama/src/serapeum/ollama/llm.py</code> <pre><code>class Ollama(ChatToCompletionMixin, FunctionCallingLLM):\n    \"\"\"Ollama.\n\n    This class integrates with the local/remote Ollama server to provide chat,\n    streaming, and structured output capabilities. It supports tool/function\n    calling and JSON mode formatting when the model allows it.\n\n    Visit https://ollama.com/ to install Ollama and run ``ollama serve`` to\n    start the server.\n\n    Args:\n        model (str):\n            Identifier of the Ollama model to use (e.g., ``\"llama3.1:latest\"``).\n        base_url (str):\n            Base URL where the Ollama server is hosted. Defaults to\n            ``\"http://localhost:11434\"``.\n        temperature (float):\n            Sampling temperature in the range [0.0, 1.0]. Higher values increase\n            randomness. Defaults to 0.75.\n        context_window (int):\n            Maximum context tokens for the model. Defaults to the project\n            ``DEFAULT_CONTEXT_WINDOW``.\n        request_timeout (float):\n            Timeout (seconds) for API calls. Defaults to 60.0.\n        prompt_key (str):\n            Key used for prompt formatting when applicable. Defaults to ``\"prompt\"``.\n        json_mode (bool):\n            Whether to request JSON-formatted responses when supported. Defaults to ``False``.\n        additional_kwargs (dict[str, Any]):\n            Extra provider-specific options forwarded under ``options``.\n        client (Client | None):\n            Pre-constructed synchronous Ollama client. When omitted, the client\n            is created lazily from ``base_url`` and ``request_timeout``.\n        async_client (AsyncClient | None):\n            Pre-constructed asynchronous Ollama client. If omitted, a client is\n            created per event loop.\n        is_function_calling_model (bool):\n            Flag indicating whether the selected model supports tool/function\n            calling. Defaults to ``True``.\n        keep_alive (float | str | None):\n            Controls how long the model stays loaded following the request\n            (e.g., ``\"5m\"``). When ``None``, provider defaults apply.\n        **kwargs (Any):\n            Reserved for future extensions and compatibility with the base class.\n\n    Examples:\n        - Basic chat using a real Ollama server (requires a running server and a pulled model)\n            ```python\n            &gt;&gt;&gt; from serapeum.core.llms import Message, MessageRole\n            &gt;&gt;&gt; from serapeum.ollama import Ollama      # type: ignore[attr-defined]\n            &gt;&gt;&gt; # Ensure `ollama serve` is running locally and the model is pulled, e.g.:\n            &gt;&gt;&gt; #   ollama pull llama3.1\n            &gt;&gt;&gt; llm = Ollama(model=\"llama3.1\", request_timeout=120)  # doctest: +SKIP\n            &gt;&gt;&gt; response = llm.chat([Message(role=MessageRole.USER, content=\"Say 'pong'.\")])  # doctest: +SKIP\n            &gt;&gt;&gt; response # doctest: +SKIP\n            ChatResponse(raw={'model': 'llama3.1', 'created_at': '2026-02-15T20:16:34.1386099Z', 'done': True, 'done_reason': 'stop', 'total_duration': 1133539400, 'load_duration': 481943100, 'prompt_eval_count': 18, 'prompt_eval_duration': 349949700, 'eval_count': 7, 'eval_duration': 81735000, 'message': Message(role='assistant', content='{ \"ok\": true }', thinking=None, images=None, tool_name=None, tool_calls=None), 'logprobs': None, 'usage': {'prompt_tokens': 18, 'completion_tokens': 7, 'total_tokens': 25}}, likelihood_score=None, additional_kwargs={}, delta=None, message=Message(role=&lt;MessageRole.ASSISTANT: 'assistant'&gt;, additional_kwargs={'tool_calls': None}, chunks=[TextChunk(content='{ \"ok\": true }', path=None, url=None, type='text')]))\n            &gt;&gt;&gt; print(response)  # doctest: +SKIP\n            assistant: Pong!\n\n            ```\n        - Enabling JSON mode for structured outputs with a real server\n            ```python\n            &gt;&gt;&gt; from serapeum.core.llms import Message, MessageRole\n            &gt;&gt;&gt; from serapeum.ollama import Ollama          # type: ignore[attr-defined]\n            &gt;&gt;&gt; # When json_mode=True, this adapter sets format=\"json\" under the hood.\n            &gt;&gt;&gt; llm = Ollama(model=\"llama3.1\", json_mode=True, request_timeout=120)  # doctest: +SKIP\n            &gt;&gt;&gt; response = llm.chat([Message(role=MessageRole.USER, content='Return {\"ok\": true} as JSON')])  # doctest: +SKIP\n            &gt;&gt;&gt; print(response)         # doctest: +SKIP\n            assistant: {\"ok\":true}\n\n            ```\n\n    See Also:\n        - chat: Synchronous chat API.\n        - stream_chat: Streaming chat API yielding deltas.\n        - structured_predict: Parse pydantic models from model output.\n    \"\"\"\n\n    base_url: str = Field(\n        default=\"http://localhost:11434\",\n        description=\"Base url the model is hosted under.\",\n    )\n    model: str = Field(description=\"The Ollama model to use.\")\n    temperature: float = Field(\n        default=0.75,\n        description=\"The temperature to use for sampling.\",\n        ge=0.0,\n        le=1.0,\n    )\n    context_window: int = Field(\n        default=DEFAULT_CONTEXT_WINDOW,\n        description=\"The maximum number of context tokens for the model.\",\n        gt=0,\n    )\n    request_timeout: float = Field(\n        default=DEFAULT_REQUEST_TIMEOUT,\n        description=\"The timeout for making http request to Ollama API server\",\n    )\n    prompt_key: str = Field(\n        default=\"prompt\", description=\"The key to use for the prompt in API calls.\"\n    )\n    json_mode: bool = Field(\n        default=False,\n        description=\"Whether to use JSON mode for the Ollama API.\",\n    )\n    additional_kwargs: dict[str, Any] = Field(\n        default_factory=dict,\n        description=\"Additional model parameters for the Ollama API.\",\n    )\n    is_function_calling_model: bool = Field(\n        default=True,\n        description=\"Whether the model is a function calling model.\",\n    )\n    keep_alive: float | str | None = Field(\n        default=\"5m\",\n        description=\"controls how long the model will stay loaded into memory following the request(default: 5m)\",\n    )\n\n    _client: ollama_sdk.Client | None = PrivateAttr()       # type: ignore\n    _async_client: ollama_sdk.AsyncClient | None = PrivateAttr()        # type: ignore\n\n    def __init__(\n        self,\n        model: str,\n        base_url: str = \"http://localhost:11434\",\n        temperature: float = 0.75,\n        context_window: int = DEFAULT_CONTEXT_WINDOW,\n        request_timeout: float = DEFAULT_REQUEST_TIMEOUT,\n        prompt_key: str = \"prompt\",\n        json_mode: bool = False,\n        additional_kwargs: dict[str, Any] | None = None,\n        client: ollama_sdk.Client | None = None,                # type: ignore\n        async_client: ollama_sdk.AsyncClient | None = None,     # type: ignore\n        is_function_calling_model: bool = True,\n        keep_alive: float | str | None = None,\n        **kwargs: Any,\n    ) -&gt; None:\n        \"\"\"Initialize the Ollama LLM adapter with configuration and optional clients.\n\n        Creates an Ollama LLM instance with the specified model and connection\n        parameters. Clients can be pre-constructed and passed in, or they will\n        be created lazily on first use. The instance inherits from FunctionCallingLLM\n        and ChatToCompletionMixin to provide complete chat and structured output support.\n\n        Args:\n            model: Identifier of the Ollama model (e.g., \"llama3.1\", \"mistral\").\n            base_url: URL where the Ollama server is hosted. Defaults to \"http://localhost:11434\".\n            temperature: Sampling temperature between 0.0 (deterministic) and 1.0 (random).\n                Defaults to 0.75.\n            context_window: Maximum context tokens the model can process. Defaults to\n                DEFAULT_CONTEXT_WINDOW.\n            request_timeout: Timeout in seconds for API requests. Defaults to 60.0.\n            prompt_key: Key used for prompt formatting in API calls. Defaults to \"prompt\".\n            json_mode: Whether to request JSON-formatted responses via the format parameter.\n                Defaults to False.\n            additional_kwargs: Extra provider-specific options passed to Ollama under \"options\".\n                Defaults to empty dict.\n            client: Pre-constructed synchronous Ollama client. If None, created lazily.\n            async_client: Pre-constructed asynchronous Ollama client. If None, created per event loop.\n            is_function_calling_model: Whether this model supports tool/function calling.\n                Defaults to True.\n            keep_alive: Duration to keep model loaded in memory (e.g., \"5m\", \"1h\", or float seconds).\n                Defaults to \"5m\".\n            **kwargs: Reserved for future extensions and base class compatibility.\n\n        Examples:\n            - Initialize with minimal configuration\n                ```python\n                &gt;&gt;&gt; from serapeum.ollama import Ollama  # type: ignore[attr-defined]\n                &gt;&gt;&gt; llm = Ollama(model=\"llama3.1\")\n                &gt;&gt;&gt; llm.model\n                'llama3.1'\n\n                ```\n            - Initialize with custom server and timeout\n                ```python\n                &gt;&gt;&gt; llm = Ollama(\n                ...     model=\"mistral\",\n                ...     base_url=\"http://custom-server:11434\",\n                ...     request_timeout=120.0,\n                ...     temperature=0.5\n                ... )\n                &gt;&gt;&gt; llm.temperature\n                0.5\n\n                ```\n            - Enable JSON mode for structured outputs\n                ```python\n                &gt;&gt;&gt; llm = Ollama(model=\"llama3.1\", json_mode=True)\n                &gt;&gt;&gt; llm.json_mode\n                True\n\n                ```\n        \"\"\"\n        if additional_kwargs is None:\n            additional_kwargs = {}\n\n        super().__init__(\n            model=model,\n            base_url=base_url,\n            temperature=temperature,\n            context_window=context_window,\n            request_timeout=request_timeout,\n            prompt_key=prompt_key,\n            json_mode=json_mode,\n            additional_kwargs=additional_kwargs,\n            is_function_calling_model=is_function_calling_model,\n            keep_alive=keep_alive,\n            **kwargs,\n        )\n\n        self._client = client\n        self._async_client = async_client\n        # Track the event loop associated with the async client to avoid\n        # reusing a client bound to a closed event loop across tests/runs\n        self._async_client_loop: asyncio.AbstractEventLoop | None = None\n\n    @classmethod\n    def class_name(cls) -&gt; str:\n        \"\"\"Return the registered class name for this provider adapter.\n\n        Returns:\n            str: Provider identifier used in registries or logs.\n\n        Examples:\n            - Retrieve the class identifier\n                ```python\n                &gt;&gt;&gt; from serapeum.ollama import Ollama      # type: ignore[attr-defined]\n                &gt;&gt;&gt; Ollama.class_name()\n                'Ollama_llm'\n\n                ```\n        \"\"\"\n        return \"Ollama_llm\"\n\n    @property\n    def metadata(self) -&gt; Metadata:\n        \"\"\"LLM metadata.\n\n        Returns:\n            Metadata: Static capabilities such as context window and chat support.\n\n        Examples:\n            - Inspect chat model capabilities\n                ```python\n                &gt;&gt;&gt; from serapeum.ollama import Ollama      # type: ignore[attr-defined]\n                &gt;&gt;&gt; Ollama(model=\"m\").metadata.is_chat_model\n                True\n\n                ```\n        \"\"\"\n        return Metadata(\n            context_window=self.context_window,\n            num_output=DEFAULT_NUM_OUTPUTS,\n            model_name=self.model,\n            is_chat_model=True,\n            is_function_calling_model=self.is_function_calling_model,\n        )\n\n    @property\n    def client(self) -&gt; ollama_sdk.Client:      # type: ignore\n        \"\"\"Synchronous Ollama client lazily bound to ``base_url``.\n\n        Returns:\n            Client: Underlying Ollama client instance.\n\n        Examples:\n            - Lazily create the client on first access\n                ```python\n                &gt;&gt;&gt; from serapeum.ollama import Ollama      # type: ignore[attr-defined]\n                &gt;&gt;&gt; llm = Ollama(model=\"m\", base_url=\"http://localhost:11434\", request_timeout=1.0)\n                &gt;&gt;&gt; c = llm.client  # doctest: +SKIP\n                &gt;&gt;&gt; type(c).__name__  # doctest: +SKIP\n                'Client'\n                &gt;&gt;&gt; hasattr(c, \"chat\")  # doctest: +SKIP\n                True\n\n                ```\n        \"\"\"\n        if self._client is None:\n            self._client = ollama_sdk.Client(host=self.base_url, timeout=self.request_timeout)      # type: ignore\n        return self._client\n\n    def _ensure_async_client(self) -&gt; ollama_sdk.AsyncClient:   # type: ignore\n        \"\"\"Return a per-event-loop AsyncClient, recreating when loop changes or closes.\n\n        This avoids ``Event loop is closed`` errors when test runners (e.g.,\n        pytest-asyncio) create and close event loops between invocations.\n\n        Returns:\n            AsyncClient: Async client instance associated with the current loop.\n\n        Examples:\n            - Re-create the client when the loop changes\n                ```python\n                &gt;&gt;&gt; import asyncio\n                &gt;&gt;&gt; from serapeum.ollama import Ollama      # type: ignore[attr-defined]\n                &gt;&gt;&gt; llm = Ollama(model=\"m\")\n                &gt;&gt;&gt; c1 = llm._ensure_async_client()  # doctest: +SKIP\n                &gt;&gt;&gt; c2 = llm._ensure_async_client()  # doctest: +SKIP\n                &gt;&gt;&gt; c1 is c2  # doctest: +SKIP\n                True\n\n                ```\n        \"\"\"\n        try:\n            current_loop = asyncio.get_running_loop()\n        except RuntimeError:\n            current_loop = None  # No running loop available in this context\n\n        cached_loop = getattr(self, \"_async_client_loop\", None)\n        if self._async_client is None:\n            # No client yet: create and bind to current loop (may be None)\n            self._async_client = ollama_sdk.AsyncClient(        # type: ignore\n                host=self.base_url, timeout=self.request_timeout\n            )\n            self._async_client_loop = current_loop\n        else:\n            # If no loop recorded yet (e.g., injected client), bind without recreation\n            if cached_loop is None:\n                self._async_client_loop = current_loop\n            # Recreate if the current loop is closed\n            elif (\n                current_loop is not None\n                and hasattr(current_loop, \"is_closed\")\n                and current_loop.is_closed()\n            ):\n                self._async_client = ollama_sdk.AsyncClient(        # type: ignore\n                    host=self.base_url, timeout=self.request_timeout\n                )\n                self._async_client_loop = current_loop\n            # Or if the cached loop has been closed since creation\n            elif hasattr(cached_loop, \"is_closed\") and cached_loop.is_closed():\n                self._async_client = ollama_sdk.AsyncClient(        # type: ignore\n                    host=self.base_url, timeout=self.request_timeout\n                )\n                self._async_client_loop = current_loop\n            else:\n                # Reuse existing client even if loop identity differs but both are open\n                self._async_client_loop = current_loop\n\n        return self._async_client\n\n    @property\n    def async_client(self) -&gt; ollama_sdk.AsyncClient:       # type: ignore\n        \"\"\"Async Ollama client bound to the current asyncio event loop.\n\n        This property lazily creates or reuses an AsyncClient instance, automatically\n        handling event loop changes and closures. It's safe to call across different\n        async contexts (e.g., multiple pytest-asyncio tests) as it detects closed\n        loops and recreates the client as needed.\n\n        Returns:\n            The async client instance used for asynchronous operations.\n\n        Examples:\n            - Access async client for manual API calls\n                ```python\n                &gt;&gt;&gt; import asyncio\n                &gt;&gt;&gt; from serapeum.ollama import Ollama      # type: ignore\n                &gt;&gt;&gt; llm = Ollama(model=\"llama3.1\")\n                &gt;&gt;&gt; async def check_client():    # doctest: +SKIP\n                ...     client = llm.async_client\n                ...     return hasattr(client, \"chat\")\n                &gt;&gt;&gt; asyncio.run(check_client())  # Returns True # doctest: +SKIP\n\n                ```\n\n        See Also:\n            _ensure_async_client: Ensures the client matches the active event loop.\n            client: Synchronous Ollama client property.\n        \"\"\"\n        return self._ensure_async_client()\n\n    @property\n    def _model_kwargs(self) -&gt; dict[str, Any]:\n        \"\"\"Assemble provider options forwarded under the ``options`` field.\n\n        Returns:\n            dict[str, Any]: Merged dictionary where ``additional_kwargs`` override\n            base defaults such as ``temperature`` and ``num_ctx``.\n\n        Examples:\n            - Merge user-provided options with defaults\n                ```python\n                &gt;&gt;&gt; from serapeum.ollama import Ollama      # type: ignore[attr-defined]\n                &gt;&gt;&gt; llm = Ollama(model=\"m\", additional_kwargs={\"mirostat\": 2, \"temperature\": 0.9})\n                &gt;&gt;&gt; print(llm._model_kwargs)\n                {'temperature': 0.9, 'num_ctx': 3900, 'mirostat': 2}\n\n                ```\n        \"\"\"\n        base_kwargs = {\n            \"temperature\": self.temperature,\n            \"num_ctx\": self.context_window,\n        }\n        return {\n            **base_kwargs,\n            **self.additional_kwargs,\n        }\n\n    @staticmethod\n    def _convert_to_ollama_messages(\n        messages: MessageList\n    ) -&gt; list[dict[str, Any]]:\n        \"\"\"Convert internal MessageList to the Ollama wire format.\n\n        Args:\n            messages (MessageList):\n                Sequence of messages to be sent to Ollama.\n\n        Returns:\n            Dict: A list of dicts compatible with the Ollama chat API (role,\n            content, optional images, and tool_calls).\n\n        Raises:\n            ValueError: If a content chunk type is unsupported.\n\n        Examples:\n            - Text-only conversion\n                ```python\n                &gt;&gt;&gt; from serapeum.core.llms import Message, MessageList, MessageRole\n                &gt;&gt;&gt; from serapeum.ollama import Ollama      # type: ignore[attr-defined]\n                &gt;&gt;&gt; llm = Ollama(model=\"m\")\n                &gt;&gt;&gt; wire = llm._convert_to_ollama_messages(\n                ...     MessageList.from_list([\n                ...         Message(role=MessageRole.USER, content=\"hello\"),\n                ...     ])\n                ... )\n                &gt;&gt;&gt; print(wire)\n                [{'role': 'user', 'content': 'hello'}]\n\n                ```\n        \"\"\"\n        ollama_messages = []\n        for message in messages:\n            cur_ollama_message = {\n                \"role\": message.role.value,\n                \"content\": \"\",\n            }\n            for block in message.chunks:\n                if isinstance(block, TextChunk):\n                    cur_ollama_message[\"content\"] += block.content\n                elif isinstance(block, Image):\n                    if \"images\" not in cur_ollama_message:\n                        cur_ollama_message[\"images\"] = []\n\n                    # Prefer an explicit base64 attribute if provided by the caller\n                    b64 = getattr(block, \"base64\", None)\n                    if b64 is None:\n                        extra = getattr(block, \"model_extra\", None) or {}\n                        b64 = extra.get(\"base64\")\n\n                    if b64 is None:\n                        try:\n                            b64 = dict(block).get(\"base64\")\n                        except Exception:\n                            b64 = None\n\n                    if b64 is None:\n                        b64 = getattr(block, \"__dict__\", {}).get(\"base64\")\n\n                    if isinstance(b64, (bytes, str)):\n                        base64_str = (\n                            b64.decode(\"utf-8\") if isinstance(b64, bytes) else b64\n                        )\n                    else:\n                        # Fall back to resolving image bytes via the helper\n                        base64_str = (\n                            block.resolve_image(as_base64=True).read().decode(\"utf-8\")\n                        )\n\n                    cur_ollama_message[\"images\"].append(base64_str)\n                else:\n                    raise ValueError(f\"Unsupported block type: {type(block)}\")\n\n            if \"tool_calls\" in message.additional_kwargs:\n                cur_ollama_message[\"tool_calls\"] = message.additional_kwargs[\n                    \"tool_calls\"\n                ]\n\n            ollama_messages.append(cur_ollama_message)\n\n        return ollama_messages\n\n    @staticmethod\n    def _get_response_token_counts(raw_response: dict[str, Any]) -&gt; dict[str, Any]:\n        \"\"\"Extract token usage fields from a raw Ollama response.\n\n        Args:\n            raw_response (dict):\n                Provider response possibly containing token counts.\n\n        Returns:\n            dict: Mapping with ``prompt_tokens``, ``completion_tokens``, and\n            ``total_tokens`` when available; otherwise an empty dict.\n\n        Examples:\n            - Compute totals when both fields are present\n                ```python\n                &gt;&gt;&gt; from serapeum.ollama import Ollama      # type: ignore\n                &gt;&gt;&gt; Ollama._get_response_token_counts({\"prompt_eval_count\": 2, \"eval_count\": 3})\n                {'prompt_tokens': 2, 'completion_tokens': 3, 'total_tokens': 5}\n\n                ```\n        \"\"\"\n        token_counts = {}\n        try:\n            prompt_tokens = raw_response[\"prompt_eval_count\"]\n            completion_tokens = raw_response[\"eval_count\"]\n            total_tokens = prompt_tokens + completion_tokens\n            token_counts = {\n                \"prompt_tokens\": prompt_tokens,\n                \"completion_tokens\": completion_tokens,\n                \"total_tokens\": total_tokens,\n            }\n        except (KeyError, TypeError):\n            pass\n\n        return token_counts\n\n    def _prepare_chat_with_tools(\n        self,\n        tools: list[\"BaseTool\"],\n        user_msg: str | Message | None = None,\n        chat_history: list[Message] | None = None,\n        verbose: bool = False,\n        allow_parallel_tool_calls: bool = False,\n        **kwargs: Any,\n    ) -&gt; dict[str, Any]:\n        \"\"\"Prepare a chat payload including tool specifications.\n\n        Args:\n            tools (List[BaseTool]): Tools to expose to the model (converted using OpenAI schema).\n            user_msg (str | Message | None): Optional user message to append.\n            chat_history (list[Message] | None): Optional existing conversation history.\n            verbose (bool): Currently unused verbosity flag.\n            allow_parallel_tool_calls (bool): Indicator forwarded to validators.\n            **kwargs (Any): Reserved for future extensions.\n\n        Returns:\n            dict[str, Any]: Dict with ``messages`` and ``tools`` entries suitable for chat calls.\n\n        Examples:\n            - Combine history, a new user message, and tool specs\n                ```python\n                &gt;&gt;&gt; from serapeum.core.llms import Message, MessageRole\n                &gt;&gt;&gt; from serapeum.ollama import Ollama      # type: ignore\n                &gt;&gt;&gt; class T:\n                ...     def __init__(self, n):\n                ...         class M:\n                ...             def to_openai_tool(self, skip_length_check=False):\n                ...                 return {\"type\": \"function\", \"function\": {\"name\": n}}\n                ...         self.metadata = M()\n                ...\n                &gt;&gt;&gt; llm = Ollama(model=\"m\")\n                &gt;&gt;&gt; payload = llm._prepare_chat_with_tools([T(\"t1\")], user_msg=\"hi\", chat_history=[Message(role=MessageRole.SYSTEM, content=\"s\")])\n                &gt;&gt;&gt; len(payload[\"messages\"])\n                2\n                &gt;&gt;&gt; payload[\"messages\"][0].role == MessageRole.SYSTEM\n                True\n                &gt;&gt;&gt; payload[\"messages\"][1].role == MessageRole.USER\n                True\n                &gt;&gt;&gt; payload[\"tools\"]\n                [{'type': 'function', 'function': {'name': 't1'}}]\n\n                ```\n        \"\"\"\n        tool_specs = [\n            tool.metadata.to_openai_tool(skip_length_check=True) for tool in tools\n        ]\n\n        if isinstance(user_msg, str):\n            user_msg = Message(role=MessageRole.USER, content=user_msg)\n\n        messages = chat_history or []\n        if user_msg:\n            messages.append(user_msg)\n\n        return {\n            \"messages\": messages,\n            \"tools\": tool_specs or None,\n        }\n\n    def _validate_chat_with_tools_response(\n        self,\n        response: ChatResponse,\n        tools: list[\"BaseTool\"],\n        allow_parallel_tool_calls: bool = False,\n        **kwargs: Any,\n    ) -&gt; ChatResponse:\n        \"\"\"Validate and normalize a chat-with-tools response.\n\n        If ``allow_parallel_tool_calls`` is ``False``, the response is mutated to\n        include at most a single tool call.\n\n        Args:\n            response (ChatResponse): Response to validate.\n            tools (List[BaseTool]): Tools originally requested (unused, reserved for future checks).\n            allow_parallel_tool_calls (bool): Whether multiple tool calls are allowed.\n            **kwargs (Any): Reserved for future options.\n\n        Returns:\n            ChatResponse: The validated response (possibly mutated in-place).\n\n        Examples:\n            - Force single tool call when multiple are present (remove the multiple tool calls and leave only the first)\n                ```python\n                &gt;&gt;&gt; from serapeum.core.llms import Message, MessageRole, ChatResponse\n                &gt;&gt;&gt; llm = Ollama(model=\"m\")\n                &gt;&gt;&gt; response = ChatResponse(\n                ...     message=Message(\n                ...         role=MessageRole.ASSISTANT, content=\"\", additional_kwargs={\n                ...             \"tool_calls\": [\n                ...                 {\"function\": {\"name\": \"a\", \"arguments\": {}}},\n                ...                 {\"function\": {\"name\": \"b\", \"arguments\": {}}},\n                ...             ]\n                ...         }\n                ...     )\n                ... )\n                &gt;&gt;&gt; validated_response = llm._validate_chat_with_tools_response(\n                ...     response,\n                ...     tools=[],\n                ...     allow_parallel_tool_calls=False,\n                ... )\n                &gt;&gt;&gt; len(validated_response.message.additional_kwargs.get(\"tool_calls\"))\n                1\n\n                ```\n        \"\"\"\n        if not allow_parallel_tool_calls:\n            force_single_tool_call(response)\n        return response\n\n    def get_tool_calls_from_response(\n        self,\n        response: \"ChatResponse\",\n        error_on_no_tool_call: bool = True,\n    ) -&gt; list[ToolCallArguments]:\n        \"\"\"Extract tool call selections from a chat response.\n\n        Args:\n            response (ChatResponse): Response potentially containing tool calls.\n            error_on_no_tool_call (bool): Whether to raise when no tool calls are present.\n\n        Returns:\n            list[ToolCallArguments]: Parsed tool selections (empty when allowed and none present).\n\n        Raises:\n            ValueError: When ``error_on_no_tool_call`` is ``True`` and no tool calls exist.\n\n        Examples:\n            - Parse a single tool call\n                ```python\n                &gt;&gt;&gt; from serapeum.core.llms import Message, MessageRole, ChatResponse\n                &gt;&gt;&gt; llm = Ollama(model=\"m\")\n                &gt;&gt;&gt; r = ChatResponse(\n                ...     message=Message(\n                ...         role=MessageRole.ASSISTANT,\n                ...         content=\"\",\n                ...         additional_kwargs={\n                ...             \"tool_calls\": [\n                ...                 {\"function\": {\"name\": \"run\", \"arguments\": {\"a\": 1}}}\n                ...             ]\n                ...         },\n                ...     )\n                ... )\n                &gt;&gt;&gt; calls = llm.get_tool_calls_from_response(r)\n                &gt;&gt;&gt; (\n                ...     calls[0].tool_id,\n                ...     calls[0].tool_name,\n                ...     calls[0].tool_kwargs[\"a\"],\n                ... ) == (\"run\", \"run\", 1)\n                True\n\n                ```\n            - Raise when no tool call is present and errors are enabled\n                ```python\n                &gt;&gt;&gt; from serapeum.core.llms import Message, MessageRole, ChatResponse\n                &gt;&gt;&gt; llm = Ollama(model=\"m\")\n                &gt;&gt;&gt; empty = ChatResponse(message=Message(role=MessageRole.ASSISTANT, content=\"\"))\n                &gt;&gt;&gt; try:\n                ...     _ = llm.get_tool_calls_from_response(empty, error_on_no_tool_call=True)\n                ... except ValueError as e:\n                ...     msg = str(e)\n                &gt;&gt;&gt; 'Expected at least one tool call' in msg\n                True\n\n                ```\n        \"\"\"\n        tool_calls = response.message.additional_kwargs.get(\"tool_calls\", [])\n        if not tool_calls or len(tool_calls) &lt; 1:\n            if error_on_no_tool_call:\n                raise ValueError(\n                    f\"Expected at least one tool call, but got {len(tool_calls) if tool_calls else 0} tool calls.\"\n                )\n            else:\n                return []\n\n        tool_selections = []\n        coercer = ArgumentCoercer()\n\n        for tool_call in tool_calls:\n            # Coerce arguments to proper types (handles JSON strings, type mismatches, etc.)\n            raw_arguments = tool_call[\"function\"][\"arguments\"]\n            argument_dict = coercer.coerce(raw_arguments)\n\n            tool_selections.append(\n                ToolCallArguments(\n                    # tool ids not provided by Ollama\n                    tool_id=tool_call[\"function\"][\"name\"],\n                    tool_name=tool_call[\"function\"][\"name\"],\n                    tool_kwargs=argument_dict,\n                )\n            )\n\n        return tool_selections\n\n    def chat(self, messages: MessageList, **kwargs: Any) -&gt; ChatResponse:\n        \"\"\"Send a chat request to Ollama and return the assistant message.\n\n        Args:\n            messages (MessageList):\n                Sequence of chat messages.\n            **kwargs (Any):\n                Provider-specific overrides such as ``tools`` or ``format``.\n\n        Returns:\n            ChatResponse: Parsed response containing the assistant message and optional token usage.\n\n        Examples:\n            - Minimal chat against a running Ollama server (requires server and model)\n                ```python\n                &gt;&gt;&gt; from serapeum.core.llms import Message, MessageRole\n                &gt;&gt;&gt; # Ensure `ollama serve` is running and the model is available locally.\n                &gt;&gt;&gt; llm = Ollama(model=\"llama3.1\", request_timeout=120)\n                &gt;&gt;&gt; resp = llm.chat([Message(role=MessageRole.USER, content=\"hi\")])  # doctest: +SKIP\n                &gt;&gt;&gt; print(resp)   # doctest: +SKIP\n                Hello! How are you today? Is there something I can help you with or would you like to chat?\n                &gt;&gt;&gt; isinstance(resp.message.content, str)   # doctest: +SKIP\n                True\n\n                ```\n        \"\"\"\n        ollama_messages = self._convert_to_ollama_messages(messages)\n\n        tools = kwargs.pop(\"tools\", None)\n        response_format = kwargs.pop(\"format\", \"json\" if self.json_mode else None)\n\n        response = self.client.chat(\n            model=self.model,\n            messages=ollama_messages,\n            stream=False,\n            format=response_format,\n            tools=tools,\n            options=self._model_kwargs,\n            keep_alive=self.keep_alive,\n        )\n\n        response = dict(response)\n\n        tool_calls = response[\"message\"].get(\"tool_calls\", [])\n        token_counts = self._get_response_token_counts(response)\n        if token_counts:\n            response[\"usage\"] = token_counts\n\n        return ChatResponse(\n            message=Message(\n                content=response[\"message\"][\"content\"],\n                role=response[\"message\"][\"role\"],\n                additional_kwargs={\"tool_calls\": tool_calls},\n            ),\n            raw=response,\n        )\n\n    @staticmethod\n    def _parse_tool_call_response(\n        tools_dict: dict[str, Any], r: dict[str, Any]\n    ) -&gt; ChatResponse:\n        \"\"\"Accumulate streaming content and unique tool calls into a ChatResponse.\n\n        This static method processes individual streaming chunks from Ollama's chat API,\n        accumulating text content and de-duplicating tool calls across multiple deltas.\n        It maintains state in the tools_dict to track cumulative response text and\n        unique tool calls seen so far.\n\n        Args:\n            tools_dict: Mutable aggregation state with keys:\n                - \"response_txt\": Accumulated text content\n                - \"seen_tool_calls\": Set of (function_name, arguments) tuples for deduplication\n                - \"all_tool_calls\": List of unique tool call dictionaries\n            r: A single streaming chunk from Ollama containing message content and metadata.\n\n        Returns:\n            ChatResponse with cumulative message content, the current delta, and all\n            unique tool calls accumulated so far.\n\n        Examples:\n            - Process streaming chunk with text content\n                ```python\n                &gt;&gt;&gt; from serapeum.ollama import Ollama      # type: ignore\n                &gt;&gt;&gt; tools_dict = {\"response_txt\": \"\", \"seen_tool_calls\": set(), \"all_tool_calls\": []}\n                &gt;&gt;&gt; chunk = {\"message\": {\"role\": \"assistant\", \"content\": \"Hello\"}}\n                &gt;&gt;&gt; response = Ollama._parse_tool_call_response(tools_dict, chunk)\n                &gt;&gt;&gt; response.message.content\n                'Hello'\n                &gt;&gt;&gt; response.delta\n                'Hello'\n\n                ```\n            - Process chunk with tool calls (deduplicated)\n                ```python\n                &gt;&gt;&gt; tools_dict = {\"response_txt\": \"\", \"seen_tool_calls\": set(), \"all_tool_calls\": []}\n                &gt;&gt;&gt; chunk1 = {\n                ...     \"message\": {\n                ...         \"role\": \"assistant\",\n                ...         \"content\": \"\",\n                ...         \"tool_calls\": [{\"function\": {\"name\": \"calc\", \"arguments\": {\"x\": 1}}}]\n                ...     }\n                ... }\n                &gt;&gt;&gt; r1 = Ollama._parse_tool_call_response(tools_dict, chunk1)\n                &gt;&gt;&gt; len(tools_dict[\"all_tool_calls\"])\n                1\n                &gt;&gt;&gt; # Same tool call again - should not duplicate\n                &gt;&gt;&gt; r2 = Ollama._parse_tool_call_response(tools_dict, chunk1)\n                &gt;&gt;&gt; len(tools_dict[\"all_tool_calls\"])\n                1\n\n                ```\n\n        See Also:\n            stream_chat: Uses this helper to materialize per-chunk responses.\n            astream_chat: Async variant that uses this helper.\n        \"\"\"\n        r = dict(r)\n\n        tools_dict[\"response_txt\"] += r[\"message\"][\"content\"]\n        new_tool_calls = [dict(t) for t in (r[\"message\"].get(\"tool_calls\", []) or [])]\n        for tool_call in new_tool_calls:\n            func_name = str(tool_call[\"function\"][\"name\"])\n            func_args = str(tool_call[\"function\"][\"arguments\"])\n            if (func_name, func_args) not in tools_dict[\"seen_tool_calls\"]:\n                tools_dict[\"seen_tool_calls\"].add((func_name, func_args))\n                tools_dict[\"all_tool_calls\"].append(tool_call)\n\n        token_counts = Ollama._get_response_token_counts(r)\n        if token_counts:\n            r[\"usage\"] = token_counts\n\n        return ChatResponse(\n            message=Message(\n                content=tools_dict[\"response_txt\"],\n                role=r[\"message\"][\"role\"],\n                additional_kwargs={\"tool_calls\": tools_dict[\"all_tool_calls\"]},\n            ),\n            delta=r[\"message\"][\"content\"],\n            raw=r,\n        )\n\n    def stream_chat(self, messages: MessageList, **kwargs: Any) -&gt; ChatResponseGen:\n        \"\"\"Stream assistant deltas for a chat request.\n\n        Args:\n            messages (MessageList): Sequence of chat messages.\n            **kwargs (Any): Provider-specific options such as ``tools`` or ``format``.\n\n        Yields:\n            ChatResponse: Incremental responses with ``delta`` and cumulative content.\n\n        Examples:\n            - Stream deltas from a real Ollama server (requires server and model)\n                ```python\n                &gt;&gt;&gt; from serapeum.core.llms import Message, MessageRole\n                &gt;&gt;&gt; # Pre-requisites:\n                &gt;&gt;&gt; #   1) Start the server: `ollama serve`\n                &gt;&gt;&gt; #   2) Pull a model:    `ollama pull llama3.1`\n                &gt;&gt;&gt; llm = Ollama(model=\"llama3.1\", request_timeout=180)\n                &gt;&gt;&gt; chunks = list(llm.stream_chat([Message(role=MessageRole.USER, content=\"Say hello succinctly\")])) # doctest: +SKIP\n                &gt;&gt;&gt; chunks  # doctest: +SKIP\n                [ChatResponse(raw={'model': 'llama3.1', 'created_at': '2026-02-15T20:33:53.7035231Z', 'done': False, 'done_reason': None, 'total_duration': None, 'load_duration': None, 'prompt_eval_count': None, 'prompt_eval_duration': None, 'eval_count': None, 'eval_duration': None, 'message': Message(role='assistant', content='Hello', thinking=None, images=None, tool_name=None, tool_calls=None), 'logprobs': None}, likelihood_score=None, additional_kwargs={}, delta='Hello', message=Message(role=&lt;MessageRole.ASSISTANT: 'assistant'&gt;, additional_kwargs={'tool_calls': []}, chunks=[TextChunk(content='Hello', path=None, url=None, type='text')])),\n                 ChatResponse(raw={'model': 'llama3.1', 'created_at': '2026-02-15T20:33:53.7201343Z', 'done': False, 'done_reason': None, 'total_duration': None, 'load_duration': None, 'prompt_eval_count': None, 'prompt_eval_duration': None, 'eval_count': None, 'eval_duration': None, 'message': Message(role='assistant', content='!', thinking=None, images=None, tool_name=None, tool_calls=None), 'logprobs': None}, likelihood_score=None, additional_kwargs={}, delta='!', message=Message(role=&lt;MessageRole.ASSISTANT: 'assistant'&gt;, additional_kwargs={'tool_calls': []}, chunks=[TextChunk(content='Hello!', path=None, url=None, type='text')])),\n                 ChatResponse(raw={'model': 'llama3.1', 'created_at': '2026-02-15T20:33:53.7350848Z', 'done': True, 'done_reason': 'stop', 'total_duration': 2473382300, 'load_duration': 2159171600, 'prompt_eval_count': 14, 'prompt_eval_duration': 278611400, 'eval_count': 3, 'eval_duration': 29859300, 'message': Message(role='assistant', content='', thinking=None, images=None, tool_name=None, tool_calls=None), 'logprobs': None, 'usage': {'prompt_tokens': 14, 'completion_tokens': 3, 'total_tokens': 17}}, likelihood_score=None, additional_kwargs={}, delta='', message=Message(role=&lt;MessageRole.ASSISTANT: 'assistant'&gt;, additional_kwargs={'tool_calls': []}, chunks=[TextChunk(content='Hello!', path=None, url=None, type='text')]))]\n                &gt;&gt;&gt; isinstance(chunks[-1].message.content, str) and len(chunks) &gt;= 1    # doctest: +SKIP\n                True\n\n                ```\n        \"\"\"\n        ollama_messages = self._convert_to_ollama_messages(messages)\n\n        tools = kwargs.pop(\"tools\", None)\n        response_format = kwargs.pop(\"format\", \"json\" if self.json_mode else None)\n\n        def gen() -&gt; ChatResponseGen:\n            response = self.client.chat(\n                model=self.model,\n                messages=ollama_messages,\n                stream=True,\n                format=response_format,\n                tools=tools,\n                options=self._model_kwargs,\n                keep_alive=self.keep_alive,\n            )\n\n            tools_dict = {\n                \"response_txt\": \"\",\n                \"seen_tool_calls\": set(),\n                \"all_tool_calls\": [],\n            }\n\n            for r in response:\n                if r[\"message\"][\"content\"] is not None:\n                    yield self._parse_tool_call_response(tools_dict, r)\n\n        return gen()\n\n    async def astream_chat(\n        self, messages: MessageList, **kwargs: Any\n    ) -&gt; ChatResponseAsyncGen:\n        \"\"\"Asynchronously stream assistant deltas for a chat request.\n\n        Async variant of stream_chat that yields ChatResponse chunks as the model\n        generates content. Each chunk includes both the current delta and cumulative\n        content. Tool calls are de-duplicated across chunks.\n\n        Args:\n            messages: Sequence of chat messages forming the conversation context.\n            **kwargs: Provider-specific options such as:\n                - tools: List of tool specifications for function calling\n                - format: Response format (e.g., \"json\")\n\n        Returns:\n            Async generator yielding ChatResponse chunks with incremental deltas\n            and cumulative content.\n\n        Examples:\n            - Async stream chat responses\n                ```python\n                &gt;&gt;&gt; import asyncio\n                &gt;&gt;&gt; from serapeum.core.llms import Message, MessageRole\n                &gt;&gt;&gt; from serapeum.ollama import Ollama      # type: ignore\n                &gt;&gt;&gt; llm = Ollama(model=\"llama3.1\", request_timeout=120) # doctest: +SKIP\n                &gt;&gt;&gt; async def stream_example():\n                ...     chunks = []\n                ...     async for chunk in await llm.astream_chat([\n                ...         Message(role=MessageRole.USER, content=\"Count to 3\")\n                ...     ]):\n                ...         chunks.append(chunk.delta)\n                ...     return len(chunks) &gt; 0\n                &gt;&gt;&gt; asyncio.run(stream_example())  # Returns True   # doctest: +SKIP\n\n                ```\n\n        See Also:\n            stream_chat: Synchronous streaming variant.\n            achat: Non-streaming async chat.\n        \"\"\"\n        ollama_messages = self._convert_to_ollama_messages(messages)\n\n        tools = kwargs.pop(\"tools\", None)\n        response_format = kwargs.pop(\"format\", \"json\" if self.json_mode else None)\n\n        async def gen() -&gt; ChatResponseAsyncGen:\n            response = await self.async_client.chat(\n                model=self.model,\n                messages=ollama_messages,\n                stream=True,\n                format=response_format,\n                tools=tools,\n                options=self._model_kwargs,\n                keep_alive=self.keep_alive,\n            )\n\n            # Some client/mocking setups may return a coroutine that resolves to\n            # an async iterator; normalize by awaiting when needed.\n            if inspect.iscoroutine(response) and not hasattr(response, \"__aiter__\"):\n                response = await response\n\n            tools_dict = {\n                \"response_txt\": \"\",\n                \"seen_tool_calls\": set(),\n                \"all_tool_calls\": [],\n            }\n\n            async for r in response:\n                if r[\"message\"][\"content\"] is not None:\n                    yield self._parse_tool_call_response(tools_dict, r)\n\n        return gen()\n\n    async def achat(self, messages: MessageList, **kwargs: Any) -&gt; ChatResponse:\n        \"\"\"Asynchronously send a chat request and return the complete assistant message.\n\n        Async variant of the chat method that sends messages to Ollama and waits\n        for the complete response. Unlike astream_chat, this returns a single\n        ChatResponse with the full assistant message after generation completes.\n\n        Args:\n            messages: Sequence of chat messages forming the conversation context.\n            **kwargs: Provider-specific overrides such as:\n                - tools: List of tool specifications for function calling\n                - format: Response format (e.g., \"json\")\n\n        Returns:\n            ChatResponse containing the complete assistant message, any tool calls,\n            and optional token usage statistics.\n\n        Examples:\n            - Async chat with minimal setup\n                ```python\n                &gt;&gt;&gt; import asyncio\n                &gt;&gt;&gt; from serapeum.core.llms import Message, MessageRole\n                &gt;&gt;&gt; from serapeum.ollama import Ollama      # type: ignore\n                &gt;&gt;&gt; llm = Ollama(model=\"llama3.1\", request_timeout=120) # doctest: +SKIP\n                &gt;&gt;&gt; async def chat_example():   # doctest: +SKIP\n                ...     response = await llm.achat([\n                ...         Message(role=MessageRole.USER, content=\"Say hello\")\n                ...     ])\n                ...     return isinstance(response.message.content, str)\n                &gt;&gt;&gt; asyncio.run(chat_example())  # Returns True     # doctest: +SKIP\n\n                ```\n\n        See Also:\n            chat: Synchronous chat variant.\n            astream_chat: Async streaming variant.\n        \"\"\"\n        ollama_messages = self._convert_to_ollama_messages(messages)\n\n        tools = kwargs.pop(\"tools\", None)\n        response_format = kwargs.pop(\"format\", \"json\" if self.json_mode else None)\n\n        response = await self.async_client.chat(\n            model=self.model,\n            messages=ollama_messages,\n            stream=False,\n            format=response_format,\n            tools=tools,\n            options=self._model_kwargs,\n            keep_alive=self.keep_alive,\n        )\n\n        response = dict(response)\n\n        tool_calls = response[\"message\"].get(\"tool_calls\", [])\n        token_counts = self._get_response_token_counts(response)\n        if token_counts:\n            response[\"usage\"] = token_counts\n\n        return ChatResponse(\n            message=Message(\n                content=response[\"message\"][\"content\"],\n                role=response[\"message\"][\"role\"],\n                additional_kwargs={\"tool_calls\": tool_calls},\n            ),\n            raw=response,\n        )\n\n    def structured_predict(\n        self,\n        output_cls: type[BaseModel],\n        prompt: PromptTemplate,\n        llm_kwargs: dict[str, Any] | None = None,\n        **prompt_args: Any,\n    ) -&gt; BaseModel:\n        \"\"\"Generate structured output conforming to a Pydantic model schema.\n\n        Instructs the Ollama model to emit JSON matching the schema of output_cls,\n        then validates and parses the response into a Pydantic instance. When using\n        StructuredLLMMode.DEFAULT, this injects the model's JSON schema into the\n        format parameter and validates the response content.\n\n        Args:\n            output_cls: Target Pydantic model class defining the expected structure.\n            prompt: PromptTemplate that will be formatted with prompt_args to create messages.\n            llm_kwargs: Additional provider arguments passed to the chat method.\n                Defaults to empty dict.\n            **prompt_args: Template variables used to format the prompt.\n\n        Returns:\n            Instance of output_cls parsed and validated from the model's JSON response.\n\n        Raises:\n            ValidationError: If the model's response doesn't match the schema.\n\n        Examples:\n            - Extract structured data from unstructured text\n                ```python\n                &gt;&gt;&gt; from pydantic import BaseModel, Field\n                &gt;&gt;&gt; from serapeum.core.prompts import PromptTemplate\n                &gt;&gt;&gt; from serapeum.ollama import Ollama      # type: ignore\n                &gt;&gt;&gt; class Person(BaseModel):\n                ...     name: str = Field(description=\"Person's full name\")\n                ...     age: int = Field(description=\"Person's age in years\")\n                &gt;&gt;&gt; llm = Ollama(model=\"llama3.1\", request_timeout=120)     # doctest: +SKIP\n                &gt;&gt;&gt; prompt = PromptTemplate(\"Extract person info: {text}\")  # doctest: +SKIP\n                &gt;&gt;&gt; result = llm.structured_predict(    # doctest: +SKIP\n                ...     Person,\n                ...     prompt,\n                ...     text=\"John Doe is 30 years old\"\n                ... )\n                &gt;&gt;&gt; result  # doctest: +SKIP\n                Person(name='John Doe', age=30)\n\n                ```\n\n        See Also:\n            astructured_predict: Async variant.\n            stream_structured_predict: Streaming counterpart yielding partial models.\n        \"\"\"\n        if self.pydantic_program_mode == StructuredLLMMode.DEFAULT:\n            llm_kwargs = llm_kwargs or {}\n            llm_kwargs[\"format\"] = output_cls.model_json_schema()\n\n            messages = prompt.format_messages(**prompt_args)\n            response = self.chat(messages, **llm_kwargs)\n\n            return output_cls.model_validate_json(response.message.content or \"\")\n        else:\n            return super().structured_predict(\n                output_cls, prompt, llm_kwargs, **prompt_args\n            )\n\n    async def astructured_predict(\n        self,\n        output_cls: type[BaseModel],\n        prompt: PromptTemplate,\n        llm_kwargs: dict[str, Any] | None = None,\n        **prompt_args: Any,\n    ) -&gt; BaseModel:\n        \"\"\"Asynchronously generate structured output conforming to a Pydantic model schema.\n\n        Async variant of structured_predict. Instructs the Ollama model to emit JSON\n        matching the schema of output_cls, then validates and parses the response\n        into a Pydantic instance using the async chat interface.\n\n        Args:\n            output_cls: Target Pydantic model class defining the expected structure.\n            prompt: PromptTemplate that will be formatted with prompt_args to create messages.\n            llm_kwargs: Additional provider arguments passed to the achat method.\n                Defaults to empty dict.\n            **prompt_args: Template variables used to format the prompt.\n\n        Returns:\n            Instance of output_cls parsed and validated from the model's JSON response.\n\n        Raises:\n            ValidationError: If the model's response doesn't match the schema.\n\n        Examples:\n            - Async structured extraction\n                ```python\n                &gt;&gt;&gt; import asyncio\n                &gt;&gt;&gt; from pydantic import BaseModel\n                &gt;&gt;&gt; from serapeum.core.prompts import PromptTemplate\n                &gt;&gt;&gt; from serapeum.ollama import Ollama      # type: ignore\n                &gt;&gt;&gt; class City(BaseModel):\n                ...     name: str\n                ...     country: str\n                &gt;&gt;&gt; llm = Ollama(model=\"llama3.1\", request_timeout=120)  # doctest: +SKIP\n                &gt;&gt;&gt; async def extract_city():       # doctest: +SKIP\n                ...     prompt = PromptTemplate(\"Extract city: {text}\")\n                ...     result = await llm.astructured_predict(\n                ...         City,\n                ...         prompt,\n                ...         text=\"Paris is in France\"\n                ...     )\n                ...     return result.name == \"Paris\"\n                &gt;&gt;&gt; asyncio.run(extract_city())  # Returns True     # doctest: +SKIP\n\n                ```\n\n        See Also:\n            structured_predict: Synchronous variant.\n            astream_structured_predict: Async streaming variant.\n        \"\"\"\n        if self.pydantic_program_mode == StructuredLLMMode.DEFAULT:\n            llm_kwargs = llm_kwargs or {}\n            llm_kwargs[\"format\"] = output_cls.model_json_schema()\n\n            messages = prompt.format_messages(**prompt_args)\n            response = await self.achat(messages, **llm_kwargs)\n\n            return output_cls.model_validate_json(response.message.content or \"\")\n        else:\n            return await super().astructured_predict(\n                output_cls, prompt, llm_kwargs, **prompt_args\n            )\n\n    def stream_structured_predict(\n        self,\n        output_cls: type[BaseModel],\n        prompt: PromptTemplate,\n        llm_kwargs: dict[str, Any] | None = None,\n        **prompt_args: Any,\n    ) -&gt; Generator[BaseModel | list[BaseModel], None, None]:\n        \"\"\"Stream incrementally parsed structured objects as the model generates JSON.\n\n        Yields partially complete Pydantic instances as the model streams JSON content,\n        allowing early access to structured data before the full response completes.\n        Uses StreamingObjectProcessor with flexible mode to handle incomplete JSON.\n\n        Args:\n            output_cls: Pydantic model class defining the expected structure.\n            prompt: PromptTemplate that will be formatted with prompt_args to create messages.\n            llm_kwargs: Additional provider arguments passed to stream_chat.\n                Defaults to empty dict.\n            **prompt_args: Template variables used to format the prompt.\n\n        Yields:\n            Parsed Pydantic instance(s) - either a single BaseModel or list of BaseModel.\n            Each yielded value represents the current state of parsing as more JSON arrives.\n\n        Examples:\n            - Stream structured data as it's generated\n                ```python\n                &gt;&gt;&gt; from pydantic import BaseModel\n                &gt;&gt;&gt; from serapeum.core.prompts import PromptTemplate\n                &gt;&gt;&gt; from serapeum.ollama import Ollama      # type: ignore\n                &gt;&gt;&gt; class Summary(BaseModel):\n                ...     title: str\n                ...     points: list[str]\n                &gt;&gt;&gt; llm = Ollama(model=\"llama3.1\", request_timeout=120)     # doctest: +SKIP\n                &gt;&gt;&gt; prompt = PromptTemplate(\"Summarize: {text}\")\n                &gt;&gt;&gt; for obj in llm.stream_structured_predict(   # doctest: +SKIP\n                ...     Summary,\n                ...     prompt,\n                ...     text=\"Long article text...\"\n                ... ):\n                ...     # obj is a Summary instance, progressively more complete\n                ...     print(f\"Current title: {obj.title if hasattr(obj, 'title') else 'N/A'}\")\n\n                ```\n\n        See Also:\n            astream_structured_predict: Asynchronous streaming counterpart.\n            structured_predict: Non-streaming variant.\n        \"\"\"\n        if self.pydantic_program_mode == StructuredLLMMode.DEFAULT:\n\n            def gen(\n                output_cls: type[BaseModel],\n                prompt: PromptTemplate,\n                llm_kwargs: dict[str, Any] | None,\n                prompt_args: dict[str, Any],\n            ) -&gt; Generator[BaseModel | list[BaseModel], None, None]:\n                llm_kwargs = llm_kwargs or {}\n                llm_kwargs[\"format\"] = output_cls.model_json_schema()\n\n                messages = prompt.format_messages(**prompt_args)\n                response_gen = self.stream_chat(messages, **llm_kwargs)\n\n                cur_objects = None\n                for response in response_gen:\n                    try:\n                        processor = StreamingObjectProcessor(\n                            output_cls=output_cls,\n                            flexible_mode=True,\n                            allow_parallel_tool_calls=False,\n                        )\n                        objects = processor.process(response, cur_objects)\n\n                        cur_objects = (\n                            objects if isinstance(objects, list) else [objects]\n                        )\n                        yield objects\n                    except Exception:\n                        continue\n\n            return gen(output_cls, prompt, llm_kwargs, prompt_args)\n        else:\n            return super().stream_structured_predict(  # type: ignore[return-value]\n                output_cls, prompt, llm_kwargs, **prompt_args\n            )\n\n    async def astream_structured_predict(\n        self,\n        output_cls: type[BaseModel],\n        prompt: PromptTemplate,\n        llm_kwargs: dict[str, Any] | None = None,\n        **prompt_args: Any,\n    ) -&gt; AsyncGenerator[BaseModel | list[BaseModel], None]:\n        \"\"\"Asynchronously stream incrementally parsed structured objects as the model generates JSON.\n\n        Async variant of stream_structured_predict. Yields partially complete Pydantic\n        instances as the model streams JSON content, allowing early access to structured\n        data before the full response completes. Uses StreamingObjectProcessor with\n        flexible mode to handle incomplete JSON.\n\n        Args:\n            output_cls: Pydantic model class defining the expected structure.\n            prompt: PromptTemplate that will be formatted with prompt_args to create messages.\n            llm_kwargs: Additional provider arguments passed to astream_chat.\n                Defaults to empty dict.\n            **prompt_args: Template variables used to format the prompt.\n\n        Returns:\n            Async generator yielding parsed Pydantic instance(s) - either a single BaseModel\n            or list of BaseModel. Each yielded value represents the current state of parsing\n            as more JSON arrives.\n\n        Examples:\n            - Async stream structured data as it's generated\n                ```python\n                &gt;&gt;&gt; import asyncio\n                &gt;&gt;&gt; from pydantic import BaseModel\n                &gt;&gt;&gt; from serapeum.core.prompts import PromptTemplate\n                &gt;&gt;&gt; from serapeum.ollama import Ollama      # type: ignore\n                &gt;&gt;&gt; class Analysis(BaseModel):\n                ...     sentiment: str\n                ...     keywords: list[str]\n                &gt;&gt;&gt; llm = Ollama(model=\"llama3.1\", request_timeout=120)     # doctest: +SKIP\n                &gt;&gt;&gt; async def stream_analysis():\n                ...     prompt = PromptTemplate(\"Analyze: {text}\")\n                ...     async for obj in await llm.astream_structured_predict(\n                ...         Analysis,\n                ...         prompt,\n                ...         text=\"Product review text...\"\n                ...     ):\n                ...         # obj is progressively more complete\n                ...         print(f\"Sentiment: {obj.sentiment if hasattr(obj, 'sentiment') else 'pending'}\")\n                &gt;&gt;&gt; asyncio.run(stream_analysis())      # doctest: +SKIP\n\n                ```\n\n        See Also:\n            stream_structured_predict: Synchronous streaming counterpart.\n            astructured_predict: Non-streaming async variant.\n        \"\"\"\n        if self.pydantic_program_mode == StructuredLLMMode.DEFAULT:\n\n            async def gen(\n                output_cls: type[BaseModel],\n                prompt: PromptTemplate,\n                llm_kwargs: dict[str, Any] | None,\n                prompt_args: dict[str, Any],\n            ) -&gt; AsyncGenerator[BaseModel | list[BaseModel], None]:\n                llm_kwargs = llm_kwargs or {}\n                llm_kwargs[\"format\"] = output_cls.model_json_schema()\n\n                messages = prompt.format_messages(**prompt_args)\n                response_gen = await self.astream_chat(messages, **llm_kwargs)\n\n                cur_objects = None\n                async for response in response_gen:\n                    try:\n                        processor = StreamingObjectProcessor(\n                            output_cls=output_cls,\n                            flexible_mode=True,\n                            allow_parallel_tool_calls=False,\n                        )\n                        objects = processor.process(response, cur_objects)\n\n                        cur_objects = (\n                            objects if isinstance(objects, list) else [objects]\n                        )\n                        yield objects\n                    except Exception:\n                        continue\n\n            return gen(output_cls, prompt, llm_kwargs, prompt_args)\n        else:\n            # Fall back to non-streaming structured predict\n            return await super().astream_structured_predict(  # type: ignore[return-value]\n                output_cls, prompt, llm_kwargs, **prompt_args\n            )\n</code></pre>"},{"location":"reference/providers/ollama/api_reference/#serapeum.ollama.llm.Ollama.async_client","title":"<code>async_client</code>  <code>property</code>","text":"<p>Async Ollama client bound to the current asyncio event loop.</p> <p>This property lazily creates or reuses an AsyncClient instance, automatically handling event loop changes and closures. It's safe to call across different async contexts (e.g., multiple pytest-asyncio tests) as it detects closed loops and recreates the client as needed.</p> <p>Returns:</p> Type Description <code>AsyncClient</code> <p>The async client instance used for asynchronous operations.</p> <p>Examples:</p> <ul> <li>Access async client for manual API calls     <pre><code>&gt;&gt;&gt; import asyncio\n&gt;&gt;&gt; from serapeum.ollama import Ollama      # type: ignore\n&gt;&gt;&gt; llm = Ollama(model=\"llama3.1\")\n&gt;&gt;&gt; async def check_client():    # doctest: +SKIP\n...     client = llm.async_client\n...     return hasattr(client, \"chat\")\n&gt;&gt;&gt; asyncio.run(check_client())  # Returns True # doctest: +SKIP\n</code></pre></li> </ul> See Also <p>_ensure_async_client: Ensures the client matches the active event loop. client: Synchronous Ollama client property.</p>"},{"location":"reference/providers/ollama/api_reference/#serapeum.ollama.llm.Ollama.client","title":"<code>client</code>  <code>property</code>","text":"<p>Synchronous Ollama client lazily bound to <code>base_url</code>.</p> <p>Returns:</p> Name Type Description <code>Client</code> <code>Client</code> <p>Underlying Ollama client instance.</p> <p>Examples:</p> <ul> <li>Lazily create the client on first access     <pre><code>&gt;&gt;&gt; from serapeum.ollama import Ollama      # type: ignore[attr-defined]\n&gt;&gt;&gt; llm = Ollama(model=\"m\", base_url=\"http://localhost:11434\", request_timeout=1.0)\n&gt;&gt;&gt; c = llm.client  # doctest: +SKIP\n&gt;&gt;&gt; type(c).__name__  # doctest: +SKIP\n'Client'\n&gt;&gt;&gt; hasattr(c, \"chat\")  # doctest: +SKIP\nTrue\n</code></pre></li> </ul>"},{"location":"reference/providers/ollama/api_reference/#serapeum.ollama.llm.Ollama.metadata","title":"<code>metadata</code>  <code>property</code>","text":"<p>LLM metadata.</p> <p>Returns:</p> Name Type Description <code>Metadata</code> <code>Metadata</code> <p>Static capabilities such as context window and chat support.</p> <p>Examples:</p> <ul> <li>Inspect chat model capabilities     <pre><code>&gt;&gt;&gt; from serapeum.ollama import Ollama      # type: ignore[attr-defined]\n&gt;&gt;&gt; Ollama(model=\"m\").metadata.is_chat_model\nTrue\n</code></pre></li> </ul>"},{"location":"reference/providers/ollama/api_reference/#serapeum.ollama.llm.Ollama.__init__","title":"<code>__init__(model, base_url='http://localhost:11434', temperature=0.75, context_window=DEFAULT_CONTEXT_WINDOW, request_timeout=DEFAULT_REQUEST_TIMEOUT, prompt_key='prompt', json_mode=False, additional_kwargs=None, client=None, async_client=None, is_function_calling_model=True, keep_alive=None, **kwargs)</code>","text":"<p>Initialize the Ollama LLM adapter with configuration and optional clients.</p> <p>Creates an Ollama LLM instance with the specified model and connection parameters. Clients can be pre-constructed and passed in, or they will be created lazily on first use. The instance inherits from FunctionCallingLLM and ChatToCompletionMixin to provide complete chat and structured output support.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>str</code> <p>Identifier of the Ollama model (e.g., \"llama3.1\", \"mistral\").</p> required <code>base_url</code> <code>str</code> <p>URL where the Ollama server is hosted. Defaults to \"http://localhost:11434\".</p> <code>'http://localhost:11434'</code> <code>temperature</code> <code>float</code> <p>Sampling temperature between 0.0 (deterministic) and 1.0 (random). Defaults to 0.75.</p> <code>0.75</code> <code>context_window</code> <code>int</code> <p>Maximum context tokens the model can process. Defaults to DEFAULT_CONTEXT_WINDOW.</p> <code>DEFAULT_CONTEXT_WINDOW</code> <code>request_timeout</code> <code>float</code> <p>Timeout in seconds for API requests. Defaults to 60.0.</p> <code>DEFAULT_REQUEST_TIMEOUT</code> <code>prompt_key</code> <code>str</code> <p>Key used for prompt formatting in API calls. Defaults to \"prompt\".</p> <code>'prompt'</code> <code>json_mode</code> <code>bool</code> <p>Whether to request JSON-formatted responses via the format parameter. Defaults to False.</p> <code>False</code> <code>additional_kwargs</code> <code>dict[str, Any] | None</code> <p>Extra provider-specific options passed to Ollama under \"options\". Defaults to empty dict.</p> <code>None</code> <code>client</code> <code>Client | None</code> <p>Pre-constructed synchronous Ollama client. If None, created lazily.</p> <code>None</code> <code>async_client</code> <code>AsyncClient | None</code> <p>Pre-constructed asynchronous Ollama client. If None, created per event loop.</p> <code>None</code> <code>is_function_calling_model</code> <code>bool</code> <p>Whether this model supports tool/function calling. Defaults to True.</p> <code>True</code> <code>keep_alive</code> <code>float | str | None</code> <p>Duration to keep model loaded in memory (e.g., \"5m\", \"1h\", or float seconds). Defaults to \"5m\".</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Reserved for future extensions and base class compatibility.</p> <code>{}</code> <p>Examples:</p> <ul> <li>Initialize with minimal configuration     <pre><code>&gt;&gt;&gt; from serapeum.ollama import Ollama  # type: ignore[attr-defined]\n&gt;&gt;&gt; llm = Ollama(model=\"llama3.1\")\n&gt;&gt;&gt; llm.model\n'llama3.1'\n</code></pre></li> <li>Initialize with custom server and timeout     <pre><code>&gt;&gt;&gt; llm = Ollama(\n...     model=\"mistral\",\n...     base_url=\"http://custom-server:11434\",\n...     request_timeout=120.0,\n...     temperature=0.5\n... )\n&gt;&gt;&gt; llm.temperature\n0.5\n</code></pre></li> <li>Enable JSON mode for structured outputs     <pre><code>&gt;&gt;&gt; llm = Ollama(model=\"llama3.1\", json_mode=True)\n&gt;&gt;&gt; llm.json_mode\nTrue\n</code></pre></li> </ul> Source code in <code>libs/providers/ollama/src/serapeum/ollama/llm.py</code> <pre><code>def __init__(\n    self,\n    model: str,\n    base_url: str = \"http://localhost:11434\",\n    temperature: float = 0.75,\n    context_window: int = DEFAULT_CONTEXT_WINDOW,\n    request_timeout: float = DEFAULT_REQUEST_TIMEOUT,\n    prompt_key: str = \"prompt\",\n    json_mode: bool = False,\n    additional_kwargs: dict[str, Any] | None = None,\n    client: ollama_sdk.Client | None = None,                # type: ignore\n    async_client: ollama_sdk.AsyncClient | None = None,     # type: ignore\n    is_function_calling_model: bool = True,\n    keep_alive: float | str | None = None,\n    **kwargs: Any,\n) -&gt; None:\n    \"\"\"Initialize the Ollama LLM adapter with configuration and optional clients.\n\n    Creates an Ollama LLM instance with the specified model and connection\n    parameters. Clients can be pre-constructed and passed in, or they will\n    be created lazily on first use. The instance inherits from FunctionCallingLLM\n    and ChatToCompletionMixin to provide complete chat and structured output support.\n\n    Args:\n        model: Identifier of the Ollama model (e.g., \"llama3.1\", \"mistral\").\n        base_url: URL where the Ollama server is hosted. Defaults to \"http://localhost:11434\".\n        temperature: Sampling temperature between 0.0 (deterministic) and 1.0 (random).\n            Defaults to 0.75.\n        context_window: Maximum context tokens the model can process. Defaults to\n            DEFAULT_CONTEXT_WINDOW.\n        request_timeout: Timeout in seconds for API requests. Defaults to 60.0.\n        prompt_key: Key used for prompt formatting in API calls. Defaults to \"prompt\".\n        json_mode: Whether to request JSON-formatted responses via the format parameter.\n            Defaults to False.\n        additional_kwargs: Extra provider-specific options passed to Ollama under \"options\".\n            Defaults to empty dict.\n        client: Pre-constructed synchronous Ollama client. If None, created lazily.\n        async_client: Pre-constructed asynchronous Ollama client. If None, created per event loop.\n        is_function_calling_model: Whether this model supports tool/function calling.\n            Defaults to True.\n        keep_alive: Duration to keep model loaded in memory (e.g., \"5m\", \"1h\", or float seconds).\n            Defaults to \"5m\".\n        **kwargs: Reserved for future extensions and base class compatibility.\n\n    Examples:\n        - Initialize with minimal configuration\n            ```python\n            &gt;&gt;&gt; from serapeum.ollama import Ollama  # type: ignore[attr-defined]\n            &gt;&gt;&gt; llm = Ollama(model=\"llama3.1\")\n            &gt;&gt;&gt; llm.model\n            'llama3.1'\n\n            ```\n        - Initialize with custom server and timeout\n            ```python\n            &gt;&gt;&gt; llm = Ollama(\n            ...     model=\"mistral\",\n            ...     base_url=\"http://custom-server:11434\",\n            ...     request_timeout=120.0,\n            ...     temperature=0.5\n            ... )\n            &gt;&gt;&gt; llm.temperature\n            0.5\n\n            ```\n        - Enable JSON mode for structured outputs\n            ```python\n            &gt;&gt;&gt; llm = Ollama(model=\"llama3.1\", json_mode=True)\n            &gt;&gt;&gt; llm.json_mode\n            True\n\n            ```\n    \"\"\"\n    if additional_kwargs is None:\n        additional_kwargs = {}\n\n    super().__init__(\n        model=model,\n        base_url=base_url,\n        temperature=temperature,\n        context_window=context_window,\n        request_timeout=request_timeout,\n        prompt_key=prompt_key,\n        json_mode=json_mode,\n        additional_kwargs=additional_kwargs,\n        is_function_calling_model=is_function_calling_model,\n        keep_alive=keep_alive,\n        **kwargs,\n    )\n\n    self._client = client\n    self._async_client = async_client\n    # Track the event loop associated with the async client to avoid\n    # reusing a client bound to a closed event loop across tests/runs\n    self._async_client_loop: asyncio.AbstractEventLoop | None = None\n</code></pre>"},{"location":"reference/providers/ollama/api_reference/#serapeum.ollama.llm.Ollama.achat","title":"<code>achat(messages, **kwargs)</code>  <code>async</code>","text":"<p>Asynchronously send a chat request and return the complete assistant message.</p> <p>Async variant of the chat method that sends messages to Ollama and waits for the complete response. Unlike astream_chat, this returns a single ChatResponse with the full assistant message after generation completes.</p> <p>Parameters:</p> Name Type Description Default <code>messages</code> <code>MessageList</code> <p>Sequence of chat messages forming the conversation context.</p> required <code>**kwargs</code> <code>Any</code> <p>Provider-specific overrides such as: - tools: List of tool specifications for function calling - format: Response format (e.g., \"json\")</p> <code>{}</code> <p>Returns:</p> Type Description <code>ChatResponse</code> <p>ChatResponse containing the complete assistant message, any tool calls,</p> <code>ChatResponse</code> <p>and optional token usage statistics.</p> <p>Examples:</p> <ul> <li>Async chat with minimal setup     <pre><code>&gt;&gt;&gt; import asyncio\n&gt;&gt;&gt; from serapeum.core.llms import Message, MessageRole\n&gt;&gt;&gt; from serapeum.ollama import Ollama      # type: ignore\n&gt;&gt;&gt; llm = Ollama(model=\"llama3.1\", request_timeout=120) # doctest: +SKIP\n&gt;&gt;&gt; async def chat_example():   # doctest: +SKIP\n...     response = await llm.achat([\n...         Message(role=MessageRole.USER, content=\"Say hello\")\n...     ])\n...     return isinstance(response.message.content, str)\n&gt;&gt;&gt; asyncio.run(chat_example())  # Returns True     # doctest: +SKIP\n</code></pre></li> </ul> See Also <p>chat: Synchronous chat variant. astream_chat: Async streaming variant.</p> Source code in <code>libs/providers/ollama/src/serapeum/ollama/llm.py</code> <pre><code>async def achat(self, messages: MessageList, **kwargs: Any) -&gt; ChatResponse:\n    \"\"\"Asynchronously send a chat request and return the complete assistant message.\n\n    Async variant of the chat method that sends messages to Ollama and waits\n    for the complete response. Unlike astream_chat, this returns a single\n    ChatResponse with the full assistant message after generation completes.\n\n    Args:\n        messages: Sequence of chat messages forming the conversation context.\n        **kwargs: Provider-specific overrides such as:\n            - tools: List of tool specifications for function calling\n            - format: Response format (e.g., \"json\")\n\n    Returns:\n        ChatResponse containing the complete assistant message, any tool calls,\n        and optional token usage statistics.\n\n    Examples:\n        - Async chat with minimal setup\n            ```python\n            &gt;&gt;&gt; import asyncio\n            &gt;&gt;&gt; from serapeum.core.llms import Message, MessageRole\n            &gt;&gt;&gt; from serapeum.ollama import Ollama      # type: ignore\n            &gt;&gt;&gt; llm = Ollama(model=\"llama3.1\", request_timeout=120) # doctest: +SKIP\n            &gt;&gt;&gt; async def chat_example():   # doctest: +SKIP\n            ...     response = await llm.achat([\n            ...         Message(role=MessageRole.USER, content=\"Say hello\")\n            ...     ])\n            ...     return isinstance(response.message.content, str)\n            &gt;&gt;&gt; asyncio.run(chat_example())  # Returns True     # doctest: +SKIP\n\n            ```\n\n    See Also:\n        chat: Synchronous chat variant.\n        astream_chat: Async streaming variant.\n    \"\"\"\n    ollama_messages = self._convert_to_ollama_messages(messages)\n\n    tools = kwargs.pop(\"tools\", None)\n    response_format = kwargs.pop(\"format\", \"json\" if self.json_mode else None)\n\n    response = await self.async_client.chat(\n        model=self.model,\n        messages=ollama_messages,\n        stream=False,\n        format=response_format,\n        tools=tools,\n        options=self._model_kwargs,\n        keep_alive=self.keep_alive,\n    )\n\n    response = dict(response)\n\n    tool_calls = response[\"message\"].get(\"tool_calls\", [])\n    token_counts = self._get_response_token_counts(response)\n    if token_counts:\n        response[\"usage\"] = token_counts\n\n    return ChatResponse(\n        message=Message(\n            content=response[\"message\"][\"content\"],\n            role=response[\"message\"][\"role\"],\n            additional_kwargs={\"tool_calls\": tool_calls},\n        ),\n        raw=response,\n    )\n</code></pre>"},{"location":"reference/providers/ollama/api_reference/#serapeum.ollama.llm.Ollama.astream_chat","title":"<code>astream_chat(messages, **kwargs)</code>  <code>async</code>","text":"<p>Asynchronously stream assistant deltas for a chat request.</p> <p>Async variant of stream_chat that yields ChatResponse chunks as the model generates content. Each chunk includes both the current delta and cumulative content. Tool calls are de-duplicated across chunks.</p> <p>Parameters:</p> Name Type Description Default <code>messages</code> <code>MessageList</code> <p>Sequence of chat messages forming the conversation context.</p> required <code>**kwargs</code> <code>Any</code> <p>Provider-specific options such as: - tools: List of tool specifications for function calling - format: Response format (e.g., \"json\")</p> <code>{}</code> <p>Returns:</p> Type Description <code>ChatResponseAsyncGen</code> <p>Async generator yielding ChatResponse chunks with incremental deltas</p> <code>ChatResponseAsyncGen</code> <p>and cumulative content.</p> <p>Examples:</p> <ul> <li>Async stream chat responses     <pre><code>&gt;&gt;&gt; import asyncio\n&gt;&gt;&gt; from serapeum.core.llms import Message, MessageRole\n&gt;&gt;&gt; from serapeum.ollama import Ollama      # type: ignore\n&gt;&gt;&gt; llm = Ollama(model=\"llama3.1\", request_timeout=120) # doctest: +SKIP\n&gt;&gt;&gt; async def stream_example():\n...     chunks = []\n...     async for chunk in await llm.astream_chat([\n...         Message(role=MessageRole.USER, content=\"Count to 3\")\n...     ]):\n...         chunks.append(chunk.delta)\n...     return len(chunks) &gt; 0\n&gt;&gt;&gt; asyncio.run(stream_example())  # Returns True   # doctest: +SKIP\n</code></pre></li> </ul> See Also <p>stream_chat: Synchronous streaming variant. achat: Non-streaming async chat.</p> Source code in <code>libs/providers/ollama/src/serapeum/ollama/llm.py</code> <pre><code>async def astream_chat(\n    self, messages: MessageList, **kwargs: Any\n) -&gt; ChatResponseAsyncGen:\n    \"\"\"Asynchronously stream assistant deltas for a chat request.\n\n    Async variant of stream_chat that yields ChatResponse chunks as the model\n    generates content. Each chunk includes both the current delta and cumulative\n    content. Tool calls are de-duplicated across chunks.\n\n    Args:\n        messages: Sequence of chat messages forming the conversation context.\n        **kwargs: Provider-specific options such as:\n            - tools: List of tool specifications for function calling\n            - format: Response format (e.g., \"json\")\n\n    Returns:\n        Async generator yielding ChatResponse chunks with incremental deltas\n        and cumulative content.\n\n    Examples:\n        - Async stream chat responses\n            ```python\n            &gt;&gt;&gt; import asyncio\n            &gt;&gt;&gt; from serapeum.core.llms import Message, MessageRole\n            &gt;&gt;&gt; from serapeum.ollama import Ollama      # type: ignore\n            &gt;&gt;&gt; llm = Ollama(model=\"llama3.1\", request_timeout=120) # doctest: +SKIP\n            &gt;&gt;&gt; async def stream_example():\n            ...     chunks = []\n            ...     async for chunk in await llm.astream_chat([\n            ...         Message(role=MessageRole.USER, content=\"Count to 3\")\n            ...     ]):\n            ...         chunks.append(chunk.delta)\n            ...     return len(chunks) &gt; 0\n            &gt;&gt;&gt; asyncio.run(stream_example())  # Returns True   # doctest: +SKIP\n\n            ```\n\n    See Also:\n        stream_chat: Synchronous streaming variant.\n        achat: Non-streaming async chat.\n    \"\"\"\n    ollama_messages = self._convert_to_ollama_messages(messages)\n\n    tools = kwargs.pop(\"tools\", None)\n    response_format = kwargs.pop(\"format\", \"json\" if self.json_mode else None)\n\n    async def gen() -&gt; ChatResponseAsyncGen:\n        response = await self.async_client.chat(\n            model=self.model,\n            messages=ollama_messages,\n            stream=True,\n            format=response_format,\n            tools=tools,\n            options=self._model_kwargs,\n            keep_alive=self.keep_alive,\n        )\n\n        # Some client/mocking setups may return a coroutine that resolves to\n        # an async iterator; normalize by awaiting when needed.\n        if inspect.iscoroutine(response) and not hasattr(response, \"__aiter__\"):\n            response = await response\n\n        tools_dict = {\n            \"response_txt\": \"\",\n            \"seen_tool_calls\": set(),\n            \"all_tool_calls\": [],\n        }\n\n        async for r in response:\n            if r[\"message\"][\"content\"] is not None:\n                yield self._parse_tool_call_response(tools_dict, r)\n\n    return gen()\n</code></pre>"},{"location":"reference/providers/ollama/api_reference/#serapeum.ollama.llm.Ollama.astream_structured_predict","title":"<code>astream_structured_predict(output_cls, prompt, llm_kwargs=None, **prompt_args)</code>  <code>async</code>","text":"<p>Asynchronously stream incrementally parsed structured objects as the model generates JSON.</p> <p>Async variant of stream_structured_predict. Yields partially complete Pydantic instances as the model streams JSON content, allowing early access to structured data before the full response completes. Uses StreamingObjectProcessor with flexible mode to handle incomplete JSON.</p> <p>Parameters:</p> Name Type Description Default <code>output_cls</code> <code>type[BaseModel]</code> <p>Pydantic model class defining the expected structure.</p> required <code>prompt</code> <code>PromptTemplate</code> <p>PromptTemplate that will be formatted with prompt_args to create messages.</p> required <code>llm_kwargs</code> <code>dict[str, Any] | None</code> <p>Additional provider arguments passed to astream_chat. Defaults to empty dict.</p> <code>None</code> <code>**prompt_args</code> <code>Any</code> <p>Template variables used to format the prompt.</p> <code>{}</code> <p>Returns:</p> Type Description <code>AsyncGenerator[BaseModel | list[BaseModel], None]</code> <p>Async generator yielding parsed Pydantic instance(s) - either a single BaseModel</p> <code>AsyncGenerator[BaseModel | list[BaseModel], None]</code> <p>or list of BaseModel. Each yielded value represents the current state of parsing</p> <code>AsyncGenerator[BaseModel | list[BaseModel], None]</code> <p>as more JSON arrives.</p> <p>Examples:</p> <ul> <li>Async stream structured data as it's generated     <pre><code>&gt;&gt;&gt; import asyncio\n&gt;&gt;&gt; from pydantic import BaseModel\n&gt;&gt;&gt; from serapeum.core.prompts import PromptTemplate\n&gt;&gt;&gt; from serapeum.ollama import Ollama      # type: ignore\n&gt;&gt;&gt; class Analysis(BaseModel):\n...     sentiment: str\n...     keywords: list[str]\n&gt;&gt;&gt; llm = Ollama(model=\"llama3.1\", request_timeout=120)     # doctest: +SKIP\n&gt;&gt;&gt; async def stream_analysis():\n...     prompt = PromptTemplate(\"Analyze: {text}\")\n...     async for obj in await llm.astream_structured_predict(\n...         Analysis,\n...         prompt,\n...         text=\"Product review text...\"\n...     ):\n...         # obj is progressively more complete\n...         print(f\"Sentiment: {obj.sentiment if hasattr(obj, 'sentiment') else 'pending'}\")\n&gt;&gt;&gt; asyncio.run(stream_analysis())      # doctest: +SKIP\n</code></pre></li> </ul> See Also <p>stream_structured_predict: Synchronous streaming counterpart. astructured_predict: Non-streaming async variant.</p> Source code in <code>libs/providers/ollama/src/serapeum/ollama/llm.py</code> <pre><code>async def astream_structured_predict(\n    self,\n    output_cls: type[BaseModel],\n    prompt: PromptTemplate,\n    llm_kwargs: dict[str, Any] | None = None,\n    **prompt_args: Any,\n) -&gt; AsyncGenerator[BaseModel | list[BaseModel], None]:\n    \"\"\"Asynchronously stream incrementally parsed structured objects as the model generates JSON.\n\n    Async variant of stream_structured_predict. Yields partially complete Pydantic\n    instances as the model streams JSON content, allowing early access to structured\n    data before the full response completes. Uses StreamingObjectProcessor with\n    flexible mode to handle incomplete JSON.\n\n    Args:\n        output_cls: Pydantic model class defining the expected structure.\n        prompt: PromptTemplate that will be formatted with prompt_args to create messages.\n        llm_kwargs: Additional provider arguments passed to astream_chat.\n            Defaults to empty dict.\n        **prompt_args: Template variables used to format the prompt.\n\n    Returns:\n        Async generator yielding parsed Pydantic instance(s) - either a single BaseModel\n        or list of BaseModel. Each yielded value represents the current state of parsing\n        as more JSON arrives.\n\n    Examples:\n        - Async stream structured data as it's generated\n            ```python\n            &gt;&gt;&gt; import asyncio\n            &gt;&gt;&gt; from pydantic import BaseModel\n            &gt;&gt;&gt; from serapeum.core.prompts import PromptTemplate\n            &gt;&gt;&gt; from serapeum.ollama import Ollama      # type: ignore\n            &gt;&gt;&gt; class Analysis(BaseModel):\n            ...     sentiment: str\n            ...     keywords: list[str]\n            &gt;&gt;&gt; llm = Ollama(model=\"llama3.1\", request_timeout=120)     # doctest: +SKIP\n            &gt;&gt;&gt; async def stream_analysis():\n            ...     prompt = PromptTemplate(\"Analyze: {text}\")\n            ...     async for obj in await llm.astream_structured_predict(\n            ...         Analysis,\n            ...         prompt,\n            ...         text=\"Product review text...\"\n            ...     ):\n            ...         # obj is progressively more complete\n            ...         print(f\"Sentiment: {obj.sentiment if hasattr(obj, 'sentiment') else 'pending'}\")\n            &gt;&gt;&gt; asyncio.run(stream_analysis())      # doctest: +SKIP\n\n            ```\n\n    See Also:\n        stream_structured_predict: Synchronous streaming counterpart.\n        astructured_predict: Non-streaming async variant.\n    \"\"\"\n    if self.pydantic_program_mode == StructuredLLMMode.DEFAULT:\n\n        async def gen(\n            output_cls: type[BaseModel],\n            prompt: PromptTemplate,\n            llm_kwargs: dict[str, Any] | None,\n            prompt_args: dict[str, Any],\n        ) -&gt; AsyncGenerator[BaseModel | list[BaseModel], None]:\n            llm_kwargs = llm_kwargs or {}\n            llm_kwargs[\"format\"] = output_cls.model_json_schema()\n\n            messages = prompt.format_messages(**prompt_args)\n            response_gen = await self.astream_chat(messages, **llm_kwargs)\n\n            cur_objects = None\n            async for response in response_gen:\n                try:\n                    processor = StreamingObjectProcessor(\n                        output_cls=output_cls,\n                        flexible_mode=True,\n                        allow_parallel_tool_calls=False,\n                    )\n                    objects = processor.process(response, cur_objects)\n\n                    cur_objects = (\n                        objects if isinstance(objects, list) else [objects]\n                    )\n                    yield objects\n                except Exception:\n                    continue\n\n        return gen(output_cls, prompt, llm_kwargs, prompt_args)\n    else:\n        # Fall back to non-streaming structured predict\n        return await super().astream_structured_predict(  # type: ignore[return-value]\n            output_cls, prompt, llm_kwargs, **prompt_args\n        )\n</code></pre>"},{"location":"reference/providers/ollama/api_reference/#serapeum.ollama.llm.Ollama.astructured_predict","title":"<code>astructured_predict(output_cls, prompt, llm_kwargs=None, **prompt_args)</code>  <code>async</code>","text":"<p>Asynchronously generate structured output conforming to a Pydantic model schema.</p> <p>Async variant of structured_predict. Instructs the Ollama model to emit JSON matching the schema of output_cls, then validates and parses the response into a Pydantic instance using the async chat interface.</p> <p>Parameters:</p> Name Type Description Default <code>output_cls</code> <code>type[BaseModel]</code> <p>Target Pydantic model class defining the expected structure.</p> required <code>prompt</code> <code>PromptTemplate</code> <p>PromptTemplate that will be formatted with prompt_args to create messages.</p> required <code>llm_kwargs</code> <code>dict[str, Any] | None</code> <p>Additional provider arguments passed to the achat method. Defaults to empty dict.</p> <code>None</code> <code>**prompt_args</code> <code>Any</code> <p>Template variables used to format the prompt.</p> <code>{}</code> <p>Returns:</p> Type Description <code>BaseModel</code> <p>Instance of output_cls parsed and validated from the model's JSON response.</p> <p>Raises:</p> Type Description <code>ValidationError</code> <p>If the model's response doesn't match the schema.</p> <p>Examples:</p> <ul> <li>Async structured extraction     <pre><code>&gt;&gt;&gt; import asyncio\n&gt;&gt;&gt; from pydantic import BaseModel\n&gt;&gt;&gt; from serapeum.core.prompts import PromptTemplate\n&gt;&gt;&gt; from serapeum.ollama import Ollama      # type: ignore\n&gt;&gt;&gt; class City(BaseModel):\n...     name: str\n...     country: str\n&gt;&gt;&gt; llm = Ollama(model=\"llama3.1\", request_timeout=120)  # doctest: +SKIP\n&gt;&gt;&gt; async def extract_city():       # doctest: +SKIP\n...     prompt = PromptTemplate(\"Extract city: {text}\")\n...     result = await llm.astructured_predict(\n...         City,\n...         prompt,\n...         text=\"Paris is in France\"\n...     )\n...     return result.name == \"Paris\"\n&gt;&gt;&gt; asyncio.run(extract_city())  # Returns True     # doctest: +SKIP\n</code></pre></li> </ul> See Also <p>structured_predict: Synchronous variant. astream_structured_predict: Async streaming variant.</p> Source code in <code>libs/providers/ollama/src/serapeum/ollama/llm.py</code> <pre><code>async def astructured_predict(\n    self,\n    output_cls: type[BaseModel],\n    prompt: PromptTemplate,\n    llm_kwargs: dict[str, Any] | None = None,\n    **prompt_args: Any,\n) -&gt; BaseModel:\n    \"\"\"Asynchronously generate structured output conforming to a Pydantic model schema.\n\n    Async variant of structured_predict. Instructs the Ollama model to emit JSON\n    matching the schema of output_cls, then validates and parses the response\n    into a Pydantic instance using the async chat interface.\n\n    Args:\n        output_cls: Target Pydantic model class defining the expected structure.\n        prompt: PromptTemplate that will be formatted with prompt_args to create messages.\n        llm_kwargs: Additional provider arguments passed to the achat method.\n            Defaults to empty dict.\n        **prompt_args: Template variables used to format the prompt.\n\n    Returns:\n        Instance of output_cls parsed and validated from the model's JSON response.\n\n    Raises:\n        ValidationError: If the model's response doesn't match the schema.\n\n    Examples:\n        - Async structured extraction\n            ```python\n            &gt;&gt;&gt; import asyncio\n            &gt;&gt;&gt; from pydantic import BaseModel\n            &gt;&gt;&gt; from serapeum.core.prompts import PromptTemplate\n            &gt;&gt;&gt; from serapeum.ollama import Ollama      # type: ignore\n            &gt;&gt;&gt; class City(BaseModel):\n            ...     name: str\n            ...     country: str\n            &gt;&gt;&gt; llm = Ollama(model=\"llama3.1\", request_timeout=120)  # doctest: +SKIP\n            &gt;&gt;&gt; async def extract_city():       # doctest: +SKIP\n            ...     prompt = PromptTemplate(\"Extract city: {text}\")\n            ...     result = await llm.astructured_predict(\n            ...         City,\n            ...         prompt,\n            ...         text=\"Paris is in France\"\n            ...     )\n            ...     return result.name == \"Paris\"\n            &gt;&gt;&gt; asyncio.run(extract_city())  # Returns True     # doctest: +SKIP\n\n            ```\n\n    See Also:\n        structured_predict: Synchronous variant.\n        astream_structured_predict: Async streaming variant.\n    \"\"\"\n    if self.pydantic_program_mode == StructuredLLMMode.DEFAULT:\n        llm_kwargs = llm_kwargs or {}\n        llm_kwargs[\"format\"] = output_cls.model_json_schema()\n\n        messages = prompt.format_messages(**prompt_args)\n        response = await self.achat(messages, **llm_kwargs)\n\n        return output_cls.model_validate_json(response.message.content or \"\")\n    else:\n        return await super().astructured_predict(\n            output_cls, prompt, llm_kwargs, **prompt_args\n        )\n</code></pre>"},{"location":"reference/providers/ollama/api_reference/#serapeum.ollama.llm.Ollama.chat","title":"<code>chat(messages, **kwargs)</code>","text":"<p>Send a chat request to Ollama and return the assistant message.</p> <p>Parameters:</p> Name Type Description Default <code>messages</code> <code>MessageList</code> <p>Sequence of chat messages.</p> required <code>**kwargs</code> <code>Any</code> <p>Provider-specific overrides such as <code>tools</code> or <code>format</code>.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>ChatResponse</code> <code>ChatResponse</code> <p>Parsed response containing the assistant message and optional token usage.</p> <p>Examples:</p> <ul> <li>Minimal chat against a running Ollama server (requires server and model)     <pre><code>&gt;&gt;&gt; from serapeum.core.llms import Message, MessageRole\n&gt;&gt;&gt; # Ensure `ollama serve` is running and the model is available locally.\n&gt;&gt;&gt; llm = Ollama(model=\"llama3.1\", request_timeout=120)\n&gt;&gt;&gt; resp = llm.chat([Message(role=MessageRole.USER, content=\"hi\")])  # doctest: +SKIP\n&gt;&gt;&gt; print(resp)   # doctest: +SKIP\nHello! How are you today? Is there something I can help you with or would you like to chat?\n&gt;&gt;&gt; isinstance(resp.message.content, str)   # doctest: +SKIP\nTrue\n</code></pre></li> </ul> Source code in <code>libs/providers/ollama/src/serapeum/ollama/llm.py</code> <pre><code>def chat(self, messages: MessageList, **kwargs: Any) -&gt; ChatResponse:\n    \"\"\"Send a chat request to Ollama and return the assistant message.\n\n    Args:\n        messages (MessageList):\n            Sequence of chat messages.\n        **kwargs (Any):\n            Provider-specific overrides such as ``tools`` or ``format``.\n\n    Returns:\n        ChatResponse: Parsed response containing the assistant message and optional token usage.\n\n    Examples:\n        - Minimal chat against a running Ollama server (requires server and model)\n            ```python\n            &gt;&gt;&gt; from serapeum.core.llms import Message, MessageRole\n            &gt;&gt;&gt; # Ensure `ollama serve` is running and the model is available locally.\n            &gt;&gt;&gt; llm = Ollama(model=\"llama3.1\", request_timeout=120)\n            &gt;&gt;&gt; resp = llm.chat([Message(role=MessageRole.USER, content=\"hi\")])  # doctest: +SKIP\n            &gt;&gt;&gt; print(resp)   # doctest: +SKIP\n            Hello! How are you today? Is there something I can help you with or would you like to chat?\n            &gt;&gt;&gt; isinstance(resp.message.content, str)   # doctest: +SKIP\n            True\n\n            ```\n    \"\"\"\n    ollama_messages = self._convert_to_ollama_messages(messages)\n\n    tools = kwargs.pop(\"tools\", None)\n    response_format = kwargs.pop(\"format\", \"json\" if self.json_mode else None)\n\n    response = self.client.chat(\n        model=self.model,\n        messages=ollama_messages,\n        stream=False,\n        format=response_format,\n        tools=tools,\n        options=self._model_kwargs,\n        keep_alive=self.keep_alive,\n    )\n\n    response = dict(response)\n\n    tool_calls = response[\"message\"].get(\"tool_calls\", [])\n    token_counts = self._get_response_token_counts(response)\n    if token_counts:\n        response[\"usage\"] = token_counts\n\n    return ChatResponse(\n        message=Message(\n            content=response[\"message\"][\"content\"],\n            role=response[\"message\"][\"role\"],\n            additional_kwargs={\"tool_calls\": tool_calls},\n        ),\n        raw=response,\n    )\n</code></pre>"},{"location":"reference/providers/ollama/api_reference/#serapeum.ollama.llm.Ollama.class_name","title":"<code>class_name()</code>  <code>classmethod</code>","text":"<p>Return the registered class name for this provider adapter.</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Provider identifier used in registries or logs.</p> <p>Examples:</p> <ul> <li>Retrieve the class identifier     <pre><code>&gt;&gt;&gt; from serapeum.ollama import Ollama      # type: ignore[attr-defined]\n&gt;&gt;&gt; Ollama.class_name()\n'Ollama_llm'\n</code></pre></li> </ul> Source code in <code>libs/providers/ollama/src/serapeum/ollama/llm.py</code> <pre><code>@classmethod\ndef class_name(cls) -&gt; str:\n    \"\"\"Return the registered class name for this provider adapter.\n\n    Returns:\n        str: Provider identifier used in registries or logs.\n\n    Examples:\n        - Retrieve the class identifier\n            ```python\n            &gt;&gt;&gt; from serapeum.ollama import Ollama      # type: ignore[attr-defined]\n            &gt;&gt;&gt; Ollama.class_name()\n            'Ollama_llm'\n\n            ```\n    \"\"\"\n    return \"Ollama_llm\"\n</code></pre>"},{"location":"reference/providers/ollama/api_reference/#serapeum.ollama.llm.Ollama.get_tool_calls_from_response","title":"<code>get_tool_calls_from_response(response, error_on_no_tool_call=True)</code>","text":"<p>Extract tool call selections from a chat response.</p> <p>Parameters:</p> Name Type Description Default <code>response</code> <code>ChatResponse</code> <p>Response potentially containing tool calls.</p> required <code>error_on_no_tool_call</code> <code>bool</code> <p>Whether to raise when no tool calls are present.</p> <code>True</code> <p>Returns:</p> Type Description <code>list[ToolCallArguments]</code> <p>list[ToolCallArguments]: Parsed tool selections (empty when allowed and none present).</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>When <code>error_on_no_tool_call</code> is <code>True</code> and no tool calls exist.</p> <p>Examples:</p> <ul> <li>Parse a single tool call     <pre><code>&gt;&gt;&gt; from serapeum.core.llms import Message, MessageRole, ChatResponse\n&gt;&gt;&gt; llm = Ollama(model=\"m\")\n&gt;&gt;&gt; r = ChatResponse(\n...     message=Message(\n...         role=MessageRole.ASSISTANT,\n...         content=\"\",\n...         additional_kwargs={\n...             \"tool_calls\": [\n...                 {\"function\": {\"name\": \"run\", \"arguments\": {\"a\": 1}}}\n...             ]\n...         },\n...     )\n... )\n&gt;&gt;&gt; calls = llm.get_tool_calls_from_response(r)\n&gt;&gt;&gt; (\n...     calls[0].tool_id,\n...     calls[0].tool_name,\n...     calls[0].tool_kwargs[\"a\"],\n... ) == (\"run\", \"run\", 1)\nTrue\n</code></pre></li> <li>Raise when no tool call is present and errors are enabled     <pre><code>&gt;&gt;&gt; from serapeum.core.llms import Message, MessageRole, ChatResponse\n&gt;&gt;&gt; llm = Ollama(model=\"m\")\n&gt;&gt;&gt; empty = ChatResponse(message=Message(role=MessageRole.ASSISTANT, content=\"\"))\n&gt;&gt;&gt; try:\n...     _ = llm.get_tool_calls_from_response(empty, error_on_no_tool_call=True)\n... except ValueError as e:\n...     msg = str(e)\n&gt;&gt;&gt; 'Expected at least one tool call' in msg\nTrue\n</code></pre></li> </ul> Source code in <code>libs/providers/ollama/src/serapeum/ollama/llm.py</code> <pre><code>def get_tool_calls_from_response(\n    self,\n    response: \"ChatResponse\",\n    error_on_no_tool_call: bool = True,\n) -&gt; list[ToolCallArguments]:\n    \"\"\"Extract tool call selections from a chat response.\n\n    Args:\n        response (ChatResponse): Response potentially containing tool calls.\n        error_on_no_tool_call (bool): Whether to raise when no tool calls are present.\n\n    Returns:\n        list[ToolCallArguments]: Parsed tool selections (empty when allowed and none present).\n\n    Raises:\n        ValueError: When ``error_on_no_tool_call`` is ``True`` and no tool calls exist.\n\n    Examples:\n        - Parse a single tool call\n            ```python\n            &gt;&gt;&gt; from serapeum.core.llms import Message, MessageRole, ChatResponse\n            &gt;&gt;&gt; llm = Ollama(model=\"m\")\n            &gt;&gt;&gt; r = ChatResponse(\n            ...     message=Message(\n            ...         role=MessageRole.ASSISTANT,\n            ...         content=\"\",\n            ...         additional_kwargs={\n            ...             \"tool_calls\": [\n            ...                 {\"function\": {\"name\": \"run\", \"arguments\": {\"a\": 1}}}\n            ...             ]\n            ...         },\n            ...     )\n            ... )\n            &gt;&gt;&gt; calls = llm.get_tool_calls_from_response(r)\n            &gt;&gt;&gt; (\n            ...     calls[0].tool_id,\n            ...     calls[0].tool_name,\n            ...     calls[0].tool_kwargs[\"a\"],\n            ... ) == (\"run\", \"run\", 1)\n            True\n\n            ```\n        - Raise when no tool call is present and errors are enabled\n            ```python\n            &gt;&gt;&gt; from serapeum.core.llms import Message, MessageRole, ChatResponse\n            &gt;&gt;&gt; llm = Ollama(model=\"m\")\n            &gt;&gt;&gt; empty = ChatResponse(message=Message(role=MessageRole.ASSISTANT, content=\"\"))\n            &gt;&gt;&gt; try:\n            ...     _ = llm.get_tool_calls_from_response(empty, error_on_no_tool_call=True)\n            ... except ValueError as e:\n            ...     msg = str(e)\n            &gt;&gt;&gt; 'Expected at least one tool call' in msg\n            True\n\n            ```\n    \"\"\"\n    tool_calls = response.message.additional_kwargs.get(\"tool_calls\", [])\n    if not tool_calls or len(tool_calls) &lt; 1:\n        if error_on_no_tool_call:\n            raise ValueError(\n                f\"Expected at least one tool call, but got {len(tool_calls) if tool_calls else 0} tool calls.\"\n            )\n        else:\n            return []\n\n    tool_selections = []\n    coercer = ArgumentCoercer()\n\n    for tool_call in tool_calls:\n        # Coerce arguments to proper types (handles JSON strings, type mismatches, etc.)\n        raw_arguments = tool_call[\"function\"][\"arguments\"]\n        argument_dict = coercer.coerce(raw_arguments)\n\n        tool_selections.append(\n            ToolCallArguments(\n                # tool ids not provided by Ollama\n                tool_id=tool_call[\"function\"][\"name\"],\n                tool_name=tool_call[\"function\"][\"name\"],\n                tool_kwargs=argument_dict,\n            )\n        )\n\n    return tool_selections\n</code></pre>"},{"location":"reference/providers/ollama/api_reference/#serapeum.ollama.llm.Ollama.stream_chat","title":"<code>stream_chat(messages, **kwargs)</code>","text":"<p>Stream assistant deltas for a chat request.</p> <p>Parameters:</p> Name Type Description Default <code>messages</code> <code>MessageList</code> <p>Sequence of chat messages.</p> required <code>**kwargs</code> <code>Any</code> <p>Provider-specific options such as <code>tools</code> or <code>format</code>.</p> <code>{}</code> <p>Yields:</p> Name Type Description <code>ChatResponse</code> <code>ChatResponseGen</code> <p>Incremental responses with <code>delta</code> and cumulative content.</p> <p>Examples:</p> <ul> <li>Stream deltas from a real Ollama server (requires server and model)     <pre><code>&gt;&gt;&gt; from serapeum.core.llms import Message, MessageRole\n&gt;&gt;&gt; # Pre-requisites:\n&gt;&gt;&gt; #   1) Start the server: `ollama serve`\n&gt;&gt;&gt; #   2) Pull a model:    `ollama pull llama3.1`\n&gt;&gt;&gt; llm = Ollama(model=\"llama3.1\", request_timeout=180)\n&gt;&gt;&gt; chunks = list(llm.stream_chat([Message(role=MessageRole.USER, content=\"Say hello succinctly\")])) # doctest: +SKIP\n&gt;&gt;&gt; chunks  # doctest: +SKIP\n[ChatResponse(raw={'model': 'llama3.1', 'created_at': '2026-02-15T20:33:53.7035231Z', 'done': False, 'done_reason': None, 'total_duration': None, 'load_duration': None, 'prompt_eval_count': None, 'prompt_eval_duration': None, 'eval_count': None, 'eval_duration': None, 'message': Message(role='assistant', content='Hello', thinking=None, images=None, tool_name=None, tool_calls=None), 'logprobs': None}, likelihood_score=None, additional_kwargs={}, delta='Hello', message=Message(role=&lt;MessageRole.ASSISTANT: 'assistant'&gt;, additional_kwargs={'tool_calls': []}, chunks=[TextChunk(content='Hello', path=None, url=None, type='text')])),\n ChatResponse(raw={'model': 'llama3.1', 'created_at': '2026-02-15T20:33:53.7201343Z', 'done': False, 'done_reason': None, 'total_duration': None, 'load_duration': None, 'prompt_eval_count': None, 'prompt_eval_duration': None, 'eval_count': None, 'eval_duration': None, 'message': Message(role='assistant', content='!', thinking=None, images=None, tool_name=None, tool_calls=None), 'logprobs': None}, likelihood_score=None, additional_kwargs={}, delta='!', message=Message(role=&lt;MessageRole.ASSISTANT: 'assistant'&gt;, additional_kwargs={'tool_calls': []}, chunks=[TextChunk(content='Hello!', path=None, url=None, type='text')])),\n ChatResponse(raw={'model': 'llama3.1', 'created_at': '2026-02-15T20:33:53.7350848Z', 'done': True, 'done_reason': 'stop', 'total_duration': 2473382300, 'load_duration': 2159171600, 'prompt_eval_count': 14, 'prompt_eval_duration': 278611400, 'eval_count': 3, 'eval_duration': 29859300, 'message': Message(role='assistant', content='', thinking=None, images=None, tool_name=None, tool_calls=None), 'logprobs': None, 'usage': {'prompt_tokens': 14, 'completion_tokens': 3, 'total_tokens': 17}}, likelihood_score=None, additional_kwargs={}, delta='', message=Message(role=&lt;MessageRole.ASSISTANT: 'assistant'&gt;, additional_kwargs={'tool_calls': []}, chunks=[TextChunk(content='Hello!', path=None, url=None, type='text')]))]\n&gt;&gt;&gt; isinstance(chunks[-1].message.content, str) and len(chunks) &gt;= 1    # doctest: +SKIP\nTrue\n</code></pre></li> </ul> Source code in <code>libs/providers/ollama/src/serapeum/ollama/llm.py</code> <pre><code>def stream_chat(self, messages: MessageList, **kwargs: Any) -&gt; ChatResponseGen:\n    \"\"\"Stream assistant deltas for a chat request.\n\n    Args:\n        messages (MessageList): Sequence of chat messages.\n        **kwargs (Any): Provider-specific options such as ``tools`` or ``format``.\n\n    Yields:\n        ChatResponse: Incremental responses with ``delta`` and cumulative content.\n\n    Examples:\n        - Stream deltas from a real Ollama server (requires server and model)\n            ```python\n            &gt;&gt;&gt; from serapeum.core.llms import Message, MessageRole\n            &gt;&gt;&gt; # Pre-requisites:\n            &gt;&gt;&gt; #   1) Start the server: `ollama serve`\n            &gt;&gt;&gt; #   2) Pull a model:    `ollama pull llama3.1`\n            &gt;&gt;&gt; llm = Ollama(model=\"llama3.1\", request_timeout=180)\n            &gt;&gt;&gt; chunks = list(llm.stream_chat([Message(role=MessageRole.USER, content=\"Say hello succinctly\")])) # doctest: +SKIP\n            &gt;&gt;&gt; chunks  # doctest: +SKIP\n            [ChatResponse(raw={'model': 'llama3.1', 'created_at': '2026-02-15T20:33:53.7035231Z', 'done': False, 'done_reason': None, 'total_duration': None, 'load_duration': None, 'prompt_eval_count': None, 'prompt_eval_duration': None, 'eval_count': None, 'eval_duration': None, 'message': Message(role='assistant', content='Hello', thinking=None, images=None, tool_name=None, tool_calls=None), 'logprobs': None}, likelihood_score=None, additional_kwargs={}, delta='Hello', message=Message(role=&lt;MessageRole.ASSISTANT: 'assistant'&gt;, additional_kwargs={'tool_calls': []}, chunks=[TextChunk(content='Hello', path=None, url=None, type='text')])),\n             ChatResponse(raw={'model': 'llama3.1', 'created_at': '2026-02-15T20:33:53.7201343Z', 'done': False, 'done_reason': None, 'total_duration': None, 'load_duration': None, 'prompt_eval_count': None, 'prompt_eval_duration': None, 'eval_count': None, 'eval_duration': None, 'message': Message(role='assistant', content='!', thinking=None, images=None, tool_name=None, tool_calls=None), 'logprobs': None}, likelihood_score=None, additional_kwargs={}, delta='!', message=Message(role=&lt;MessageRole.ASSISTANT: 'assistant'&gt;, additional_kwargs={'tool_calls': []}, chunks=[TextChunk(content='Hello!', path=None, url=None, type='text')])),\n             ChatResponse(raw={'model': 'llama3.1', 'created_at': '2026-02-15T20:33:53.7350848Z', 'done': True, 'done_reason': 'stop', 'total_duration': 2473382300, 'load_duration': 2159171600, 'prompt_eval_count': 14, 'prompt_eval_duration': 278611400, 'eval_count': 3, 'eval_duration': 29859300, 'message': Message(role='assistant', content='', thinking=None, images=None, tool_name=None, tool_calls=None), 'logprobs': None, 'usage': {'prompt_tokens': 14, 'completion_tokens': 3, 'total_tokens': 17}}, likelihood_score=None, additional_kwargs={}, delta='', message=Message(role=&lt;MessageRole.ASSISTANT: 'assistant'&gt;, additional_kwargs={'tool_calls': []}, chunks=[TextChunk(content='Hello!', path=None, url=None, type='text')]))]\n            &gt;&gt;&gt; isinstance(chunks[-1].message.content, str) and len(chunks) &gt;= 1    # doctest: +SKIP\n            True\n\n            ```\n    \"\"\"\n    ollama_messages = self._convert_to_ollama_messages(messages)\n\n    tools = kwargs.pop(\"tools\", None)\n    response_format = kwargs.pop(\"format\", \"json\" if self.json_mode else None)\n\n    def gen() -&gt; ChatResponseGen:\n        response = self.client.chat(\n            model=self.model,\n            messages=ollama_messages,\n            stream=True,\n            format=response_format,\n            tools=tools,\n            options=self._model_kwargs,\n            keep_alive=self.keep_alive,\n        )\n\n        tools_dict = {\n            \"response_txt\": \"\",\n            \"seen_tool_calls\": set(),\n            \"all_tool_calls\": [],\n        }\n\n        for r in response:\n            if r[\"message\"][\"content\"] is not None:\n                yield self._parse_tool_call_response(tools_dict, r)\n\n    return gen()\n</code></pre>"},{"location":"reference/providers/ollama/api_reference/#serapeum.ollama.llm.Ollama.stream_structured_predict","title":"<code>stream_structured_predict(output_cls, prompt, llm_kwargs=None, **prompt_args)</code>","text":"<p>Stream incrementally parsed structured objects as the model generates JSON.</p> <p>Yields partially complete Pydantic instances as the model streams JSON content, allowing early access to structured data before the full response completes. Uses StreamingObjectProcessor with flexible mode to handle incomplete JSON.</p> <p>Parameters:</p> Name Type Description Default <code>output_cls</code> <code>type[BaseModel]</code> <p>Pydantic model class defining the expected structure.</p> required <code>prompt</code> <code>PromptTemplate</code> <p>PromptTemplate that will be formatted with prompt_args to create messages.</p> required <code>llm_kwargs</code> <code>dict[str, Any] | None</code> <p>Additional provider arguments passed to stream_chat. Defaults to empty dict.</p> <code>None</code> <code>**prompt_args</code> <code>Any</code> <p>Template variables used to format the prompt.</p> <code>{}</code> <p>Yields:</p> Type Description <code>BaseModel | list[BaseModel]</code> <p>Parsed Pydantic instance(s) - either a single BaseModel or list of BaseModel.</p> <code>BaseModel | list[BaseModel]</code> <p>Each yielded value represents the current state of parsing as more JSON arrives.</p> <p>Examples:</p> <ul> <li>Stream structured data as it's generated     <pre><code>&gt;&gt;&gt; from pydantic import BaseModel\n&gt;&gt;&gt; from serapeum.core.prompts import PromptTemplate\n&gt;&gt;&gt; from serapeum.ollama import Ollama      # type: ignore\n&gt;&gt;&gt; class Summary(BaseModel):\n...     title: str\n...     points: list[str]\n&gt;&gt;&gt; llm = Ollama(model=\"llama3.1\", request_timeout=120)     # doctest: +SKIP\n&gt;&gt;&gt; prompt = PromptTemplate(\"Summarize: {text}\")\n&gt;&gt;&gt; for obj in llm.stream_structured_predict(   # doctest: +SKIP\n...     Summary,\n...     prompt,\n...     text=\"Long article text...\"\n... ):\n...     # obj is a Summary instance, progressively more complete\n...     print(f\"Current title: {obj.title if hasattr(obj, 'title') else 'N/A'}\")\n</code></pre></li> </ul> See Also <p>astream_structured_predict: Asynchronous streaming counterpart. structured_predict: Non-streaming variant.</p> Source code in <code>libs/providers/ollama/src/serapeum/ollama/llm.py</code> <pre><code>def stream_structured_predict(\n    self,\n    output_cls: type[BaseModel],\n    prompt: PromptTemplate,\n    llm_kwargs: dict[str, Any] | None = None,\n    **prompt_args: Any,\n) -&gt; Generator[BaseModel | list[BaseModel], None, None]:\n    \"\"\"Stream incrementally parsed structured objects as the model generates JSON.\n\n    Yields partially complete Pydantic instances as the model streams JSON content,\n    allowing early access to structured data before the full response completes.\n    Uses StreamingObjectProcessor with flexible mode to handle incomplete JSON.\n\n    Args:\n        output_cls: Pydantic model class defining the expected structure.\n        prompt: PromptTemplate that will be formatted with prompt_args to create messages.\n        llm_kwargs: Additional provider arguments passed to stream_chat.\n            Defaults to empty dict.\n        **prompt_args: Template variables used to format the prompt.\n\n    Yields:\n        Parsed Pydantic instance(s) - either a single BaseModel or list of BaseModel.\n        Each yielded value represents the current state of parsing as more JSON arrives.\n\n    Examples:\n        - Stream structured data as it's generated\n            ```python\n            &gt;&gt;&gt; from pydantic import BaseModel\n            &gt;&gt;&gt; from serapeum.core.prompts import PromptTemplate\n            &gt;&gt;&gt; from serapeum.ollama import Ollama      # type: ignore\n            &gt;&gt;&gt; class Summary(BaseModel):\n            ...     title: str\n            ...     points: list[str]\n            &gt;&gt;&gt; llm = Ollama(model=\"llama3.1\", request_timeout=120)     # doctest: +SKIP\n            &gt;&gt;&gt; prompt = PromptTemplate(\"Summarize: {text}\")\n            &gt;&gt;&gt; for obj in llm.stream_structured_predict(   # doctest: +SKIP\n            ...     Summary,\n            ...     prompt,\n            ...     text=\"Long article text...\"\n            ... ):\n            ...     # obj is a Summary instance, progressively more complete\n            ...     print(f\"Current title: {obj.title if hasattr(obj, 'title') else 'N/A'}\")\n\n            ```\n\n    See Also:\n        astream_structured_predict: Asynchronous streaming counterpart.\n        structured_predict: Non-streaming variant.\n    \"\"\"\n    if self.pydantic_program_mode == StructuredLLMMode.DEFAULT:\n\n        def gen(\n            output_cls: type[BaseModel],\n            prompt: PromptTemplate,\n            llm_kwargs: dict[str, Any] | None,\n            prompt_args: dict[str, Any],\n        ) -&gt; Generator[BaseModel | list[BaseModel], None, None]:\n            llm_kwargs = llm_kwargs or {}\n            llm_kwargs[\"format\"] = output_cls.model_json_schema()\n\n            messages = prompt.format_messages(**prompt_args)\n            response_gen = self.stream_chat(messages, **llm_kwargs)\n\n            cur_objects = None\n            for response in response_gen:\n                try:\n                    processor = StreamingObjectProcessor(\n                        output_cls=output_cls,\n                        flexible_mode=True,\n                        allow_parallel_tool_calls=False,\n                    )\n                    objects = processor.process(response, cur_objects)\n\n                    cur_objects = (\n                        objects if isinstance(objects, list) else [objects]\n                    )\n                    yield objects\n                except Exception:\n                    continue\n\n        return gen(output_cls, prompt, llm_kwargs, prompt_args)\n    else:\n        return super().stream_structured_predict(  # type: ignore[return-value]\n            output_cls, prompt, llm_kwargs, **prompt_args\n        )\n</code></pre>"},{"location":"reference/providers/ollama/api_reference/#serapeum.ollama.llm.Ollama.structured_predict","title":"<code>structured_predict(output_cls, prompt, llm_kwargs=None, **prompt_args)</code>","text":"<p>Generate structured output conforming to a Pydantic model schema.</p> <p>Instructs the Ollama model to emit JSON matching the schema of output_cls, then validates and parses the response into a Pydantic instance. When using StructuredLLMMode.DEFAULT, this injects the model's JSON schema into the format parameter and validates the response content.</p> <p>Parameters:</p> Name Type Description Default <code>output_cls</code> <code>type[BaseModel]</code> <p>Target Pydantic model class defining the expected structure.</p> required <code>prompt</code> <code>PromptTemplate</code> <p>PromptTemplate that will be formatted with prompt_args to create messages.</p> required <code>llm_kwargs</code> <code>dict[str, Any] | None</code> <p>Additional provider arguments passed to the chat method. Defaults to empty dict.</p> <code>None</code> <code>**prompt_args</code> <code>Any</code> <p>Template variables used to format the prompt.</p> <code>{}</code> <p>Returns:</p> Type Description <code>BaseModel</code> <p>Instance of output_cls parsed and validated from the model's JSON response.</p> <p>Raises:</p> Type Description <code>ValidationError</code> <p>If the model's response doesn't match the schema.</p> <p>Examples:</p> <ul> <li>Extract structured data from unstructured text     <pre><code>&gt;&gt;&gt; from pydantic import BaseModel, Field\n&gt;&gt;&gt; from serapeum.core.prompts import PromptTemplate\n&gt;&gt;&gt; from serapeum.ollama import Ollama      # type: ignore\n&gt;&gt;&gt; class Person(BaseModel):\n...     name: str = Field(description=\"Person's full name\")\n...     age: int = Field(description=\"Person's age in years\")\n&gt;&gt;&gt; llm = Ollama(model=\"llama3.1\", request_timeout=120)     # doctest: +SKIP\n&gt;&gt;&gt; prompt = PromptTemplate(\"Extract person info: {text}\")  # doctest: +SKIP\n&gt;&gt;&gt; result = llm.structured_predict(    # doctest: +SKIP\n...     Person,\n...     prompt,\n...     text=\"John Doe is 30 years old\"\n... )\n&gt;&gt;&gt; result  # doctest: +SKIP\nPerson(name='John Doe', age=30)\n</code></pre></li> </ul> See Also <p>astructured_predict: Async variant. stream_structured_predict: Streaming counterpart yielding partial models.</p> Source code in <code>libs/providers/ollama/src/serapeum/ollama/llm.py</code> <pre><code>def structured_predict(\n    self,\n    output_cls: type[BaseModel],\n    prompt: PromptTemplate,\n    llm_kwargs: dict[str, Any] | None = None,\n    **prompt_args: Any,\n) -&gt; BaseModel:\n    \"\"\"Generate structured output conforming to a Pydantic model schema.\n\n    Instructs the Ollama model to emit JSON matching the schema of output_cls,\n    then validates and parses the response into a Pydantic instance. When using\n    StructuredLLMMode.DEFAULT, this injects the model's JSON schema into the\n    format parameter and validates the response content.\n\n    Args:\n        output_cls: Target Pydantic model class defining the expected structure.\n        prompt: PromptTemplate that will be formatted with prompt_args to create messages.\n        llm_kwargs: Additional provider arguments passed to the chat method.\n            Defaults to empty dict.\n        **prompt_args: Template variables used to format the prompt.\n\n    Returns:\n        Instance of output_cls parsed and validated from the model's JSON response.\n\n    Raises:\n        ValidationError: If the model's response doesn't match the schema.\n\n    Examples:\n        - Extract structured data from unstructured text\n            ```python\n            &gt;&gt;&gt; from pydantic import BaseModel, Field\n            &gt;&gt;&gt; from serapeum.core.prompts import PromptTemplate\n            &gt;&gt;&gt; from serapeum.ollama import Ollama      # type: ignore\n            &gt;&gt;&gt; class Person(BaseModel):\n            ...     name: str = Field(description=\"Person's full name\")\n            ...     age: int = Field(description=\"Person's age in years\")\n            &gt;&gt;&gt; llm = Ollama(model=\"llama3.1\", request_timeout=120)     # doctest: +SKIP\n            &gt;&gt;&gt; prompt = PromptTemplate(\"Extract person info: {text}\")  # doctest: +SKIP\n            &gt;&gt;&gt; result = llm.structured_predict(    # doctest: +SKIP\n            ...     Person,\n            ...     prompt,\n            ...     text=\"John Doe is 30 years old\"\n            ... )\n            &gt;&gt;&gt; result  # doctest: +SKIP\n            Person(name='John Doe', age=30)\n\n            ```\n\n    See Also:\n        astructured_predict: Async variant.\n        stream_structured_predict: Streaming counterpart yielding partial models.\n    \"\"\"\n    if self.pydantic_program_mode == StructuredLLMMode.DEFAULT:\n        llm_kwargs = llm_kwargs or {}\n        llm_kwargs[\"format\"] = output_cls.model_json_schema()\n\n        messages = prompt.format_messages(**prompt_args)\n        response = self.chat(messages, **llm_kwargs)\n\n        return output_cls.model_validate_json(response.message.content or \"\")\n    else:\n        return super().structured_predict(\n            output_cls, prompt, llm_kwargs, **prompt_args\n        )\n</code></pre>"},{"location":"reference/providers/ollama/api_reference/#serapeum.ollama.llm.force_single_tool_call","title":"<code>force_single_tool_call(response)</code>","text":"<p>Mutate a response to include at most a single tool call.</p> <p>Ollama may return multiple tool calls within a single assistant message. Some consumers require a single call at a time. This helper trims the list to the first occurrence in-place.</p> <p>Parameters:</p> Name Type Description Default <code>response</code> <code>ChatResponse</code> <p>Parsed chat response whose <code>message.additional_kwargs['tool_calls']</code> may contain multiple entries.</p> required <p>Returns:</p> Name Type Description <code>None</code> <code>None</code> <p>The function mutates <code>response</code> and returns nothing.</p> <p>Examples:</p> <ul> <li>Truncate multiple tool calls to one     <pre><code>&gt;&gt;&gt; from serapeum.core.llms import Message, MessageRole, ChatResponse\n&gt;&gt;&gt; r = ChatResponse(message=Message(\n...     role=MessageRole.ASSISTANT,\n...     content=\"\",\n...     additional_kwargs={\n...         \"tool_calls\": [\n...             {\"function\": {\"name\": \"a\", \"arguments\": {}}},\n...             {\"function\": {\"name\": \"b\", \"arguments\": {}}},\n...         ]\n...     },\n... ))\n&gt;&gt;&gt; force_single_tool_call(r)\n&gt;&gt;&gt; len(r.message.additional_kwargs.get(\"tool_calls\", []))\n1\n</code></pre></li> <li>Leave empty or single tool call lists unchanged     <pre><code>&gt;&gt;&gt; from serapeum.core.llms import Message, MessageRole, ChatResponse\n&gt;&gt;&gt; r = ChatResponse(message=Message(role=MessageRole.ASSISTANT, content=\"\"))\n&gt;&gt;&gt; force_single_tool_call(r)\n&gt;&gt;&gt; r.message.additional_kwargs.get(\"tool_calls\") is None or len(r.message.additional_kwargs.get(\"tool_calls\", [])) == 0\nTrue\n</code></pre></li> </ul> Source code in <code>libs/providers/ollama/src/serapeum/ollama/llm.py</code> <pre><code>def force_single_tool_call(response: ChatResponse) -&gt; None:\n    \"\"\"Mutate a response to include at most a single tool call.\n\n    Ollama may return multiple tool calls within a single assistant message. Some\n    consumers require a single call at a time. This helper trims the list to the\n    first occurrence in-place.\n\n    Args:\n        response (ChatResponse):\n            Parsed chat response whose ``message.additional_kwargs['tool_calls']``\n            may contain multiple entries.\n\n    Returns:\n        None: The function mutates ``response`` and returns nothing.\n\n    Examples:\n        - Truncate multiple tool calls to one\n            ```python\n            &gt;&gt;&gt; from serapeum.core.llms import Message, MessageRole, ChatResponse\n            &gt;&gt;&gt; r = ChatResponse(message=Message(\n            ...     role=MessageRole.ASSISTANT,\n            ...     content=\"\",\n            ...     additional_kwargs={\n            ...         \"tool_calls\": [\n            ...             {\"function\": {\"name\": \"a\", \"arguments\": {}}},\n            ...             {\"function\": {\"name\": \"b\", \"arguments\": {}}},\n            ...         ]\n            ...     },\n            ... ))\n            &gt;&gt;&gt; force_single_tool_call(r)\n            &gt;&gt;&gt; len(r.message.additional_kwargs.get(\"tool_calls\", []))\n            1\n\n            ```\n        - Leave empty or single tool call lists unchanged\n            ```python\n            &gt;&gt;&gt; from serapeum.core.llms import Message, MessageRole, ChatResponse\n            &gt;&gt;&gt; r = ChatResponse(message=Message(role=MessageRole.ASSISTANT, content=\"\"))\n            &gt;&gt;&gt; force_single_tool_call(r)\n            &gt;&gt;&gt; r.message.additional_kwargs.get(\"tool_calls\") is None or len(r.message.additional_kwargs.get(\"tool_calls\", [])) == 0\n            True\n\n            ```\n    \"\"\"\n    tool_calls = response.message.additional_kwargs.get(\"tool_calls\", [])\n    if tool_calls and len(tool_calls) &gt; 1:\n        response.message.additional_kwargs[\"tool_calls\"] = [tool_calls[0]]\n</code></pre>"},{"location":"reference/providers/ollama/api_reference/#serapeum.ollama.llm.get_additional_kwargs","title":"<code>get_additional_kwargs(response, exclude)</code>","text":"<p>Filter out excluded keys from a response dictionary.</p> <p>Parameters:</p> Name Type Description Default <code>response</code> <code>dict[str, Any]</code> <p>Source dictionary, typically a raw provider response.</p> required <code>exclude</code> <code>Tuple[str, ...]</code> <p>Keys that should be omitted from the returned mapping.</p> required <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>dict[str, Any]: A new dictionary containing only entries whose keys are not present in <code>exclude</code>.</p> <p>Examples:</p> <ul> <li>Keep only non-excluded keys     <pre><code>&gt;&gt;&gt; from serapeum.ollama.llm import get_additional_kwargs  # type: ignore\n&gt;&gt;&gt; get_additional_kwargs({\"a\": 1, \"b\": 2, \"keep\": 3}, (\"a\", \"b\"))\n{'keep': 3}\n</code></pre></li> <li>Return all keys when no exclusions are provided     <pre><code>&gt;&gt;&gt; get_additional_kwargs({\"x\": 10}, tuple())\n{'x': 10}\n</code></pre></li> </ul> Source code in <code>libs/providers/ollama/src/serapeum/ollama/llm.py</code> <pre><code>def get_additional_kwargs(\n    response: dict[str, Any], exclude: tuple[str, ...]\n) -&gt; dict[str, Any]:\n    \"\"\"Filter out excluded keys from a response dictionary.\n\n    Args:\n        response (dict[str, Any]):\n            Source dictionary, typically a raw provider response.\n        exclude (Tuple[str, ...]):\n            Keys that should be omitted from the returned mapping.\n\n    Returns:\n        dict[str, Any]:\n            A new dictionary containing only entries whose keys are not present in ``exclude``.\n\n    Examples:\n        - Keep only non-excluded keys\n            ```python\n            &gt;&gt;&gt; from serapeum.ollama.llm import get_additional_kwargs  # type: ignore\n            &gt;&gt;&gt; get_additional_kwargs({\"a\": 1, \"b\": 2, \"keep\": 3}, (\"a\", \"b\"))\n            {'keep': 3}\n\n            ```\n        - Return all keys when no exclusions are provided\n            ```python\n            &gt;&gt;&gt; get_additional_kwargs({\"x\": 10}, tuple())\n            {'x': 10}\n\n            ```\n    \"\"\"\n    return {k: v for k, v in response.items() if k not in exclude}\n</code></pre>"},{"location":"reference/providers/ollama/api_reference/#embeddings-module","title":"embeddings module","text":""},{"location":"reference/providers/ollama/api_reference/#serapeum.ollama.embedding","title":"<code>serapeum.ollama.embedding</code>","text":"<p>Ollama embeddings implementation for text and query vectorization.</p> <p>This module provides the OllamaEmbedding class for generating embeddings using Ollama models. It supports both symmetric and asymmetric embedding patterns, allowing different instructions for queries vs. documents to optimize retrieval performance. All operations support both synchronous and asynchronous execution.</p>"},{"location":"reference/providers/ollama/api_reference/#serapeum.ollama.embedding.OllamaEmbedding","title":"<code>OllamaEmbedding</code>","text":"<p>               Bases: <code>BaseEmbedding</code></p> <p>Ollama-based embedding model for generating text and query vector representations.</p> <p>This class provides a complete embedding interface using Ollama models, supporting both symmetric and asymmetric embedding patterns. Asymmetric embeddings allow different instructions for queries vs. documents, which can significantly improve retrieval accuracy by optimizing each embedding type for its specific role.</p> <p>The class manages both synchronous and asynchronous Ollama clients, automatically handling connection pooling and model lifecycle. All embedding operations support batching for efficient processing of multiple texts.</p> <p>Attributes:</p> Name Type Description <code>base_url</code> <code>str</code> <p>Base URL where Ollama is hosted. Defaults to \"http://localhost:11434\".</p> <code>model_name</code> <code>str</code> <p>Name of the Ollama model to use for embeddings (e.g., \"nomic-embed-text\").</p> <code>ollama_additional_kwargs</code> <code>dict[str, Any]</code> <p>Additional options passed to Ollama's embed API.</p> <code>query_instruction</code> <code>str | None</code> <p>Optional instruction prepended to search queries for asymmetric embedding. Example: \"search_query:\" or \"Represent this query for retrieval.\"</p> <code>text_instruction</code> <code>str | None</code> <p>Optional instruction prepended to documents for asymmetric embedding. Example: \"search_document:\" or \"Represent this document:\".</p> <code>keep_alive</code> <code>float | str | None</code> <p>Duration to keep model loaded in memory after requests. Can be a duration string (e.g., \"5m\", \"1h\") or float (seconds). Defaults to \"5m\".</p> <code>client_kwargs</code> <code>dict[str, Any]</code> <p>Additional kwargs for Ollama client initialization.</p> <p>Examples:</p> <ul> <li>Basic embedding with default settings     <pre><code>&gt;&gt;&gt; from serapeum.ollama import OllamaEmbedding         # type: ignore\n&gt;&gt;&gt; embedder = OllamaEmbedding(model_name=\"nomic-embed-text\")  # doctest: +SKIP\n&gt;&gt;&gt; embedding = embedder.get_text_embedding(\"Hello world\")  # doctest: +SKIP\n&gt;&gt;&gt; len(embedding) &gt; 0  # doctest: +SKIP\nTrue\n</code></pre></li> <li>Asymmetric embeddings for retrieval     <pre><code>&gt;&gt;&gt; from serapeum.ollama import OllamaEmbedding     # type: ignore\n&gt;&gt;&gt; embedder = OllamaEmbedding(  # doctest: +SKIP\n...     model_name=\"nomic-embed-text\",\n...     query_instruction=\"search_query:\",\n...     text_instruction=\"search_document:\"\n... )\n</code></pre><ul> <li>Query embedding optimized for search <pre><code>&gt;&gt;&gt; query_emb = embedder.get_query_embedding(\"What is Python?\")  # doctest: +SKIP\n[0.011942148,\n 0.023432752,\n -0.14708464,\n 0.0131236715,\n ...\n -0.01889455,\n -0.00888129]\n</code></pre></li> <li>Document embedding optimized for retrieval <pre><code>&gt;&gt;&gt; doc_emb = embedder.get_text_embedding(\"Python is a programming language\")  # doctest: +SKIP\n[0.011555489,\n 0.02302966,\n -0.104428634,\n -0.04198733,\n -0.047405742]\n</code></pre></li> </ul> </li> <li>Batch processing with async     <pre><code>&gt;&gt;&gt; import asyncio\n&gt;&gt;&gt; from serapeum.ollama import OllamaEmbedding     # type: ignore\n&gt;&gt;&gt; embedder = OllamaEmbedding(model_name=\"nomic-embed-text\")  # doctest: +SKIP\n&gt;&gt;&gt; async def embed_batch():  # doctest: +SKIP\n...     texts = [\"First doc\", \"Second doc\", \"Third doc\"]\n...     embeddings = await embedder._aget_text_embeddings(texts)\n...     return len(embeddings)\n&gt;&gt;&gt; asyncio.run(embed_batch())  # Returns 3 # doctest : +SKIP\n</code></pre></li> <li>Custom Ollama server configuration     <pre><code>&gt;&gt;&gt; from serapeum.ollama import OllamaEmbedding         # type: ignore\n&gt;&gt;&gt; embedder = OllamaEmbedding(  # doctest: +SKIP\n...     model_name=\"llama2\",\n...     base_url=\"http://custom-server:11434\",\n...     keep_alive=\"10m\",\n...     ollama_additional_kwargs={\"temperature\": 0.0}\n... )\n</code></pre></li> </ul> See Also <p>BaseEmbedding: Abstract base class defining the embedding interface. serapeum.core.embeddings: Core embedding abstractions and utilities.</p> Source code in <code>libs/providers/ollama/src/serapeum/ollama/embedding.py</code> <pre><code>class OllamaEmbedding(BaseEmbedding):\n    \"\"\"Ollama-based embedding model for generating text and query vector representations.\n\n    This class provides a complete embedding interface using Ollama models, supporting\n    both symmetric and asymmetric embedding patterns. Asymmetric embeddings allow\n    different instructions for queries vs. documents, which can significantly improve\n    retrieval accuracy by optimizing each embedding type for its specific role.\n\n    The class manages both synchronous and asynchronous Ollama clients, automatically\n    handling connection pooling and model lifecycle. All embedding operations support\n    batching for efficient processing of multiple texts.\n\n    Attributes:\n        base_url: Base URL where Ollama is hosted. Defaults to \"http://localhost:11434\".\n        model_name: Name of the Ollama model to use for embeddings (e.g., \"nomic-embed-text\").\n        ollama_additional_kwargs: Additional options passed to Ollama's embed API.\n        query_instruction: Optional instruction prepended to search queries for asymmetric\n            embedding. Example: \"search_query:\" or \"Represent this query for retrieval.\"\n        text_instruction: Optional instruction prepended to documents for asymmetric\n            embedding. Example: \"search_document:\" or \"Represent this document:\".\n        keep_alive: Duration to keep model loaded in memory after requests. Can be a\n            duration string (e.g., \"5m\", \"1h\") or float (seconds). Defaults to \"5m\".\n        client_kwargs: Additional kwargs for Ollama client initialization.\n\n    Examples:\n        - Basic embedding with default settings\n            ```python\n            &gt;&gt;&gt; from serapeum.ollama import OllamaEmbedding         # type: ignore\n            &gt;&gt;&gt; embedder = OllamaEmbedding(model_name=\"nomic-embed-text\")  # doctest: +SKIP\n            &gt;&gt;&gt; embedding = embedder.get_text_embedding(\"Hello world\")  # doctest: +SKIP\n            &gt;&gt;&gt; len(embedding) &gt; 0  # doctest: +SKIP\n            True\n\n            ```\n        - Asymmetric embeddings for retrieval\n            ```python\n            &gt;&gt;&gt; from serapeum.ollama import OllamaEmbedding     # type: ignore\n            &gt;&gt;&gt; embedder = OllamaEmbedding(  # doctest: +SKIP\n            ...     model_name=\"nomic-embed-text\",\n            ...     query_instruction=\"search_query:\",\n            ...     text_instruction=\"search_document:\"\n            ... )\n\n            ```\n            -  Query embedding optimized for search\n            ```python\n            &gt;&gt;&gt; query_emb = embedder.get_query_embedding(\"What is Python?\")  # doctest: +SKIP\n            [0.011942148,\n             0.023432752,\n             -0.14708464,\n             0.0131236715,\n             ...\n             -0.01889455,\n             -0.00888129]\n\n            ```\n            - Document embedding optimized for retrieval\n            ```python\n            &gt;&gt;&gt; doc_emb = embedder.get_text_embedding(\"Python is a programming language\")  # doctest: +SKIP\n            [0.011555489,\n             0.02302966,\n             -0.104428634,\n             -0.04198733,\n             -0.047405742]\n\n            ```\n        - Batch processing with async\n            ```python\n            &gt;&gt;&gt; import asyncio\n            &gt;&gt;&gt; from serapeum.ollama import OllamaEmbedding     # type: ignore\n            &gt;&gt;&gt; embedder = OllamaEmbedding(model_name=\"nomic-embed-text\")  # doctest: +SKIP\n            &gt;&gt;&gt; async def embed_batch():  # doctest: +SKIP\n            ...     texts = [\"First doc\", \"Second doc\", \"Third doc\"]\n            ...     embeddings = await embedder._aget_text_embeddings(texts)\n            ...     return len(embeddings)\n            &gt;&gt;&gt; asyncio.run(embed_batch())  # Returns 3 # doctest : +SKIP\n\n            ```\n        - Custom Ollama server configuration\n            ```python\n            &gt;&gt;&gt; from serapeum.ollama import OllamaEmbedding         # type: ignore\n            &gt;&gt;&gt; embedder = OllamaEmbedding(  # doctest: +SKIP\n            ...     model_name=\"llama2\",\n            ...     base_url=\"http://custom-server:11434\",\n            ...     keep_alive=\"10m\",\n            ...     ollama_additional_kwargs={\"temperature\": 0.0}\n            ... )\n\n            ```\n\n    See Also:\n        BaseEmbedding: Abstract base class defining the embedding interface.\n        serapeum.core.embeddings: Core embedding abstractions and utilities.\n    \"\"\"\n\n    base_url: str = Field(\n        default=\"http://localhost:11434\",\n        description=\"Base url the model is hosted by Ollama\",\n    )\n    model_name: str = Field(description=\"The Ollama model to use.\")\n    ollama_additional_kwargs: dict[str, Any] = Field(\n        default_factory=dict, description=\"Additional kwargs for the Ollama API.\"\n    )\n    query_instruction: str | None = Field(\n        default=None,\n        description=(\n            \"Instruction to prepend to search queries for asymmetric embedding. \"\n            \"Used by get_query_embedding() when embedding user questions/searches. \"\n            \"Example: 'search_query:' or 'Represent this query for retrieving relevant documents:'. \"\n            \"This helps the model optimize query embeddings to match against document embeddings.\"\n        ),\n    )\n    text_instruction: str | None = Field(\n        default=None,\n        description=(\n            \"Instruction to prepend to documents/text for asymmetric embedding. \"\n            \"Used by get_text_embedding() when embedding documents to be searched. \"\n            \"Example: 'search_document:' or 'Represent this document for retrieval:'. \"\n            \"This helps the model create document embeddings optimized for retrieval.\"\n        ),\n    )\n    keep_alive: float | str | None = Field(\n        default=\"5m\",\n        description=\"controls how long the model will stay loaded into memory following the request(default: 5m)\",\n    )\n    client_kwargs: dict[str, Any] = Field(\n        default_factory=dict,\n        description=\"Additional kwargs for the Ollama client initialization.\",\n    )\n\n    _client: ollama_sdk.Client = PrivateAttr()      # type: ignore\n    _async_client: ollama_sdk.AsyncClient = PrivateAttr()       # type: ignore\n\n    @model_validator(mode=\"after\")\n    def _initialize_clients(self) -&gt; OllamaEmbedding:\n        \"\"\"Initialize Ollama synchronous and asynchronous clients after model validation.\n\n        This validator runs automatically after all fields are validated during\n        instance creation. It creates both sync and async Ollama clients configured\n        with the specified base_url and any additional client kwargs.\n\n        Returns:\n            The OllamaEmbedding instance with initialized clients.\n\n        Examples:\n            - Clients are initialized automatically on instantiation\n                ```python\n                &gt;&gt;&gt; from serapeum.ollama import OllamaEmbedding     # type: ignore\n                &gt;&gt;&gt; embedder = OllamaEmbedding(model_name=\"nomic-embed-text\")  # doctest: +SKIP\n                &gt;&gt;&gt; # Both _client and _async_client are now initialized\n\n                ```\n        \"\"\"\n        self._client = ollama_sdk.Client(host=self.base_url, **self.client_kwargs)      # type: ignore\n        self._async_client = ollama_sdk.AsyncClient(host=self.base_url, **self.client_kwargs)       # type: ignore\n        return self\n\n    @classmethod\n    def class_name(cls) -&gt; str:\n        \"\"\"Return the canonical class name for this embedding implementation.\n\n        Returns:\n            The string \"OllamaEmbedding\".\n\n        Examples:\n            - Get the class name identifier\n                ```python\n                &gt;&gt;&gt; from serapeum.ollama import OllamaEmbedding     # type: ignore\n                &gt;&gt;&gt; OllamaEmbedding.class_name()\n                'OllamaEmbedding'\n\n                ```\n        \"\"\"\n        return \"OllamaEmbedding\"\n\n    def _get_query_embedding(self, query: str) -&gt; Sequence[float]:\n        \"\"\"Generate an embedding vector for a search query.\n\n        Formats the query with the optional query_instruction prefix (if configured)\n        to optimize the embedding for search/retrieval tasks, then generates the\n        embedding vector using the Ollama model.\n\n        Args:\n            query: The search query text to embed.\n\n        Returns:\n            A sequence of floats representing the query's embedding vector.\n\n        Raises:\n            ValueError: If the query is empty or whitespace-only.\n\n        Examples:\n            - Embed a search query\n                ```python\n                &gt;&gt;&gt; from serapeum.ollama import OllamaEmbedding     # type: ignore\n                &gt;&gt;&gt; embedder = OllamaEmbedding(  # doctest: +SKIP\n                ...     model_name=\"nomic-embed-text\",\n                ...     query_instruction=\"search_query:\"\n                ... )\n                &gt;&gt;&gt; query_vec = embedder.get_query_embedding(\"What is machine learning?\")  # doctest: +SKIP\n                &gt;&gt;&gt; len(query_vec) &gt; 0  # doctest: +SKIP\n                True\n\n                ```\n\n        See Also:\n            _aget_query_embedding: Async version of this method.\n            _format_query: Formats query with instruction prefix.\n        \"\"\"\n        formatted_query = self._format_query(query)\n        return self._embed_raw(formatted_query)\n\n    async def _aget_query_embedding(self, query: str) -&gt; Sequence[float]:\n        \"\"\"Asynchronously generate an embedding vector for a search query.\n\n        Async version of _get_query_embedding. Formats the query with the optional\n        query_instruction prefix, then generates the embedding using async Ollama client.\n\n        Args:\n            query: The search query text to embed.\n\n        Returns:\n            A sequence of floats representing the query's embedding vector.\n\n        Raises:\n            ValueError: If the query is empty or whitespace-only.\n\n        Examples:\n            - Async query embedding\n                ```python\n                &gt;&gt;&gt; import asyncio\n                &gt;&gt;&gt; from serapeum.ollama import OllamaEmbedding     # type: ignore\n                &gt;&gt;&gt; embedder = OllamaEmbedding(model_name=\"nomic-embed-text\")  # doctest: +SKIP\n                &gt;&gt;&gt; async def embed_query():  # doctest: +SKIP\n                ...     vec = await embedder.aget_query_embedding(\"neural networks\")\n                ...     return len(vec) &gt; 0\n                &gt;&gt;&gt; # asyncio.run(embed_query())  # Returns True\n\n                ```\n\n        See Also:\n            _get_query_embedding: Synchronous version of this method.\n            _format_query: Formats query with instruction prefix.\n        \"\"\"\n        formatted_query = self._format_query(query)\n        return await self._a_embed_raw(formatted_query)\n\n    def _get_text_embedding(self, text: str) -&gt; Sequence[float]:\n        \"\"\"Generate an embedding vector for a document or text passage.\n\n        Formats the text with the optional text_instruction prefix (if configured)\n        to optimize the embedding for document retrieval, then generates the\n        embedding vector using the Ollama model.\n\n        Args:\n            text: The document or text passage to embed.\n\n        Returns:\n            A sequence of floats representing the text's embedding vector.\n\n        Raises:\n            ValueError: If the text is empty or whitespace-only.\n\n        Examples:\n            - Embed a document\n                ```python\n                &gt;&gt;&gt; from serapeum.ollama import OllamaEmbedding     # type: ignore\n                &gt;&gt;&gt; embedder = OllamaEmbedding(  # doctest: +SKIP\n                ...     model_name=\"nomic-embed-text\",\n                ...     text_instruction=\"search_document:\"\n                ... )\n                &gt;&gt;&gt; doc_vec = embedder.get_text_embedding(\"Python is a programming language\")  # doctest: +SKIP\n                &gt;&gt;&gt; len(doc_vec) &gt; 0  # doctest: +SKIP\n                True\n\n                ```\n\n        See Also:\n            _aget_text_embedding: Async version of this method.\n            _format_text: Formats text with instruction prefix.\n        \"\"\"\n        formatted_text = self._format_text(text)\n        return self._embed_raw(formatted_text)\n\n    async def _aget_text_embedding(self, text: str) -&gt; Sequence[float]:\n        \"\"\"Asynchronously generate an embedding vector for a document or text passage.\n\n        Async version of _get_text_embedding. Formats the text with the optional\n        text_instruction prefix, then generates the embedding using async Ollama client.\n\n        Args:\n            text: The document or text passage to embed.\n\n        Returns:\n            A sequence of floats representing the text's embedding vector.\n\n        Raises:\n            ValueError: If the text is empty or whitespace-only.\n\n        Examples:\n            - Async document embedding\n                ```python\n                &gt;&gt;&gt; import asyncio\n                &gt;&gt;&gt; from serapeum.ollama import OllamaEmbedding     # type: ignore\n                &gt;&gt;&gt; embedder = OllamaEmbedding(model_name=\"nomic-embed-text\")  # doctest: +SKIP\n                &gt;&gt;&gt; async def embed_doc():  # doctest: +SKIP\n                ...     vec = await embedder.aget_text_embedding(\"Machine learning basics\")\n                ...     return len(vec) &gt; 0\n                &gt;&gt;&gt; # asyncio.run(embed_doc())  # Returns True\n\n                ```\n\n        See Also:\n            _get_text_embedding: Synchronous version of this method.\n            _format_text: Formats text with instruction prefix.\n        \"\"\"\n        formatted_text = self._format_text(text)\n        return await self._a_embed_raw(formatted_text)\n\n    def _get_text_embeddings(self, texts: list[str]) -&gt; Sequence[Sequence[float]]:\n        \"\"\"Generate embedding vectors for multiple documents or text passages.\n\n        Batch version of _get_text_embedding. Formats all texts with the optional\n        text_instruction prefix, then generates embeddings for all texts in a single\n        API call for efficiency.\n\n        Args:\n            texts: List of documents or text passages to embed.\n\n        Returns:\n            A sequence of embedding vectors, one for each input text, in the same order.\n\n        Raises:\n            ValueError: If any text is empty or whitespace-only.\n\n        Examples:\n            - Batch embed multiple documents\n                ```python\n                &gt;&gt;&gt; from serapeum.ollama import OllamaEmbedding     # type: ignore\n                &gt;&gt;&gt; embedder = OllamaEmbedding(model_name=\"nomic-embed-text\")  # doctest: +SKIP\n                &gt;&gt;&gt; docs = [\"First document\", \"Second document\", \"Third document\"]  # doctest: +SKIP\n                &gt;&gt;&gt; embeddings = embedder.get_text_embeddings(docs)  # doctest: +SKIP\n                &gt;&gt;&gt; len(embeddings) == 3  # doctest: +SKIP\n                True\n\n                ```\n\n        See Also:\n            _aget_text_embeddings: Async version of this method.\n            _get_text_embedding: Single text version.\n        \"\"\"\n        formatted_texts = [self._format_text(text) for text in texts]\n        return self._embed_batch_raw(formatted_texts)\n\n    async def _aget_text_embeddings(\n        self, texts: list[str]\n    ) -&gt; Sequence[Sequence[float]]:\n        \"\"\"Asynchronously generate embedding vectors for multiple documents or text passages.\n\n        Async batch version of _get_text_embedding. Formats all texts with the optional\n        text_instruction prefix, then generates all embeddings in a single async API call.\n\n        Args:\n            texts: List of documents or text passages to embed.\n\n        Returns:\n            A sequence of embedding vectors, one for each input text, in the same order.\n\n        Raises:\n            ValueError: If any text is empty or whitespace-only.\n\n        Examples:\n            - Async batch embedding\n                ```python\n                &gt;&gt;&gt; import asyncio\n                &gt;&gt;&gt; from serapeum.ollama import OllamaEmbedding     # type: ignore\n                &gt;&gt;&gt; embedder = OllamaEmbedding(model_name=\"nomic-embed-text\")  # doctest: +SKIP\n                &gt;&gt;&gt; async def batch_embed():  # doctest: +SKIP\n                ...     docs = [\"Doc 1\", \"Doc 2\", \"Doc 3\"]\n                ...     vecs = await embedder.aget_text_embeddings(docs)\n                ...     return len(vecs)\n                &gt;&gt;&gt; # asyncio.run(batch_embed())  # Returns 3\n\n                ```\n\n        See Also:\n            _get_text_embeddings: Synchronous version of this method.\n            _aget_text_embedding: Single text async version.\n        \"\"\"\n        formatted_texts = [self._format_text(text) for text in texts]\n        return await self._a_embed_batch_raw(formatted_texts)\n\n    def _embed_batch_raw(\n        self, texts: list[str]\n    ) -&gt; Sequence[Sequence[float]]:\n        \"\"\"Generate raw embeddings for multiple texts using the Ollama API.\n\n        Low-level private method that directly calls the Ollama embed API without any\n        text formatting or instruction prefixes. Used internally by higher-level\n        methods after text formatting is applied.\n\n        Args:\n            texts: List of text strings to embed (should already be formatted).\n\n        Returns:\n            A sequence of embedding vectors from the Ollama model.\n\n        See Also:\n            _a_embed_batch_raw: Async version of this method.\n            _embed_raw: Single text version.\n        \"\"\"\n        result = self._client.embed(\n            model=self.model_name,\n            input=texts,\n            options=self.ollama_additional_kwargs,\n            keep_alive=self.keep_alive,\n        )\n        return result.embeddings\n\n    async def _a_embed_batch_raw(\n        self, texts: list[str]\n    ) -&gt; Sequence[Sequence[float]]:\n        \"\"\"Asynchronously generate raw embeddings for multiple texts using the Ollama API.\n\n        Async low-level private method that directly calls the Ollama embed API without any\n        text formatting or instruction prefixes. Used internally by higher-level\n        async methods after text formatting is applied.\n\n        Args:\n            texts: List of text strings to embed (should already be formatted).\n\n        Returns:\n            A sequence of embedding vectors from the Ollama model.\n\n        See Also:\n            _embed_batch_raw: Synchronous version of this method.\n            _a_embed_raw: Single text async version.\n        \"\"\"\n        result = await self._async_client.embed(\n            model=self.model_name,\n            input=texts,\n            options=self.ollama_additional_kwargs,\n            keep_alive=self.keep_alive,\n        )\n        return result.embeddings\n\n    def _embed_raw(self, text: str) -&gt; Sequence[float]:\n        \"\"\"Generate a raw embedding for a single text using the Ollama API.\n\n        Low-level private method that directly calls the Ollama embed API without any\n        text formatting or instruction prefixes. Used internally by higher-level\n        methods after text formatting is applied. Returns the first embedding\n        from the API response.\n\n        Args:\n            text: The text string to embed (should already be formatted).\n\n        Returns:\n            An embedding vector from the Ollama model.\n\n        See Also:\n            _a_embed_raw: Async version of this method.\n            _embed_batch_raw: Batch version.\n        \"\"\"\n        result = self._client.embed(\n            model=self.model_name,\n            input=text,\n            options=self.ollama_additional_kwargs,\n            keep_alive=self.keep_alive,\n        )\n        return result.embeddings[0]\n\n    async def _a_embed_raw(self, text: str) -&gt; Sequence[float]:\n        \"\"\"Asynchronously generate a raw embedding for a single text using the Ollama API.\n\n        Async low-level private method that directly calls the Ollama embed API without any\n        text formatting or instruction prefixes. Used internally by higher-level\n        async methods after text formatting is applied. Returns the first embedding\n        from the API response.\n\n        Args:\n            text: The text string to embed (should already be formatted).\n\n        Returns:\n            An embedding vector from the Ollama model.\n\n        See Also:\n            _embed_raw: Synchronous version of this method.\n            _a_embed_batch_raw: Batch async version.\n        \"\"\"\n        result = await self._async_client.embed(\n            model=self.model_name,\n            input=text,\n            options=self.ollama_additional_kwargs,\n            keep_alive=self.keep_alive,\n        )\n        return result.embeddings[0]\n\n    def _format_query(self, query: str) -&gt; str:\n        \"\"\"Format query with instruction if provided.\n\n        Args:\n            query: The query string to format.\n\n        Returns:\n            Formatted query string.\n\n        Raises:\n            ValueError: If query is empty or whitespace-only after stripping.\n        \"\"\"\n        stripped_query = query.strip()\n\n        if not stripped_query:\n            raise ValueError(\n                \"Cannot embed empty or whitespace-only query. \"\n                \"Query becomes empty after stripping whitespace.\"\n            )\n\n        if self.query_instruction:\n            return f\"{self.query_instruction.strip()} {stripped_query}\"\n        return stripped_query\n\n    def _format_text(self, text: str) -&gt; str:\n        \"\"\"Format text with instruction if provided.\n\n        Args:\n            text: The text string to format.\n\n        Returns:\n            Formatted text string.\n\n        Raises:\n            ValueError: If text is empty or whitespace-only after stripping.\n        \"\"\"\n        stripped_text = text.strip()\n\n        if not stripped_text:\n            raise ValueError(\n                \"Cannot embed empty or whitespace-only text. \"\n                \"Text becomes empty after stripping whitespace.\"\n            )\n\n        if self.text_instruction:\n            return f\"{self.text_instruction.strip()} {stripped_text}\"\n        return stripped_text\n</code></pre>"},{"location":"reference/providers/ollama/api_reference/#serapeum.ollama.embedding.OllamaEmbedding.class_name","title":"<code>class_name()</code>  <code>classmethod</code>","text":"<p>Return the canonical class name for this embedding implementation.</p> <p>Returns:</p> Type Description <code>str</code> <p>The string \"OllamaEmbedding\".</p> <p>Examples:</p> <ul> <li>Get the class name identifier     <pre><code>&gt;&gt;&gt; from serapeum.ollama import OllamaEmbedding     # type: ignore\n&gt;&gt;&gt; OllamaEmbedding.class_name()\n'OllamaEmbedding'\n</code></pre></li> </ul> Source code in <code>libs/providers/ollama/src/serapeum/ollama/embedding.py</code> <pre><code>@classmethod\ndef class_name(cls) -&gt; str:\n    \"\"\"Return the canonical class name for this embedding implementation.\n\n    Returns:\n        The string \"OllamaEmbedding\".\n\n    Examples:\n        - Get the class name identifier\n            ```python\n            &gt;&gt;&gt; from serapeum.ollama import OllamaEmbedding     # type: ignore\n            &gt;&gt;&gt; OllamaEmbedding.class_name()\n            'OllamaEmbedding'\n\n            ```\n    \"\"\"\n    return \"OllamaEmbedding\"\n</code></pre>"},{"location":"reference/providers/ollama/examples/","title":"Ollama Usage Examples","text":"<p>This guide provides comprehensive examples covering all possible ways to use the <code>Ollama</code> LLM class based on real test cases from the codebase.</p>"},{"location":"reference/providers/ollama/examples/#table-of-contents","title":"Table of Contents","text":"<ol> <li>Basic Usage</li> <li>Initialization Patterns</li> <li>Chat Operations</li> <li>Completion Operations</li> <li>Streaming Operations</li> <li>Tool/Function Calling</li> <li>Integration with Orchestrators</li> <li>Async Operations</li> </ol>"},{"location":"reference/providers/ollama/examples/#basic-usage","title":"Basic Usage","text":""},{"location":"reference/providers/ollama/examples/#simple-chat","title":"Simple Chat","text":"<p>The most straightforward way to use <code>Ollama</code>:</p> <pre><code>from serapeum.core.base.llms.types import Message, MessageRole\nfrom serapeum.ollama import Ollama\n\n# Initialize Ollama LLM\nllm = Ollama(\n    model=\"llama3.1\",\n    request_timeout=180,\n)\n\n# Create a message\nmessages = [Message(role=MessageRole.USER, content=\"Say 'pong'.\")]\n\n# Send chat request\nresponse = llm.chat(messages)\nprint(response.message.content)  # \"Pong!\"\n</code></pre>"},{"location":"reference/providers/ollama/examples/#simple-completion","title":"Simple Completion","text":"<p>Using the completion API:</p> <pre><code>from serapeum.ollama import Ollama\n\n# Initialize Ollama LLM\nllm = Ollama(\n    model=\"llama3.1\",\n    request_timeout=180,\n)\n\n# Send completion request\nresponse = llm.complete(\"Say 'pong'.\")\nprint(response.text)  # \"Pong!\"\n</code></pre>"},{"location":"reference/providers/ollama/examples/#initialization-patterns","title":"Initialization Patterns","text":""},{"location":"reference/providers/ollama/examples/#1-basic-initialization","title":"1. Basic Initialization","text":"<p>Minimal configuration:</p> <pre><code>from serapeum.ollama import Ollama\n\nllm = Ollama(model=\"llama3.1\")\n</code></pre>"},{"location":"reference/providers/ollama/examples/#2-full-configuration","title":"2. Full Configuration","text":"<p>With all common parameters:</p> <pre><code>from serapeum.ollama import Ollama\n\nllm = Ollama(\n    model=\"llama3.1\",\n    base_url=\"http://localhost:11434\",\n    temperature=0.8,\n    context_window=4096,\n    request_timeout=180.0,\n    json_mode=True,\n    keep_alive=\"5m\",\n    additional_kwargs={\"top_p\": 0.9, \"top_k\": 40}\n)\n</code></pre>"},{"location":"reference/providers/ollama/examples/#3-with-custom-client","title":"3. With Custom Client","text":"<p>Pre-configured Ollama client:</p> <pre><code>from ollama import Client\nfrom serapeum.ollama import Ollama\n\n# Create custom client\nclient = Client(host=\"http://localhost:11434\", timeout=300)\n\n# Pass to Ollama\nllm = Ollama(\n    model=\"llama3.1\",\n    client=client,\n)\n</code></pre>"},{"location":"reference/providers/ollama/examples/#4-json-mode-for-structured-outputs","title":"4. JSON Mode for Structured Outputs","text":"<p>Enable JSON formatting:</p> <pre><code>from serapeum.ollama import Ollama\n\nllm = Ollama(\n    model=\"llama3.1\",\n    json_mode=True,  # Forces JSON output\n    request_timeout=180,\n)\n</code></pre>"},{"location":"reference/providers/ollama/examples/#chat-operations","title":"Chat Operations","text":""},{"location":"reference/providers/ollama/examples/#1-single-turn-chat","title":"1. Single Turn Chat","text":"<p>Basic conversation:</p> <pre><code>from serapeum.core.base.llms.types import Message, MessageRole\nfrom serapeum.ollama import Ollama\n\nllm = Ollama(model=\"llama3.1\", request_timeout=180)\n\nmessages = [\n    Message(role=MessageRole.USER, content=\"What is 2+2?\")\n]\n\nresponse = llm.chat(messages)\nprint(response.message.content)  # \"4\"\n</code></pre>"},{"location":"reference/providers/ollama/examples/#2-multi-turn-conversation","title":"2. Multi-turn Conversation","text":"<p>With conversation history:</p> <pre><code>from serapeum.core.base.llms.types import Message, MessageRole\nfrom serapeum.ollama import Ollama\n\nllm = Ollama(model=\"llama3.1\", request_timeout=180)\n\nmessages = [\n    Message(role=MessageRole.SYSTEM, content=\"You are a helpful math tutor.\"),\n    Message(role=MessageRole.USER, content=\"What is 2+2?\"),\n    Message(role=MessageRole.ASSISTANT, content=\"2+2 equals 4.\"),\n    Message(role=MessageRole.USER, content=\"What about 3+3?\"),\n]\n\nresponse = llm.chat(messages)\nprint(response.message.content)  # \"3+3 equals 6.\"\n</code></pre>"},{"location":"reference/providers/ollama/examples/#3-chat-with-parameters","title":"3. Chat with Parameters","text":"<p>Passing custom parameters:</p> <pre><code>from serapeum.core.base.llms.types import Message, MessageRole\nfrom serapeum.ollama import Ollama\n\nllm = Ollama(model=\"llama3.1\", request_timeout=180)\n\nmessages = [Message(role=MessageRole.USER, content=\"Write a creative story.\")]\n\n# Override default settings\nresponse = llm.chat(\n    messages,\n    temperature=0.9,      # Higher for creativity\n    top_p=0.95,\n    max_tokens=500,\n)\n</code></pre>"},{"location":"reference/providers/ollama/examples/#4-chat-with-images","title":"4. Chat with Images","text":"<p>Multi-modal input (if supported by model):</p> <pre><code>from serapeum.core.base.llms.types import Message, MessageRole, Image\nfrom serapeum.ollama import Ollama\n\nllm = Ollama(model=\"llama4\", request_timeout=180)  # Vision model\n\n# Create message with image\nimage = Image(path=\"docs/reference/providers/ollama/images/baharia-oasis.jpg\")\nmessages = [\n    Message(\n        role=MessageRole.USER,\n        content=\"What's in this image?\",\n        images=[image]\n    )\n]\n\nresponse = llm.chat(messages)\nprint(response.message.content)\n</code></pre>"},{"location":"reference/providers/ollama/examples/#completion-operations","title":"Completion Operations","text":""},{"location":"reference/providers/ollama/examples/#1-basic-completion","title":"1. Basic Completion","text":"<p>Simple text completion:</p> <pre><code>from serapeum.ollama import Ollama\n\nllm = Ollama(model=\"llama3.1\", request_timeout=180)\n\nprompt = \"The capital of France is\"\nresponse = llm.complete(prompt)\nprint(response.text)  # \"Paris\"\n</code></pre>"},{"location":"reference/providers/ollama/examples/#2-completion-with-parameters","title":"2. Completion with Parameters","text":"<p>Custom generation settings:</p> <pre><code>from serapeum.ollama import Ollama\n\nllm = Ollama(model=\"llama3.1\", request_timeout=180)\n\nresponse = llm.complete(\n    \"Once upon a time\",\n    temperature=0.8,\n    max_tokens=200,\n)\nprint(response.text)\n</code></pre>"},{"location":"reference/providers/ollama/examples/#3-json-completion","title":"3. JSON Completion","text":"<p>Force JSON output:</p> <pre><code>from serapeum.ollama import Ollama\n\nllm = Ollama(\n    model=\"llama3.1\",\n    json_mode=True,\n    request_timeout=180,\n)\n\nprompt = 'Return {\"name\": \"John\", \"age\": 30} as JSON'\nresponse = llm.complete(prompt)\nprint(response.text)  # {\"name\": \"John\", \"age\": 30}\n</code></pre>"},{"location":"reference/providers/ollama/examples/#streaming-operations","title":"Streaming Operations","text":""},{"location":"reference/providers/ollama/examples/#1-stream-chat","title":"1. Stream Chat","text":"<p>Real-time streaming chat:</p> <pre><code>from serapeum.core.base.llms.types import Message, MessageRole\nfrom serapeum.ollama import Ollama\n\nllm = Ollama(model=\"llama3.1\", request_timeout=180)\n\nmessages = [Message(role=MessageRole.USER, content=\"Count from 1 to 5.\")]\n\n# Stream responses\nfor chunk in llm.stream_chat(messages):\n    print(chunk.message.content, end=\"\", flush=True)\n    # Outputs: \"1\" \" 2\" \" 3\" \" 4\" \" 5\"\n</code></pre>"},{"location":"reference/providers/ollama/examples/#2-stream-completion","title":"2. Stream Completion","text":"<p>Real-time streaming completion:</p> <pre><code>from serapeum.ollama import Ollama\n\nllm = Ollama(model=\"llama3.1\", request_timeout=180)\n\nprompt = \"Write a haiku about coding:\"\n\n# Stream completion\nfor chunk in llm.stream_complete(prompt):\n    print(chunk.text, end=\"\", flush=True)\n</code></pre>"},{"location":"reference/providers/ollama/examples/#3-processing-stream-with-delta","title":"3. Processing Stream with Delta","text":"<p>Access incremental content:</p> <pre><code>from serapeum.core.base.llms.types import Message, MessageRole\nfrom serapeum.ollama import Ollama\n\nllm = Ollama(model=\"llama3.1\", request_timeout=180)\n\nmessages = [Message(role=MessageRole.USER, content=\"Tell me a joke.\")]\n\nfull_response = \"\"\nfor chunk in llm.stream_chat(messages):\n    delta = chunk.delta  # Incremental content\n    if delta:\n        full_response += delta\n        print(delta, end=\"\", flush=True)\n\nprint(f\"\\n\\nFull response: {full_response}\")\n</code></pre>"},{"location":"reference/providers/ollama/examples/#toolfunction-calling","title":"Tool/Function Calling","text":""},{"location":"reference/providers/ollama/examples/#1-basic-tool-calling","title":"1. Basic Tool Calling","text":"<p>Using tools with Ollama:</p> <pre><code>from pydantic import BaseModel\nfrom serapeum.core.base.llms.types import Message, MessageRole\nfrom serapeum.core.tools import CallableTool\nfrom serapeum.ollama import Ollama\n\n\nclass Album(BaseModel):\n    title: str\n    artist: str\n    songs: list[str]\n\n\ndef create_album(title: str, artist: str, songs: list[str]) -&gt; Album:\n    \"\"\"Create an album with the given information.\"\"\"\n    return Album(title=title, artist=artist, songs=songs)\n\n\nllm = Ollama(model=\"llama3.1\", request_timeout=180)\n\n# Create tool from function\ntool = CallableTool.from_function(create_album)\n\nmessage =  Message(\n    role=MessageRole.USER,\n    content=\"Create a rock album with two songs\"\n)\n\n# Call with tools\nresponse = llm.chat_with_tools(tools=[tool], user_msg=message)\n\n# Extract tool calls\ntool_calls = llm.get_tool_calls_from_response(response)\nprint(tool_calls)\n</code></pre>"},{"location":"reference/providers/ollama/examples/#2-tool-calling-from-pydantic-model","title":"2. Tool Calling from Pydantic Model","text":"<p>Create tools from Pydantic models:</p> <pre><code>from pydantic import BaseModel, Field\nfrom serapeum.core.base.llms.types import Message, MessageRole\nfrom serapeum.core.tools import CallableTool\nfrom serapeum.ollama import Ollama\n\n\nclass Album(BaseModel):\n    \"\"\"An music album.\"\"\"\n    title: str = Field(description=\"Album title\")\n    artist: str = Field(description=\"Artist name\")\n    songs: list[str] = Field(description=\"List of song titles\")\n\n\nllm = Ollama(model=\"llama3.1\", request_timeout=180)\n\n# Create tool from Pydantic model\ntool = CallableTool.from_model(Album)\n\nmessage = Message(\n    role=MessageRole.USER,\n    content=\"Create a jazz album with title 'Blue Notes' by Miles Davis with 3 songs\"\n)\n\nresponse = llm.chat_with_tools(tools=[tool], user_msg=message)\n\n# Extract and execute tool call\ntool_calls = llm.get_tool_calls_from_response(response)\nfor tool_call in tool_calls:\n    # Execute tool\n    result = tool.call(**tool_call.tool_kwargs)\n    print(result)  # Album instance\n</code></pre>"},{"location":"reference/providers/ollama/examples/#3-single-tool-call-mode","title":"3. Single Tool Call Mode","text":"<p>Force single tool call:</p> <pre><code>from pydantic import BaseModel\nfrom serapeum.core.base.llms.types import Message, MessageRole\nfrom serapeum.core.tools import CallableTool\nfrom serapeum.ollama import Ollama\n\n\nclass Album(BaseModel):\n    title: str\n    artist: str\n    songs: list[str]\n\n\nllm = Ollama(model=\"llama3.1\", request_timeout=180)\n\ntool = CallableTool.from_model(Album)\n\nmessage = Message(\n    role=MessageRole.USER,\n    content=\"Create two albums\"\n)\n\n# Force single tool call\nresponse = llm.chat_with_tools(\n    tools=[tool],\n    user_msg=message,\n    allow_parallel_tool_calls=False,  # Only one tool call allowed\n)\n\ntool_calls = llm.get_tool_calls_from_response(response)\nprint(len(tool_calls))  # 1 (even if model tried to return multiple)\n</code></pre>"},{"location":"reference/providers/ollama/examples/#4-parallel-tool-calls","title":"4. Parallel Tool Calls","text":"<p>Allow multiple tool calls:</p> <pre><code>from pydantic import BaseModel\nfrom serapeum.core.base.llms.types import Message, MessageRole\nfrom serapeum.core.tools import CallableTool\nfrom serapeum.ollama import Ollama\n\n\nclass Album(BaseModel):\n    title: str\n    artist: str\n    songs: list[str]\n\n\nllm = Ollama(model=\"llama3.1\", request_timeout=180)\n\ntool = CallableTool.from_model(Album)\n\nmessage = Message(\n    role=MessageRole.USER,\n    content=\"Create two albums: one rock album and one jazz album\"\n)\n\n# Allow parallel tool calls\nresponse = llm.chat_with_tools(\n    tools=[tool],\n    user_msg=message,\n    allow_parallel_tool_calls=True,\n)\n\ntool_calls = llm.get_tool_calls_from_response(response)\nprint(len(tool_calls))  # 2 (if model returns multiple)\n\nfor tool_call in tool_calls:\n    result = tool.call(**tool_call.tool_kwargs)\n    print(result)\n</code></pre>"},{"location":"reference/providers/ollama/examples/#5-streaming-with-tools","title":"5. Streaming with Tools","text":"<p>Stream tool calls:</p> <pre><code>from pydantic import BaseModel\nfrom serapeum.core.base.llms.types import Message, MessageRole\nfrom serapeum.core.tools import CallableTool\nfrom serapeum.ollama import Ollama\n\n\nclass Album(BaseModel):\n    title: str\n    artist: str\n    songs: list[str]\n\n\nllm = Ollama(model=\"llama3.1\", request_timeout=180)\n\ntool = CallableTool.from_model(Album)\n\nmessage = Message(\n    role=MessageRole.USER,\n    content=\"Create a pop album\"\n)\n\n# Stream with tools\nfor chunk in llm.stream_chat_with_tools(tools=[tool], user_msg=message):\n    # Process streaming tool calls\n    if chunk.message.additional_kwargs.get(\"tool_calls\"):\n        print(f\"Tool call chunk: {chunk.message.additional_kwargs['tool_calls']}\")\n</code></pre>"},{"location":"reference/providers/ollama/examples/#integration-with-orchestrators","title":"Integration with Orchestrators","text":""},{"location":"reference/providers/ollama/examples/#1-with-textcompletionllm","title":"1. With TextCompletionLLM","text":"<p>Use Ollama with <code>TextCompletionLLM</code> for structured outputs:</p> <pre><code>from pydantic import BaseModel\nfrom serapeum.core.output_parsers import PydanticParser\nfrom serapeum.core.llms import TextCompletionLLM\nfrom serapeum.ollama import Ollama\n\n\nclass DummyModel(BaseModel):\n    value: str\n\n\n# Initialize Ollama\nllm = Ollama(model=\"llama3.1\", request_timeout=180)\n\n# Create parser\nparser = PydanticParser(output_cls=DummyModel)\n\n# Create TextCompletionLLM\ntext_llm = TextCompletionLLM(\n    output_parser=parser,\n    prompt=\"Value: {value}\",\n    llm=llm,\n)\n\n# Execute\nresult = text_llm(value=\"input\")\nprint(result.value)  # \"input\"\n</code></pre>"},{"location":"reference/providers/ollama/examples/#2-with-toolorchestratingllm","title":"2. With ToolOrchestratingLLM","text":"<p>Use Ollama with <code>ToolOrchestratingLLM</code> for tool-based workflows:</p> <pre><code>from pydantic import BaseModel\nfrom serapeum.core.llms import ToolOrchestratingLLM\nfrom serapeum.ollama import Ollama\n\n\nclass Album(BaseModel):\n    title: str\n    artist: str\n    songs: list[str]\n\n\n# Initialize Ollama\nllm = Ollama(model=\"llama3.1\", request_timeout=180)\n\n# Create ToolOrchestratingLLM\ntools_llm = ToolOrchestratingLLM(\n    output_cls=Album,\n    prompt=\"Create an album about {topic} with two random songs\",\n    llm=llm,\n)\n\n# Execute - returns Album instance\nresult = tools_llm(topic=\"rock\")\nprint(result.title)\nprint(result.artist)\nprint(result.songs)\n</code></pre>"},{"location":"reference/providers/ollama/examples/#3-parallel-tool-execution","title":"3. Parallel Tool Execution","text":"<p>Using <code>ToolOrchestratingLLM</code> with parallel tools:</p> <pre><code>from pydantic import BaseModel\nfrom serapeum.core.llms import ToolOrchestratingLLM\nfrom serapeum.llms.ollama import Ollama\n\n\nclass Album(BaseModel):\n    title: str\n    artist: str\n    songs: list[str]\n\n\nllm = Ollama(model=\"llama3.1\", request_timeout=180)\n\n# Enable parallel tool calls\ntools_llm = ToolOrchestratingLLM(\n    output_cls=Album,\n    prompt=\"Create albums about {topic}\",\n    llm=llm,\n    allow_parallel_tool_calls=True,\n)\n\n# Returns list of Album instances\nresults = tools_llm(topic=\"jazz\")\nprint(len(results))  # Potentially multiple albums\nfor album in results:\n    print(f\"{album.title} by {album.artist}\")\n</code></pre>"},{"location":"reference/providers/ollama/examples/#4-streaming-with-toolorchestratingllm","title":"4. Streaming with ToolOrchestratingLLM","text":"<p>Stream tool execution results:</p> <pre><code>from pydantic import BaseModel\nfrom serapeum.core.llms import ToolOrchestratingLLM\nfrom serapeum.llms.ollama import Ollama\n\n\nclass Album(BaseModel):\n    title: str\n    artist: str\n    songs: list[str]\n\n\nllm = Ollama(model=\"llama3.1\", request_timeout=180)\n\ntools_llm = ToolOrchestratingLLM(\n    output_cls=Album,\n    prompt=\"Create albums about {topic}\",\n    llm=llm,\n    allow_parallel_tool_calls=False,\n)\n\n# Stream results\nfor album in tools_llm.stream_call(topic=\"rock\"):\n    print(f\"Received: {album.title}\")\n</code></pre>"},{"location":"reference/providers/ollama/examples/#async-operations","title":"Async Operations","text":""},{"location":"reference/providers/ollama/examples/#1-async-chat","title":"1. Async Chat","text":"<p>Non-blocking chat:</p> <pre><code>import asyncio\nfrom serapeum.core.base.llms.types import Message, MessageRole\nfrom serapeum.llms.ollama import Ollama\n\n\nasync def async_chat_example():\n    llm = Ollama(model=\"llama3.1\", request_timeout=180)\n\n    messages = [Message(role=MessageRole.USER, content=\"Hello!\")]\n\n    response = await llm.achat(messages)\n    print(response.message.content)\n\n\nasyncio.run(async_chat_example())\n</code></pre>"},{"location":"reference/providers/ollama/examples/#2-async-completion","title":"2. Async Completion","text":"<p>Non-blocking completion:</p> <pre><code>import asyncio\nfrom serapeum.llms.ollama import Ollama\n\n\nasync def async_complete_example():\n    llm = Ollama(model=\"llama3.1\", request_timeout=180)\n\n    response = await llm.acomplete(\"Say hello\")\n    print(response.text)\n\n\nasyncio.run(async_complete_example())\n</code></pre>"},{"location":"reference/providers/ollama/examples/#3-async-streaming-chat","title":"3. Async Streaming Chat","text":"<p>Non-blocking streaming:</p> <pre><code>import asyncio\nfrom serapeum.core.base.llms.types import Message, MessageRole\nfrom serapeum.llms.ollama import Ollama\n\n\nasync def async_stream_example():\n    llm = Ollama(model=\"llama3.1\", request_timeout=180)\n\n    messages = [Message(role=MessageRole.USER, content=\"Count to 5\")]\n\n    async for chunk in await llm.astream_chat(messages):\n        print(chunk.message.content, end=\"\", flush=True)\n\n\nasyncio.run(async_stream_example())\n</code></pre>"},{"location":"reference/providers/ollama/examples/#4-concurrent-async-requests","title":"4. Concurrent Async Requests","text":"<p>Process multiple requests concurrently:</p> <pre><code>import asyncio\nfrom serapeum.core.base.llms.types import Message, MessageRole\nfrom serapeum.llms.ollama import Ollama\n\n\nasync def process_multiple():\n    llm = Ollama(model=\"llama3.1\", request_timeout=180)\n\n    prompts = [\"What is 2+2?\", \"What is 3+3?\", \"What is 4+4?\"]\n\n    # Create tasks\n    tasks = [\n        llm.achat([Message(role=MessageRole.USER, content=prompt)])\n        for prompt in prompts\n    ]\n\n    # Execute concurrently\n    responses = await asyncio.gather(*tasks)\n\n    for prompt, response in zip(prompts, responses):\n        print(f\"{prompt} -&gt; {response.message.content}\")\n\n\nasyncio.run(process_multiple())\n</code></pre>"},{"location":"reference/providers/ollama/examples/#5-async-with-toolorchestratingllm","title":"5. Async with ToolOrchestratingLLM","text":"<p>Async tool orchestration:</p> <pre><code>import asyncio\nfrom pydantic import BaseModel\nfrom serapeum.core.llms import ToolOrchestratingLLM\nfrom serapeum.llms.ollama import Ollama\n\n\nclass Album(BaseModel):\n    title: str\n    artist: str\n    songs: list[str]\n\n\nasync def async_tool_example():\n    llm = Ollama(model=\"llama3.1\", request_timeout=180)\n\n    tools_llm = ToolOrchestratingLLM(\n        output_cls=Album,\n        prompt=\"Create an album about {topic}\",\n        llm=llm,\n    )\n\n    result = await tools_llm.acall(topic=\"pop\")\n    print(result.title)\n\n\nasyncio.run(async_tool_example())\n</code></pre>"},{"location":"reference/providers/ollama/examples/#6-async-streaming-with-tools","title":"6. Async Streaming with Tools","text":"<p>Async streaming tool execution:</p> <pre><code>import asyncio\nfrom pydantic import BaseModel\nfrom serapeum.core.llms import ToolOrchestratingLLM\nfrom serapeum.llms.ollama import Ollama\n\n\nclass Album(BaseModel):\n    title: str\n    artist: str\n    songs: list[str]\n\n\nasync def async_stream_tool_example():\n    llm = Ollama(model=\"llama3.1\", request_timeout=180)\n\n    tools_llm = ToolOrchestratingLLM(\n        output_cls=Album,\n        prompt=\"Create albums about {topic}\",\n        llm=llm,\n        allow_parallel_tool_calls=False,\n    )\n\n    stream = await tools_llm.astream_call(topic=\"rock\")\n    async for album in stream:\n        print(f\"Received: {album.title}\")\n\n\nasyncio.run(async_stream_tool_example())\n</code></pre>"},{"location":"reference/providers/ollama/examples/#best-practices","title":"Best Practices","text":""},{"location":"reference/providers/ollama/examples/#1-reuse-llm-instances","title":"1. Reuse LLM Instances","text":"<p>Create once, use many times:</p> <pre><code>from serapeum.llms.ollama import Ollama\n\n# \u2713 Good: Create once\nllm = Ollama(model=\"llama3.1\", request_timeout=180)\n\n# Reuse for multiple calls\nresponse1 = llm.chat(messages1)\nresponse2 = llm.chat(messages2)\n\n# \u2717 Bad: Don't recreate for each call\ndef process(messages):\n    llm = Ollama(model=\"llama3.1\")  # Inefficient\n    return llm.chat(messages)\n</code></pre>"},{"location":"reference/providers/ollama/examples/#2-use-appropriate-timeout","title":"2. Use Appropriate Timeout","text":"<p>Set timeout based on expected response time:</p> <pre><code>from serapeum.llms.ollama import Ollama\n\n# Short timeout for simple queries\nquick_llm = Ollama(model=\"llama3.1\", request_timeout=30)\n\n# Longer timeout for complex queries\ncomplex_llm = Ollama(model=\"llama3.1\", request_timeout=300)\n</code></pre>"},{"location":"reference/providers/ollama/examples/#3-handle-errors-gracefully","title":"3. Handle Errors Gracefully","text":"<p>Always handle potential errors:</p> <pre><code>from serapeum.core.base.llms.types import Message, MessageRole\nfrom serapeum.llms.ollama import Ollama\n\nllm = Ollama(model=\"llama3.1\", request_timeout=180)\n\ntry:\n    response = llm.chat([Message(role=MessageRole.USER, content=\"Hello\")])\nexcept TimeoutError:\n    print(\"Request timed out\")\nexcept ConnectionError:\n    print(\"Could not connect to Ollama server\")\nexcept Exception as e:\n    print(f\"Unexpected error: {e}\")\n</code></pre>"},{"location":"reference/providers/ollama/examples/#4-use-json-mode-for-structured-outputs","title":"4. Use JSON Mode for Structured Outputs","text":"<p>Enable when expecting JSON:</p> <pre><code>from serapeum.llms.ollama import Ollama\n\n# Enable JSON mode\nllm = Ollama(\n    model=\"llama3.1\",\n    json_mode=True,\n    request_timeout=180,\n)\n\n# LLM will always return valid JSON\n</code></pre>"},{"location":"reference/providers/ollama/examples/#5-monitor-response-metadata","title":"5. Monitor Response Metadata","text":"<p>Use metadata for monitoring:</p> <pre><code>from serapeum.core.base.llms.types import Message, MessageRole\nfrom serapeum.llms.ollama import Ollama\n\nllm = Ollama(model=\"llama3.1\", request_timeout=180)\n\nresponse = llm.chat([Message(role=MessageRole.USER, content=\"Hello\")])\n\n# Access metadata\nprint(f\"Model: {response.additional_kwargs.get('model')}\")\nprint(f\"Tokens: {response.additional_kwargs.get('eval_count')}\")\nprint(f\"Duration: {response.additional_kwargs.get('total_duration')}\")\n</code></pre>"},{"location":"reference/providers/ollama/examples/#see-also","title":"See Also","text":"<ul> <li>Execution Flow and Method Calls - Detailed sequence diagrams</li> <li>Architecture and Class Relationships - Class structure</li> <li>Data Transformations and Validation - Data flow details</li> <li>Component Boundaries and Interactions - System components</li> <li>Lifecycle and State Management - State management</li> </ul>"},{"location":"reference/providers/ollama/general/","title":"Ollama LLM Integration","text":"<p>This directory contains comprehensive documentation explaining the complete workflow of the <code>Ollama</code> class, from initialization to execution across various modes (chat, completion, streaming, tool calling, async).</p>"},{"location":"reference/providers/ollama/general/#overview","title":"Overview","text":"<p>The <code>Ollama</code> class is a production-ready LLM integration that provides: 1. Connection to Ollama server (local or remote) 2. Chat and completion APIs with sync/async support 3. Streaming responses for real-time output 4. Tool/function calling for structured interactions 5. Integration with orchestrators (TextCompletionLLM, ToolOrchestratingLLM)</p>"},{"location":"reference/providers/ollama/general/#example-usage","title":"Example Usage","text":""},{"location":"reference/providers/ollama/general/#basic-chat","title":"Basic Chat","text":"<pre><code>from serapeum.core.base.llms.types import Message, MessageRole\nfrom serapeum.ollama import Ollama\n\n# Initialize Ollama\nllm = Ollama(\n    model=\"llama3.1\",\n    request_timeout=180,\n)\n\n# Send chat request\nmessages = [Message(role=MessageRole.USER, content=\"Say 'pong'.\")]\nresponse = llm.chat(messages)\nprint(response.message.content)  # \"Pong!\"\n</code></pre>"},{"location":"reference/providers/ollama/general/#with-textcompletionllm","title":"With TextCompletionLLM","text":"<pre><code>from pydantic import BaseModel\nfrom serapeum.core.output_parsers import PydanticParser\nfrom serapeum.core.llms import TextCompletionLLM\nfrom serapeum.ollama import Ollama\n\n\nclass DummyModel(BaseModel):\n    value: str\n\n\n# Initialize Ollama\nllm = Ollama(model=\"llama3.1\", request_timeout=180)\n\n# Create structured completion runner\ntext_llm = TextCompletionLLM(\n    output_parser=PydanticParser(output_cls=DummyModel),\n    prompt=\"Value: {value}\",\n    llm=llm,\n)\n\n# Execute and get structured output\nresult = text_llm(value=\"input\")\n# Returns: DummyModel(value=\"input\")\n</code></pre>"},{"location":"reference/providers/ollama/general/#with-toolorchestratingllm","title":"With ToolOrchestratingLLM","text":"<pre><code>from pydantic import BaseModel\nfrom serapeum.core.llms import ToolOrchestratingLLM\nfrom serapeum.ollama import Ollama\n\n\nclass Album(BaseModel):\n    title: str\n    artist: str\n    songs: list[str]\n\n\n# Initialize Ollama\nllm = Ollama(model=\"llama3.1\", request_timeout=180)\n\n# Create tool orchestrator\ntools_llm = ToolOrchestratingLLM(\n    output_cls=Album,\n    prompt=\"Create an album about {topic} with two random songs\",\n    llm=llm,\n)\n\n# Execute and get structured output via tool calling\nresult = tools_llm(topic=\"rock\")\n# Returns: Album(title=\"...\", artist=\"...\", songs=[...])\n</code></pre>"},{"location":"reference/providers/ollama/general/#understanding-the-workflow","title":"Understanding the Workflow","text":""},{"location":"reference/providers/ollama/general/#1-execution-flow-and-method-calls","title":"1. Execution Flow and Method Calls","text":"<p>Shows the chronological flow of method calls and interactions across all usage patterns.</p> <p>Best for: - Understanding the order of operations - Seeing how Ollama communicates with the server - Debugging execution flow - Understanding integration patterns</p> <p>Key Flows: - Initialization phase (lazy client creation) - Direct chat/completion calls - Tool calling with schema conversion - Streaming execution - Integration with TextCompletionLLM and ToolOrchestratingLLM - Async operations</p>"},{"location":"reference/providers/ollama/general/#2-architecture-and-class-relationships","title":"2. Architecture and Class Relationships","text":"<p>Illustrates the static structure, inheritance hierarchy, and relationships.</p> <p>Best for: - Understanding the architecture - Seeing inheritance chain (BaseLLM \u2192 LLM \u2192 FunctionCallingLLM \u2192 Ollama) - Identifying class responsibilities - Understanding integration points</p> <p>Key Classes: - <code>Ollama</code>: Main LLM implementation - <code>FunctionCallingLLM</code>: Tool calling abstraction - <code>LLM</code>: High-level orchestration - <code>BaseLLM</code>: Core interface - <code>Client</code>/<code>AsyncClient</code>: HTTP communication - Response models: <code>ChatResponse</code>, <code>CompletionResponse</code>, <code>Message</code></p>"},{"location":"reference/providers/ollama/general/#3-data-transformations-and-validation","title":"3. Data Transformations and Validation","text":"<p>Tracks how data transforms through the system across different operation modes.</p> <p>Best for: - Understanding data transformations - Identifying validation points - Seeing error handling paths - Understanding request/response formats</p> <p>Key Flows: - Initialization and configuration - Chat request building and response parsing - Completion via decorator pattern - Tool schema conversion - Streaming chunk processing - Error handling pipelines</p>"},{"location":"reference/providers/ollama/general/#4-component-boundaries-and-interactions","title":"4. Component Boundaries and Interactions","text":"<p>Shows component boundaries, responsibilities, and interaction patterns.</p> <p>Best for: - Understanding system architecture - Seeing component responsibilities - Identifying interaction patterns - Understanding integration layers</p> <p>Key Components: - User space (application code) - Ollama core (request building, response parsing, tool handling) - Client layer (HTTP communication) - Ollama server (model runtime, inference) - Orchestrator layer (TextCompletionLLM, ToolOrchestratingLLM)</p>"},{"location":"reference/providers/ollama/general/#5-lifecycle-and-state-management","title":"5. Lifecycle and State Management","text":"<p>Depicts the lifecycle states, transitions, and state variables.</p> <p>Best for: - Understanding instance lifecycle - Seeing state transitions - Identifying error states - Understanding concurrency considerations</p> <p>Key States: - Uninitialized \u2192 Configured (initialization) - Configured \u2192 ClientInitialized (lazy client creation) - Idle \u2194 Processing* (request handling) - Processing \u2192 Error \u2192 Idle (error handling)</p>"},{"location":"reference/providers/ollama/general/#6-usage-examples","title":"6. Usage Examples","text":"<p>Comprehensive examples from real test cases.</p> <p>Best for: - Learning by example - Understanding practical usage - Seeing all API variants - Integration patterns</p> <p>Key Examples: - Basic chat and completion - Streaming operations - Tool/function calling - Integration with orchestrators - Async operations - Error handling</p>"},{"location":"reference/providers/ollama/general/#core-capabilities","title":"Core Capabilities","text":""},{"location":"reference/providers/ollama/general/#1-chat-api","title":"1. Chat API","text":"<pre><code>Direct conversation with the model:\n- Single and multi-turn conversations\n- System messages for context\n- Image inputs (if model supports)\n- Custom parameters (temperature, top_p, etc.)\n</code></pre>"},{"location":"reference/providers/ollama/general/#2-completion-api","title":"2. Completion API","text":"<pre><code>Text completion via decorator pattern:\n- Converts prompt to chat message\n- Delegates to chat API\n- Extracts text from response\n</code></pre>"},{"location":"reference/providers/ollama/general/#3-streaming","title":"3. Streaming","text":"<pre><code>Real-time response generation:\n- Stream chat responses\n- Stream completion responses\n- Chunk-by-chunk processing\n- Delta content access\n</code></pre>"},{"location":"reference/providers/ollama/general/#4-toolfunction-calling","title":"4. Tool/Function Calling","text":"<pre><code>Structured interactions with tools:\n- Automatic schema conversion\n- Single or parallel tool calls\n- Tool call validation\n- Streaming tool calls\n</code></pre>"},{"location":"reference/providers/ollama/general/#5-async-operations","title":"5. Async Operations","text":"<pre><code>Non-blocking execution:\n- Async chat and completion\n- Async streaming\n- Concurrent request handling\n- Separate async client per event loop\n</code></pre>"},{"location":"reference/providers/ollama/general/#key-design-patterns","title":"Key Design Patterns","text":""},{"location":"reference/providers/ollama/general/#1-lazy-initialization","title":"1. Lazy Initialization","text":"<p>Clients are created on first use, not during <code>__init__</code>: <pre><code>@property\ndef client(self) -&gt; Client:\n    if self._client is None:\n        self._client = Client(host=self.base_url, timeout=self.request_timeout)\n    return self._client\n</code></pre></p>"},{"location":"reference/providers/ollama/general/#2-decorator-pattern","title":"2. Decorator Pattern","text":"<p>Completion API wraps chat API for code reuse: <pre><code>@chat_to_completion_decorator\ndef complete(self, prompt: str, **kwargs) -&gt; CompletionResponse:\n    # Decorator handles conversion\n    pass\n</code></pre></p>"},{"location":"reference/providers/ollama/general/#3-template-method-pattern","title":"3. Template Method Pattern","text":"<p>FunctionCallingLLM defines workflow, Ollama implements specifics: <pre><code>def chat_with_tools(self, messages, tools, **kwargs):\n    prepared = self._prepare_chat_with_tools(messages, tools, **kwargs)  # Subclass\n    response = self.chat(prepared)\n    validated = self._validate_chat_with_tools_response(response, tools)  # Subclass\n    return validated\n</code></pre></p>"},{"location":"reference/providers/ollama/general/#4-adapter-pattern","title":"4. Adapter Pattern","text":"<p>Ollama adapts between internal types and Ollama server format: - <code>Message</code> \u2192 Ollama message dict - <code>BaseTool</code> \u2192 Ollama tool schema - Raw response dict \u2192 <code>ChatResponse</code>/<code>CompletionResponse</code></p>"},{"location":"reference/providers/ollama/general/#integration-architecture","title":"Integration Architecture","text":"<pre><code>User Application\n    \u2193\nToolOrchestratingLLM / TextCompletionLLM\n    \u2193\nOllama\n    \u2193\nClient / AsyncClient\n    \u2193\nOllama Server (HTTP)\n    \u2193\nModel Runtime (llama3.1, etc.)\n</code></pre>"},{"location":"reference/providers/ollama/general/#textcompletionllm-integration","title":"TextCompletionLLM Integration","text":"<pre><code>1. Formats prompt with variables\n2. Checks is_chat_model \u2192 True\n3. Calls Ollama.chat()\n4. Parses response with PydanticParser\n5. Returns validated model instance\n</code></pre>"},{"location":"reference/providers/ollama/general/#toolorchestratingllm-integration","title":"ToolOrchestratingLLM Integration","text":"<pre><code>1. Converts output_cls to CallableTool\n2. Formats prompt with variables\n3. Calls Ollama.chat_with_tools()\n4. Ollama converts tool to schema\n5. Server returns tool_calls\n6. Executes tool to create instance\n7. Returns model instance(s)\n</code></pre>"},{"location":"reference/providers/ollama/general/#performance-considerations","title":"Performance Considerations","text":"<ol> <li>Client Reuse: Client created once and reused for all requests</li> <li>Async Support: Separate async client for concurrent operations</li> <li>Streaming: Reduces latency for long responses</li> <li>Connection Pooling: HTTP client handles connection reuse</li> <li>Lazy Initialization: Only create clients when needed</li> </ol>"},{"location":"reference/providers/ollama/general/#configuration-options","title":"Configuration Options","text":""},{"location":"reference/providers/ollama/general/#essential","title":"Essential","text":"<ul> <li><code>model</code>: Model name (e.g., \"llama3.1\")</li> <li><code>base_url</code>: Ollama server URL (default: \"http://localhost:11434\")</li> <li><code>request_timeout</code>: Timeout in seconds (default: 60.0)</li> </ul>"},{"location":"reference/providers/ollama/general/#generation","title":"Generation","text":"<ul> <li><code>temperature</code>: Sampling temperature (0.0-1.0, default: 0.75)</li> <li><code>context_window</code>: Maximum context tokens (default: 3900)</li> <li><code>json_mode</code>: Force JSON output (default: False)</li> </ul>"},{"location":"reference/providers/ollama/general/#advanced","title":"Advanced","text":"<ul> <li><code>keep_alive</code>: Model keep-alive duration (default: None)</li> <li><code>additional_kwargs</code>: Additional Ollama options</li> <li><code>client</code>: Pre-configured client (default: None, lazy-created)</li> <li><code>async_client</code>: Pre-configured async client (default: None, lazy-created)</li> </ul>"},{"location":"reference/providers/ollama/general/#error-handling","title":"Error Handling","text":""},{"location":"reference/providers/ollama/general/#network-errors","title":"Network Errors","text":"<pre><code>TimeoutError: Request timeout exceeded\nConnectionError: Cannot reach Ollama server\nHTTPError: Server returned error status\n</code></pre>"},{"location":"reference/providers/ollama/general/#parsing-errors","title":"Parsing Errors","text":"<pre><code>JSONDecodeError: Invalid JSON response\nKeyError: Missing required field in response\nValueError: Invalid response format\n</code></pre>"},{"location":"reference/providers/ollama/general/#configuration-errors","title":"Configuration Errors","text":"<pre><code>ValueError: Invalid model or URL\nTypeError: Missing required field\nAssertionError: Invalid parameter value\n</code></pre>"},{"location":"reference/providers/ollama/general/#prerequisites","title":"Prerequisites","text":""},{"location":"reference/providers/ollama/general/#server-requirements","title":"Server Requirements","text":"<pre><code># Install Ollama\ncurl -fsSL https://ollama.com/install.sh | sh\n\n# Pull model\nollama pull llama3.1\n\n# Start server (runs on port 11434 by default)\nollama serve\n</code></pre>"},{"location":"reference/providers/ollama/general/#python-requirements","title":"Python Requirements","text":"<pre><code># Install serapeum-ollama\nuv pip install serapeum-ollama\n\n# Or install from source\nuv pip install -e libs/providers/serapeum-ollama\n</code></pre>"},{"location":"reference/providers/ollama/general/#common-patterns","title":"Common Patterns","text":""},{"location":"reference/providers/ollama/general/#pattern-1-reusable-instance","title":"Pattern 1: Reusable Instance","text":"<pre><code># Create once\nllm = Ollama(model=\"llama3.1\", request_timeout=180)\n\n# Reuse many times\nresponse1 = llm.chat(messages1)\nresponse2 = llm.chat(messages2)\n</code></pre>"},{"location":"reference/providers/ollama/general/#pattern-2-streaming-for-long-responses","title":"Pattern 2: Streaming for Long Responses","text":"<pre><code>llm = Ollama(model=\"llama3.1\", request_timeout=180)\n\nfor chunk in llm.stream_chat(messages):\n    print(chunk.message.content, end=\"\", flush=True)\n</code></pre>"},{"location":"reference/providers/ollama/general/#pattern-3-tool-calling-for-structured-outputs","title":"Pattern 3: Tool Calling for Structured Outputs","text":"<pre><code>llm = Ollama(model=\"llama3.1\", request_timeout=180)\n\n# Define tool from Pydantic model\ntool = CallableTool.from_model(MyModel)\n\n# Get structured output via tool calling\nresponse = llm.chat_with_tools(messages, tools=[tool])\n</code></pre>"},{"location":"reference/providers/ollama/general/#pattern-4-async-for-concurrency","title":"Pattern 4: Async for Concurrency","text":"<pre><code>llm = Ollama(model=\"llama3.1\", request_timeout=180)\n\n# Process multiple requests concurrently\ntasks = [llm.achat(messages) for messages in message_list]\nresponses = await asyncio.gather(*tasks)\n</code></pre>"},{"location":"reference/providers/ollama/general/#troubleshooting","title":"Troubleshooting","text":""},{"location":"reference/providers/ollama/general/#issue-connection-refused","title":"Issue: Connection Refused","text":"<pre><code>Solution: Ensure Ollama server is running\n  $ ollama serve\n</code></pre>"},{"location":"reference/providers/ollama/general/#issue-model-not-found","title":"Issue: Model Not Found","text":"<pre><code>Solution: Pull the model first\n  $ ollama pull llama3.1\n</code></pre>"},{"location":"reference/providers/ollama/general/#issue-timeout","title":"Issue: Timeout","text":"<pre><code>Solution: Increase request_timeout\n  llm = Ollama(model=\"llama3.1\", request_timeout=300)\n</code></pre>"},{"location":"reference/providers/ollama/general/#issue-invalid-json-response","title":"Issue: Invalid JSON Response","text":"<pre><code>Solution: Enable json_mode\n  llm = Ollama(model=\"llama3.1\", json_mode=True)\n</code></pre>"},{"location":"reference/providers/ollama/general/#next-steps","title":"Next Steps","text":"<ol> <li>Start with Examples for practical usage patterns</li> <li>Review Sequence Diagrams to understand execution flow</li> <li>Study Class Diagram to understand architecture</li> <li>Explore Data Flow to understand transformations</li> <li>Check State Management for lifecycle details</li> </ol>"},{"location":"reference/providers/ollama/general/#see-also","title":"See Also","text":"<ul> <li>TextCompletionLLM - Structured completion   orchestrator</li> <li>ToolOrchestratingLLM - Tool-based orchestrator</li> <li>Ollama Official Documentation - Ollama server documentation</li> </ul>"},{"location":"reference/providers/ollama/ollama_class/","title":"Architecture and Class Relationships","text":"<p>This diagram shows the class relationships and inheritance hierarchy for the <code>Ollama</code> LLM implementation.</p>  Hold \"Ctrl\" to enable pan &amp; zoom  <pre><code>classDiagram\n    class BaseLLM {\n        &lt;&lt;abstract&gt;&gt;\n        +metadata: Metadata\n        +chat(messages, **kwargs) ChatResponse\n        +stream_chat(messages, **kwargs) ChatResponseGen\n        +achat(messages, **kwargs) ChatResponse\n        +astream_chat(messages, **kwargs) ChatResponseAsyncGen\n        +complete(prompt, **kwargs) CompletionResponse\n        +stream_complete(prompt, **kwargs) CompletionResponseGen\n        +acomplete(prompt, **kwargs) CompletionResponse\n        +astream_complete(prompt, **kwargs) CompletionResponseAsyncGen\n    }\n\n    class LLM {\n        +system_prompt: Optional[str]\n        +messages_to_prompt: Callable\n        +completion_to_prompt: Callable\n        +output_parser: Optional[BaseParser]\n        +pydantic_program_mode: StructuredLLMMode\n        +_get_prompt(prompt, **kwargs) str\n        +_get_messages(prompt, **kwargs) List[Message]\n        +_parse_output(output) str\n        +_extend_prompt(formatted_prompt) str\n        +_extend_messages(messages) List[Message]\n        +predict(prompt, **kwargs) str\n        +stream(prompt, **kwargs) TokenGen\n        +apredict(prompt, **kwargs) str\n        +astream(prompt, **kwargs) TokenAsyncGen\n        +structured_predict(output_cls, prompt, **kwargs) Model\n    }\n\n    class FunctionCallingLLM {\n        &lt;&lt;abstract&gt;&gt;\n        +chat_with_tools(messages, tools, **kwargs) ChatResponse\n        +achat_with_tools(messages, tools, **kwargs) ChatResponse\n        +stream_chat_with_tools(messages, tools, **kwargs) ChatResponseGen\n        +astream_chat_with_tools(messages, tools, **kwargs) ChatResponseAsyncGen\n        +get_tool_calls_from_response(response, error_on_no_tool_call) List[ToolSelection]\n        #_prepare_chat_with_tools(messages, tools, **kwargs) dict\n        #_validate_chat_with_tools_response(response, tools, **kwargs) ChatResponse\n    }\n\n    class Ollama {\n        +model: str\n        +base_url: str\n        +temperature: float\n        +context_window: int\n        +request_timeout: float\n        +prompt_key: str\n        +json_mode: bool\n        +additional_kwargs: dict\n        +keep_alive: Optional[str]\n        -_client: Optional[Client]\n        -_async_client: Optional[AsyncClient]\n        -_is_function_calling_model: bool\n        +__init__(model, base_url, temperature, ...)\n        +metadata: Metadata\n        +client: Client\n        +async_client: AsyncClient\n        +chat(messages, **kwargs) ChatResponse\n        +stream_chat(messages, **kwargs) ChatResponseGen\n        +achat(messages, **kwargs) ChatResponse\n        +astream_chat(messages, **kwargs) ChatResponseAsyncGen\n        +complete(prompt, **kwargs) CompletionResponse\n        +stream_complete(prompt, **kwargs) CompletionResponseGen\n        +acomplete(prompt, **kwargs) CompletionResponse\n        +astream_complete(prompt, **kwargs) CompletionResponseAsyncGen\n        +chat_with_tools(messages, tools, **kwargs) ChatResponse\n        +stream_chat_with_tools(messages, tools, **kwargs) ChatResponseGen\n        +achat_with_tools(messages, tools, **kwargs) ChatResponse\n        +astream_chat_with_tools(messages, tools, **kwargs) ChatResponseAsyncGen\n        -_chat(messages, stream, **kwargs) ChatResponse\n        -_achat(messages, stream, **kwargs) ChatResponse\n        -_prepare_chat_with_tools(messages, tools, **kwargs) dict\n        -_validate_chat_with_tools_response(response, tools, **kwargs) ChatResponse\n        -_chat_from_response(response) ChatResponse\n        -_chat_stream_from_response(response) ChatResponse\n        #_get_model_kwargs(**kwargs) dict\n    }\n\n    class Client {\n        &lt;&lt;ollama.Client&gt;&gt;\n        +chat(**kwargs) dict\n        +generate(**kwargs) dict\n        +__init__(host, timeout)\n    }\n\n    class AsyncClient {\n        &lt;&lt;ollama.AsyncClient&gt;&gt;\n        +chat(**kwargs) dict\n        +generate(**kwargs) dict\n        +__init__(host, timeout)\n    }\n\n    class Metadata {\n        +model_name: str\n        +context_window: int\n        +num_output: int\n        +is_chat_model: bool\n        +is_function_calling_model: bool\n        +system_role: MessageRole\n    }\n\n    class Message {\n        +role: MessageRole\n        +content: str\n        +additional_kwargs: dict\n        +images: Optional[List[Image]]\n    }\n\n    class MessageRole {\n        &lt;&lt;enumeration&gt;&gt;\n        SYSTEM\n        USER\n        ASSISTANT\n        TOOL\n    }\n\n    class ChatResponse {\n        +message: Message\n        +raw: Optional[dict]\n        +delta: Optional[str]\n        +logprobs: Optional[List]\n        +additional_kwargs: dict\n    }\n\n    class CompletionResponse {\n        +text: str\n        +raw: Optional[dict]\n        +delta: Optional[str]\n        +logprobs: Optional[List]\n        +additional_kwargs: dict\n    }\n\n    class BaseTool {\n        &lt;&lt;protocol&gt;&gt;\n        +metadata: ToolMetadata\n        +call(**kwargs) ToolOutput\n        +acall(**kwargs) ToolOutput\n    }\n\n    class CallableTool {\n        +metadata: ToolMetadata\n        -_fn: Callable\n        +__init__(fn, metadata)\n        +call(**kwargs) ToolOutput\n        +acall(**kwargs) ToolOutput\n        +from_function(fn) CallableTool\n        +from_model(model_cls) CallableTool\n    }\n\n    class ToolMetadata {\n        +name: str\n        +description: str\n        +fn_schema: dict\n    }\n\n    class TextCompletionLLM {\n        -_llm: LLM\n        -_prompt: BasePromptTemplate\n        -_output_parser: PydanticParser\n        -_output_cls: Type[BaseModel]\n        +__call__(**kwargs) BaseModel\n        +acall(**kwargs) BaseModel\n    }\n\n    class ToolOrchestratingLLM {\n        -_llm: FunctionCallingLLM\n        -_prompt: BasePromptTemplate\n        -_output_cls: Type[BaseModel]\n        -_tools: List[BaseTool]\n        -_allow_parallel_tool_calls: bool\n        +__call__(**kwargs) BaseModel | List[BaseModel]\n        +acall(**kwargs) BaseModel | List[BaseModel]\n        +stream_call(**kwargs) Generator[BaseModel]\n        +astream_call(**kwargs) AsyncGenerator[BaseModel]\n    }\n\n    class BaseModel {\n        &lt;&lt;pydantic&gt;&gt;\n        +model_validate_json(json_data) BaseModel\n        +model_json_schema() dict\n    }\n\n    class DummyModel {\n        +value: str\n    }\n\n    class Album {\n        +title: str\n        +artist: str\n        +songs: List[str]\n    }\n\n    %% Inheritance relationships\n    BaseLLM &lt;|-- LLM\n    LLM &lt;|-- FunctionCallingLLM\n    FunctionCallingLLM &lt;|-- Ollama\n    BaseTool &lt;|.. CallableTool\n    BaseModel &lt;|-- DummyModel\n    BaseModel &lt;|-- Album\n\n    %% Composition relationships\n    Ollama o-- Client : uses (lazy init)\n    Ollama o-- AsyncClient : uses (lazy init)\n    Ollama ..&gt; Metadata : provides\n    Ollama ..&gt; ChatResponse : produces\n    Ollama ..&gt; CompletionResponse : produces\n    Ollama ..&gt; Message : consumes/produces\n\n    %% Message relationships\n    Message o-- MessageRole : has\n    ChatResponse o-- Message : contains\n    Message ..&gt; Image : may contain\n\n    %% Tool relationships\n    BaseTool o-- ToolMetadata : has\n    CallableTool ..&gt; ToolMetadata : creates\n    BaseTool ..&gt; BaseModel : may wrap\n\n    %% Orchestrator relationships\n    TextCompletionLLM o-- Ollama : uses\n    TextCompletionLLM ..&gt; DummyModel : produces\n    ToolOrchestratingLLM o-- Ollama : uses\n    ToolOrchestratingLLM o-- CallableTool : uses\n    ToolOrchestratingLLM ..&gt; Album : produces\n\n    note for Ollama \"Main LLM implementation that:\\n1. Connects to Ollama server\\n2. Supports chat and completion\\n3. Handles tool/function calling\\n4. Manages streaming responses\\n5. Provides sync/async interfaces\"\n    note for FunctionCallingLLM \"Abstract class providing:\\n- Tool calling interface\\n- Tool response validation\\n- Tool preparation helpers\"\n    note for Client \"Synchronous Ollama client\\nfrom ollama package\"\n    note for AsyncClient \"Asynchronous Ollama client\\nfrom ollama package\"</code></pre>"},{"location":"reference/providers/ollama/ollama_class/#class-hierarchy","title":"Class Hierarchy","text":""},{"location":"reference/providers/ollama/ollama_class/#inheritance-chain","title":"Inheritance Chain","text":"<pre><code>BaseLLM (abstract)\n  \u2514\u2500\u2192 LLM (adds prompting and structured outputs)\n      \u2514\u2500\u2192 FunctionCallingLLM (abstract, adds tool calling)\n          \u2514\u2500\u2192 Ollama (concrete implementation)\n</code></pre>"},{"location":"reference/providers/ollama/ollama_class/#component-responsibilities","title":"Component Responsibilities","text":""},{"location":"reference/providers/ollama/ollama_class/#ollama","title":"Ollama","text":"<p>Core LLM Implementation - Connection Management: Manages sync/async clients for Ollama server - Request Handling: Builds and executes chat/completion requests - Response Parsing: Converts raw responses to typed models - Tool Integration: Prepares tools in Ollama format, validates responses - Streaming Support: Handles incremental response chunks - Configuration: Manages model settings, temperature, context window, etc.</p>"},{"location":"reference/providers/ollama/ollama_class/#functioncallingllm-parent-class","title":"FunctionCallingLLM (Parent Class)","text":"<p>Tool Calling Abstraction - Tool Interface: Defines standard methods for tool-calling interactions - Tool Preparation: Abstract method for preparing tools in provider format - Response Validation: Ensures tool calls are properly structured - Tool Extraction: Gets tool calls from chat responses</p>"},{"location":"reference/providers/ollama/ollama_class/#llm-grandparent-class","title":"LLM (Grandparent Class)","text":"<p>High-Level Orchestration - Prompt Management: Extends prompts with system messages - Message Formatting: Converts between formats - Structured Outputs: Forces Pydantic model outputs via <code>structured_predict</code> - Parser Integration: Applies output parsers to responses</p>"},{"location":"reference/providers/ollama/ollama_class/#basellm-root-class","title":"BaseLLM (Root Class)","text":"<p>Core Interface - Standard Methods: Defines chat, complete, and their variants - Sync/Async: Requires both synchronous and asynchronous implementations - Streaming: Requires streaming variants of all methods - Metadata: Requires metadata property for capabilities</p>"},{"location":"reference/providers/ollama/ollama_class/#clientasyncclient","title":"Client/AsyncClient","text":"<p>HTTP Communication - API Requests: Handles HTTP communication with Ollama server - Streaming: Supports streaming responses - Configuration: Manages host, timeout, and connection settings</p>"},{"location":"reference/providers/ollama/ollama_class/#messagechatresponsecompletionresponse","title":"Message/ChatResponse/CompletionResponse","text":"<p>Data Models - Message: Represents a single chat message with role and content - ChatResponse: Wraps assistant response with metadata - CompletionResponse: Wraps text completion with metadata</p>"},{"location":"reference/providers/ollama/ollama_class/#tool-classes","title":"Tool Classes","text":"<p>Function Calling - BaseTool: Protocol defining tool interface - CallableTool: Concrete implementation wrapping Python functions or Pydantic models - ToolMetadata: Describes tool name, description, and schema</p>"},{"location":"reference/providers/ollama/ollama_class/#orchestration-classes","title":"Orchestration Classes","text":"<p>High-Level Patterns - TextCompletionLLM: Formats prompts \u2192 calls LLM \u2192 parses to Pydantic - ToolOrchestratingLLM: Formats prompts \u2192 calls LLM with tools \u2192 executes tools \u2192 returns instances</p>"},{"location":"reference/providers/ollama/ollama_class/#design-patterns","title":"Design Patterns","text":""},{"location":"reference/providers/ollama/ollama_class/#1-lazy-initialization","title":"1. Lazy Initialization","text":"<pre><code>@property\ndef client(self) -&gt; Client:\n    if self._client is None:\n        self._client = Client(host=self.base_url, timeout=self.request_timeout)\n    return self._client\n</code></pre>"},{"location":"reference/providers/ollama/ollama_class/#2-decorator-pattern-completion-via-chat","title":"2. Decorator Pattern (Completion via Chat)","text":"<pre><code>@chat_to_completion_decorator\ndef complete(self, prompt: str, **kwargs) -&gt; CompletionResponse:\n    # Decorator handles conversion\n    pass\n</code></pre>"},{"location":"reference/providers/ollama/ollama_class/#3-template-method-pattern","title":"3. Template Method Pattern","text":"<pre><code># FunctionCallingLLM defines workflow\ndef chat_with_tools(self, messages, tools, **kwargs):\n    prepared = self._prepare_chat_with_tools(messages, tools, **kwargs)  # Subclass implements\n    response = self.chat(prepared)\n    validated = self._validate_chat_with_tools_response(response, tools)  # Subclass implements\n    return validated\n</code></pre>"},{"location":"reference/providers/ollama/ollama_class/#4-protocol-based-tools","title":"4. Protocol-Based Tools","text":"<pre><code># BaseTool is a protocol, not a base class\nclass BaseTool(Protocol):\n    def call(self, **kwargs) -&gt; ToolOutput: ...\n</code></pre>"},{"location":"reference/providers/ollama/ollama_class/#integration-points","title":"Integration Points","text":""},{"location":"reference/providers/ollama/ollama_class/#with-textcompletionllm","title":"With TextCompletionLLM","text":"<pre><code>TextCompletionLLM uses Ollama for:\n  - Checking is_chat_model via metadata\n  - Calling chat() or complete()\n  - Getting raw text responses for parsing\n</code></pre>"},{"location":"reference/providers/ollama/ollama_class/#with-toolorchestratingllm","title":"With ToolOrchestratingLLM","text":"<pre><code>ToolOrchestratingLLM uses Ollama for:\n  - Tool-calling capabilities\n  - chat_with_tools() method\n  - Tool call extraction from responses\n</code></pre>"},{"location":"reference/providers/ollama/ollama_class/#with-external-packages","title":"With External Packages","text":"<pre><code>Ollama depends on:\n  - ollama package (Client, AsyncClient)\n  - pydantic (for configuration and models)\n  - serapeum.core (for base classes and types)\n</code></pre>"},{"location":"reference/providers/ollama/ollama_components/","title":"Component Boundaries and Interactions","text":"<p>This diagram shows how components interact during the complete lifecycle of the Ollama LLM.</p>  Hold \"Ctrl\" to enable pan &amp; zoom  <pre><code>graph TB\n    subgraph User Space\n        UC[User Code]\n        PM[Pydantic Models: DummyModel, Album]\n    end\n\n    subgraph Ollama Core\n        OL[Ollama Instance]\n\n        subgraph Configuration\n            CFG[Configuration Fields]\n            MD[Metadata]\n        end\n\n        subgraph Client Management\n            CL[Client Property]\n            ACL[AsyncClient Property]\n            LINIT[Lazy Initialization]\n        end\n\n        subgraph Request Building\n            BCRQ[_chat Request Builder]\n            BCRQ_MSG[Message Converter]\n            BCRQ_OPT[Options Builder]\n            BCRQ_TOOL[Tool Converter]\n        end\n\n        subgraph Response Parsing\n            PRSP[_chat_from_response]\n            PRSP_MSG[Message Parser]\n            PRSP_TOOL[Tool Calls Parser]\n            PRSP_META[Metadata Extractor]\n        end\n\n        subgraph Stream Handling\n            STRM[_chat_stream_from_response]\n            ACC[Accumulator]\n        end\n\n        subgraph Tool Handling\n            PREP[_prepare_chat_with_tools]\n            VAL[_validate_chat_with_tools_response]\n            FORCE[force_single_tool_call]\n        end\n\n        subgraph Decorators\n            C2C[chat_to_completion_decorator]\n            SC2C[stream_chat_to_completion]\n            AC2C[achat_to_completion]\n        end\n    end\n\n    subgraph External Client Layer\n        CLI[ollama.Client]\n        ACLI[ollama.AsyncClient]\n\n        subgraph Client Operations\n            CHAT_OP[chat method]\n            GEN_OP[generate method]\n        end\n    end\n\n    subgraph Ollama Server\n        SRV[Ollama Server Process]\n\n        subgraph API Endpoints\n            EP_CHAT[\"/api/chat\"]\n            EP_GEN[\"/api/generate\"]\n        end\n\n        subgraph Model Runtime\n            MDL[Loaded Model: llama3.1]\n            CTX[Context Window]\n            INF[Inference Engine]\n        end\n    end\n\n    subgraph Orchestrator Layer\n        TCL[TextCompletionLLM]\n        TOL[ToolOrchestratingLLM]\n\n        subgraph Orchestrator Components\n            PRS[PydanticParser]\n            PTMP[PromptTemplate]\n            CTOOL[CallableTool]\n        end\n    end\n\n    subgraph Response Models\n        CRESP[ChatResponse]\n        CORESP[CompletionResponse]\n        MSG[Message]\n    end\n\n    %% Initialization Flow\n    UC --&gt;|1. Initialize| OL\n    OL --&gt;|Store config| CFG\n    OL --&gt;|Create| MD\n    MD --&gt;|is_chat_model=True| OL\n    MD --&gt;|is_function_calling_model=True| OL\n\n    %% Client lazy init\n    OL --&gt;|On first use| LINIT\n    LINIT --&gt;|Create if None| CL\n    LINIT --&gt;|Create if None| ACL\n    CL -.-&gt;|Wraps| CLI\n    ACL -.-&gt;|Wraps| ACLI\n\n    %% Chat Flow\n    UC --&gt;|2a. chat(messages)| OL\n    OL --&gt;|Check tools| PREP\n    PREP --&gt;|Convert tools| BCRQ_TOOL\n    BCRQ_TOOL --&gt;|Merge| BCRQ\n\n    OL --&gt;|Build request| BCRQ\n    BCRQ --&gt;|Convert messages| BCRQ_MSG\n    BCRQ_MSG --&gt;|Add options| BCRQ_OPT\n    BCRQ_OPT --&gt;|Final payload| CL\n\n    CL --&gt;|chat(**request)| CLI\n    CLI --&gt;|HTTP POST| EP_CHAT\n    EP_CHAT --&gt;|Route to| MDL\n    MDL --&gt;|Use| CTX\n    MDL --&gt;|Run| INF\n    INF --&gt;|Generate| EP_CHAT\n    EP_CHAT --&gt;|Response dict| CLI\n    CLI --&gt;|Return| CL\n\n    CL --&gt;|Raw response| PRSP\n    PRSP --&gt;|Extract message| PRSP_MSG\n    PRSP --&gt;|Extract tool_calls| PRSP_TOOL\n    PRSP --&gt;|Extract metadata| PRSP_META\n    PRSP_META --&gt;|Create| CRESP\n    CRESP --&gt;|Contains| MSG\n    CRESP --&gt;|Return| UC\n\n    %% Complete Flow (via decorator)\n    UC --&gt;|2b. complete(prompt)| C2C\n    C2C --&gt;|Wrap to Message| OL\n    OL --&gt;|Delegate chat| CL\n    CL --&gt;|ChatResponse| C2C\n    C2C --&gt;|Extract text| CORESP\n    CORESP --&gt;|Return| UC\n\n    %% Stream Flow\n    UC --&gt;|2c. stream_chat(messages)| OL\n    OL --&gt;|stream=True| CLI\n    CLI --&gt;|Streaming POST| EP_CHAT\n    EP_CHAT -.-&gt;|Chunk 1| CLI\n    CLI -.-&gt;|Chunk 1| STRM\n    STRM -.-&gt;|Accumulate| ACC\n    ACC -.-&gt;|Yield| CRESP\n    CRESP -.-&gt;|Yield| UC\n    EP_CHAT -.-&gt;|Chunk N| CLI\n    CLI -.-&gt;|Chunk N| STRM\n\n    %% Tool calling flow\n    UC --&gt;|2d. chat_with_tools(messages, tools)| OL\n    OL --&gt;|Prepare| PREP\n    PREP --&gt;|Convert to schema| BCRQ_TOOL\n    OL --&gt;|Call chat| CLI\n    CLI --&gt;|Response with tool_calls| PRSP\n    PRSP --&gt;|Parse| PRSP_TOOL\n    PRSP_TOOL --&gt;|Return to| VAL\n    VAL --&gt;|Check parallel| FORCE\n    FORCE --&gt;|Trim if needed| CRESP\n    CRESP --&gt;|Return| UC\n\n    %% TextCompletionLLM Integration\n    UC --&gt;|3a. TextCompletionLLM(llm=Ollama)| TCL\n    TCL --&gt;|Store| OL\n    TCL --&gt;|Use| PTMP\n    TCL --&gt;|Use| PRS\n    UC --&gt;|Call| TCL\n    TCL --&gt;|Format prompt| PTMP\n    PTMP --&gt;|Messages| OL\n    OL --&gt;|chat| CRESP\n    CRESP --&gt;|message.content| PRS\n    PRS --&gt;|Parse JSON| PM\n    PM --&gt;|Return| UC\n\n    %% ToolOrchestratingLLM Integration\n    UC --&gt;|3b. ToolOrchestratingLLM(llm=Ollama)| TOL\n    TOL --&gt;|Store| OL\n    TOL --&gt;|Create tool from| PM\n    PM --&gt;|Schema| CTOOL\n    TOL --&gt;|Use| PTMP\n    UC --&gt;|Call| TOL\n    TOL --&gt;|Format prompt| PTMP\n    PTMP --&gt;|Messages| OL\n    TOL --&gt;|Pass tools| CTOOL\n    CTOOL --&gt;|Convert| OL\n    OL --&gt;|chat_with_tools| CRESP\n    CRESP --&gt;|tool_calls| TOL\n    TOL --&gt;|Execute tool| CTOOL\n    CTOOL --&gt;|Create| PM\n    PM --&gt;|Return| UC\n\n    %% Styling\n    classDef userClass fill:#e1f5ff,stroke:#01579b\n    classDef ollamaClass fill:#e0f2f1,stroke:#004d40\n    classDef configClass fill:#fff9c4,stroke:#f57f17\n    classDef clientClass fill:#f3e5f5,stroke:#4a148c\n    classDef parserClass fill:#e8f5e9,stroke:#1b5e20\n    classDef serverClass fill:#efebe9,stroke:#3e2723\n    classDef orchestratorClass fill:#fce4ec,stroke:#880e4f\n    classDef responseClass fill:#fff3e0,stroke:#e65100\n\n    class UC,PM userClass\n    class OL ollamaClass\n    class CFG,MD,BCRQ,BCRQ_MSG,BCRQ_OPT,BCRQ_TOOL,PRSP,PRSP_MSG,PRSP_TOOL,PRSP_META configClass\n    class CL,ACL,LINIT,CLI,ACLI,CHAT_OP,GEN_OP clientClass\n    class PREP,VAL,FORCE parserClass\n    class SRV,EP_CHAT,EP_GEN,MDL,CTX,INF serverClass\n    class TCL,TOL,PRS,PTMP,CTOOL orchestratorClass\n    class CRESP,CORESP,MSG responseClass</code></pre>"},{"location":"reference/providers/ollama/ollama_components/#component-interaction-patterns","title":"Component Interaction Patterns","text":""},{"location":"reference/providers/ollama/ollama_components/#1-initialization-pattern","title":"1. Initialization Pattern","text":"<pre><code>User Code\n  \u2514\u2500\u2192 Ollama.__init__\n      \u251c\u2500\u2192 Store: model, base_url, temperature, request_timeout, json_mode, additional_kwargs\n      \u251c\u2500\u2192 Create Metadata: is_chat_model=True, is_function_calling_model=True\n      \u2514\u2500\u2192 Set _client=None, _async_client=None (lazy init)\n</code></pre>"},{"location":"reference/providers/ollama/ollama_components/#2-client-lazy-initialization-pattern","title":"2. Client Lazy Initialization Pattern","text":"<pre><code>User \u2192 Ollama.chat\n  \u2514\u2500\u2192 Access self.client property\n      \u2514\u2500\u2192 Check if self._client is None\n          \u251c\u2500\u2192 If None: Create Client(host=base_url, timeout=request_timeout)\n          \u2514\u2500\u2192 Return self._client\n</code></pre>"},{"location":"reference/providers/ollama/ollama_components/#3-chat-request-pattern","title":"3. Chat Request Pattern","text":"<pre><code>User \u2192 Ollama.chat(messages, **kwargs)\n  \u251c\u2500\u2192 _prepare_chat_with_tools (if tools present)\n  \u2502   \u2514\u2500\u2192 For each tool:\n  \u2502       \u251c\u2500\u2192 Extract tool.metadata\n  \u2502       \u251c\u2500\u2192 Get fn_schema from metadata\n  \u2502       \u2514\u2500\u2192 Build Ollama tool dict\n  \u251c\u2500\u2192 _chat(messages, stream=False, **kwargs)\n  \u2502   \u251c\u2500\u2192 Build request dict:\n  \u2502   \u2502   \u251c\u2500\u2192 model: self.model\n  \u2502   \u2502   \u251c\u2500\u2192 messages: [msg.dict() for msg in messages]\n  \u2502   \u2502   \u251c\u2500\u2192 options: {temperature, ...}\n  \u2502   \u2502   \u251c\u2500\u2192 format: \"json\" if json_mode\n  \u2502   \u2502   \u2514\u2500\u2192 tools: converted tool dicts\n  \u2502   \u251c\u2500\u2192 Ensure client initialized\n  \u2502   \u251c\u2500\u2192 client.chat(**request)\n  \u2502   \u2514\u2500\u2192 _chat_from_response(raw_response)\n  \u2502       \u251c\u2500\u2192 Extract message dict\n  \u2502       \u251c\u2500\u2192 Parse role, content, tool_calls\n  \u2502       \u251c\u2500\u2192 Create Message object\n  \u2502       \u251c\u2500\u2192 Extract metadata: model, times, tokens\n  \u2502       \u2514\u2500\u2192 Create ChatResponse\n  \u2514\u2500\u2192 Return ChatResponse\n</code></pre>"},{"location":"reference/providers/ollama/ollama_components/#4-completion-via-decorator-pattern","title":"4. Completion via Decorator Pattern","text":"<pre><code>User \u2192 Ollama.complete(prompt, **kwargs)\n  \u2514\u2500\u2192 @chat_to_completion_decorator wrapper\n      \u251c\u2500\u2192 Convert prompt to Message(role=USER, content=prompt)\n      \u251c\u2500\u2192 Call self.chat([message], **kwargs)\n      \u251c\u2500\u2192 Receive ChatResponse\n      \u251c\u2500\u2192 Extract text = response.message.content\n      \u2514\u2500\u2192 Return CompletionResponse(text=text, raw=response.raw, ...)\n</code></pre>"},{"location":"reference/providers/ollama/ollama_components/#5-streaming-pattern","title":"5. Streaming Pattern","text":"<pre><code>User \u2192 Ollama.stream_chat(messages, **kwargs)\n  \u2514\u2500\u2192 _chat(messages, stream=True, **kwargs)\n      \u251c\u2500\u2192 Build request with stream=True\n      \u251c\u2500\u2192 client.chat(**request) returns iterator\n      \u2514\u2500\u2192 For each chunk:\n          \u251c\u2500\u2192 _chat_stream_from_response(chunk)\n          \u2502   \u251c\u2500\u2192 Extract delta content\n          \u2502   \u251c\u2500\u2192 Accumulate tool_calls\n          \u2502   \u2514\u2500\u2192 Create ChatResponse with delta\n          \u2514\u2500\u2192 Yield ChatResponse\n</code></pre>"},{"location":"reference/providers/ollama/ollama_components/#6-tool-calling-pattern","title":"6. Tool Calling Pattern","text":"<pre><code>User \u2192 Ollama.chat_with_tools(messages, tools, **kwargs)\n  \u251c\u2500\u2192 _prepare_chat_with_tools(messages, tools, **kwargs)\n  \u2502   \u2514\u2500\u2192 For each tool in tools:\n  \u2502       \u251c\u2500\u2192 Get tool.metadata.fn_schema\n  \u2502       \u2514\u2500\u2192 Build: {\"type\": \"function\", \"function\": {\"name\": ..., \"parameters\": schema}}\n  \u251c\u2500\u2192 Merge tools into kwargs\n  \u251c\u2500\u2192 Call self.chat(messages, **kwargs)\n  \u251c\u2500\u2192 Receive ChatResponse with tool_calls\n  \u251c\u2500\u2192 _validate_chat_with_tools_response(response, tools, **kwargs)\n  \u2502   \u2514\u2500\u2192 If not allow_parallel_tool_calls:\n  \u2502       \u2514\u2500\u2192 force_single_tool_call(response)\n  \u2514\u2500\u2192 Return ChatResponse\n</code></pre>"},{"location":"reference/providers/ollama/ollama_components/#7-textcompletionllm-integration-pattern","title":"7. TextCompletionLLM Integration Pattern","text":"<pre><code>User \u2192 TextCompletionLLM(output_parser=parser, prompt=prompt, llm=Ollama(...))\n  \u2514\u2500\u2192 TextCompletionLLM stores Ollama instance\n\nUser \u2192 text_llm(key=\"value\")\n  \u251c\u2500\u2192 Check llm.metadata.is_chat_model \u2192 True\n  \u251c\u2500\u2192 Format prompt with variables \u2192 List[Message]\n  \u251c\u2500\u2192 Ollama.chat(messages) \u2192 ChatResponse\n  \u251c\u2500\u2192 Extract response.message.content\n  \u251c\u2500\u2192 PydanticParser.parse(content) \u2192 DummyModel\n  \u2514\u2500\u2192 Return DummyModel instance\n</code></pre>"},{"location":"reference/providers/ollama/ollama_components/#8-toolorchestratingllm-integration-pattern","title":"8. ToolOrchestratingLLM Integration Pattern","text":"<pre><code>User \u2192 ToolOrchestratingLLM(output_cls=Album, prompt=prompt, llm=Ollama(...))\n  \u251c\u2500\u2192 Convert Album Pydantic model to CallableTool\n  \u2514\u2500\u2192 Store Ollama instance\n\nUser \u2192 tools_llm(topic=\"rock\")\n  \u251c\u2500\u2192 Format prompt with topic \u2192 List[Message]\n  \u251c\u2500\u2192 CallableTool.metadata.fn_schema \u2192 Album JSON schema\n  \u251c\u2500\u2192 Ollama.chat_with_tools(messages, tools=[album_tool])\n  \u2502   \u251c\u2500\u2192 _prepare_chat_with_tools converts tool to Ollama format\n  \u2502   \u251c\u2500\u2192 Server returns tool_calls with arguments\n  \u2502   \u2514\u2500\u2192 Return ChatResponse with tool_calls\n  \u251c\u2500\u2192 Extract tool_calls from response\n  \u251c\u2500\u2192 For each tool_call:\n  \u2502   \u251c\u2500\u2192 Get function name and arguments\n  \u2502   \u251c\u2500\u2192 Execute CallableTool.call(**arguments)\n  \u2502   \u2514\u2500\u2192 Creates Album instance from arguments\n  \u2514\u2500\u2192 Return Album instance (or list if parallel)\n</code></pre>"},{"location":"reference/providers/ollama/ollama_components/#component-state-management","title":"Component State Management","text":""},{"location":"reference/providers/ollama/ollama_components/#ollama-instance-state","title":"Ollama Instance State","text":"<pre><code>Initialization:\n  - model: str (immutable after init)\n  - base_url: str (immutable after init)\n  - request_timeout: float (immutable after init)\n  - temperature: float (immutable after init)\n  - json_mode: bool (immutable after init)\n  - additional_kwargs: dict (immutable after init)\n  - _client: Optional[Client] (mutable, lazy-initialized)\n  - _async_client: Optional[AsyncClient] (mutable, lazy-initialized)\n\nRuntime:\n  - _client: None \u2192 Client instance (on first use)\n  - _async_client: None \u2192 AsyncClient instance (on first async use)\n</code></pre>"},{"location":"reference/providers/ollama/ollama_components/#request-state-per-call","title":"Request State (Per Call)","text":"<pre><code>Input:\n  - messages: List[Message]\n  - tools: Optional[List[BaseTool]]\n  - stream: bool\n  - **kwargs: Additional options\n\nProcessing:\n  - request_dict: Built from inputs\n  - raw_response: dict from server\n  - parsed_response: ChatResponse/CompletionResponse\n\nOutput:\n  - ChatResponse or CompletionResponse\n</code></pre>"},{"location":"reference/providers/ollama/ollama_components/#streaming-state-per-stream","title":"Streaming State (Per Stream)","text":"<pre><code>Initialization:\n  - iterator: From client.chat(stream=True)\n\nPer Chunk:\n  - chunk_dict: Raw chunk from server\n  - accumulated_content: Growing string\n  - accumulated_tool_calls: Growing list\n  - delta: New content in this chunk\n\nOutput:\n  - Generator yielding ChatResponse objects\n</code></pre>"},{"location":"reference/providers/ollama/ollama_components/#error-boundaries","title":"Error Boundaries","text":""},{"location":"reference/providers/ollama/ollama_components/#1-configuration-errors-initialization","title":"1. Configuration Errors (Initialization)","text":"<pre><code>Ollama.__init__\n  \u2514\u2500\u2192 Validate inputs\n      \u251c\u2500\u2192 model: must be non-empty string\n      \u251c\u2500\u2192 base_url: must be valid URL\n      \u251c\u2500\u2192 temperature: must be in [0.0, 1.0]\n      \u2514\u2500\u2192 request_timeout: must be positive\n</code></pre>"},{"location":"reference/providers/ollama/ollama_components/#2-client-creation-errors-first-use","title":"2. Client Creation Errors (First Use)","text":"<pre><code>client property\n  \u2514\u2500\u2192 Create Client(host, timeout)\n      \u2514\u2500\u2192 Catch: ValueError, ConnectionError\n</code></pre>"},{"location":"reference/providers/ollama/ollama_components/#3-request-errors-during-call","title":"3. Request Errors (During Call)","text":"<pre><code>client.chat(**request)\n  \u2514\u2500\u2192 Catch: TimeoutError, ConnectionError, HTTPError\n      \u2514\u2500\u2192 Wrap and re-raise with context\n</code></pre>"},{"location":"reference/providers/ollama/ollama_components/#4-parsing-errors-response-processing","title":"4. Parsing Errors (Response Processing)","text":"<pre><code>_chat_from_response(raw_response)\n  \u2514\u2500\u2192 Extract required fields\n      \u2514\u2500\u2192 Catch: KeyError, TypeError\n          \u2514\u2500\u2192 Log warning and return default\n</code></pre>"},{"location":"reference/providers/ollama/ollama_components/#5-tool-validation-errors","title":"5. Tool Validation Errors","text":"<pre><code>_prepare_chat_with_tools(messages, tools)\n  \u2514\u2500\u2192 For each tool:\n      \u2514\u2500\u2192 Validate metadata.fn_schema exists\n          \u2514\u2500\u2192 Raise ValueError if missing\n</code></pre>"},{"location":"reference/providers/ollama/ollama_components/#component-dependencies","title":"Component Dependencies","text":""},{"location":"reference/providers/ollama/ollama_components/#ollama-depends-on","title":"Ollama Depends On:","text":"<ul> <li><code>ollama.Client</code> and <code>ollama.AsyncClient</code> (external package)</li> <li><code>serapeum.core.base.llms.types</code> (Message, ChatResponse, CompletionResponse, Metadata)</li> <li><code>serapeum.core.llms.function_calling.FunctionCallingLLM</code> (base class)</li> <li><code>serapeum.core.base.llms.utils</code> (decorators)</li> <li><code>pydantic</code> (for configuration)</li> </ul>"},{"location":"reference/providers/ollama/ollama_components/#ollama-is-used-by","title":"Ollama Is Used By:","text":"<ul> <li><code>TextCompletionLLM</code> (as the LLM engine)</li> <li><code>ToolOrchestratingLLM</code> (as the function-calling LLM)</li> <li>Direct user code (standalone usage)</li> </ul>"},{"location":"reference/providers/ollama/ollama_components/#external-dependencies","title":"External Dependencies:","text":"<ul> <li>Ollama Server: Must be running and accessible at base_url</li> <li>Model: Must be pulled and available on the server</li> </ul>"},{"location":"reference/providers/ollama/ollama_dataflow/","title":"Data Transformations and Validation","text":"<p>This diagram shows how data flows and transforms through the Ollama LLM system.</p>  Hold \"Ctrl\" to enable pan &amp; zoom  <pre><code>flowchart TD\n    Start([User Code]) --&gt; Init{Initialize Ollama}\n\n    Init --&gt; SetConfig[Set Configuration]\n    SetConfig --&gt; StoreModel[Store model name]\n    StoreModel --&gt; StoreURL[Store base_url]\n    StoreURL --&gt; StoreTimeout[Store request_timeout]\n    StoreTimeout --&gt; StoreTemp[Store temperature]\n    StoreTemp --&gt; StoreJSON[Store json_mode flag]\n    StoreJSON --&gt; StoreKwargs[Store additional_kwargs]\n\n    StoreKwargs --&gt; CreateMetadata[Create Metadata]\n    CreateMetadata --&gt; SetChatFlag[Set is_chat_model=True]\n    SetChatFlag --&gt; SetFnCallFlag[Set is_function_calling_model]\n    SetFnCallFlag --&gt; SetContext[Set context_window]\n    SetContext --&gt; Ready([Ollama Instance Ready])\n\n    Ready --&gt; CallType{Call Type?}\n\n    CallType --&gt;|chat| ChatPath[Chat Path]\n    CallType --&gt;|complete| CompletePath[Complete Path]\n    CallType --&gt;|chat_with_tools| ToolsPath[Tools Path]\n    CallType --&gt;|stream_chat| StreamPath[Stream Path]\n\n    %% Chat Path\n    ChatPath --&gt; ValidateMessages{Messages Valid?}\n    ValidateMessages --&gt;|No| Error1[Raise ValueError]\n    ValidateMessages --&gt;|Yes| BuildChatReq[Build Chat Request]\n\n    BuildChatReq --&gt; AddModel[Add model name]\n    AddModel --&gt; ConvertMessages[Convert Messages to dicts]\n    ConvertMessages --&gt; AddOptions[Add options: temperature, etc.]\n    AddOptions --&gt; CheckJSON{json_mode?}\n    CheckJSON --&gt;|True| AddFormat[Add format: json]\n    CheckJSON --&gt;|False| AddKeepAlive\n    AddFormat --&gt; AddKeepAlive[Add keep_alive]\n\n    AddKeepAlive --&gt; EnsureClient[Ensure client initialized]\n    EnsureClient --&gt; CheckClient{Client exists?}\n    CheckClient --&gt;|No| CreateClient[Create Client with base_url, timeout]\n    CheckClient --&gt;|Yes| SendRequest\n    CreateClient --&gt; SendRequest[Send HTTP POST /api/chat]\n\n    SendRequest --&gt; ReceiveRaw[Receive raw dict response]\n    ReceiveRaw --&gt; ParseResponse[_chat_from_response]\n\n    ParseResponse --&gt; ExtractMsg[Extract message dict]\n    ExtractMsg --&gt; ParseRole[Parse role: assistant]\n    ParseRole --&gt; ParseContent[Parse content: str]\n    ParseContent --&gt; CheckTools{tool_calls present?}\n    CheckTools --&gt;|Yes| ParseToolCalls[Parse tool_calls array]\n    CheckTools --&gt;|No| CreateMessage1\n    ParseToolCalls --&gt; CreateMessage1[Create Message object]\n\n    CreateMessage1 --&gt; ExtractMeta[Extract metadata: model, times, tokens]\n    ExtractMeta --&gt; CreateChatResp[Create ChatResponse]\n    CreateChatResp --&gt; ReturnChat([Return ChatResponse])\n\n    %% Complete Path\n    CompletePath --&gt; Decorator[@chat_to_completion_decorator]\n    Decorator --&gt; WrapPrompt[Wrap prompt in Message]\n    WrapPrompt --&gt; SetRole[role=USER, content=prompt]\n    SetRole --&gt; CallChat[Delegate to chat method]\n    CallChat --&gt; ChatPath\n    ReturnChat --&gt; UnwrapDecorator[Decorator unwraps response]\n    UnwrapDecorator --&gt; ExtractText[Extract message.content as text]\n    ExtractText --&gt; CreateCompleteResp[Create CompletionResponse]\n    CreateCompleteResp --&gt; ReturnComplete([Return CompletionResponse])\n\n    %% Tools Path\n    ToolsPath --&gt; PrepareTools[_prepare_chat_with_tools]\n    PrepareTools --&gt; ConvertToolsLoop[For each tool in tools]\n    ConvertToolsLoop --&gt; ExtractToolMeta[Extract tool.metadata]\n    ExtractToolMeta --&gt; GetSchema[Get fn_schema from metadata]\n    GetSchema --&gt; BuildToolDict[Build Ollama tool dict]\n    BuildToolDict --&gt; AddToolType[Add type: function]\n    AddToolType --&gt; AddToolFunc[Add function: name, description, parameters]\n\n    AddToolFunc --&gt; MergeKwargs[Merge tools into kwargs]\n    MergeKwargs --&gt; CallChatWithTools[Call chat with tools kwarg]\n    CallChatWithTools --&gt; SendRequestTools[HTTP POST with tools array]\n    SendRequestTools --&gt; ReceiveToolResp[Receive response with tool_calls]\n    ReceiveToolResp --&gt; ValidateTools[_validate_chat_with_tools_response]\n    ValidateTools --&gt; CheckParallel{allow_parallel?}\n    CheckParallel --&gt;|No| ForceSingle[force_single_tool_call]\n    CheckParallel --&gt;|Yes| ReturnToolResp\n    ForceSingle --&gt; ReturnToolResp([Return ChatResponse with tools])\n\n    %% Stream Path\n    StreamPath --&gt; BuildStreamReq[Build chat request with stream=True]\n    BuildStreamReq --&gt; SendStreamReq[HTTP POST /api/chat streaming]\n    SendStreamReq --&gt; StreamLoop{For each chunk}\n\n    StreamLoop --&gt; ReceiveChunk[Receive chunk dict]\n    ReceiveChunk --&gt; ParseChunk[_chat_stream_from_response]\n    ParseChunk --&gt; ExtractDelta[Extract message delta]\n    ExtractDelta --&gt; AccumContent[Accumulate content]\n    AccumContent --&gt; CheckToolChunk{tool_calls in chunk?}\n    CheckToolChunk --&gt;|Yes| AccumTools[Accumulate tool_calls]\n    CheckToolChunk --&gt;|No| CreateStreamResp\n    AccumTools --&gt; CreateStreamResp[Create ChatResponse with delta]\n    CreateStreamResp --&gt; YieldResp[Yield ChatResponse]\n    YieldResp --&gt; CheckDone{done=True?}\n    CheckDone --&gt;|No| StreamLoop\n    CheckDone --&gt;|Yes| EndStream([Stream Complete])\n\n    %% Error paths\n    Error1 --&gt; ErrorEnd([Raise Exception])\n\n    %% Styling\n    style Start fill:#e1f5ff\n    style Ready fill:#e1f5ff\n    style ReturnChat fill:#c8e6c9\n    style ReturnComplete fill:#c8e6c9\n    style ReturnToolResp fill:#c8e6c9\n    style EndStream fill:#c8e6c9\n    style Error1 fill:#ffcdd2\n    style ErrorEnd fill:#ffcdd2\n    style SendRequest fill:#fff9c4\n    style SendRequestTools fill:#fff9c4\n    style SendStreamReq fill:#fff9c4</code></pre>"},{"location":"reference/providers/ollama/ollama_dataflow/#data-transformation-examples","title":"Data Transformation Examples","text":""},{"location":"reference/providers/ollama/ollama_dataflow/#1-initialization","title":"1. Initialization","text":"<pre><code>Input:\n  Ollama(model=\"llama3.1\", base_url=\"http://localhost:11434\", request_timeout=180)\n\nTransformations:\n  1. Store configuration:\n     - model = \"llama3.1\"\n     - base_url = \"http://localhost:11434\"\n     - request_timeout = 180\n     - temperature = 0.75 (default)\n     - json_mode = False (default)\n\n  2. Create metadata:\n     - model_name = \"llama3.1\"\n     - is_chat_model = True\n     - is_function_calling_model = True\n     - context_window = 3900\n     - num_output = 256\n\nOutput:\n  Ollama instance with lazy-initialized client\n</code></pre>"},{"location":"reference/providers/ollama/ollama_dataflow/#2-chat-request","title":"2. Chat Request","text":"<pre><code>Input:\n  messages = [Message(role=MessageRole.USER, content=\"Say 'pong'.\")]\n  kwargs = {\"temperature\": 0.2}\n\nTransformations:\n  1. Convert messages to dicts:\n     [{\"role\": \"user\", \"content\": \"Say 'pong'.\"}]\n\n  2. Build request payload:\n     {\n       \"model\": \"llama3.1\",\n       \"messages\": [{\"role\": \"user\", \"content\": \"Say 'pong'.\"}],\n       \"options\": {\"temperature\": 0.2},\n       \"stream\": False,\n       \"keep_alive\": None\n     }\n\n  3. HTTP POST to /api/chat\n\n  4. Raw response:\n     {\n       \"model\": \"llama3.1\",\n       \"created_at\": \"2025-01-22T...\",\n       \"message\": {\n         \"role\": \"assistant\",\n         \"content\": \"Pong!\"\n       },\n       \"done\": True,\n       \"total_duration\": 1234567890,\n       \"prompt_eval_count\": 10,\n       \"eval_count\": 2\n     }\n\n  5. Parse to ChatResponse:\n     ChatResponse(\n       message=Message(\n         role=MessageRole.ASSISTANT,\n         content=\"Pong!\",\n         additional_kwargs={}\n       ),\n       raw={...},\n       additional_kwargs={\n         \"model\": \"llama3.1\",\n         \"created_at\": \"...\",\n         \"total_duration\": 1234567890,\n         \"prompt_eval_count\": 10,\n         \"eval_count\": 2\n       }\n     )\n\nOutput:\n  ChatResponse with assistant message\n</code></pre>"},{"location":"reference/providers/ollama/ollama_dataflow/#3-complete-request-via-decorator","title":"3. Complete Request (via Decorator)","text":"<pre><code>Input:\n  prompt = \"Say 'pong'.\"\n  kwargs = {}\n\nTransformations:\n  1. Decorator wraps prompt:\n     Message(role=MessageRole.USER, content=\"Say 'pong'.\")\n\n  2. Delegates to chat([message], **kwargs)\n     [Follows Chat Request flow above]\n\n  3. Decorator extracts text:\n     text = chat_response.message.content  # \"Pong!\"\n\n  4. Creates CompletionResponse:\n     CompletionResponse(\n       text=\"Pong!\",\n       raw={...},\n       additional_kwargs={...}\n     )\n\nOutput:\n  CompletionResponse with text\n</code></pre>"},{"location":"reference/providers/ollama/ollama_dataflow/#4-chat-with-tools","title":"4. Chat with Tools","text":"<pre><code>Input:\n  messages = [Message(role=MessageRole.USER, content=\"Create album about rock\")]\n  tools = [CallableTool(fn=create_album, metadata=ToolMetadata(...))]\n  kwargs = {}\n\nTransformations:\n  1. Convert each tool to Ollama format:\n     {\n       \"type\": \"function\",\n       \"function\": {\n         \"name\": \"create_album\",\n         \"description\": \"Create an album with title and songs\",\n         \"parameters\": {\n           \"type\": \"object\",\n           \"properties\": {\n             \"title\": {\"type\": \"string\"},\n             \"artist\": {\"type\": \"string\"},\n             \"songs\": {\"type\": \"array\", \"items\": {\"type\": \"string\"}}\n           },\n           \"required\": [\"title\", \"artist\", \"songs\"]\n         }\n       }\n     }\n\n  2. Build request with tools:\n     {\n       \"model\": \"llama3.1\",\n       \"messages\": [...],\n       \"tools\": [&lt;converted tool dicts&gt;],\n       \"stream\": False\n     }\n\n  3. HTTP POST to /api/chat\n\n  4. Raw response with tool_calls:\n     {\n       \"message\": {\n         \"role\": \"assistant\",\n         \"content\": \"\",\n         \"tool_calls\": [\n           {\n             \"function\": {\n               \"name\": \"create_album\",\n               \"arguments\": {\n                 \"title\": \"Rock Legends\",\n                 \"artist\": \"Various Artists\",\n                 \"songs\": [\"Song 1\", \"Song 2\"]\n               }\n             }\n           }\n         ]\n       },\n       ...\n     }\n\n  5. Parse tool_calls in message:\n     Message(\n       role=MessageRole.ASSISTANT,\n       content=\"\",\n       additional_kwargs={\n         \"tool_calls\": [\n           {\n             \"function\": {\n               \"name\": \"create_album\",\n               \"arguments\": {...}\n             }\n           }\n         ]\n       }\n     )\n\n  6. If not allow_parallel, force_single_tool_call:\n     Keep only first tool call\n\nOutput:\n  ChatResponse with tool_calls in message.additional_kwargs\n</code></pre>"},{"location":"reference/providers/ollama/ollama_dataflow/#5-streaming-chat","title":"5. Streaming Chat","text":"<pre><code>Input:\n  messages = [Message(role=MessageRole.USER, content=\"Count to 3\")]\n  stream = True\n\nTransformations:\n  1. Build request with stream=True\n\n  2. HTTP POST returns chunk iterator\n\n  3. For each chunk:\n     Chunk 1: {\"message\": {\"content\": \"1\"}, \"done\": False}\n       \u2192 ChatResponse(message=Message(content=\"1\"), delta=\"1\")\n       \u2192 Yield\n\n     Chunk 2: {\"message\": {\"content\": \", 2\"}, \"done\": False}\n       \u2192 ChatResponse(message=Message(content=\", 2\"), delta=\", 2\")\n       \u2192 Yield\n\n     Chunk 3: {\"message\": {\"content\": \", 3\"}, \"done\": True}\n       \u2192 ChatResponse(message=Message(content=\", 3\"), delta=\", 3\")\n       \u2192 Yield\n\nOutput:\n  Generator yielding ChatResponse objects\n</code></pre>"},{"location":"reference/providers/ollama/ollama_dataflow/#validation-points","title":"Validation Points","text":""},{"location":"reference/providers/ollama/ollama_dataflow/#1-configuration-validation","title":"1. Configuration Validation","text":"<ul> <li>model: Must be non-empty string</li> <li>base_url: Must be valid URL format</li> <li>temperature: Must be float in range [0.0, 1.0]</li> <li>request_timeout: Must be positive float</li> </ul>"},{"location":"reference/providers/ollama/ollama_dataflow/#2-message-validation","title":"2. Message Validation","text":"<ul> <li>messages: Must be non-empty list</li> <li>role: Must be valid MessageRole enum</li> <li>content: Must be string (can be empty for tool calls)</li> </ul>"},{"location":"reference/providers/ollama/ollama_dataflow/#3-tool-validation","title":"3. Tool Validation","text":"<ul> <li>tools: Must be list of BaseTool</li> <li>tool.metadata: Must have name, description, fn_schema</li> <li>fn_schema: Must be valid JSON schema dict</li> </ul>"},{"location":"reference/providers/ollama/ollama_dataflow/#4-response-validation","title":"4. Response Validation","text":"<ul> <li>HTTP status: Must be 200, else raise error</li> <li>JSON parsing: Must be valid JSON</li> <li>Required fields: Must have message/text in response</li> <li>tool_calls format: Must match expected structure</li> </ul>"},{"location":"reference/providers/ollama/ollama_dataflow/#error-handling","title":"Error Handling","text":""},{"location":"reference/providers/ollama/ollama_dataflow/#network-errors","title":"Network Errors","text":"<pre><code>Request \u2192 Timeout \u2192 Raise RequestException with timeout info\nRequest \u2192 Connection Error \u2192 Raise ConnectionError with URL\nRequest \u2192 HTTP Error \u2192 Raise HTTPError with status code\n</code></pre>"},{"location":"reference/providers/ollama/ollama_dataflow/#parsing-errors","title":"Parsing Errors","text":"<pre><code>Response \u2192 Invalid JSON \u2192 Raise JSONDecodeError\nResponse \u2192 Missing fields \u2192 Raise KeyError\nResponse \u2192 Invalid tool_calls \u2192 Log warning, return empty list\n</code></pre>"},{"location":"reference/providers/ollama/ollama_dataflow/#configuration-errors","title":"Configuration Errors","text":"<pre><code>Invalid model \u2192 Raise ValueError\nInvalid URL \u2192 Raise ValueError\nMissing required field \u2192 Raise TypeError\n</code></pre>"},{"location":"reference/providers/ollama/ollama_dataflow/#data-flow-summary","title":"Data Flow Summary","text":"<pre><code>User Input\n  \u2193\nConfiguration/Validation\n  \u2193\nRequest Building (convert to Ollama format)\n  \u2193\nClient Initialization (lazy)\n  \u2193\nHTTP Request (sync/async)\n  \u2193\nRaw Response (dict)\n  \u2193\nResponse Parsing (to typed models)\n  \u2193\nValidation/Post-processing\n  \u2193\nTyped Response (ChatResponse/CompletionResponse)\n  \u2193\nUser Output\n</code></pre>"},{"location":"reference/providers/ollama/ollama_sequence/","title":"Execution Flow and Method Calls","text":"<p>This diagram shows the complete workflow from initialization to execution of the <code>Ollama</code> class.</p>  Hold \"Ctrl\" to enable pan &amp; zoom  <pre><code>sequenceDiagram\n    participant User\n    participant Ollama\n    participant Client/AsyncClient\n    participant OllamaServer\n    participant TextCompletionLLM\n    participant ToolOrchestratingLLM\n    participant PydanticParser\n\n    Note over User: Initialization Phase\n    User-&gt;&gt;Ollama: __init__(model, base_url, request_timeout, ...)\n    activate Ollama\n\n    Ollama-&gt;&gt;Ollama: Set model configuration\n    Note over Ollama: Store: model, base_url, temperature,&lt;br/&gt;request_timeout, json_mode, etc.\n\n    Ollama-&gt;&gt;Ollama: Initialize metadata\n    Note over Ollama: Set is_chat_model=True&lt;br/&gt;is_function_calling_model=True&lt;br/&gt;context_window, num_output\n\n    alt client not provided\n        Ollama-&gt;&gt;Client/AsyncClient: Create lazy client\n        Note over Client/AsyncClient: Client created on first use&lt;br/&gt;with base_url and timeout\n    else client provided\n        Ollama-&gt;&gt;Ollama: Store provided client\n    end\n\n    Ollama--&gt;&gt;User: Ollama instance\n    deactivate Ollama\n\n    Note over User: Direct Usage - Chat Method\n    User-&gt;&gt;Ollama: chat(messages, **kwargs)\n    activate Ollama\n\n    Ollama-&gt;&gt;Ollama: _prepare_chat_with_tools(messages, tools)\n    Note over Ollama: Convert tools to Ollama format&lt;br/&gt;if tools provided\n\n    Ollama-&gt;&gt;Ollama: _chat(messages, stream=False, **kwargs)\n    Ollama-&gt;&gt;Client/AsyncClient: Ensure client initialized\n    activate Client/AsyncClient\n    Client/AsyncClient--&gt;&gt;Ollama: client instance\n    deactivate Client/AsyncClient\n\n    Ollama-&gt;&gt;Ollama: Build request payload\n    Note over Ollama: Combine: model, messages,&lt;br/&gt;options (temp, etc.), format, tools\n\n    Ollama-&gt;&gt;Client/AsyncClient: client.chat(**request)\n    activate Client/AsyncClient\n    Client/AsyncClient-&gt;&gt;OllamaServer: HTTP POST /api/chat\n    activate OllamaServer\n    OllamaServer--&gt;&gt;Client/AsyncClient: JSON response\n    deactivate OllamaServer\n    Client/AsyncClient--&gt;&gt;Ollama: raw response dict\n    deactivate Client/AsyncClient\n\n    Ollama-&gt;&gt;Ollama: _chat_from_response(response)\n    Note over Ollama: Parse message, tool_calls,&lt;br/&gt;additional_kwargs\n\n    Ollama--&gt;&gt;User: ChatResponse\n    deactivate Ollama\n\n    Note over User: Direct Usage - Complete Method\n    User-&gt;&gt;Ollama: complete(prompt, **kwargs)\n    activate Ollama\n\n    Ollama-&gt;&gt;Ollama: @chat_to_completion_decorator\n    Note over Ollama: Converts prompt to Message&lt;br/&gt;and delegates to chat()\n\n    Ollama-&gt;&gt;Ollama: chat([Message(USER, prompt)], **kwargs)\n    Note over Ollama: Follows chat flow above\n\n    Ollama-&gt;&gt;Ollama: Extract text from ChatResponse\n    Ollama--&gt;&gt;User: CompletionResponse\n    deactivate Ollama\n\n    Note over User: Usage with TextCompletionLLM\n    User-&gt;&gt;PydanticParser: Create with output_cls\n    activate PydanticParser\n    PydanticParser--&gt;&gt;User: parser\n    deactivate PydanticParser\n\n    User-&gt;&gt;TextCompletionLLM: __init__(parser, prompt, llm=Ollama)\n    activate TextCompletionLLM\n    TextCompletionLLM-&gt;&gt;TextCompletionLLM: Validate components\n    TextCompletionLLM--&gt;&gt;User: text_llm instance\n    deactivate TextCompletionLLM\n\n    User-&gt;&gt;TextCompletionLLM: __call__(value=\"input\")\n    activate TextCompletionLLM\n\n    TextCompletionLLM-&gt;&gt;Ollama: Check metadata.is_chat_model\n    activate Ollama\n    Ollama--&gt;&gt;TextCompletionLLM: True\n    deactivate Ollama\n\n    TextCompletionLLM-&gt;&gt;TextCompletionLLM: Format prompt with variables\n    TextCompletionLLM-&gt;&gt;Ollama: chat(formatted_messages)\n    activate Ollama\n    Ollama-&gt;&gt;OllamaServer: HTTP POST /api/chat\n    activate OllamaServer\n    OllamaServer--&gt;&gt;Ollama: JSON response\n    deactivate OllamaServer\n    Ollama--&gt;&gt;TextCompletionLLM: ChatResponse\n    deactivate Ollama\n\n    TextCompletionLLM-&gt;&gt;PydanticParser: parse(response.message.content)\n    activate PydanticParser\n    PydanticParser--&gt;&gt;TextCompletionLLM: Parsed model instance\n    deactivate PydanticParser\n\n    TextCompletionLLM--&gt;&gt;User: DummyModel instance\n    deactivate TextCompletionLLM\n\n    Note over User: Usage with ToolOrchestratingLLM\n    User-&gt;&gt;ToolOrchestratingLLM: __init__(output_cls=Album, prompt, llm=Ollama)\n    activate ToolOrchestratingLLM\n    ToolOrchestratingLLM-&gt;&gt;ToolOrchestratingLLM: Create tool from output_cls\n    Note over ToolOrchestratingLLM: Convert Album Pydantic model&lt;br/&gt;to CallableTool\n    ToolOrchestratingLLM--&gt;&gt;User: tools_llm instance\n    deactivate ToolOrchestratingLLM\n\n    User-&gt;&gt;ToolOrchestratingLLM: __call__(topic=\"rock\")\n    activate ToolOrchestratingLLM\n\n    ToolOrchestratingLLM-&gt;&gt;ToolOrchestratingLLM: Format prompt with topic\n    ToolOrchestratingLLM-&gt;&gt;Ollama: chat(messages, tools=[Album tool])\n    activate Ollama\n\n    Ollama-&gt;&gt;Ollama: _prepare_chat_with_tools(messages, tools)\n    Note over Ollama: Convert CallableTool to&lt;br/&gt;Ollama tool format with schema\n\n    Ollama-&gt;&gt;OllamaServer: HTTP POST /api/chat with tools\n    activate OllamaServer\n    OllamaServer--&gt;&gt;Ollama: Response with tool_calls\n    deactivate OllamaServer\n\n    Ollama--&gt;&gt;ToolOrchestratingLLM: ChatResponse(tool_calls=[...])\n    deactivate Ollama\n\n    ToolOrchestratingLLM-&gt;&gt;ToolOrchestratingLLM: Extract tool calls\n    ToolOrchestratingLLM-&gt;&gt;ToolOrchestratingLLM: Execute tool with arguments\n    Note over ToolOrchestratingLLM: Create Album instance&lt;br/&gt;from tool arguments\n\n    ToolOrchestratingLLM--&gt;&gt;User: Album instance\n    deactivate ToolOrchestratingLLM\n\n    Note over User: Streaming Usage\n    User-&gt;&gt;Ollama: stream_chat(messages)\n    activate Ollama\n\n    Ollama-&gt;&gt;Ollama: _chat(messages, stream=True)\n    Ollama-&gt;&gt;Client/AsyncClient: client.chat(stream=True)\n    activate Client/AsyncClient\n    Client/AsyncClient-&gt;&gt;OllamaServer: HTTP POST /api/chat (streaming)\n    activate OllamaServer\n\n    loop For each chunk\n        OllamaServer--&gt;&gt;Client/AsyncClient: Stream chunk\n        Client/AsyncClient--&gt;&gt;Ollama: Raw chunk dict\n        Ollama-&gt;&gt;Ollama: _chat_stream_from_response(chunk)\n        Note over Ollama: Parse incremental message,&lt;br/&gt;accumulate tool_calls\n        Ollama--&gt;&gt;User: Yield ChatResponse\n    end\n\n    deactivate OllamaServer\n    deactivate Client/AsyncClient\n    deactivate Ollama\n\n    Note over User: Async Usage\n    User-&gt;&gt;Ollama: await achat(messages)\n    activate Ollama\n\n    Ollama-&gt;&gt;Ollama: _achat(messages, stream=False)\n    Ollama-&gt;&gt;Client/AsyncClient: await async_client.chat()\n    activate Client/AsyncClient\n    Client/AsyncClient-&gt;&gt;OllamaServer: HTTP POST /api/chat (async)\n    activate OllamaServer\n    OllamaServer--&gt;&gt;Client/AsyncClient: JSON response\n    deactivate OllamaServer\n    Client/AsyncClient--&gt;&gt;Ollama: raw response dict\n    deactivate Client/AsyncClient\n\n    Ollama-&gt;&gt;Ollama: _chat_from_response(response)\n    Ollama--&gt;&gt;User: ChatResponse\n    deactivate Ollama</code></pre>"},{"location":"reference/providers/ollama/ollama_sequence/#key-execution-paths","title":"Key Execution Paths","text":""},{"location":"reference/providers/ollama/ollama_sequence/#1-direct-chat-call","title":"1. Direct Chat Call","text":"<pre><code>User \u2192 Ollama.chat\n  \u251c\u2500\u2192 _prepare_chat_with_tools (if tools provided)\n  \u251c\u2500\u2192 _chat (build request)\n  \u251c\u2500\u2192 Client.chat \u2192 HTTP POST /api/chat\n  \u251c\u2500\u2192 _chat_from_response (parse response)\n  \u2514\u2500\u2192 Return ChatResponse\n</code></pre>"},{"location":"reference/providers/ollama/ollama_sequence/#2-complete-call-via-decorator","title":"2. Complete Call (via Decorator)","text":"<pre><code>User \u2192 Ollama.complete\n  \u251c\u2500\u2192 @chat_to_completion_decorator\n  \u251c\u2500\u2192 Convert prompt to Message\n  \u251c\u2500\u2192 Delegate to chat()\n  \u2514\u2500\u2192 Return CompletionResponse\n</code></pre>"},{"location":"reference/providers/ollama/ollama_sequence/#3-with-textcompletionllm","title":"3. With TextCompletionLLM","text":"<pre><code>User \u2192 TextCompletionLLM(llm=Ollama)\n  \u251c\u2500\u2192 Format prompt with variables\n  \u251c\u2500\u2192 Ollama.chat (get raw response)\n  \u251c\u2500\u2192 PydanticParser.parse\n  \u2514\u2500\u2192 Return validated model instance\n</code></pre>"},{"location":"reference/providers/ollama/ollama_sequence/#4-with-toolorchestratingllm","title":"4. With ToolOrchestratingLLM","text":"<pre><code>User \u2192 ToolOrchestratingLLM(llm=Ollama)\n  \u251c\u2500\u2192 Convert output_cls to CallableTool\n  \u251c\u2500\u2192 Format prompt with variables\n  \u251c\u2500\u2192 Ollama.chat with tools parameter\n  \u251c\u2500\u2192 Ollama converts tools to schema\n  \u251c\u2500\u2192 Server returns tool_calls\n  \u251c\u2500\u2192 Execute tool to create instance\n  \u2514\u2500\u2192 Return model instance\n</code></pre>"},{"location":"reference/providers/ollama/ollama_sequence/#5-streaming","title":"5. Streaming","text":"<pre><code>User \u2192 Ollama.stream_chat\n  \u251c\u2500\u2192 _chat(stream=True)\n  \u251c\u2500\u2192 Client.chat(stream=True)\n  \u2514\u2500\u2192 For each chunk:\n      \u251c\u2500\u2192 _chat_stream_from_response\n      \u2514\u2500\u2192 Yield ChatResponse\n</code></pre>"},{"location":"reference/providers/ollama/ollama_sequence/#important-implementation-details","title":"Important Implementation Details","text":"<ol> <li>Client Lazy Initialization: The Ollama client is created lazily on first use, not during <code>__init__</code></li> <li>Tool Conversion: When tools are provided, they're converted from <code>BaseTool</code> to Ollama's tool schema format</li> <li>Decorator Pattern: The <code>complete</code> method uses decorators to wrap the <code>chat</code> method for consistency</li> <li>Streaming Accumulation: In streaming mode, tool calls are accumulated across chunks</li> <li>Metadata Handling: Response metadata includes model info, timing, and token counts</li> <li>Error Handling: Network errors, timeout errors, and parsing errors are handled at each stage</li> </ol>"},{"location":"reference/providers/ollama/ollama_state/","title":"Lifecycle and State Management","text":"<p>This diagram shows the complete lifecycle and state transitions of the Ollama LLM.</p>  Hold \"Ctrl\" to enable pan &amp; zoom  <pre><code>stateDiagram-v2\n    [*] --&gt; Uninitialized\n\n    Uninitialized --&gt; Configured: __init__(model, base_url, ...)\n\n    state Configured {\n        [*] --&gt; ClientNotCreated\n\n        note right of ClientNotCreated\n            State: Configuration stored\n            - model: str\n            - base_url: str\n            - request_timeout: float\n            - temperature: float\n            - json_mode: bool\n            - _client: None\n            - _async_client: None\n        end note\n\n        ClientNotCreated --&gt; ClientInitialized: First chat/complete call\n\n        state ClientInitialized {\n            [*] --&gt; Idle\n\n            note right of Idle\n                State: Ready for requests\n                - _client: Client instance\n                - _async_client: None or AsyncClient\n            end note\n\n            Idle --&gt; ProcessingChat: chat(messages)\n            Idle --&gt; ProcessingComplete: complete(prompt)\n            Idle --&gt; ProcessingStream: stream_chat(messages)\n            Idle --&gt; ProcessingAsync: achat(messages)\n            Idle --&gt; ProcessingTools: chat_with_tools(messages, tools)\n\n            state ProcessingChat {\n                [*] --&gt; BuildingRequest\n                BuildingRequest --&gt; ConvertingMessages: Convert Message objects\n                ConvertingMessages --&gt; AddingOptions: Add temperature, etc.\n                AddingOptions --&gt; AddingFormat: Add json format if enabled\n                AddingFormat --&gt; SendingRequest: HTTP POST to server\n                SendingRequest --&gt; WaitingResponse: Awaiting response\n                WaitingResponse --&gt; ParsingResponse: Parse raw dict\n                ParsingResponse --&gt; CreatingChatResponse: Create ChatResponse\n                CreatingChatResponse --&gt; [*]\n            }\n\n            state ProcessingComplete {\n                [*] --&gt; DecoratorWrap\n                DecoratorWrap --&gt; ConvertToMessage: Wrap prompt as Message\n                ConvertToMessage --&gt; DelegateToChat: Call chat([message])\n                DelegateToChat --&gt; ProcessingChat\n                ProcessingChat --&gt; DecoratorUnwrap: Extract text\n                DecoratorUnwrap --&gt; CreateCompletionResponse: Create CompletionResponse\n                CreateCompletionResponse --&gt; [*]\n            }\n\n            state ProcessingStream {\n                [*] --&gt; BuildingStreamRequest\n                BuildingStreamRequest --&gt; SendingStreamRequest: stream=True\n                SendingStreamRequest --&gt; StreamLoop\n\n                state StreamLoop {\n                    [*] --&gt; WaitingChunk\n                    WaitingChunk --&gt; ReceivingChunk: Chunk arrives\n                    ReceivingChunk --&gt; ParsingChunk: Parse chunk dict\n                    ParsingChunk --&gt; AccumulatingContent: Accumulate content\n                    AccumulatingContent --&gt; AccumulatingTools: Accumulate tool_calls\n                    AccumulatingTools --&gt; YieldingResponse: Yield ChatResponse\n                    YieldingResponse --&gt; CheckDone: Check done flag\n                    CheckDone --&gt; WaitingChunk: done=False\n                    CheckDone --&gt; [*]: done=True\n                }\n\n                StreamLoop --&gt; [*]\n            }\n\n            state ProcessingAsync {\n                [*] --&gt; EnsureAsyncClient\n                EnsureAsyncClient --&gt; BuildingAsyncRequest: Create AsyncClient if needed\n                BuildingAsyncRequest --&gt; SendingAsyncRequest: await async_client.chat()\n                SendingAsyncRequest --&gt; WaitingAsyncResponse: Awaiting\n                WaitingAsyncResponse --&gt; ParsingAsyncResponse: Parse response\n                ParsingAsyncResponse --&gt; CreatingAsyncChatResponse: Create ChatResponse\n                CreatingAsyncChatResponse --&gt; [*]\n            }\n\n            state ProcessingTools {\n                [*] --&gt; PreparingTools\n                PreparingTools --&gt; ConvertingTools: Convert to Ollama format\n                ConvertingTools --&gt; ExtractingSchemas: Extract fn_schema from metadata\n                ExtractingSchemas --&gt; BuildingToolDicts: Build tool dicts\n                BuildingToolDicts --&gt; MergingKwargs: Add tools to kwargs\n                MergingKwargs --&gt; CallingChat: Call chat(messages, tools=...)\n                CallingChat --&gt; ProcessingChat\n                ProcessingChat --&gt; ValidatingToolResponse: Check tool_calls present\n                ValidatingToolResponse --&gt; CheckParallel: Check allow_parallel_tool_calls\n                CheckParallel --&gt; ForcingSingle: False - trim to 1\n                CheckParallel --&gt; ReturningMultiple: True - keep all\n                ForcingSingle --&gt; [*]\n                ReturningMultiple --&gt; [*]\n            }\n\n            ProcessingChat --&gt; Idle: Return ChatResponse\n            ProcessingComplete --&gt; Idle: Return CompletionResponse\n            ProcessingStream --&gt; Idle: Stream complete\n            ProcessingAsync --&gt; Idle: Return ChatResponse\n            ProcessingTools --&gt; Idle: Return ChatResponse with tools\n\n            Idle --&gt; Error: Exception occurs\n            ProcessingChat --&gt; Error: Network/Parse error\n            ProcessingComplete --&gt; Error: Network/Parse error\n            ProcessingStream --&gt; Error: Network/Parse error\n            ProcessingAsync --&gt; Error: Network/Parse error\n            ProcessingTools --&gt; Error: Tool validation error\n\n            state Error {\n                [*] --&gt; LoggingError\n                LoggingError --&gt; RaisingException: Raise appropriate exception\n                RaisingException --&gt; [*]\n            }\n\n            Error --&gt; Idle: Error handled by caller\n        }\n    }\n\n    Configured --&gt; [*]: Delete instance\n\n    note left of Configured\n        Lifecycle Phases:\n        1. Uninitialized: Before __init__\n        2. Configured: After __init__, clients lazy\n        3. ClientInitialized: After first use\n        4. Idle: Ready for requests\n        5. Processing*: Handling request\n        6. Error: Exception state\n    end note</code></pre>"},{"location":"reference/providers/ollama/ollama_state/#state-transitions","title":"State Transitions","text":""},{"location":"reference/providers/ollama/ollama_state/#1-initialization-configured","title":"1. Initialization \u2192 Configured","text":"<pre><code>llm = Ollama(\n    model=\"llama3.1\",\n    base_url=\"http://localhost:11434\",\n    request_timeout=180\n)\n\n# State: Configured\n# - Configuration fields populated\n# - Metadata created (is_chat_model=True, is_function_calling_model=True)\n# - _client = None (not yet created)\n# - _async_client = None (not yet created)\n</code></pre>"},{"location":"reference/providers/ollama/ollama_state/#2-configured-clientinitialized-lazy","title":"2. Configured \u2192 ClientInitialized (Lazy)","text":"<pre><code># First call triggers client creation\nresponse = llm.chat([Message(role=MessageRole.USER, content=\"Hello\")])\n\n# Transition:\n# - Access self.client property\n# - Check if self._client is None \u2192 True\n# - Create Client(host=self.base_url, timeout=self.request_timeout)\n# - Store in self._client\n\n# State: ClientInitialized \u2192 Idle\n# - _client = Client instance\n# - Ready to process requests\n</code></pre>"},{"location":"reference/providers/ollama/ollama_state/#3-idle-processingchat-idle","title":"3. Idle \u2192 ProcessingChat \u2192 Idle","text":"<pre><code># Idle state: Ready for requests\nresponse = llm.chat(messages)\n\n# Transition to ProcessingChat:\n# 1. BuildingRequest: Create request dict\n# 2. ConvertingMessages: Message objects to dicts\n# 3. AddingOptions: Merge temperature, etc.\n# 4. AddingFormat: Add json format if enabled\n# 5. SendingRequest: client.chat(**request)\n# 6. WaitingResponse: Block until response\n# 7. ParsingResponse: _chat_from_response(raw)\n# 8. CreatingChatResponse: Build ChatResponse object\n\n# Transition back to Idle:\n# - Return ChatResponse to caller\n</code></pre>"},{"location":"reference/providers/ollama/ollama_state/#4-idle-processingcomplete-idle","title":"4. Idle \u2192 ProcessingComplete \u2192 Idle","text":"<pre><code># Complete uses decorator pattern\nresponse = llm.complete(prompt)\n\n# Transition to ProcessingComplete:\n# 1. DecoratorWrap: @chat_to_completion_decorator intercepts\n# 2. ConvertToMessage: prompt \u2192 Message(role=USER, content=prompt)\n# 3. DelegateToChat: Call self.chat([message])\n#    [Enters ProcessingChat state]\n# 4. DecoratorUnwrap: Extract message.content\n# 5. CreateCompletionResponse: Wrap in CompletionResponse\n\n# Transition back to Idle:\n# - Return CompletionResponse to caller\n</code></pre>"},{"location":"reference/providers/ollama/ollama_state/#5-idle-processingstream-idle","title":"5. Idle \u2192 ProcessingStream \u2192 Idle","text":"<pre><code># Streaming maintains state across multiple yields\nfor chunk in llm.stream_chat(messages):\n    print(chunk.message.content)\n\n# Transition to ProcessingStream:\n# 1. BuildingStreamRequest: Create request with stream=True\n# 2. SendingStreamRequest: client.chat(stream=True)\n# 3. StreamLoop - for each chunk:\n#    a. WaitingChunk: Block for next chunk\n#    b. ReceivingChunk: Chunk dict arrives\n#    c. ParsingChunk: _chat_stream_from_response(chunk)\n#    d. AccumulatingContent: Append to content buffer\n#    e. AccumulatingTools: Append to tool_calls buffer\n#    f. YieldingResponse: Create and yield ChatResponse with delta\n#    g. CheckDone: If done=True, exit loop\n# 4. StreamLoop exits when done=True\n\n# Transition back to Idle:\n# - Generator exhausted\n</code></pre>"},{"location":"reference/providers/ollama/ollama_state/#6-idle-processingasync-idle","title":"6. Idle \u2192 ProcessingAsync \u2192 Idle","text":"<pre><code># Async uses separate client and event loop\nresponse = await llm.achat(messages)\n\n# Transition to ProcessingAsync:\n# 1. EnsureAsyncClient: Check self._async_client\n#    - If None, create AsyncClient(host=base_url, timeout=timeout)\n# 2. BuildingAsyncRequest: Create request dict\n# 3. SendingAsyncRequest: await async_client.chat(**request)\n# 4. WaitingAsyncResponse: Coroutine awaits response\n# 5. ParsingAsyncResponse: _chat_from_response(raw)\n# 6. CreatingAsyncChatResponse: Build ChatResponse\n\n# Transition back to Idle:\n# - Return ChatResponse to caller\n</code></pre>"},{"location":"reference/providers/ollama/ollama_state/#7-idle-processingtools-idle","title":"7. Idle \u2192 ProcessingTools \u2192 Idle","text":"<pre><code># Tool calling adds preparation and validation steps\nresponse = llm.chat_with_tools(messages, tools)\n\n# Transition to ProcessingTools:\n# 1. PreparingTools: _prepare_chat_with_tools(messages, tools)\n# 2. ConvertingTools: For each tool:\n#    a. ExtractingSchemas: Get tool.metadata.fn_schema\n#    b. BuildingToolDicts: Create Ollama tool dict format\n# 3. MergingKwargs: Add tools to kwargs\n# 4. CallingChat: Delegate to chat(messages, **kwargs)\n#    [Enters ProcessingChat state]\n# 5. ValidatingToolResponse: _validate_chat_with_tools_response\n# 6. CheckParallel: Check allow_parallel_tool_calls flag\n#    - If False: ForcingSingle \u2192 trim to first tool call\n#    - If True: ReturningMultiple \u2192 keep all tool calls\n\n# Transition back to Idle:\n# - Return ChatResponse with tool_calls\n</code></pre>"},{"location":"reference/providers/ollama/ollama_state/#8-any-state-error-idle","title":"8. Any State \u2192 Error \u2192 Idle","text":"<pre><code>try:\n    response = llm.chat(messages)\nexcept Exception as e:\n    # Handle error\n\n# Error transition can occur from:\n# - ProcessingChat: Network timeout, invalid response\n# - ProcessingComplete: Any chat error propagates\n# - ProcessingStream: Chunk parsing error\n# - ProcessingAsync: Async operation failure\n# - ProcessingTools: Tool schema validation error\n\n# Error state:\n# 1. LoggingError: Log exception details\n# 2. RaisingException: Raise appropriate exception type\n#    - TimeoutError: request_timeout exceeded\n#    - ConnectionError: Cannot reach server\n#    - ValueError: Invalid response format\n#    - KeyError: Missing required field in response\n\n# Transition back to Idle:\n# - Exception handled by caller\n# - Instance still usable for next call\n</code></pre>"},{"location":"reference/providers/ollama/ollama_state/#state-variables","title":"State Variables","text":""},{"location":"reference/providers/ollama/ollama_state/#configuration-state-immutable-after-init","title":"Configuration State (Immutable after init)","text":"<pre><code># Set during __init__, never change\nself.model: str = \"llama3.1\"\nself.base_url: str = \"http://localhost:11434\"\nself.request_timeout: float = 180.0\nself.temperature: float = 0.75\nself.context_window: int = 3900\nself.prompt_key: str = \"prompt\"\nself.json_mode: bool = False\nself.additional_kwargs: dict[str, Any] = {}\nself.keep_alive: Optional[str] = None\nself._is_function_calling_model: bool = True\n</code></pre>"},{"location":"reference/providers/ollama/ollama_state/#client-state-mutable-lazy-initialized","title":"Client State (Mutable, lazy-initialized)","text":"<pre><code># None until first use\nself._client: Optional[Client] = None\nself._async_client: Optional[AsyncClient] = None\n\n# After first sync call\nself._client: Client = Client(host=self.base_url, timeout=self.request_timeout)\n\n# After first async call\nself._async_client: AsyncClient = AsyncClient(host=self.base_url, timeout=self.request_timeout)\n</code></pre>"},{"location":"reference/providers/ollama/ollama_state/#request-state-per-call-transient","title":"Request State (Per-call, transient)","text":"<pre><code># Created fresh for each call, not stored\nrequest_dict = {\n    \"model\": self.model,\n    \"messages\": [...],\n    \"options\": {\"temperature\": self.temperature, ...},\n    \"stream\": False,\n    \"format\": \"json\" if self.json_mode else None,\n    \"tools\": [...] if tools else None,\n    \"keep_alive\": self.keep_alive\n}\n</code></pre>"},{"location":"reference/providers/ollama/ollama_state/#streaming-state-per-stream-transient","title":"Streaming State (Per-stream, transient)","text":"<pre><code># Maintained during stream, not stored on instance\naccumulated_content: str = \"\"\naccumulated_tool_calls: list[dict] = []\ncurrent_chunk: dict = {}\ndone: bool = False\n</code></pre>"},{"location":"reference/providers/ollama/ollama_state/#response-state-per-call-returned","title":"Response State (Per-call, returned)","text":"<pre><code># Created and returned, not stored\nchat_response = ChatResponse(\n    message=Message(\n        role=MessageRole.ASSISTANT,\n        content=\"...\",\n        additional_kwargs={\"tool_calls\": [...]}\n    ),\n    raw={...},\n    additional_kwargs={...}\n)\n</code></pre>"},{"location":"reference/providers/ollama/ollama_state/#lifecycle-diagram","title":"Lifecycle Diagram","text":"Hold \"Ctrl\" to enable pan &amp; zoom  <pre><code>graph LR\n    A[Create Instance] --&gt; B[Configured State]\n    B --&gt; C{First Call?}\n    C --&gt;|Yes| D[Initialize Client]\n    C --&gt;|No| E[Use Existing Client]\n    D --&gt; E\n    E --&gt; F[Process Request]\n    F --&gt; G{Success?}\n    G --&gt;|Yes| H[Return Response]\n    G --&gt;|No| I[Handle Error]\n    H --&gt; J{More Calls?}\n    I --&gt; J\n    J --&gt;|Yes| E\n    J --&gt;|No| K[Delete Instance]\n\n    style A fill:#e1f5ff\n    style B fill:#fff9c4\n    style D fill:#f3e5f5\n    style F fill:#e0f2f1\n    style H fill:#c8e6c9\n    style I fill:#ffcdd2\n    style K fill:#efebe9</code></pre>"},{"location":"reference/providers/ollama/ollama_state/#concurrency-considerations","title":"Concurrency Considerations","text":""},{"location":"reference/providers/ollama/ollama_state/#thread-safety","title":"Thread Safety","text":"<pre><code>Ollama instance is NOT thread-safe by default:\n  - _client and _async_client are shared state\n  - Lazy initialization is not synchronized\n\nRecommendation:\n  - Use separate Ollama instance per thread\n  - Or use locks around lazy initialization\n</code></pre>"},{"location":"reference/providers/ollama/ollama_state/#async-safety","title":"Async Safety","text":"<pre><code>Ollama async methods are event-loop safe:\n  - Uses separate _async_client per event loop\n  - No shared mutable state in async methods\n\nSafe to use:\n  - Multiple concurrent achat() calls in same loop\n  - Multiple event loops with same instance (separate clients)\n</code></pre>"},{"location":"reference/providers/ollama/ollama_state/#streaming-state","title":"Streaming State","text":"<pre><code>Each stream maintains its own state:\n  - Generator/async generator has local variables\n  - No shared state between streams\n\nSafe to have:\n  - Multiple concurrent streams from same instance\n</code></pre>"},{"location":"reference/providers/ollama/ollama_state/#state-management-best-practices","title":"State Management Best Practices","text":""},{"location":"reference/providers/ollama/ollama_state/#1-initialization","title":"1. Initialization","text":"<pre><code># \u2713 Good: Initialize once, reuse\nllm = Ollama(model=\"llama3.1\", request_timeout=180)\n\n# \u2717 Bad: Create new instance per call\ndef get_response(prompt):\n    llm = Ollama(model=\"llama3.1\")  # Inefficient\n    return llm.complete(prompt)\n</code></pre>"},{"location":"reference/providers/ollama/ollama_state/#2-client-reuse","title":"2. Client Reuse","text":"<pre><code># \u2713 Good: Client automatically reused\nllm = Ollama(model=\"llama3.1\")\nresponse1 = llm.chat(messages1)  # Creates client\nresponse2 = llm.chat(messages2)  # Reuses client\n\n# \u2717 Bad: Don't access _client directly\nllm._client = None  # Don't do this\n</code></pre>"},{"location":"reference/providers/ollama/ollama_state/#3-configuration","title":"3. Configuration","text":"<pre><code># \u2713 Good: Set configuration at init\nllm = Ollama(model=\"llama3.1\", temperature=0.8, json_mode=True)\n\n# \u2717 Bad: Don't modify config after init\nllm.temperature = 0.5  # Config is immutable\n</code></pre>"},{"location":"reference/providers/ollama/ollama_state/#4-error-handling","title":"4. Error Handling","text":"<pre><code># \u2713 Good: Instance remains usable after error\nllm = Ollama(model=\"llama3.1\")\ntry:\n    response = llm.chat(messages)\nexcept TimeoutError:\n    # Can still use llm for next call\n    response = llm.chat(messages, temperature=0.2)\n\n# \u2713 Good: Instance is reusable\n</code></pre>"},{"location":"reference/providers/ollama/ollama_state/#5-streaming","title":"5. Streaming","text":"<pre><code># \u2713 Good: Complete stream before next call\nfor chunk in llm.stream_chat(messages1):\n    process(chunk)\nresponse = llm.chat(messages2)  # Safe\n\n# \u26a0 Warning: Interleaving streams\nstream1 = llm.stream_chat(messages1)\nstream2 = llm.stream_chat(messages2)  # Both use same client\n</code></pre>"}]}